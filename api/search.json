[{"id":"047a0157ce631e99397979ea9f68fcda","title":"meta","content":"11.18上午开始写大数据综合实训作业\nonedrive不给力，world保存会报错，反反复复搞了很久\n中午美美的吃了一顿烧烤，一觉醒来三点钟。。。\n11.17晚上看李景亮看嗨了，睡不着\n上午摸鱼一直睡到十一点\n下午晚上复习整理了一下Hadoop\n哎\n11.16上午看了一会hive\n下午和导师交流毕设，又弄了一会hive\n晚上代课＋背面经+刷算法\n11.15上午写了论文引言\n下午完成安卓作业\n晚上摸鱼助教，整理了一下编程辅导班的课件\n11.14躺床上看一天JAVA视频\n11.13昨天晚上做到三点的大数据作业，早上发现做错了，应该做后面的实验，白给了\n学院楼有考试，周末在床上躺着\n上午躺着刷B站\n11.12上午下午一直在整理Java笔记\n下午把之前的Python笔记上传到博客\n晚上做大数据的作业\n写在前面此博客记录本人近期的学习情况\n个人情况简介：\n中国矿业大学大四，已保研，最近一个月有两次考试，一个报告作业\n每天在实验室学习，准备寒假去北京实习。\n","slug":"000meta","date":"2021-11-12T06:22:18.000Z","categories_index":"学习规划","tags_index":"","author_index":"YFR718"},{"id":"a3f5b417ccee185425acbc3a549133c3","title":"LeetCode做题记录","content":"刷题汇总\n\n\n\n题号\n题目\n算法\n最优解\n时间复杂度\n\n\n\n\n1\n两数之和\n哈希\n:green_heart:\n$$\n\n\n2\n两数相加\n模拟\n:green_heart:\n\n\n\n3\n无重复字符的最长子串\n\n\n\n\n\n4\n\n\n\n\n\n\n5\n\n\n\n\n\n\n6\n\n\n\n\n\n\n7\n\n\n\n\n\n\n8\n\n\n\n\n\n\n9\n\n\n\n\n\n\n10\n\n\n\n\n\n\n11\n\n\n\n\n\n\n12\n\n\n\n\n\n\n13\n\n\n\n\n\n\n14\n\n\n\n\n\n\n15\n\n\n\n\n\n\n16\n\n\n\n\n\n\n17\n\n\n\n\n\n\n18\n\n\n\n\n\n\n19\n\n\n\n\n\n\n20\n\n\n\n\n\n\n21\n\n\n\n\n\n\n22\n\n\n\n\n\n\n23\n\n\n\n\n\n\n24\n\n\n\n\n\n\n25\n\n\n\n\n\n\n26\n\n\n\n\n\n\n27\n\n\n\n\n\n\n28\n\n\n\n\n\n\n29\n\n\n\n\n\n\n30\n\n\n\n\n\n\n31\n\n\n\n\n\n\n32\n\n\n\n\n\n\n33\n\n\n\n\n\n\n34\n\n\n\n\n\n\n35\n\n\n\n\n\n\n36\n\n\n\n\n\n\n37\n\n\n\n\n\n\n38\n\n\n\n\n\n\n39\n\n\n\n\n\n\n40\n\n\n\n\n\n\n41\n\n\n\n\n\n\n42\n\n\n\n\n\n\n43\n\n\n\n\n\n\n44\n\n\n\n\n\n\n45\n\n\n\n\n\n\n46\n\n\n\n\n\n\n47\n\n\n\n\n\n\n48\n\n\n\n\n\n\n49\n\n\n\n\n\n\n50\n\n\n\n\n\n\n51\n\n\n\n\n\n\n52\n\n\n\n\n\n\n53\n\n\n\n\n\n\n54\n\n\n\n\n\n\n55\n\n\n\n\n\n\n56\n\n\n\n\n\n\n57\n\n\n\n\n\n\n58\n\n\n\n\n\n\n59\n\n\n\n\n\n\n60\n\n\n\n\n\n\n61\n\n\n\n\n\n\n62\n\n\n\n\n\n\n63\n\n\n\n\n\n\n64\n\n\n\n\n\n\n65\n\n\n\n\n\n\n66\n\n\n\n\n\n\n67\n\n\n\n\n\n\n68\n\n\n\n\n\n\n69\n\n\n\n\n\n\n70\n\n\n\n\n\n\n71\n\n\n\n\n\n\n72\n\n\n\n\n\n\n73\n\n\n\n\n\n\n74\n\n\n\n\n\n\n75\n\n\n\n\n\n\n76\n\n\n\n\n\n\n77\n\n\n\n\n\n\n78\n\n\n\n\n\n\n79\n\n\n\n\n\n\n80\n\n\n\n\n\n\n81\n\n\n\n\n\n\n82\n\n\n\n\n\n\n83\n\n\n\n\n\n\n84\n\n\n\n\n\n\n85\n\n\n\n\n\n\n86\n\n\n\n\n\n\n87\n\n\n\n\n\n\n88\n\n\n\n\n\n\n89\n\n\n\n\n\n\n90\n\n\n\n\n\n\n91\n\n\n\n\n\n\n92\n\n\n\n\n\n\n93\n\n\n\n\n\n\n94\n\n\n\n\n\n\n95\n\n\n\n\n\n\n96\n\n\n\n\n\n\n97\n\n\n\n\n\n\n98\n\n\n\n\n\n\n\n算法分类对应题目分类","slug":"l00","date":"2021-11-05T13:51:29.000Z","categories_index":"LeetCode","tags_index":"","author_index":"YFR718"},{"id":"71dcb878b30b2fb16e5265f7515744d5","title":"疑难杂症队列","content":"Java\n整理笔记，等待发布\n尚硅谷整理的Java学习\n书5.7跳过\n\nLinux\n笔记整理，待发布\nshell编程笔记整理，待发布\n\n算法\nleetcode每日三道题\nACwing课程学习\n\nHadoop\nP88 job提交源码\nP89 切片源码\nP97 、98自定义分区\nP100~102\nP104 Combiner\nP107 OutPutFormat\nP101 源码解析\n\n\nHive\nP12 hive启动脚本\n\n\n","slug":"1-疑难杂症队列","date":"2021-11-04T05:27:30.000Z","categories_index":"学习规划","tags_index":"","author_index":"YFR718"},{"id":"5703ea14901e0d80ea4ba70122c1bc16","title":"B6.Hive基础","content":"111111111111111111111111111\nDDL数据定义","slug":"B6-Hive基础","date":"2021-11-17T14:38:57.000Z","categories_index":"数据库","tags_index":"Hive","author_index":"YFR718"},{"id":"60afbd6afe5dbba07afaea373a4f245b","title":"B5.Hadoop案例","content":"MapReduceWordCount案例实操\n创建maven工程，MapReduceDemo\n\n在pom.xml文件中添加如下依赖\n&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n        &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;\n        &lt;version&gt;3.1.3&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;junit&lt;/groupId&gt;\n        &lt;artifactId&gt;junit&lt;/artifactId&gt;\n        &lt;version&gt;4.12&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.slf4j&lt;/groupId&gt;\n        &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;\n        &lt;version&gt;1.7.30&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n\n\n在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入。\nlog4j.rootLogger=INFO, stdout  \nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender  \nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  \nlog4j.appender.logfile=org.apache.log4j.FileAppender  \nlog4j.appender.logfile.File=target/spring.log  \nlog4j.appender.logfile.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n\n\n创建包名：com.atguigu.mapreduce.wordcount\n\n编写Mapper类\npackage com.atguigu.mapreduce.wordcount;\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\npublic class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{\n\t\n\tText k = new Text();\n\tIntWritable v = new IntWritable(1);\n\t\n\t@Override\n\tprotected void map(LongWritable key, Text value, Context context)\tthrows IOException, InterruptedException {\n\t\t\n\t\t// 1 获取一行\n\t\tString line = value.toString();\n\t\t\n\t\t// 2 切割\n\t\tString[] words = line.split(\" \");\n\t\t\n\t\t// 3 输出\n\t\tfor (String word : words) {\n\t\t\t\n\t\t\tk.set(word);\n\t\t\tcontext.write(k, v);\n\t\t}\n\t}\n}\n\n编写Reducer类\npackage com.atguigu.mapreduce.wordcount;\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\npublic class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{\n\nint sum;\nIntWritable v = new IntWritable();\n\n\t@Override\n\tprotected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException {\n\t\t\n\t\t// 1 累加求和\n\t\tsum = 0;\n\t\tfor (IntWritable count : values) {\n\t\t\tsum += count.get();\n\t\t}\n\t\t\n\t\t// 2 输出\n         v.set(sum);\n\t\tcontext.write(key,v);\n\t}\n}\n\n编写Driver驱动类\npackage com.atguigu.mapreduce.wordcount;\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCountDriver {\n\n\tpublic static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n\t\t// 1 获取配置信息以及获取job对象\n\t\tConfiguration conf = new Configuration();\n\t\tJob job = Job.getInstance(conf);\n\n\t\t// 2 关联本Driver程序的jar\n\t\tjob.setJarByClass(WordCountDriver.class);\n\n\t\t// 3 关联Mapper和Reducer的jar\n\t\tjob.setMapperClass(WordCountMapper.class);\n\t\tjob.setReducerClass(WordCountReducer.class);\n\n\t\t// 4 设置Mapper输出的kv类型\n\t\tjob.setMapOutputKeyClass(Text.class);\n\t\tjob.setMapOutputValueClass(IntWritable.class);\n\n\t\t// 5 设置最终输出kv类型\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(IntWritable.class);\n\t\t\n\t\t// 6 设置输入和输出路径\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\t// 7 提交job\n\t\tboolean result = job.waitForCompletion(true);\n\t\tSystem.exit(result ? 0 : 1);\n\t}\n}\n\n本地测试\n（1）需要首先配置好HADOOP_HOME变量以及Windows运行依赖\n（2）在IDEA/Eclipse上运行程序\n\n集群上测试\n（1）用maven打jar包，需要添加的打包插件依赖\n&lt;build&gt;\n    &lt;plugins&gt;\n        &lt;plugin&gt;\n            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n            &lt;version&gt;3.6.1&lt;/version&gt;\n            &lt;configuration&gt;\n                &lt;source&gt;1.8&lt;/source&gt;\n                &lt;target&gt;1.8&lt;/target&gt;\n            &lt;/configuration&gt;\n        &lt;/plugin&gt;\n        &lt;plugin&gt;\n            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;\n            &lt;configuration&gt;\n                &lt;descriptorRefs&gt;\n                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;\n                &lt;/descriptorRefs&gt;\n            &lt;/configuration&gt;\n            &lt;executions&gt;\n                &lt;execution&gt;\n                    &lt;id&gt;make-assembly&lt;/id&gt;\n                    &lt;phase&gt;package&lt;/phase&gt;\n                    &lt;goals&gt;\n                        &lt;goal&gt;single&lt;/goal&gt;\n                    &lt;/goals&gt;\n                &lt;/execution&gt;\n            &lt;/executions&gt;\n        &lt;/plugin&gt;\n    &lt;/plugins&gt;\n&lt;/build&gt;\n（2）将程序打成jar包\n（3）修改不带依赖的jar包名称为wc.jar，并拷贝该jar包到Hadoop集群的/opt/module/hadoop-3.1.3路径。\n（4）启动Hadoop集群\n[atguigu@hadoop102 hadoop-3.1.3]sbin/start-dfs.sh\n\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh\n（5）执行WordCount程序\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar\n\n com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/input /user/atguigu/output\n\n\n序列化案例实操统计每一个手机号耗费的总上行流量、总下行流量、总流量\n输入数据格式：\n\n\n\n\nid\n手机号码\n网络ip\n上行流\n下行流量\n网络状态码\n\n\n\n\n7\n13560436666\n13560436666\n1116\n954\n200\n\n\n\n\n期望输出数据格式:\n\n\n\n\n手机号码\n上行流量\n下行流量\n总流量\n\n\n\n\n13560436666\n1116\n954\n2070\n\n\n\n\nMap阶段(1)读取一行数据,切分字段(2) 抽取手机号、上行流量、下行流量(3)以手机号为key, bean对象为value输出,即context. write(手机号,bean);(4) bean对象要想能够传输，必须实现序列化接口\nReduce阶段(1) 累加上行流量和下行流量得到总流量。\n编写流量统计的Bean对象\npackage com.atguigu.mapreduce.writable;\n\nimport org.apache.hadoop.io.Writable;\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n//1 继承Writable接口\npublic class FlowBean implements Writable {\n\n    private long upFlow; //上行流量\n    private long downFlow; //下行流量\n    private long sumFlow; //总流量\n\n    //2 提供无参构造\n    public FlowBean() {\n    }\n\n    //3 提供三个参数的getter和setter方法\n    public long getUpFlow() {\n        return upFlow;\n    }\n\n    public void setUpFlow(long upFlow) {\n        this.upFlow = upFlow;\n    }\n\n    public long getDownFlow() {\n        return downFlow;\n    }\n\n    public void setDownFlow(long downFlow) {\n        this.downFlow = downFlow;\n    }\n\n    public long getSumFlow() {\n        return sumFlow;\n    }\n\n    public void setSumFlow(long sumFlow) {\n        this.sumFlow = sumFlow;\n    }\n\n    public void setSumFlow() {\n        this.sumFlow = this.upFlow + this.downFlow;\n    }\n\n    //4 实现序列化和反序列化方法,注意顺序一定要保持一致\n    @Override\n    public void write(DataOutput dataOutput) throws IOException {\n        dataOutput.writeLong(upFlow);\n        dataOutput.writeLong(downFlow);\n        dataOutput.writeLong(sumFlow);\n    }\n\n    @Override\n    public void readFields(DataInput dataInput) throws IOException {\n        this.upFlow = dataInput.readLong();\n        this.downFlow = dataInput.readLong();\n        this.sumFlow = dataInput.readLong();\n    }\n\n    //5 重写ToString\n    @Override\n    public String toString() {\n        return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow;\n    }\n}\n编写Mapper类\npackage com.atguigu.mapreduce.writable;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport java.io.IOException;\n\npublic class FlowMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; {\n    private Text outK = new Text();\n    private FlowBean outV = new FlowBean();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        //1 获取一行数据,转成字符串\n        String line = value.toString();\n\n        //2 切割数据\n        String[] split = line.split(\"\\t\");\n\n        //3 抓取我们需要的数据:手机号,上行流量,下行流量\n        String phone = split[1];\n        String up = split[split.length - 3];\n        String down = split[split.length - 2];\n\n        //4 封装outK outV\n        outK.set(phone);\n        outV.setUpFlow(Long.parseLong(up));\n        outV.setDownFlow(Long.parseLong(down));\n        outV.setSumFlow();\n\n        //5 写出outK outV\n        context.write(outK, outV);\n    }\n}\n编写Reducer类\npackage com.atguigu.mapreduce.writable;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;\n\npublic class FlowReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; {\n    private FlowBean outV = new FlowBean();\n    @Override\n    protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException {\n\n        long totalUp = 0;\n        long totalDown = 0;\n\n        //1 遍历values,将其中的上行流量,下行流量分别累加\n        for (FlowBean flowBean : values) {\n            totalUp += flowBean.getUpFlow();\n            totalDown += flowBean.getDownFlow();\n        }\n\n        //2 封装outKV\n        outV.setUpFlow(totalUp);\n        outV.setDownFlow(totalDown);\n        outV.setSumFlow();\n\n        //3 写出outK outV\n        context.write(key,outV);\n    }\n}\n编写Driver驱动类\npackage com.atguigu.mapreduce.writable;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport java.io.IOException;\n\npublic class FlowDriver {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        //1 获取job对象\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        //2 关联本Driver类\n        job.setJarByClass(FlowDriver.class);\n\n        //3 关联Mapper和Reducer\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n        \n\t\t//4 设置Map端输出KV类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(FlowBean.class);\n        \n\t\t//5 设置程序最终输出的KV类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n        \n\t\t//6 设置程序的输入输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\inputflow\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\flowoutput\"));\n        \n\t\t//7 提交Job\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\nCombineTextInputFormat案例实操将输入的大量小文件合并成一个切片统一处理。\n（1）不做任何处理，运行1.8节的WordCount案例程序，观察切片个数为4。\nnumber of splits:4\n（2）在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数为3。（a）驱动类中添加代码如下：\n// 如果不设置InputFormat，它默认用的是TextInputFormat.class\njob.setInputFormatClass(CombineTextInputFormat.class);\n\n//虚拟存储切片最大值设置4m\nCombineTextInputFormat.setMaxInputSplitSize(job, 4194304);\n​        （b）运行如果为3个切片。\nnumber of splits:3\n（3）在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数为1。        （a）驱动中添加代码如下：\n// 如果不设置InputFormat，它默认用的是TextInputFormat.class\njob.setInputFormatClass(CombineTextInputFormat.class);\n\n//虚拟存储切片最大值设置20m\nCombineTextInputFormat.setMaxInputSplitSize(job, 20971520);\n​        （b）运行如果为1个切片\nnumber of splits:1\nPartition分区案例实操将统计结果按照手机归属地不同省份输出到不同文件中（分区）\n在上面的基础上，增加一个分区类\npackage com.atguigu.mapreduce.partitioner;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\npublic class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; {\n\n    @Override\n    public int getPartition(Text text, FlowBean flowBean, int numPartitions) {\n        //获取手机号前三位prePhone\n        String phone = text.toString();\n        String prePhone = phone.substring(0, 3);\n\n        //定义一个分区号变量partition,根据prePhone设置分区号\n        int partition;\n\n        if(\"136\".equals(prePhone)){\n            partition = 0;\n        }else if(\"137\".equals(prePhone)){\n            partition = 1;\n        }else if(\"138\".equals(prePhone)){\n            partition = 2;\n        }else if(\"139\".equals(prePhone)){\n            partition = 3;\n        }else {\n            partition = 4;\n        }\n\n        //最后返回分区号partition\n        return partition;\n    }\n}\n在驱动函数中增加自定义数据分区设置和ReduceTask设置\npackage com.atguigu.mapreduce.partitioner;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport java.io.IOException;\n\npublic class FlowDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        //1 获取job对象\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        //2 关联本Driver类\n        job.setJarByClass(FlowDriver.class);\n\n        //3 关联Mapper和Reducer\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n\n        //4 设置Map端输出数据的KV类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(FlowBean.class);\n\n        //5 设置程序最终输出的KV类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        //8 指定自定义分区器\n        job.setPartitionerClass(ProvincePartitioner.class);\n\n        //9 同时指定相应数量的ReduceTask\n        job.setNumReduceTasks(5);\n\n        //6 设置输入输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\inputflow\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D\\\\partitionout\"));\n\n        //7 提交Job\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\nWritableComparable排序案例实操（全排序）根据前面序列化案例产生的结果再次对总流量进行倒序排序。\nFlowBean对象在在需求1基础上增加了比较功能\npackage com.atguigu.mapreduce.writablecompable;\n\nimport org.apache.hadoop.io.WritableComparable;\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\npublic class FlowBean implements WritableComparable&lt;FlowBean&gt; {\n\n    private long upFlow; //上行流量\n    private long downFlow; //下行流量\n    private long sumFlow; //总流量\n\n    //提供无参构造\n    public FlowBean() {\n    }\n\n    //生成三个属性的getter和setter方法\n    public long getUpFlow() {\n        return upFlow;\n    }\n\n    public void setUpFlow(long upFlow) {\n        this.upFlow = upFlow;\n    }\n\n    public long getDownFlow() {\n        return downFlow;\n    }\n\n    public void setDownFlow(long downFlow) {\n        this.downFlow = downFlow;\n    }\n\n    public long getSumFlow() {\n        return sumFlow;\n    }\n\n    public void setSumFlow(long sumFlow) {\n        this.sumFlow = sumFlow;\n    }\n\n    public void setSumFlow() {\n        this.sumFlow = this.upFlow + this.downFlow;\n    }\n\n    //实现序列化和反序列化方法,注意顺序一定要一致\n    @Override\n    public void write(DataOutput out) throws IOException {\n        out.writeLong(this.upFlow);\n        out.writeLong(this.downFlow);\n        out.writeLong(this.sumFlow);\n\n    }\n\n    @Override\n    public void readFields(DataInput in) throws IOException {\n        this.upFlow = in.readLong();\n        this.downFlow = in.readLong();\n        this.sumFlow = in.readLong();\n    }\n\n    //重写ToString,最后要输出FlowBean\n    @Override\n    public String toString() {\n        return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow;\n    }\n\n    @Override\n    public int compareTo(FlowBean o) {\n\n        //按照总流量比较,倒序排列\n        if(this.sumFlow &gt; o.sumFlow){\n            return -1;\n        }else if(this.sumFlow &lt; o.sumFlow){\n            return 1;\n        }else {\n            return 0;\n        }\n    }\n}\n编写Mapper类\npackage com.atguigu.mapreduce.writablecompable;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport java.io.IOException;\n\npublic class FlowMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; {\n    private FlowBean outK = new FlowBean();\n    private Text outV = new Text();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        //1 获取一行数据\n        String line = value.toString();\n\n        //2 按照\"\\t\",切割数据\n        String[] split = line.split(\"\\t\");\n\n        //3 封装outK outV\n        outK.setUpFlow(Long.parseLong(split[1]));\n        outK.setDownFlow(Long.parseLong(split[2]));\n        outK.setSumFlow();\n        outV.set(split[0]);\n\n        //4 写出outK outV\n        context.write(outK,outV);\n    }\n}\n编写Reducer类\npackage com.atguigu.mapreduce.writablecompable;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;\n\npublic class FlowReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; {\n    @Override\n    protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {\n\n        //遍历values集合,循环写出,避免总流量相同的情况\n        for (Text value : values) {\n            //调换KV位置,反向写出\n            context.write(value,key);\n        }\n    }\n}\n编写Driver类\npackage com.atguigu.mapreduce.writablecompable;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport java.io.IOException;\n\npublic class FlowDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        //1 获取job对象\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        //2 关联本Driver类\n        job.setJarByClass(FlowDriver.class);\n\n        //3 关联Mapper和Reducer\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n\n        //4 设置Map端输出数据的KV类型\n        job.setMapOutputKeyClass(FlowBean.class);\n        job.setMapOutputValueClass(Text.class);\n\n        //5 设置程序最终输出的KV类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        //6 设置输入输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\inputflow2\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\comparout\"));\n\n        //7 提交Job\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\nWritableComparable排序案例实操（区内排序）要求每个省份手机号输出的文件中按照总流量内部排序。基于前一个需求，增加自定义分区类，分区按照省份手机号设置。\n增加自定义分区类\npackage com.atguigu.mapreduce.partitionercompable;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\npublic class ProvincePartitioner2 extends Partitioner&lt;FlowBean, Text&gt; {\n\n    @Override\n    public int getPartition(FlowBean flowBean, Text text, int numPartitions) {\n        //获取手机号前三位\n        String phone = text.toString();\n        String prePhone = phone.substring(0, 3);\n\n        //定义一个分区号变量partition,根据prePhone设置分区号\n        int partition;\n        if(\"136\".equals(prePhone)){\n            partition = 0;\n        }else if(\"137\".equals(prePhone)){\n            partition = 1;\n        }else if(\"138\".equals(prePhone)){\n            partition = 2;\n        }else if(\"139\".equals(prePhone)){\n            partition = 3;\n        }else {\n            partition = 4;\n        }\n\n        //最后返回分区号partition\n        return partition;\n    }\n}\n在驱动类中添加分区类\n// 设置自定义分区器\njob.setPartitionerClass(ProvincePartitioner2.class);\n\n// 设置对应的ReduceTask的个数\njob.setNumReduceTasks(5);\nCombiner合并案例实操统计过程中对每一个MapTask的输出进行局部汇总，以减小网络传输量即采用Combiner功能。 \n期望：Combine输入数据多，输出时经过合并，输出数据降低。\n方案一1)增加一个WordcountCombiner类继承Reducer2)在WordcountCombiner中(1) 统计单词汇总(2)将统计结果输出\n增加一个WordCountCombiner类继承Reducer\npackage com.atguigu.mapreduce.combiner;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;\n\npublic class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n\nprivate IntWritable outV = new IntWritable();\n\n    @Override\n    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {\n\n        int sum = 0;\n        for (IntWritable value : values) {\n            sum += value.get();\n        }\n\n        //封装outKV\n        outV.set(sum);\n\n        //写出outKV\n        context.write(key,outV);\n    }\n}\n在WordcountDriver驱动类中指定Combiner\n// 指定需要使用combiner，以及用哪个类作为combiner的逻辑\njob.setCombinerClass(WordCountCombiner.class);\n方案二1 )将Wordc ountReducer作为Combiner在    WWordc ountDriver驱动类中指定    job. setCombiner Clas s(WordcountR educer. class);\n将WordcountReducer作为Combiner在WordcountDriver驱动类中指定\n// 指定需要使用Combiner，以及用哪个类作为Combiner的逻辑\njob.setCombinerClass(WordCountReducer.class);\n自定义OutputFormat案例实操过滤输入的log日志，包含atguigu的网站输出到e:/atguigu.log，不包含atguigu的网站输出到e:/other.log。\n输入数据：\nhttp://www.baidu.comhttp://www.google.com。。。\n自定义一个OutputFormat类(1) 创建一个类L ogRecordWniter继承RecordWniter(a)创建两个文件的输出流: atguiguOut、otherOut(b)如果输入数据包含atguigu,输出到atguiguOut流如果不包含atguigu,输出到otherOut流\n5、驱动类Driver.//要将自定义的输出格式组件设置到job中job. setOutputF ormatCla s(Lo gOutputF ormat. class);\n编写LogMapper类\npackage com.atguigu.mapreduce.outputformat;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class LogMapper extends Mapper&lt;LongWritable, Text,Text, NullWritable&gt; {\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        //不做任何处理,直接写出一行log数据\n        context.write(value,NullWritable.get());\n    }\n}\n编写LogReducer类\npackage com.atguigu.mapreduce.outputformat;\n\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class LogReducer extends Reducer&lt;Text, NullWritable,Text, NullWritable&gt; {\n    @Override\n    protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {\n        // 防止有相同的数据,迭代写出\n        for (NullWritable value : values) {\n            context.write(key,NullWritable.get());\n        }\n    }\n}\n自定义一个LogOutputFormat类\npackage com.atguigu.mapreduce.outputformat;\n\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class LogOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; {\n    @Override\n    public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {\n        //创建一个自定义的RecordWriter返回\n        LogRecordWriter logRecordWriter = new LogRecordWriter(job);\n        return logRecordWriter;\n    }\n}\n编写LogRecordWriter类\npackage com.atguigu.mapreduce.outputformat;\n\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\n\nimport java.io.IOException;\n\npublic class LogRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; {\n\n    private FSDataOutputStream atguiguOut;\n    private FSDataOutputStream otherOut;\n\n    public LogRecordWriter(TaskAttemptContext job) {\n        try {\n            //获取文件系统对象\n            FileSystem fs = FileSystem.get(job.getConfiguration());\n            //用文件系统对象创建两个输出流对应不同的目录\n            atguiguOut = fs.create(new Path(\"d:/hadoop/atguigu.log\"));\n            otherOut = fs.create(new Path(\"d:/hadoop/other.log\"));\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n    @Override\n    public void write(Text key, NullWritable value) throws IOException, InterruptedException {\n        String log = key.toString();\n        //根据一行的log数据是否包含atguigu,判断两条输出流输出的内容\n        if (log.contains(\"atguigu\")) {\n            atguiguOut.writeBytes(log + \"\\n\");\n        } else {\n            otherOut.writeBytes(log + \"\\n\");\n        }\n    }\n\n    @Override\n    public void close(TaskAttemptContext context) throws IOException, InterruptedException {\n        //关流\n        IOUtils.closeStream(atguiguOut);\n        IOUtils.closeStream(otherOut);\n    }\n}\n编写LogDriver类\npackage com.atguigu.mapreduce.outputformat;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class LogDriver {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(LogDriver.class);\n        job.setMapperClass(LogMapper.class);\n        job.setReducerClass(LogReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        //设置自定义的outputformat\n        job.setOutputFormatClass(LogOutputFormat.class);\n\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\input\"));\n        //虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat\n        //而fileoutputformat要输出一个_SUCCESS文件，所以在这还得指定一个输出目录\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\logoutput\"));\n\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\nReduce Join案例实操订单数据表t_order\n\n\n\n\nid\npid\namount\n\n\n\n\n1001\n01\n1\n\n\n1002\n02\n2\n\n\n1003\n03\n3\n\n\n1004\n01\n4\n\n\n1005\n02\n5\n\n\n1006\n03\n6\n\n\n\n\n商品信息表t_product\n\n\n\n\npid\npname\n\n\n\n\n01\n小米\n\n\n02\n华为\n\n\n03\n格力\n\n\n\n\n将商品信息表中数据根据商品pid合并到订单数据表中。\n\n\n\n\nid\npname\namount\n\n\n\n\n1001\n小米\n1\n\n\n1004\n小米\n4\n\n\n1002\n华为\n2\n\n\n1005\n华为\n5\n\n\n1003\n格力\n3\n\n\n1006\n格力\n6\n\n\n\n\n通过将关联条件作为Map输出的key，将两表满足Join条件的数据并携带数据所来源的文件信息，发往同一个ReduceTask，在Reduce中进行数据的串联。\n创建商品和订单合并后的TableBean类\npackage com.atguigu.mapreduce.reducejoin;\n\nimport org.apache.hadoop.io.Writable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\npublic class TableBean implements Writable {\n\n    private String id; //订单id\n    private String pid; //产品id\n    private int amount; //产品数量\n    private String pname; //产品名称\n    private String flag; //判断是order表还是pd表的标志字段\n\n    public TableBean() {\n    }\n\n    public String getId() {\n        return id;\n    }\n\n    public void setId(String id) {\n        this.id = id;\n    }\n\n    public String getPid() {\n        return pid;\n    }\n\n    public void setPid(String pid) {\n        this.pid = pid;\n    }\n\n    public int getAmount() {\n        return amount;\n    }\n\n    public void setAmount(int amount) {\n        this.amount = amount;\n    }\n\n    public String getPname() {\n        return pname;\n    }\n\n    public void setPname(String pname) {\n        this.pname = pname;\n    }\n\n    public String getFlag() {\n        return flag;\n    }\n\n    public void setFlag(String flag) {\n        this.flag = flag;\n    }\n\n    @Override\n    public String toString() {\n        return id + \"\\t\" + pname + \"\\t\" + amount;\n    }\n\n    @Override\n    public void write(DataOutput out) throws IOException {\n        out.writeUTF(id);\n        out.writeUTF(pid);\n        out.writeInt(amount);\n        out.writeUTF(pname);\n        out.writeUTF(flag);\n    }\n\n    @Override\n    public void readFields(DataInput in) throws IOException {\n        this.id = in.readUTF();\n        this.pid = in.readUTF();\n        this.amount = in.readInt();\n        this.pname = in.readUTF();\n        this.flag = in.readUTF();\n    }\n}\n编写TableMapper类\npackage com.atguigu.mapreduce.reducejoin;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.InputSplit;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.FileSplit;\n\nimport java.io.IOException;\n\npublic class TableMapper extends Mapper&lt;LongWritable,Text,Text,TableBean&gt; {\n\n    private String filename;\n    private Text outK = new Text();\n    private TableBean outV = new TableBean();\n\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n        //获取对应文件名称\n        InputSplit split = context.getInputSplit();\n        FileSplit fileSplit = (FileSplit) split;\n        filename = fileSplit.getPath().getName();\n    }\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        //获取一行\n        String line = value.toString();\n\n        //判断是哪个文件,然后针对文件进行不同的操作\n        if(filename.contains(\"order\")){  //订单表的处理\n            String[] split = line.split(\"\\t\");\n            //封装outK\n            outK.set(split[1]);\n            //封装outV\n            outV.setId(split[0]);\n            outV.setPid(split[1]);\n            outV.setAmount(Integer.parseInt(split[2]));\n            outV.setPname(\"\");\n            outV.setFlag(\"order\");\n        }else {                             //商品表的处理\n            String[] split = line.split(\"\\t\");\n            //封装outK\n            outK.set(split[0]);\n            //封装outV\n            outV.setId(\"\");\n            outV.setPid(split[0]);\n            outV.setAmount(0);\n            outV.setPname(split[1]);\n            outV.setFlag(\"pd\");\n        }\n\n        //写出KV\n        context.write(outK,outV);\n    }\n}\n编写TableReducer类\npackage com.atguigu.mapreduce.reducejoin;\n\nimport org.apache.commons.beanutils.BeanUtils;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\nimport java.lang.reflect.InvocationTargetException;\nimport java.util.ArrayList;\n\npublic class TableReducer extends Reducer&lt;Text,TableBean,TableBean, NullWritable&gt; {\n\n    @Override\n    protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException {\n\n        ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;();\n        TableBean pdBean = new TableBean();\n\n        for (TableBean value : values) {\n\n            //判断数据来自哪个表\n            if(\"order\".equals(value.getFlag())){   //订单表\n\n\t\t\t  //创建一个临时TableBean对象接收value\n                TableBean tmpOrderBean = new TableBean();\n\n                try {\n                    BeanUtils.copyProperties(tmpOrderBean,value);\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                } catch (InvocationTargetException e) {\n                    e.printStackTrace();\n                }\n\n\t\t\t  //将临时TableBean对象添加到集合orderBeans\n                orderBeans.add(tmpOrderBean);\n            }else {                                    //商品表\n                try {\n                    BeanUtils.copyProperties(pdBean,value);\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                } catch (InvocationTargetException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n\n        //遍历集合orderBeans,替换掉每个orderBean的pid为pname,然后写出\n        for (TableBean orderBean : orderBeans) {\n\n            orderBean.setPname(pdBean.getPname());\n\n\t\t   //写出修改后的orderBean对象\n            context.write(orderBean,NullWritable.get());\n        }\n    }\n}\n编写TableDriver类\npackage com.atguigu.mapreduce.reducejoin;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class TableDriver {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        Job job = Job.getInstance(new Configuration());\n\n        job.setJarByClass(TableDriver.class);\n        job.setMapperClass(TableMapper.class);\n        job.setReducerClass(TableReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(TableBean.class);\n\n        job.setOutputKeyClass(TableBean.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\input\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\output\"));\n\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\n缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。\n解决方案：Map端实现数据合并。\nMap Join案例实操先在MapJoinDriver驱动类中添加缓存文件\npackage com.atguigu.mapreduce.mapjoin;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\n\npublic class MapJoinDriver {\n\n    public static void main(String[] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException {\n\n        // 1 获取job信息\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n        // 2 设置加载jar包路径\n        job.setJarByClass(MapJoinDriver.class);\n        // 3 关联mapper\n        job.setMapperClass(MapJoinMapper.class);\n        // 4 设置Map输出KV类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n        // 5 设置最终输出KV类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        // 加载缓存数据\n        job.addCacheFile(new URI(\"file:///D:/input/tablecache/pd.txt\"));\n        // Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0\n        job.setNumReduceTasks(0);\n\n        // 6 设置输入输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\input\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\output\"));\n        // 7 提交\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\n在MapJoinMapper类中的setup方法中读取缓存文件\npackage com.atguigu.mapreduce.mapjoin;\n\nimport org.apache.commons.lang.StringUtils;\nimport org.apache.hadoop.fs.FSDataInputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.URI;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class MapJoinMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {\n\n    private Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;();\n    private Text text = new Text();\n\n    //任务开始前将pd数据缓存进pdMap\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n\n        //通过缓存文件得到小表数据pd.txt\n        URI[] cacheFiles = context.getCacheFiles();\n        Path path = new Path(cacheFiles[0]);\n\n        //获取文件系统对象,并开流\n        FileSystem fs = FileSystem.get(context.getConfiguration());\n        FSDataInputStream fis = fs.open(path);\n\n        //通过包装流转换为reader,方便按行读取\n        BufferedReader reader = new BufferedReader(new InputStreamReader(fis, \"UTF-8\"));\n\n        //逐行读取，按行处理\n        String line;\n        while (StringUtils.isNotEmpty(line = reader.readLine())) {\n            //切割一行    \n//01\t小米\n            String[] split = line.split(\"\\t\");\n            pdMap.put(split[0], split[1]);\n        }\n\n        //关流\n        IOUtils.closeStream(reader);\n    }\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        //读取大表数据    \n//1001\t01\t1\n        String[] fields = value.toString().split(\"\\t\");\n\n        //通过大表每行数据的pid,去pdMap里面取出pname\n        String pname = pdMap.get(fields[1]);\n\n        //将大表每行数据的pid替换为pname\n        text.set(fields[0] + \"\\t\" + pname + \"\\t\" + fields[2]);\n\n        //写出\n        context.write(text,NullWritable.get());\n    }\n}\n数据清洗（ETL）去除日志中字段个数小于等于11的日志。\n需要在Map阶段对输入的数据根据规则进行过滤清洗。\n编写WebLogMapper类\npackage com.atguigu.mapreduce.weblog;\nimport java.io.IOException;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\npublic class WebLogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;{\n\t\n\t@Override\n\tprotected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\t\t\n\t\t// 1 获取1行数据\n\t\tString line = value.toString();\n\t\t\n\t\t// 2 解析日志\n\t\tboolean result = parseLog(line,context);\n\t\t\n\t\t// 3 日志不合法退出\n\t\tif (!result) {\n\t\t\treturn;\n\t\t}\n\t\t\n\t\t// 4 日志合法就直接写出\n\t\tcontext.write(value, NullWritable.get());\n\t}\n\n\t// 2 封装解析日志的方法\n\tprivate boolean parseLog(String line, Context context) {\n\n\t\t// 1 截取\n\t\tString[] fields = line.split(\" \");\n\t\t\n\t\t// 2 日志长度大于11的为合法\n\t\tif (fields.length &gt; 11) {\n\t\t\treturn true;\n\t\t}else {\n\t\t\treturn false;\n\t\t}\n\t}\n}\n编写WebLogDriver类\npackage com.atguigu.mapreduce.weblog;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WebLogDriver {\n\tpublic static void main(String[] args) throws Exception {\n\n// 输入输出路径需要根据自己电脑上实际的输入输出路径设置\n        args = new String[] { \"D:/input/inputlog\", \"D:/output1\" };\n\n\t\t// 1 获取job信息\n\t\tConfiguration conf = new Configuration();\n\t\tJob job = Job.getInstance(conf);\n\n\t\t// 2 加载jar包\n\t\tjob.setJarByClass(LogDriver.class);\n\n\t\t// 3 关联map\n\t\tjob.setMapperClass(WebLogMapper.class);\n\n\t\t// 4 设置最终输出类型\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(NullWritable.class);\n\n\t\t// 设置reducetask个数为0\n\t\tjob.setNumReduceTasks(0);\n\n\t\t// 5 设置输入和输出路径\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\t// 6 提交\n         boolean b = job.waitForCompletion(true);\n         System.exit(b ? 0 : 1);\n\t}\n}\n压缩实操案例Map输出端采用压缩即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。\n给大家提供的Hadoop源码支持的压缩格式有：BZip2Codec、DefaultCodec\npackage com.atguigu.mapreduce.compress;\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.BZip2Codec;\t\nimport org.apache.hadoop.io.compress.CompressionCodec;\nimport org.apache.hadoop.io.compress.GzipCodec;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCountDriver {\n\n\tpublic static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n\t\tConfiguration conf = new Configuration();\n\n\t\t// 开启map端输出压缩\n\t\tconf.setBoolean(\"mapreduce.map.output.compress\", true);\n\n\t\t// 设置map端输出压缩方式\n\t\tconf.setClass(\"mapreduce.map.output.compress.codec\", BZip2Codec.class,CompressionCodec.class);\n\n\t\tJob job = Job.getInstance(conf);\n\n\t\tjob.setJarByClass(WordCountDriver.class);\n\n\t\tjob.setMapperClass(WordCountMapper.class);\n\t\tjob.setReducerClass(WordCountReducer.class);\n\n\t\tjob.setMapOutputKeyClass(Text.class);\n\t\tjob.setMapOutputValueClass(IntWritable.class);\n\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(IntWritable.class);\n\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\tboolean result = job.waitForCompletion(true);\n\n\t\tSystem.exit(result ? 0 : 1);\n\t}\n}\nMapper保持不变\npackage com.atguigu.mapreduce.compress;\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\npublic class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{\n\n\tText k = new Text();\n\tIntWritable v = new IntWritable(1);\n\n\t@Override\n\tprotected void map(LongWritable key, Text value, Context context)throws IOException, InterruptedException {\n\n\t\t// 1 获取一行\n\t\tString line = value.toString();\n\n\t\t// 2 切割\n\t\tString[] words = line.split(\" \");\n\n\t\t// 3 循环写出\n\t\tfor(String word:words){\n\t\t\tk.set(word);\n\t\t\tcontext.write(k, v);\n\t\t}\n\t}\n}\nReducer保持不变\npackage com.atguigu.mapreduce.compress;\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\npublic class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{\n\n\tIntWritable v = new IntWritable();\n\n\t@Override\n\tprotected void reduce(Text key, Iterable&lt;IntWritable&gt; values,\n\t\t\tContext context) throws IOException, InterruptedException {\n\t\t\n\t\tint sum = 0;\n\n\t\t// 1 汇总\n\t\tfor(IntWritable value:values){\n\t\t\tsum += value.get();\n\t\t}\n\t\t\n         v.set(sum);\n\n         // 2 输出\n\t\tcontext.write(key, v);\n\t}\n}\nReduce输出端采用压缩基于WordCount案例处理。\n修改驱动\npackage com.atguigu.mapreduce.compress;\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.BZip2Codec;\nimport org.apache.hadoop.io.compress.DefaultCodec;\nimport org.apache.hadoop.io.compress.GzipCodec;\nimport org.apache.hadoop.io.compress.Lz4Codec;\nimport org.apache.hadoop.io.compress.SnappyCodec;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCountDriver {\n\n\tpublic static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\t\t\n\t\tConfiguration conf = new Configuration();\n\t\t\n\t\tJob job = Job.getInstance(conf);\n\t\t\n\t\tjob.setJarByClass(WordCountDriver.class);\n\t\t\n\t\tjob.setMapperClass(WordCountMapper.class);\n\t\tjob.setReducerClass(WordCountReducer.class);\n\t\t\n\t\tjob.setMapOutputKeyClass(Text.class);\n\t\tjob.setMapOutputValueClass(IntWritable.class);\n\t\t\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(IntWritable.class);\n\t\t\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\t\t\n\t\t// 设置reduce端输出压缩开启\n\t\tFileOutputFormat.setCompressOutput(job, true);\n\n\t\t// 设置压缩的方式\n\t    FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); \n//\t    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); \n//\t    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); \n\t    \n\t\tboolean result = job.waitForCompletion(true);\n\t\t\n\t\tSystem.exit(result?0:1);\n\t}\n}\nMapper和Reducer保持不变\nYarn注：调整下列参数之前尽量拍摄Linux快照，否则后续的案例，还需要重写准备集群。\nYarn生产环境核心参数配置案例1）需求：从1G数据中，统计每个单词出现次数。服务器3台，每台配置4G内存，4核CPU，4线程。\n2）需求分析：\n1G / 128m = 8个MapTask；1个ReduceTask；1个mrAppMaster\n平均每个节点运行10个 / 3台 ≈ 3个任务（4   3   3）\n3）修改yarn-site.xml配置参数如下：\n&lt;!-- 选择调度器，默认容量 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt;\n\t&lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;\n\t&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Number of threads to handle scheduler interface.&lt;/description&gt;\n\t&lt;name&gt;yarn.resourcemanager.scheduler.client.thread-count&lt;/name&gt;\n\t&lt;value&gt;8&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Enable auto-detection of node capabilities such as\n\tmemory and CPU.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.resource.detect-hardware-capabilities&lt;/name&gt;\n\t&lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Flag to determine if logical processors(such as\n\thyperthreads) should be counted as cores. Only applicable on Linux\n\twhen yarn.nodemanager.resource.cpu-vcores is set to -1 and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is true.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.resource.count-logical-processors-as-cores&lt;/name&gt;\n\t&lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 虚拟核数和物理核数乘数，默认是1.0 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Multiplier to determine how to convert phyiscal cores to\n\tvcores. This value is used if yarn.nodemanager.resource.cpu-vcores\n\tis set to -1(which implies auto-calculate vcores) and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is set to true. The\tnumber of vcores will be calculated as\tnumber of CPUs * multiplier.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.resource.pcores-vcores-multiplier&lt;/name&gt;\n\t&lt;value&gt;1.0&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- NodeManager使用内存数，默认8G，修改为4G内存 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Amount of physical memory, in MB, that can be allocated \n\tfor containers. If set to -1 and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is true, it is\n\tautomatically calculated(in case of Windows and Linux).\n\tIn other cases, the default is 8192MB.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n\t&lt;value&gt;4096&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Number of vcores that can be allocated\n\tfor containers. This is used by the RM scheduler when allocating\n\tresources for containers. This is not used to limit the number of\n\tCPUs used by YARN containers. If it is set to -1 and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is true, it is\n\tautomatically determined from the hardware in case of Windows and Linux.\n\tIn other cases, number of vcores is 8 by default.&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n\t&lt;value&gt;4&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 容器最小内存，默认1G --&gt;\n&lt;property&gt;\n\t&lt;description&gt;The minimum allocation for every container request at the RM\tin MBs. Memory requests lower than this will be set to the value of this\tproperty. Additionally, a node manager that is configured to have less memory\tthan this value will be shut down by the resource manager.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n\t&lt;value&gt;1024&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 容器最大内存，默认8G，修改为2G --&gt;\n&lt;property&gt;\n\t&lt;description&gt;The maximum allocation for every container request at the RM\tin MBs. Memory requests higher than this will throw an\tInvalidResourceRequestException.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n\t&lt;value&gt;2048&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 容器最小CPU核数，默认1个 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;The minimum allocation for every container request at the RM\tin terms of virtual CPU cores. Requests lower than this will be set to the\tvalue of this property. Additionally, a node manager that is configured to\thave fewer virtual cores than this value will be shut down by the resource\tmanager.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;\n\t&lt;value&gt;1&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 容器最大CPU核数，默认4个，修改为2个 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;The maximum allocation for every container request at the RM\tin terms of virtual CPU cores. Requests higher than this will throw an\n\tInvalidResourceRequestException.&lt;/description&gt;\n\t&lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;\n\t&lt;value&gt;2&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 虚拟内存检查，默认打开，修改为关闭 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Whether virtual memory limits will be enforced for\n\tcontainers.&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n\t&lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 虚拟内存和物理内存设置比例,默认2.1 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Ratio between virtual memory to physical memory when\tsetting memory limits for containers. Container allocations are\texpressed in terms of physical memory, and virtual memory usage\tis allowed to exceed this allocation by this ratio.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;\n\t&lt;value&gt;2.1&lt;/value&gt;\n&lt;/property&gt;\n\n关闭虛拟内存检查原因\n\n&lt;property&gt;\n&lt;description&gt;Ratio between virtual memory to physi cal memory when setting memory liits for C0 ntainers. Container allocations are expressed in terms of physical memory, and vitual memory usage is allowed to exceed this allocation by this ratio.\n&lt;/description&gt;\n&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;\n&lt;value&gt;2.1 &lt;/value&gt;\n&lt;/property&gt;\n4）分发配置。\n注意：如果集群的硬件资源不一致，要每个NodeManager单独配置\n5）重启集群\n[atguigu@hadoop102 hadoop-3.1.3]$ sbin/stop-yarn.sh\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh\n6）执行WordCount程序\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output\n7）观察Yarn任务执行页面\nhttp://hadoop103:8088/cluster/apps\n容量调度器多队列提交案例1）在生产环境怎么创建队列？\n（1）调度器默认就1个default队列，不能满足生产要求。\n   （2）按照框架：hive /spark/ flink 每个框架的任务放入指定的队列（企业用的不是特别多）\n（3）按照业务模块：登录注册、购物车、下单、业务部门1、业务部门2\n2）创建多队列的好处？\n（1）因为担心员工不小心，写递归死循环代码，把所有资源全部耗尽。\n（2）实现任务的降级使用，特殊时期保证重要的任务队列资源充足。11.11 6.18\n业务部门1（重要）=》业务部门2（比较重要）=》下单（一般）=》购物车（一般）=》登录注册（次要）\n​    需求1：default队列占总内存的40%，最大资源容量占总资源60%，hive队列占总内存的60%，最大资源容量占总资源80%。\n​    需求2：配置队列优先级\n配置多队列的容量调度器1）在capacity-scheduler.xml中配置如下：\n（1）修改如下配置\n&lt;!-- 指定多队列，增加hive队列 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;\n    &lt;value&gt;default,hive&lt;/value&gt;\n    &lt;description&gt;\n      The queues at the this level (root is the root queue).\n    &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;!-- 降低default队列资源额定容量为40%，默认100% --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.default.capacity&lt;/name&gt;\n    &lt;value&gt;40&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 降低default队列资源最大容量为60%，默认100% --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.default.maximum-capacity&lt;/name&gt;\n    &lt;value&gt;60&lt;/value&gt;\n&lt;/property&gt;\n（2）为新加队列添加必要属性：\n&lt;!-- 指定hive队列的资源额定容量 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.capacity&lt;/name&gt;\n    &lt;value&gt;60&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 用户最多可以使用队列多少资源，1表示 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.user-limit-factor&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 指定hive队列的资源最大容量 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-capacity&lt;/name&gt;\n    &lt;value&gt;80&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 启动hive队列 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.state&lt;/name&gt;\n    &lt;value&gt;RUNNING&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 哪些用户有权向队列提交作业 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_submit_applications&lt;/name&gt;\n    &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 哪些用户有权操作队列，管理员权限（查看/杀死） --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_administer_queue&lt;/name&gt;\n    &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 哪些用户有权配置提交任务优先级 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_application_max_priority&lt;/name&gt;\n    &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 任务的超时时间设置：yarn application -appId appId -updateLifetime Timeout\n参考资料：https://blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ --&gt;\n\n&lt;!-- 如果application指定了超时时间，则提交到该队列的application能够指定的最大超时时间不能超过该值。 \n--&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-application-lifetime&lt;/name&gt;\n    &lt;value&gt;-1&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 如果application没指定超时时间，则用default-application-lifetime作为默认值 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.default-application-lifetime&lt;/name&gt;\n    &lt;value&gt;-1&lt;/value&gt;\n&lt;/property&gt;\n2）分发配置文件\n3）重启Yarn或者执行yarn rmadmin -refreshQueues刷新队列，就可以看到两条队列：\n[atguigu@hadoop102 hadoop-3.1.3]$ yarn rmadmin -refreshQueues\n公平调度器案例创建两个队列，分别是test和atguigu（以用户所属组命名）。期望实现以下效果：若用户提交任务时指定队列，则任务提交到指定队列运行；若未指定队列，test用户提交的任务到root.group.test队列运行，atguigu提交的任务到root.group.atguigu队列运行（注：group为用户所属组）。\n公平调度器的配置涉及到两个文件，一个是yarn-site.xml，另一个是公平调度器队列分配文件fair-scheduler.xml（文件名可自定义）。\n（1）配置文件参考资料：\nhttps://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html\n（2）任务队列放置规则参考资料：\nhttps://blog.cloudera.com/untangling-apache-hadoop-yarn-part-4-fair-scheduler-queue-basics/\n修改yarn-site.xml文件，加入以下参数\n&lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;\n    &lt;description&gt;配置使用公平调度器&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.fair.allocation.file&lt;/name&gt;\n    &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml&lt;/value&gt;\n    &lt;description&gt;指明公平调度器队列分配配置文件&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.fair.preemption&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n    &lt;description&gt;禁止队列间资源抢占&lt;/description&gt;\n&lt;/property&gt;\n配置fair-scheduler.xml\n&lt;?xml version=\"1.0\"?&gt;\n&lt;allocations&gt;\n  &lt;!-- 单个队列中Application Master占用资源的最大比例,取值0-1 ，企业一般配置0.1 --&gt;\n  &lt;queueMaxAMShareDefault&gt;0.5&lt;/queueMaxAMShareDefault&gt;\n  &lt;!-- 单个队列最大资源的默认值 test atguigu default --&gt;\n  &lt;queueMaxResourcesDefault&gt;4096mb,4vcores&lt;/queueMaxResourcesDefault&gt;\n\n  &lt;!-- 增加一个队列test --&gt;\n  &lt;queue name=\"test\"&gt;\n    &lt;!-- 队列最小资源 --&gt;\n    &lt;minResources&gt;2048mb,2vcores&lt;/minResources&gt;\n    &lt;!-- 队列最大资源 --&gt;\n    &lt;maxResources&gt;4096mb,4vcores&lt;/maxResources&gt;\n    &lt;!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 --&gt;\n    &lt;maxRunningApps&gt;4&lt;/maxRunningApps&gt;\n    &lt;!-- 队列中Application Master占用资源的最大比例 --&gt;\n    &lt;maxAMShare&gt;0.5&lt;/maxAMShare&gt;\n    &lt;!-- 该队列资源权重,默认值为1.0 --&gt;\n    &lt;weight&gt;1.0&lt;/weight&gt;\n    &lt;!-- 队列内部的资源分配策略 --&gt;\n    &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;\n  &lt;/queue&gt;\n  &lt;!-- 增加一个队列atguigu --&gt;\n  &lt;queue name=\"atguigu\" type=\"parent\"&gt;\n    &lt;!-- 队列最小资源 --&gt;\n    &lt;minResources&gt;2048mb,2vcores&lt;/minResources&gt;\n    &lt;!-- 队列最大资源 --&gt;\n    &lt;maxResources&gt;4096mb,4vcores&lt;/maxResources&gt;\n    &lt;!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 --&gt;\n    &lt;maxRunningApps&gt;4&lt;/maxRunningApps&gt;\n    &lt;!-- 队列中Application Master占用资源的最大比例 --&gt;\n    &lt;maxAMShare&gt;0.5&lt;/maxAMShare&gt;\n    &lt;!-- 该队列资源权重,默认值为1.0 --&gt;\n    &lt;weight&gt;1.0&lt;/weight&gt;\n    &lt;!-- 队列内部的资源分配策略 --&gt;\n    &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;\n  &lt;/queue&gt;\n\n  &lt;!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 --&gt;\n  &lt;queuePlacementPolicy&gt;\n    &lt;!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; false表示：如果指定队列不存在,不允许自动创建--&gt;\n    &lt;rule name=\"specified\" create=\"false\"/&gt;\n    &lt;!-- 提交到root.group.username队列,若root.group不存在,不允许自动创建；若root.group.user不存在,允许自动创建 --&gt;\n    &lt;rule name=\"nestedUserQueue\" create=\"true\"&gt;\n        &lt;rule name=\"primaryGroup\" create=\"false\"/&gt;\n    &lt;/rule&gt;\n    &lt;!-- 最后一个规则必须为reject或者default。Reject表示拒绝创建提交失败，default表示把任务提交到default队列 --&gt;\n    &lt;rule name=\"reject\" /&gt;\n  &lt;/queuePlacementPolicy&gt;\n&lt;/allocations&gt;\n分发配置并重启Yarn\n[atguigu@hadoop102 hadoop]$ xsync yarn-site.xml\n[atguigu@hadoop102 hadoop]$ xsync fair-scheduler.xml\n\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh\n测试提交任务1）提交任务时指定队列，按照配置规则，任务会到指定的root.test队列 \n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -Dmapreduce.job.queuename=root.test 1 1\n​                                   \n2）提交任务时不指定队列，按照配置规则，任务会到root.atguigu.atguigu队列\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 1 1\n​     \nYarn的Tool接口案例0）回顾：\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.atguigu.mapreduce.wordcount2.WordCountDriver /input /output1\n​    期望可以动态传参，结果报错，误认为是第一个输入参数。\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.atguigu.mapreduce.wordcount2.WordCountDriver -Dmapreduce.job.queuename=root.test /input /output1\n1）需求：自己写的程序也可以动态修改参数。编写Yarn的Tool接口。\n2）具体步骤：\n（1）新建Maven项目YarnDemo，pom如下：\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;com.atguigu.hadoop&lt;/groupId&gt;\n    &lt;artifactId&gt;yarn_tool_test&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;\n            &lt;version&gt;3.1.3&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n&lt;/project&gt;\n（2）新建com.atguigu.yarn报名\n（3）创建类WordCount并实现Tool接口：\npackage com.atguigu.yarn;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\n\nimport java.io.IOException;\n\npublic class WordCount implements Tool {\n\n    private Configuration conf;\n\n    @Override\n    public int run(String[] args) throws Exception {\n\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(WordCountDriver.class);\n\n        job.setMapperClass(WordCountMapper.class);\n        job.setReducerClass(WordCountReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        return job.waitForCompletion(true) ? 0 : 1;\n    }\n\n    @Override\n    public void setConf(Configuration conf) {\n        this.conf = conf;\n    }\n\n    @Override\n    public Configuration getConf() {\n        return conf;\n    }\n\n    public static class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {\n\n        private Text outK = new Text();\n        private IntWritable outV = new IntWritable(1);\n\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n            String line = value.toString();\n            String[] words = line.split(\" \");\n\n            for (String word : words) {\n                outK.set(word);\n\n                context.write(outK, outV);\n            }\n        }\n    }\n\n    public static class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n        private IntWritable outV = new IntWritable();\n\n        @Override\n        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {\n\n            int sum = 0;\n\n            for (IntWritable value : values) {\n                sum += value.get();\n            }\n            outV.set(sum);\n\n            context.write(key, outV);\n        }\n    }\n}\n（4）新建WordCountDriver\npackage com.atguigu.yarn;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport java.util.Arrays;\n\npublic class WordCountDriver {\n\n    private static Tool tool;\n\n    public static void main(String[] args) throws Exception {\n        // 1. 创建配置文件\n        Configuration conf = new Configuration();\n\n        // 2. 判断是否有tool接口\n        switch (args[0]){\n            case \"wordcount\":\n                tool = new WordCount();\n                break;\n            default:\n                throw new RuntimeException(\" No such tool: \"+ args[0] );\n        }\n        // 3. 用Tool执行程序\n        // Arrays.copyOfRange 将老数组的元素放到新数组里面\n        int run = ToolRunner.run(conf, tool, Arrays.copyOfRange(args, 1, args.length));\n\n        System.exit(run);\n    }\n}\n3）在HDFS上准备输入文件，假设为/input目录，向集群提交该Jar包\n3）在HDFS上准备输入文件，假设为/input目录，向集群提交该Jar包\n[atguigu@hadoop102 hadoop-3.1.3]$ yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver wordcount /input /output\n注意此时提交的3个参数，第一个用于生成特定的Tool，第二个和第三个为输入输出目录。此时如果我们希望加入设置参数，可以在wordcount后面添加参数，例如：\n[atguigu@hadoop102 hadoop-3.1.3]$ yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver wordcount -Dmapreduce.job.queuename=root.test /input /output1\n4）注：以上操作全部做完过后，快照回去或者手动将配置文件修改成之前的状态，因为本身资源就不够，分成了这么多，不方便以后测试。\n","slug":"B5-MapReduce案例","date":"2021-11-17T11:17:27.000Z","categories_index":"大数据","tags_index":"Hadoop","author_index":"YFR718"},{"id":"486fbd19636de541976c51f999e27abf","title":"B0.linux","content":"","slug":"B0-linux","date":"2021-11-16T09:01:33.000Z","categories_index":"","tags_index":"","author_index":"YFR718"},{"id":"c8964ce203e761cbb3c6f466ff1217cb","title":"B4.大数据常用shell脚本","content":"常见shell脚本集群见文件同步#!/bin/bash\n\n#1. 判断参数个数\nif [ $# -lt 1 ]\nthen\n    echo Not Enough Arguement!\n    exit;\nfi\n\n#2. 遍历集群所有机器\nfor host in hadoop102 hadoop103 hadoop104\ndo\n    echo ====================  $host  ====================\n    #3. 遍历所有目录，挨个发送\n\n    for file in $@\n    do\n        #4. 判断文件是否存在\n        if [ -e $file ]\n            then\n                #5. 获取父目录\n                pdir=$(cd -P $(dirname $file); pwd)\n\n                #6. 获取当前文件的名称\n                fname=$(basename $file)\n                ssh $host \"mkdir -p $pdir\"\n                rsync -av $pdir/$fname $host:$pdir\n            else\n                echo $file does not exists!\n        fi\n    done\ndone\n\n启动停止hadoop集群\n#!/bin/bash\n\nif [ $# -lt 1 ]\nthen\n    echo \"No Args Input...\"\n    exit ;\nfi\n\ncase $1 in\n\"start\")\n        echo \" =================== 启动 hadoop集群 ===================\"\n\n        echo \" --------------- 启动 hdfs ---------------\"\n        ssh hadoop102 \"/opt/module/hadoop-3.1.3/sbin/start-dfs.sh\"\n        echo \" --------------- 启动 yarn ---------------\"\n        ssh hadoop103 \"/opt/module/hadoop-3.1.3/sbin/start-yarn.sh\"\n        echo \" --------------- 启动 historyserver ---------------\"\n        ssh hadoop102 \"/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver\"\n;;\n\"stop\")\n        echo \" =================== 关闭 hadoop集群 ===================\"\n\n        echo \" --------------- 关闭 historyserver ---------------\"\n        ssh hadoop102 \"/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver\"\n        echo \" --------------- 关闭 yarn ---------------\"\n        ssh hadoop103 \"/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh\"\n        echo \" --------------- 关闭 hdfs ---------------\"\n        ssh hadoop102 \"/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh\"\n;;\n*)\n    echo \"Input Args Error...\"\n;;\nesac\n\n\n查看集群jps#!/bin/bash\n\nfor host in hadoop102 hadoop103 hadoop104\ndo\n        echo =============== $host ===============\n        ssh $host jps\ndone\n\nhive#!/bin/bash \nHIVE_LOG_DIR=$HIVE_HOME/logs\nif [ ! -d $HIVE_LOG_DIR ]\nthen\n    mkdir -p $HIVE_LOG_DIR\nfi\n#检查进程是否运行正常，参数 1为进程名，参数 2为进程端口\nfunction check_process() \n{\n   pid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i $1 | awk '{print $2}')\n   ppid=$(netstat -nltp 2&gt;/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)\n   echo $pid  \n   [[ \"$pid\" =~ \"$ppid\" ]] &amp;&amp; [ \"$ppid\" ] &amp;&amp; return 0 || return 1\n}\nfunction hive_start() \n{\n   metapid=$(check_process HiveMetastore 9083)\n   cmd=\"nohup hive --service metastore &gt;$HIVE_LOG_DIR/metastore.log 2&gt;&amp;1 &amp;\"\n   [ -z \"$metapid\" ] &amp;&amp; eval $cmd || echo \"Metastroe服务已启动\"\n   server2pid=$(check_process HiveServer2 10000)\n   cmd=\"nohup hiveserver2 &gt;$HIVE_LOG_DIR/hiveServer2.log 2&gt;&amp;1 &amp;\"\n   [ -z \"$server2pid\" ] &amp;&amp; eval $cmd || echo \"HiveServer2服务已启动\"\n}\nfunction hive_stop() \n{\n    metapid=$(check_process HiveMetastore 9083)\n    [ \"$metapid\" ] &amp;&amp; kill $metapid || echo \"Metastore服务未启动\"\n    server2pid=$(check_process HiveServer2 10000)\n    [ \"$server2pid\" ] &amp;&amp; kill $server2pid || echo \"HiveServer2服务未启动\"\n}\ncase $1 in\n\"start\")\nhive_start\n;;\n\"stop\")\nhive_stop\n;;\n\"restart\")\nhive_stop\nsleep 2\nhive_start\n;;\n\n","slug":"B4-大数据常用shell脚本","date":"2021-11-16T08:46:58.000Z","categories_index":"大数据","tags_index":"shell","author_index":"YFR718"},{"id":"35292c50fe93e1c9375839d3cf5431cb","title":"shell编程ing","content":"shell编程\nshell基础Shell是一个命令行解释器，它接收应用程序/用户命令,然后调用操作系统内核。\nShel解析器（1）Linux提供的Shell解析器有：\n[atguigu@hadoop101 ~]$ cat /etc/shells \n/bin/sh\n/bin/bash\n/sbin/nologin\n/bin/dash\n/bin/tcsh\n/bin/csh\n（2）bash和sh的关系\n[atguigu@hadoop101 bin]$ ll | grep bash\n-rwxr-xr-x. 1 root root 941880 5月  11 2016 bash\nlrwxrwxrwx. 1 root root      4 5月  27 2017 sh -&gt; bash\n（3）Centos默认的解析器是bash\n[atguigu@hadoop102 bin]$ echo $SHELL\n/bin/bash\nShell脚本入门1．脚本格式脚本以==#!/bin/bash==开头（指定解析器）2．第一个Shell脚本：helloworld\n[atguigu@hadoop101 datas]$ touch helloworld.sh\n[atguigu@hadoop101 datas]$ vi helloworld.sh\n在helloworld.sh中输入如下内容\n#!/bin/bash\necho \"helloworld\"\n脚本的常用执行方式第一种：采用bash或sh+脚本的相对路径或绝对路径（不用赋予脚本+x权限）\n#\tsh+脚本的相对路径\n[atguigu@hadoop101 datas]$ sh helloworld.sh \nHelloworld\n#\tsh+脚本的绝对路径\n[atguigu@hadoop101 datas]$ sh /home/atguigu/datas/helloworld.sh \nhelloworld\n#\tbash+脚本的相对路径\n[atguigu@hadoop101 datas]$ bash helloworld.sh \nHelloworld\n#\tbash+脚本的绝对路径\n[atguigu@hadoop101 datas]$ bash /home/atguigu/datas/helloworld.sh \nHelloworld\n第二种：采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）\n#首先要赋予helloworld.sh 脚本的+x权限\n[atguigu@hadoop101 datas]$ chmod 777 helloworld.sh\n#执行脚本\n相对路径\n[atguigu@hadoop101 datas]$ ./helloworld.sh \nHelloworld\n#绝对路径\n[atguigu@hadoop101 datas]$ /home/atguigu/datas/helloworld.sh \nHelloworld\n注意：第一种执行方法，本质是bash解析器帮你执行脚本，所以脚本本身不需要执行权限。第二种执行方法，本质是脚本需要自己执行，所以需要执行权限。\nShell中变量\n\n\n\n常用系统变量\n\n\n\n\n\n$HOME\n\n\n\n$PWD\n\n\n\n$SHELL\n\n\n\n$USER\n\n\n\n\n显示当前Shell中所有变量：set\n自定义变量#定义变量：\n变量=值 \n#撤销变量：\nunset 变量\n#声明静态变量：\nreadonly变量，注意：不能unset\n#可把变量提升为全局环境变量，可供其他Shell程序使用\nexport 变量名\n[atguigu@hadoop101 datas]$ vim helloworld.sh \n\n# 在helloworld.sh文件中增加echo $B\n#!/bin/bash\necho \"helloworld\"\necho $B\n[atguigu@hadoop101 datas]$ ./helloworld.sh \nHelloworld\n发现并没有打印输出变量B的值。\n[atguigu@hadoop101 datas]$ export B\n[atguigu@hadoop101 datas]$ ./helloworld.sh \nhelloworld\n2\n变量定义规则    （1）变量名称可以由字母、数字和下划线组成，但是不能以数字开头，环境变量名建议大写。    （2）等号两侧不能有空格    （3）在bash中，变量默认类型都是字符串类型，无法直接进行数值运算。    （4）变量的值如果有空格，需要使用双引号或单引号括起来。\n特殊变量\n\n\n\n特殊变量\n\n\n\n\n\n$n\nn为数字，代表该脚本名称，1-代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如{10}\n\n\n\n$#\n获取所有输入参数==个数==，常用于循环\n\n\n$*\n这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体\n\n\n\n$@\n这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待\n\n\n\n$？\n最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。\n\n\n\n\n\n\n\n# $0 $# $* $@用法\n[atguigu@hadoop101 datas]$ vim parameter.sh\n#!/bin/bash\necho \"$0  $1   $2\"\necho $#\necho $*\necho $@\n[atguigu@hadoop101 datas]$ bash parameter.sh 1 2 3\nparameter.sh  1   2\n3\n1 2 3\n1 2 3\n# $？ 用法\n[atguigu@hadoop101 datas]$ ./helloworld.sh \nhello world\t\n[atguigu@hadoop101 datas]$ echo $?\n0\n运算符\n\n\n\n运算符\n\n\n\n\n\n$((运算式))\n\n\n\n$[运算式]\nS=$[(2+3)*4]\n\n\n\nexpr  + , - , *, /, %\nexpr `expr 2 + 3` * 4\n\n\n\n\n条件判断\n\n\n\n[ condition ]\n（注意condition前后要有空格）\n\n\n\n\n\n\n两个整数之间比较\n\n\n\n\n\n=\n字符串比较\n-lt\n小于（less than）\n\n\n-le\n小于等于（less equal）\n-eq\n等于（equal）\n\n\n-gt\n大于（greater than）\n-ge\n大于等于（greater equal）\n\n\n-ne\n不等于（Not equal）\n\n\n\n\n按照文件权限进行判断\n\n\n\n\n\n-r\n读的权限（read）\n-w\n有写的权限（write）\n\n\n-x\n有执行的权限（execute）\n\n\n\n\n按照文件类型进行判断\n\n\n\n\n\n-f\n文件存在并且是一个常规的文件（file）\n-e\n文件存在（existence）\n\n\n-d\n文件存在并是一个目录（directory）\n\n\n\n\n\n# 23是否大于等于22\n[atguigu@hadoop101 datas]$ [ 23 -ge 22 ]\n[atguigu@hadoop101 datas]$ echo $?\n0\n# helloworld.sh是否具有写权限\n[atguigu@hadoop101 datas]$ [ -w helloworld.sh ]\n[atguigu@hadoop101 datas]$ echo $?\n0\n# /home/atguigu/cls.txt目录中的文件是否存在\n[atguigu@hadoop101 datas]$ [ -e /home/atguigu/cls.txt ]\n[atguigu@hadoop101 datas]$ echo $?\n1\n# 多条件判断（&amp;&amp; 表示前一条命令执行成功时，才执行后一条命令，|| 表示上一条命令执行失败后，才执行下一条命令）\n[atguigu@hadoop101 ~]$ [ condition ] &amp;&amp; echo OK || echo notok\nOK\n[atguigu@hadoop101 datas]$ [ condition ] &amp;&amp; [ ] || echo notok\nnotok\n\n流程控制if. case. for. whileif [ 条件判断式 ];then \n  程序 \nfi \n# [ 条件判断式 ]，中括号和条件判断式之间必须有空格\n# if后要有空格\n\nif [ 条件判断式 ] \n  then \n    程序 \nfi\n# 列\nif [ $1 -eq \"1\" ]\nthen\n        echo \"banzhang zhen shuai\"\nelif [ $1 -eq \"2\" ]\nthen\n        echo \"cls zhen mei\"\nfi\n\ncase $变量名 in \n  \"值1\"） \n    如果变量的值等于值1，则执行程序1 \n    ;; \n  \"值2\"） \n    如果变量的值等于值2，则执行程序2 \n    ;; \n  …省略其他分支… \n  *） \n    如果变量的值都不是以上的值，则执行此程序 \n    ;; \nesac\n#注意事项：\n#1)\tcase行尾必须为单词“in”，每一个模式匹配必须以右括号“）”结束。\n#2)\t双分号“;;”表示命令序列结束，相当于java中的break。\n#3)\t最后的“*）”表示默认模式，相当于java中的default。\n#\ncase $1 in\n\"1\")\n        echo \"banzhang\"\n;;\n\n\"2\")\n        echo \"cls\"\n;;\n*)\n        echo \"renyao\"\n;;\nesac\n\nfor (( 初始值;循环控制条件;变量变化 )) \n  do \n    程序 \n  done\n# 列\ns=0\nfor((i=0;i&lt;=100;i++))\ndo\n        s=$[$s+$i]\ndone\n\n\nfor 变量 in 值1 值2 值3… \n  do \n    程序 \n  done\n \n# 列\nfor i in $*\n    do\n      echo \"ban zhang love $i \"\n    done\nwhile [ 条件判断式 ] \n  do \n    程序\n  done\n\n# 列\ns=0\ni=1\nwhile [ $i -le 100 ]\ndo\n        s=$[$s+$i]\n        i=$[$i+1]\ndone\n\nread读取控制台输入read(选项)(参数)\n选项：\n-p：指定读取值时的提示符；\n-t：指定读取值时等待的时间（秒）。\n参数：变量，指定读取值的变量名\nread -t 7 -p \"Enter your name in 7 seconds \" NAME\necho $NAME\n\nshell高级系统函数自定义函数Shell工具cutShell工具sedShell工具awrkShellI具sort .","slug":"B3-shell编程","date":"2021-11-16T08:46:40.000Z","categories_index":"大数据","tags_index":"Hadoop","author_index":"YFR718"},{"id":"144f0b4d9b9f80ea377106e9c6a54a45","title":"Linux基础","content":"Linux概述Linux的创始人:Linus Torvalds 林纳斯\nLinux主要的发行版:Ubuntu(乌班图)、RedHat(红帽)、CentOS、Debain[蝶变]、Fedora、SuSE、OpenSUSE\nLinux由Unix发展而来。\nlinux与windows区别VMWare安装见学习资料\n⭐虚拟机的网络连接三种模式\n\n\n\n模式\n特点\n\n\n\n\n桥连接\nVMWare虚拟出来的操作系统就像是局域网中的一台独立的主机，它可以访问网内任何一台机器。需要手工为虚拟系统配置IP地址、子网掩码，而且还要和宿主机器处于同一网段，这样虚拟系统才能和宿主机器进行通信。同时，由于这个虚拟系统是局域网中的一个独立的主机系统，那么就可以手工配置它的TCP/IP配置信息，以实现通过局域网的网关或路由器访问互联网。可能照成ip冲突。\n\n\nNAT\n网络地址转换方式，通过宿主机器所在的网络来访问公网。使用NAT模式可以实现在虚拟系统里访问互联网。NAT模式下的虚拟系统的TCP/IP配置信息是由VMnet8(NAT)虚拟网络的DHCP服务器提供的，无法进行手工修改，因此虚拟系统也就无法和本局域网中的其他真实主机进行通讯。\n\n\n仅主机模式\n所有的虚拟系统是可以相互通信的，但虚拟系统和真实的网络是被隔离开的。\n\n\n\n\nCentOS安装见学习资料\nlinux目录结构linux的文件系统是采用级层式的树状目录结构，在此结构中的最上层是根目录“/”，然后在此目录下再创建其他的目录。\n\n\n\n\n目录结构\n含义\n\n\n\n\n/bin  (/usr/bin 、 /usr/local/bin)⭐\nBinary的缩写, 这个目录存放二进制可执行文件和最经常使用的命令\n\n\n/sbin  (/usr/sbin 、 /usr/local/sbin)\nSuper User，这里存放的是系统管理员使用的系统管理程序。\n\n\n/home⭐\n存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。\n\n\n/root ⭐\n系统管理员，也称作超级权限者的用户主目录。\n\n\n/lib\n系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。\n\n\n/lost+found\n这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。\n\n\n/etc ⭐\n所有的系统管理所需要的配置文件和子目录 my.conf\n\n\n/usr ⭐\n用户很多应用程序和文件都放在这个目录下，类似与windows下的program files目录。\n\n\n/boot ⭐\n启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件\n\n\n/proc\n虚拟的目录，它是系统内存的映射，访问这个目录来获取系统信息。\n\n\n/srv\nservice，该目录存放一些服务启动之后需要提取的数据。\n\n\n/sys\n这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。\n\n\n/tmp\n存放一些临时文件的。\n\n\n/dev\n类似于windows的设备管理器，把所有的硬件用文件的形式存储。\n\n\n/media ⭐\nlinux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。\n\n\n/mnt ⭐\n让用户临时挂载别的文件系统的，我们可以将外部的存储挂载在/mnt/上，然后进入该目录就可以查看里的内容了。 d:/myshare\n\n\n/opt\n给主机额外安装软件所摆放的目录。\n\n\n/usr/local ⭐\n另一个给主机额外安装软件所==安装的目录==。一般是通过编译源码方式安装的程序。\n\n\n/var ⭐\n不断扩充着的东西，习惯将经常被修改的目录放在这个目录下。包括各种日志文件。\n\n\n/selinux  [security-enhanced linux]  类似 360\nSELinux是一种安全子系统,它能控制程序只能访问特定文件。\n\n\n\n\nVI/VIMVI/VIM一般模式以 vim 打开一个文件就直接进入一般模式了(这是默认的模式)。在这个模式中， 你可以使用『上下左右』按键来移动光标，你可以使用『删除字符』或『删除整行』来处理档案内容， 也可以使用『复制、贴上』来处理你的文件数据。在正常模式下可以使用快捷键\nVI/VIM编辑模式按下i（insert）, I, o, O, a, A, r, R等任何一个字母之后才会进入编辑模式, 一般来说按i即可.\nVI/VIM指令模式在这个模式当中， 可以提供你相关指令，完成读取、存盘、替换、离开 vim 、显示行号等的动作则是在此模式中达成的！\nVI/VIM模式间转换一般模式+i=&gt;编辑模式        编辑模式+ESC=&gt;一般模式\n一般模式+：=&gt;命令模式      命令模式+ESC=&gt;一般模式\n\n\n\n\n命令\n功能\n\n\n\n\n:w\n保存\n\n\n:q\n退出\n\n\n:!\n强制执行\n\n\n/要查找的词\nn 查找下一个，N 往上查找\n\n\n? 要查找的词\nn是查找上一个，shift+n是往下查找\n\n\n:set nu\n显示行号\n\n\n:set nonu\n关闭行号\n\n\n\n\n\n\n\n\n语法\n功能描述\n\n\n\n\nyy\n复制光标当前一行\n\n\ny数字y\n复制一段（从第几行到第几行）\n\n\np\n箭头移动到目的行粘贴\n\n\nu\n撤销上一步\n\n\ndd\n删除光标当前行\n\n\nd数字d\n删除光标（含）后多少行\n\n\nx\n删除一个字母，相当于del\n\n\nX\n删除一个字母，相当于Backspace\n\n\nyw\n复制一个词\n\n\ndw\n删除一个词\n\n\nshift+^\n移动到行头\n\n\nshift+$\n移动到行尾\n\n\n1+shift+g\n移动到页头，数字\n\n\nshift+g\n移动到页尾\n\n\n数字N+shift+g\n移动到目标行\n\n\n\n\n系统管理操作和远程操作# 查看网络IP和网关\nifconfig\n# 配置网络ip地址\n# 配置主机名\n# 防火墙配置\n\n# 测试主机之间网络连通性\nping 目的主机\n\n\n\n\n关机重启命令\n\n\n\n\n\nshutdown  –h  now\n立该进行关机\n\n\nshutdown  –r   now\n现在重新启动计算机\n\n\nhalt\n立该进行关机\n\n\nreboot\n现在重新启动计算机\n\n\nsync\n把内存的数据同步到磁盘.\n\n\n\n\n切换成系统管理员身份.: su - 用户名\n注意细节:不管是重启系统还是关闭系统，首先要运行sync命令，把内存中的数据写到磁盘中\n用户管理Linux系统是一个多用户多任务的操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。\n用户组:类似于角色，系统可以对有共性的多个用户进行统一的管理。\n#添加用户\nuseradd [-d 家目录] [-g 用户组]  用户名\n#指定/修改密码\npasswd    用户名   \n#删除用户\nuserdel   用户名\n#查询用户信息\nid  用户名\n#切换用户\nsu  –  切换用户名\n#查看当前用户/登录用户\nwhoami/ who am I\n#新增组\ngroupadd 组名\n#修改用户的组\nusermod  –g 新的组名 用户名\n#查看文件/目录所在组/所有者\nls –ahl\n#修改文件所有者\nchown 用户名 文件名\n#修改文件所在的组\nchgrp 组名 文件名\n\n\n\n细节说明从权限高的用户切换到权限低的用户，不需要输入密码，反之需要。当需要返回到原来用户时，使用exit指令如果 su – 没有带用户名，则默认切换到root用户\n\n\n\n\n\n用户和组的相关文件\n含义\n每行含义\n\n\n\n\n/etc/passwd\n用户（user）的配置文件，记录用户的各种信息\n用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录Shell\n\n\n/etc/shadow\n口令的配置文件\n登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:保留\n\n\n/etc/group\n组(group)的配置文件，记录Linux包含的组的信息\n组名:口令:组标识号:组内用户列表\n\n\n\n\n\n\n\n\n运行级别\n\n\n\n级别\n\n\n\n\n\n0\n关机\n\n\n1\n单用户 [类似安全模式， 这个模式可以帮助找回root密码]\n\n\n2\n多用户状态没有网络服务\n\n\n3\n多用户状态有网络服务 [使用]\n\n\n4\n系统未使用保留给用户\n\n\n5\n图形界面 【】\n\n\n6\n系统重启\n\n\n\n\n\n\n\n常用运行级别是3和5 ，要修改默认的运行级别可改文件\n/etc/inittab的id:5:initdefault:这一行中的数字\n命令：init [012356] https协议\n找回root密码⭐1.启动时-&gt;快速输入enter-&gt;输入e-&gt; 进入到编辑界面-&gt; 选择中间有kernel 项-&gt; 输入e(edit)-&gt; 在该行的最后写入 1 [表示修改内核，临时生效]-&gt; 输入enter-&gt; 输入b [boot]-&gt; 进入到单用模式 【这里就可以做补救工作】\n常用命令\n\n\n\n帮助命令\n\n\n\n\n\nman [命令或配置文件]\n获得帮助信息\n\n\nhelp 命令\n获得shell内置命令的帮助信息\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n文件目录美命令\n\n\n\n\n\n\npwd\n显示当前工作目录的绝对路径\n\n\n\nls  [选项]\n-a ：显示当前目录所有的文件和目录，包括隐藏的 (文件名以.开头就是隐藏)。-l  ：以列表的方式显示信息-h  : 显示文件大小时，以 k , m, G单位显示\n\n\n\ncd [参数]\ncd ~  或者cd ：回到自己的家目录cd ..  回到当前目录的上一级目录\n\n\n\nmkdir 文件名\n创建目录-p ：创建多级目录\nmkdir -p /home/animal/tiger\n\n\nrmdir 文件名\n删除空目录\n\n\n\ntouch 文件名\n创建空文件\ntouch hello.txt\n\n\ncp [参数] 源文件 目的目录\n拷贝文件到指定目录-r ：递归复制整个文件夹\n\n\n\nrm [选项] 文件/目\n-r ：递归删除整个文件夹-f ： 强制删除不提示-v:    显示指令的详细执行过程\nrm –rf /home/bbb\n\n\nmv\n移动文件与目录：mv /temp/movefile /targetFolder 重命名 ：mv oldNameFile newNameFile\n\n\n\ncat [选项] 要查看的文件\n查看文件内容-n ：显示行号\ncat a.txt \\\nmore\n\n\nmore\n按页显示文本内容\n\n\n\nless\n分屏查看文件内容\n\n\n\necho [选项] [内容]\n输出内容到控制台 -e： 支持反斜线控制的字符转换\n\n\n\nhead\n显示文件头部内容-n : 查看前几行内容\n\n\n\ntai\n输出文件中尾部的内容-n : 查看前几行内容\n\n\n\n&gt;\n输出重定向，覆盖写\nls -l &gt;文件\n\n\n&gt;&gt;\n列表的内容追加到文件\nls -al &gt;&gt;文件\n\n\nln\n软链接，类似于windows里的快捷方式rm -rf houzi （删除软连接）\nln -s [原文件或目录] [软链接名]\n\n\nhistory\n查看已经执行过历史命令\n\n\n\n\n\n\n\n\nmore 操作\n功能说明\n\n\n\n\n空白键 (space)\n代表向下翻一页；\n\n\nEnter\n代表向下翻『一行』；\n\n\nq\n代表立刻离开 more ，不再显示该文件内容。\n\n\nCtrl+F\n向下滚动一屏\n\n\nCtrl+B\n返回上一屏\n\n\n=\n输出当前行的行号\n\n\n:f\n输出文件名和当前行的行号\n\n\n\n\n\n\n\n\nless  操作\n功能说明\n\n\n\n\n空白键\n向下翻动一页；\n\n\n[pagedown]\n向下翻动一页\n\n\n[pageup]\n向上翻动一页；\n\n\n/字串\n向下搜寻『字串』的功能；n：向下查找；N：向上查找；\n\n\n?字串\n向上搜寻『字串』的功能；n：向上查找；N：向下查找；\n\n\nq\n离开 less 这个程序；\n\n\n\n\n\n\n\n\necho  控制字符\n作用\n\n\n\n\n\\\n输出\\本身\n\n\n\\n\n换行符\n\n\n\\t\n制表符，也就是Tab键\n\n\n\n\n时间日期类命令1) date （功能描述：显示当前时间）2) date +%Y （功能描述：显示当前年份）3) date +%m （功能描述：显示当前月份）4) date +%d （功能描述：显示当前是哪一天）5) date “+%Y-%m-%d %H:%M:%S”（功能描述：显示年月日时分秒）6) date -s 字符串时间 （设定系统时间）7) cal 查看日历\n权限类命令-rwxrw-r— 1 root police 1213 Feb 2 09:39 abc.txt\n0-9位说明\n\n第0位确定文件类型(说明: -:普通文件, d:目录，l : 连接文件, c: 字符设备文件[键盘,鼠标] b: 块设备文件[硬盘] )\n第1-3位确定所有者（该文件的所有者）拥有该文件的权限。R: 读 ， w : 写权限 x: 执行权限 \n第4-6位确定所属组（同用户组的）拥有该文件的权限\n第7-9位确定其他用户拥有该文件的权限\n1: 如果是文件，表示硬链接的数目， 如果是目录，则表示有多少个子目录\n1213： 表示文件大小，如果是目录，则统一为 4096\n\nrwx作用到文件⭐\n1) [ r ]代表可读(read): 可以读取,查看2) [ w ]代表可写(write): 可以修改,但是不代表可以删除该文件,删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件.3) [ x ]代表可执行(execute):可以被执行  \nrwx作用到目录⭐\n1) [ r ]代表可读(read): 可以读取，ls查看目录内容2) [ w ]代表可写(write): 可以修改,目录内创建+删除+重命名目录3) [ x ]代表可执行(execute):可以进入该目录  \n变更权限第一种方式：+ 、-、= u:所有者  g:所有组  o:其他人  a:所有人(u、g、o的总和)\n  1) chmod   u=rwx,g=rx,o=x   文件、目录 【表示：给所有者rwx, 给所在组的用户 rx, 给其他人 x】  2) chmod   o+w    文件、目录 【表示：给其它用户增加w 的权限】  3) chmod   a-x    文件、目录    【表示：给所有用户 去掉 x权限】\n第二种方式：通过数字变更权限 r=4 w=2 x=1   rwx=4+2+1=7 chmod u=rwx,g=rx,o=x    文件、目录相当于 chmod   751  文件、目录\n修改文件所有者chown  newowner  file  改变文件的所有者chown  newowner:newgroup  file  改变用户的所有者和所有组-R   如果是目录 则使其下所有子文件或目录递归生效\n修改文件所在组chgrp newgroup file \n搜索查找类命令\nfind：find指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件或者目录显示在终端。\n\n\n\n\n选项\n功能\n\n\n\n\n-name&lt;查询方式&gt;\n按照指定的文件名查找模式查找文件 , 可以使用通配符 * ？\n\n\n-user&lt;用户名&gt;\n查找属于指定用户名所有文件\n\n\n-size&lt;文件大小&gt;\n按照指定的文件大小查找文件。\n\n\n\n\nfind /home  -name hello.txt \nlocate：快速定位文件路径。locate hello.txt\ngrep：过滤查找 \n\n\n\n\n选项\n功能\n\n\n\n\n-n\n显示匹配行及行号。\n\n\n-i\n忽略字母大小写\n\n\n\n\ngrep –n  if /etc/profile [在/etc/profile 中查找 if ,并显示行，区别大小写]\n\n\n\n\n压缩和解压类命令\n\n\n\n\n\ngzip 文件\n压缩文件，只能将文件压缩为*.gz文件\n\n\ngunzip 文件.gz\n解压缩文件命令\n\n\nzip   [选项] XXX.zip\n压缩文件和目录-r：递归压缩，即压缩目录\n\n\nunzip [选项] XXX.zip\n解压缩文件-d&lt;目录&gt; ：指定解压后文件的存放目录\n\n\ntar\n打包指令\n\n\n\n\n\n\n\n\ntar 选项\n功能\n\n\n\n\n-c\n产生.tar打包文件\n\n\n-v\n显示详细信息\n\n\n-f\n指定压缩后的文件名\n\n\n-z\n打包同时压缩\n\n\n-x\n解包.tar文件\n\n\n\n\n# 压缩多个文件, 将/homne/a1.txt 和/homne/a2.txt 压缩成 a.tar.gz\ntar -zcvf a.tar.gz al.txt a2.txt [注意， 路径要写清楚]\n# 将/home 的文件夹压缩成myhome.tar.gz\ntar -zcvf myhome.tar:gz /home/ [注意， 路径写清楚]\n# 将 a.tar.gz 解压到当前目录\ntar -zxvf a.tar.gz\n# 将myhome.tar:gz解压到/opt/tmp2 目录下[ -C ]\ntar -zxvf myhome.tar.gz _C /opt/tmp2 [注 意; /opt/tmp2事先需要创建好]\n磁盘分区类\n\n\n\n磁盘分区类\n\n\n\n\n\ndf 选项\n列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况  -h    以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；\n\n\nfdisk -l\n查看磁盘分区详情-l 显示所有硬盘的分区列表\n\n\nmount 参数\n挂载设备\n\n\numount 设备文件名或挂载点\n卸载设备\n\n\n\n\n\n\n\n\nmount   参数\n功能\n\n\n\n\n-t vfstype\n指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。常用类型有：  光盘或光盘镜像：iso9660  DOS fat16文件系统：msdos  Windows 9x fat32文件系统：vfat  Windows NT ntfs文件系统：ntfs  Mount Windows文件网络共享：smbfs  UNIX(LINUX) 文件网络共享：nfs\n\n\n-o options\n主要用来描述设备或档案的挂接方式。常用的参数有：  loop：用来把一个文件当成硬盘分区挂接上系统  ro：采用只读方式挂接设备  rw：采用读写方式挂接设备  　 iocharset：指定访问文件系统所用字符集\n\n\ndevice\n要挂接(mount)的设备\n\n\ndir\n设备在系统上的挂接点(mount point)\n\n\n\n\n进程线程类命令\n\n\n\n进程线程类\n\n\n\n\n\nps\n查看当前系统进程状态\n\n\nps aux  \\\ngrep xxx\n查看系统中所有进程\n\n\nps -ef \\\ngrep xxx\n可以查看子父进程之间的关系\n\n\n\n\n\n\n\n\n\n\n\nps  选项\n功能\n\n\n\n\n-a\n选择所有进程\n\n\n-u\n显示所有用户的所有进程\n\n\n-x\n显示没有终端的进程\n\n\n\n\n功能说明（1）ps aux显示信息说明USER：该进程是由哪个用户产生的PID：进程的ID号%CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；VSZ：该进程占用虚拟内存的大小，单位KB；RSS：该进程占用实际物理内存的大小，单位KB；TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台START：该进程的启动时间TIME：该进程占用CPU的运算时间，注意不是系统时间COMMAND：产生此进程的命令名（2）ps -ef显示信息说明UID：用户IDPID：进程IDPPID：父进程IDC：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高STIME：进程启动的时间TTY：完整的终端名称TIME：CPU时间CMD：启动进程所用的命令和参数\n\n\n\n\nkill 终止进程\n\n\n\n\n\nkill [选项] 进程号\n通过进程号杀死进程-9 表示强迫进程立即停止\n\n\nkillall 进程名称\n通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用\n\n\n\n\n\n\n\n\npstree 查看进程树\n\n\n\n\n\npstree [选项]\n\n\n\n-p\n显示进程的PID\n\n\n-u\n显示进程的所属用户\n\n\n\n\n\n\n\n\ntop 查看系统健康状态\n\n\n\n\n\n\ntop [选项]\n\n\n\n\n\n-d 秒数\n指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：\n\n\n\n-i\n使top不显示任何闲置或者僵死进程。\n\n\n\n-p\n通过指定监控进程ID来仅仅监控某个进程的状态。\n\n\n\n\n\n\n\n\ntop 操作\n功能\n\n\n\n\nP\n以CPU使用率排序，默认就是此项\n\n\nM\n以内存的使用率排序\n\n\nN\n以PID排序\n\n\nq\n退出top\n\n\n\n\n查询结果字段解释\n第一行信息为任务队列信息\n\n\n\n\n内容\n说明\n\n\n\n\n12:26:46\n系统当前时间\n\n\nup 1 day, 13:32\n系统的运行时间，本机已经运行1天  13小时32分钟\n\n\n2 users\n当前登录了两个用户\n\n\nload average:   0.00, 0.00, 0.00\n系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。\n\n\n\n\n第二行为进程信息\n\n\n\n\nTasks: 95 total\n系统中的进程总数\n\n\n\n\n1 running\n正在运行的进程数\n\n\n94 sleeping\n睡眠的进程\n\n\n0 stopped\n正在停止的进程\n\n\n0 zombie\n僵尸进程。如果不是0，需要手工检查僵尸进程\n\n\n\n\n第三行为CPU信息\n\n\n\n\nCpu(s): 0.1%us\n用户模式占用的CPU百分比\n\n\n\n\n0.1%sy\n系统模式占用的CPU百分比\n\n\n0.0%ni\n改变过优先级的用户进程占用的CPU百分比\n\n\n99.7%id\n空闲CPU的CPU百分比\n\n\n0.1%wa\n等待输入/输出的进程的占用CPU百分比\n\n\n0.0%hi\n硬中断请求服务占用的CPU百分比\n\n\n0.1%si\n软中断请求服务占用的CPU百分比\n\n\n0.0%st\nst（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。\n\n\n\n\n第四行为物理内存信息\n\n\n\n\nMem:  625344k total\n物理内存的总量，单位KB\n\n\n\n\n571504k used\n已经使用的物理内存数量\n\n\n53840k free\n空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了\n\n\n65800k buffers\n作为缓冲的内存数量\n\n\n\n\n第五行为交换分区（swap）信息\n\n\n\n\nSwap:  524280k total\n交换分区（虚拟内存）的总大小\n\n\n\n\n0k used\n已经使用的交互分区的大小\n\n\n524280k free\n空闲交换分区的大小\n\n\n409280k cached\n作为缓存的交互分区的大小\n\n\n\n\n\n\n\n\nnetstat\n示网络统计信息和端口占用情况\n\n\n\n\n\nnetstat -anp \\\ngrep 进程号\n查看该进程网络信息\n\n\n\nnetstat -nlp \\\ngrep 端口号\n查看网络端口号占用情况\n\n\n\n\n-n\n拒绝显示别名，能显示数字的全部转化成数字\n\n\n\n-l\n仅列出有在listen（监听）的服务状态\n\n\n\n\ncrond系统定时任务\n\n\n\ncrond 服务管理\n\n\n\n\n\n\nservice crond restart\n重新启动crond服务\n\n\n\ncrontab [选项]\n-e\n编辑crontab定时任务\n\n\n\n-l\n查询crontab任务\n\n\n\n-r\n删除当前用户所有的crontab任务\n\n\n\n\n（1）进入crontab编辑界面。会打开vim编辑你的工作。\n*     执行的任务\n\n\n\n\n项目\n含义\n范围\n\n\n\n\n第一个“*”\n一小时当中的第几分钟\n0-59\n\n\n第二个“*”\n一天当中的第几小时\n0-23\n\n\n第三个“*”\n一个月当中的第几天\n1-31\n\n\n第四个“*”\n一年当中的第几月\n1-12\n\n\n第五个“*”\n一周当中的星期几\n0-7（0和7都代表星期日）\n\n\n\n\n（2）特殊符号\n\n\n\n\n特殊符号\n含义\n\n\n\n\n*\n代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。\n\n\n，\n代表不连续的时间。比如“0 8,12,16   * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令\n\n\n-\n代表连续的时间范围。比如“0 5     1-6命令”，代表在周一到周六的凌晨5点0分执行命令\n\n\n*/n\n代表每隔多久执行一次。比如“/10      * 命令”，代表每隔10分钟就执行一遍命令\n\n\n\n\n（3）特定时间执行命令\n\n\n\n\n时间\n含义\n\n\n\n\n45 22   * 命令\n在22点45分执行命令\n\n\n0 17   1 命令\n每周1 的17点0分执行命令\n\n\n0 5 1,15   命令\n每月1号和15号的凌晨5点0分执行命令\n\n\n40 4   1-5 命令\n每周一到周五的凌晨4点40分执行命令\n\n\n/10 4    命令\n每天的凌晨4点，每隔10分钟执行一次命令\n\n\n0 0 1,15 * 1 命令\n每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。\n\n\n\n\n# 每隔1分钟，向/root/bailongma.txt文件中添加一个11的数字\n*/1 * * * * /bin/echo ”11” &gt;&gt; /root/bailongma.txt\n\n软件包管理RPMRPM（RedHat Package Manager），RedHat软件包管理工具，类似windows里面的setup.exe 是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。RPM包的名称格式Apache-1.3.23-11.i386.rpm\n\n“apache” 软件名称\n“1.3.23-11”软件的版本号，主版本和此版本\n“i386”是软件所运行的硬件平台，Intel 32位微处理器的统称\n“rpm”文件扩展名，代表RPM包\n\n\n\n\n\nRPM命令\n\n\n\n\n\n\nrpm -qa\n查询所安装的所有rpm软件包\n\n\n\nrpm -e RPM软件包\n卸载软件包\n\n\n\nrpm -e —nodeps 软件包\n卸载软件时，不检查依赖。\n\n\n\nrpm -ivh RPM包全名\n-i\n-i=install，安装\n\n\n\n-v\n-v=verbose，显示详细信息\n\n\n\n-h\n-h=hash，进度条\n\n\n\n—nodeps\n—nodeps，不检测依赖进度\n\n\n\n\n# 由于软件包比较多，一般都会采取过滤。rpm -qa | grep rpm软件包\n[root@hadoop101 Packages]# rpm -qa |grep firefox \nfirefox-45.0.1-1.el6.centos.x86_64\n# 卸载 firefox\n[root@hadoop101 Packages]# rpm -e firefox\n# 安装 firefox\n[root@hadoop101 Packages]# rpm -ivh firefox-45.0.1-1.el6.centos.x86_64.rpm \nwarning: firefox-45.0.1-1.el6.centos.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY\nPreparing...                ########################################### [100%]\n   1:firefox                ########################################### [100%]\nYUMYUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。\n\n\n\n\nYUM的常用命令\n\n\n\n\n\n\nyum [选项] [参数]\n选项\n功能\n\n\n\n-y\n对所有提问都回答“yes”\n\n\n\n参数\n功能\n\n\n\ninstall\n安装rpm软件包\n\n\n\nupdate\n更新rpm软件包\n\n\n\ncheck-update\n检查是否有可用的更新rpm软件包\n\n\n\nremove\n删除指定的rpm软件包\n\n\n\nlist\n显示软件包信息\n\n\n\nclean\n清理yum过期的缓存\n\n\n\ndeplist\n显示yum软件包的所有依赖关系\n\n\n\n\n# 采用yum方式安装firefox\n[root@hadoop101 ~]#yum -y install firefox.x86_64\n\n修改网络YUM源\n#前提条件linux系统必须可以联网\n#在Linux环境中访问该网络地址：http://mirrors.163.com/.help/centos.html，在使用说明中点击CentOS6-&gt;再点击保存\n#查看文件保存的位置，在打开的终端中输入如下命令，就可以找到文件的保存位置。\n[atguigu@hadoop101 下载]$ pwd\n/home/atguigu/下载\n#2.替换本地yum文件\n#把下载的文件移动到/etc/yum.repos.d/目录\n[root@hadoop101 下载]# mv CentOS6-Base-163.repo /etc/yum.repos.d/\t\n#进入到/etc/yum.repos.d/目录\n[root@hadoop101 yum.repos.d]# pwd\n/etc/yum.repos.d\n#用CentOS6-Base-163.repo替换CentOS-Base.repo\n[root@hadoop101 yum.repos.d]# mv CentOS6-Base-163.repo  CentOS-Base.repo\n#3．安装命令\n[root@hadoop101 yum.repos.d]#yum clean all\n[root@hadoop101 yum.repos.d]#yum makecache\nyum makecache就是把服务器的包信息下载到本地电脑缓存起来\n#4．测试\n[root@hadoop101 yum.repos.d]#yum list | grep firefox\n[root@hadoop101 ~]#yum -y install firefox.x86_64\n\n常见错误及解决方案百度&amp;考满分问题：Linux常用命令参考答案：find、df、tar、ps、top、netstat等。（尽量说一些高级命令）10.2 瓜子二手车问题：Linux查看内存、磁盘存储、io 读写、端口占用、进程等命令答案：1、查看内存：top2、查看磁盘存储情况：df -h3、查看磁盘IO读写情况：iotop（需要安装一下：yum install iotop）、iotop -o（直接查看输出比较高的磁盘读写程序）4、查看端口占用情况：netstat -tunlp | grep 端口号5、查看进程：ps aux\n","slug":"B0-Linux基础","date":"2021-11-16T08:35:41.000Z","categories_index":"大数据","tags_index":"Linux","author_index":"YFR718"},{"id":"d627fd9c1bed4de81fff7ae3f54e977a","title":"Hadoop基础","content":"0. 大数据概念大数据（Big Data）：指无法在==一定时间范围内==用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的==海量、高增长率和多样化==的==信息资产==。大数据主要解决，海量数据的采集、存储和分析计算问题。\n⭐大数据特点（4V） ：\n\nVolume（大量）\nVelocity（高速）\nVariety（多样）\nValue（低价值密度）\n\n1. Hadoop 概述\nHadoop是什么1）Hadoop是一个由Apache基金会所开发的分布式系统基础架构。2）主要解决，海量数据的存储和海量数据的分析计算问题。3）广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。\n\nHadoop发展历史\n1）Hadoop创始人Doug\n2）2001年年底Lucene成为Apache基金会的一个子项目。3）对于海量数据的场景，Lucene框架面对与Google同样的困难，存储海量数据困难，检索海量速度慢。4）学习和模仿Google解决这些问题的办法 ：微型版Nutch。5）可以说Google是Hadoop的思想之源（Google在大数据方面的三篇论文）\nGFS —-&gt;HDFS、Map-Reduce —-&gt;MR、BigTable —-&gt;HBase6）2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。\n7）2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。8）2006 年 3 月份，Map-Reduce和Nutch Distributed File System （NDFS）分别被纳入到 Hadoop 项目中，Hadoop就此正式诞生，标志着大数据时代来临。\n9）名字来源于Doug Cutting儿子的玩具大象\n\n\nHadoop 优势（4 高）\n高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。\n高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。\n高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。\n高容错性：能够自动将失败的任务重新分配。\n\nHadoop 组成（面试重点） ⭐⭐\n\n\n\nHadoop1.x\nHadoop2.x\n\n\n\n\nMapReduce（计算+资源调度）\nMapReduce（计算）\n\n\n\nYarn（资源调度）\n\n\nHDFS（数据存储）\nHDFS（数据存储）\n\n\nCommon（辅助工具）\nCommon（辅助工具）\n\n\n\n\n在 Hadoop1.x 时代 ， Hadoop中的MapReduce同时处理业务逻辑运算和资源的调度，耦合性较大。在Hadoop2.x时代，增加了Yarn。Yarn只负责资 源 的 调 度 ， MapReduce 只负责运算。在Hadoop3.x在组成上没有变化。\nHDFS 架构概述Hadoop Distributed File System，简称 HDFS，是一个分布式文件系统。 \nHDFS架构组成1）NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。2）DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。3）Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份。\nYARN 架构概述Yet Another Resource Negotiator 简称 YARN ，另一种资源协调者，是 Hadoop 的资源管理器。 \nYarn架构的组成\n1）    ResourceManager（RM）：整个集群资源（内存、CPU等）的老大2）    NodeManager（NM）：单个节点服务器资源老大3）    ApplicationMaster（AM）：单个任务运行的老大4）    Container：容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络\n\nMapReduce 架构概述MapReduce 将计算过程分为两个阶段：Map 和Reduce \n1）Map 阶段并行处理输入数据2）Reduce 阶段对 Map 结果进行汇总\n大数据技术生态体系HDFS、YARN、MapReduce三者关系\n\n大数据技术生态体系\n\n图中涉及的技术名词解释如下：1）Sqoop：Sqoop 是一款开源的工具，主要用于在Hadoop、Hive 与传统的数据库（MySQL）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop 的HDFS 中，也可以将HDFS 的数据导进到关系型数据库中。2）Flume：Flume 是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统， Flume 支持在日志系统中定制各类数据发送方，用于收集数据；3）Kafka：Kafka 是一种高吞吐量的分布式发布订阅消息系统；更多Java –大数据 –前端 –python 人工智能资料下载，可百度访问：尚硅谷官网4）Spark：Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数据进行计算。5）Flink：Flink 是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。 6）Oozie：Oozie 是一个管理Hadoop 作业（job）的工作流程调度管理系统。7）Hbase：HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。8）Hive：Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开发专门的MapReduce 应用，十分适合数据仓库的统计分析。9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。 \n推荐系统框架图 \n\n2. Hadoop 运行环境搭建（开发重点）基础环境准备\n安装VMware、xshell\n配置静态IP、hostname、hosts\n创建用户，设置权限，克隆环境\n安装JDK、安装Hadoop\n\nHadoop 运行模式本地运行模式啥都不用配置，直接运行即可\n完全分布式运行模式（开发重点）⭐⭐⭐\n配置台虚拟机\n设置免密通信、shell脚本分发安装的JDK、Hadoop环境\n配置集群、历史服务器、日志服务器\nshell控制集群启停和jps情况\n\n集群部署规划：\n\nNameNode和 SecondaryNameNode不要安装在同一台服务器\nResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在 配置在 同一台机器上。\n\n\n\n\n\n\nhadoop102\nhadoop103\nhadoop104\n\n\n\n\nHDFS\nNameNodeDataNode\nDataNode\nSecondaryNameNodeDataNode\n\n\nYARN\nNodeManager\nResourceManagerNodeManager\nNodeManager\n\n\n\n\n常用端口号说明⭐⭐\n\n\n\n端口名称\nHadoop2.x\nHadoop3.x\n\n\n\n\nNameNode内部通信端口\n8020 / 9000\n8020 / 9000/9820\n\n\nNameNode HTTP UI\n50070\n9870\n\n\nMapReduce查看执行任务端口\n8088\n8088\n\n\n历史服务器通信端口\n19888\n19888\n\n\n\n\n\n\n\n\n\nweb\n服务\n\n\n\n\nHDFS\n9870\n8020\n\n\nYARN\n8088\n8032\n\n\nhistory\n19888\n10020\n\n\n\n\n常用配置文件⭐⭐\n\n\n\n配置文件\n2.x\n3.x\n\n\n\n\n核心配置文件\ncore-site.xml\ncore-site.xml\n\n\nHDFS配置文件\nhdfs-site.xml\nhdfs-site.xml\n\n\nYARN配置文件\nyarn-site.xml\nyarn-site.xml\n\n\nMapReduce配置文件\nmapred-site.xml\nmapred-site.xml\n\n\n\nslaves\nworkers\n\n\n\n\n配置 core-site.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;configuration&gt;\n    &lt;!-- 指定NameNode的地址 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://hadoop102:8020&lt;/value&gt;\n    &lt;/property&gt;\n    \n    &lt;!-- 指定hadoop数据的存储目录 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n        &lt;value&gt;/opt/module/hadoop-3.1.3/data&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;\n        &lt;value&gt;atguigu&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n\n配置 hdfs-site.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;configuration&gt;\n\t&lt;!-- nn web端访问地址--&gt;\n\t&lt;property&gt;\n        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n        &lt;value&gt;hadoop102:9870&lt;/value&gt;\n    &lt;/property&gt;\n\t&lt;!-- 2nn web端访问地址--&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;\n        &lt;value&gt;hadoop104:9868&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n\n配置 yarn-site.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n&lt;configuration&gt;\n    &lt;!-- 指定MR走shuffle --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 指定ResourceManager的地址--&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n        &lt;value&gt;hadoop103&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 环境变量的继承 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;\n        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n\n\n配置 mapred-site.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;configuration&gt;\n\t&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n\n配置历史服务器：配置 mapred-site.xml\n&lt;!-- 历史服务器端地址 --&gt;\n&lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;\n    &lt;value&gt;hadoop102:10020&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 历史服务器web端地址 --&gt;\n&lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;\n    &lt;value&gt;hadoop102:19888&lt;/value&gt;\n&lt;/property&gt;\n\n配置日志的聚集：配置 yarn-site.xml\n&lt;!-- 开启日志聚集功能 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;!-- 设置日志聚集服务器地址 --&gt;\n&lt;property&gt;  \n    &lt;name&gt;yarn.log.server.url&lt;/name&gt;  \n    &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt;\n&lt;/property&gt;\n&lt;!-- 设置日志保留时间为7天 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;\n    &lt;value&gt;604800&lt;/value&gt;\n&lt;/property&gt;\n\n3. HDFSHDFS概述HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。\nHDFS的使用场景：适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变。\nHDFS优缺点HDFS优点\n1）    高容错性➢    数据自动保存多个副本。它通过增加副本的形式，提高容错性。\n➢    某一个副本丢失以后，它可以自动恢复。\n2）    适合处理大数据➢    数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据；➢    文件规模：能够处理百万规模以上的文件数量，数量相当之大。3）    可构建在廉价机器上，通过多副本机制，提高可靠性。\nHDFS缺点\n1）    不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。2）    无法高效的对大量小文件进行存储。➢    存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；➢    小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。\n3）    不支持并发写入、文件随机修改。➢    一个文件只能有一个写，不允许多个线程同时写；\n➢    仅支持数据append（追加），不支持文件的随机修改。\nHDFS组成架构\n\n\n\nHDFS组成架构\n\n\n\n\n\nNameNode（nn）\n就是Master，它是一个主管、管理者。\n\n\n\n（1）    管理HDFS的名称空间；（2）    配置副本策略；（3）    管理数据块（Block）映射信息；（4）    处理客户端读写请求。\n\n\nDataNode\n就是Slave。NameNode下达命令，DataNode执行实际的操作。\n\n\n\n（1）    存储实际的数据块；（2）    执行数据块的读/写操作。\n\n\nClient\n就是客户端。\n\n\n\n（1）    文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传；（2）    与NameNode交互，获取文件的位置信息；（3）    与DataNode交互，读取或者写入数据；（4）    Client提供一些命令来管理HDFS，比如NameNode格式化；\n\n\nSecondary NameNode\n并非NameNode的热备份，当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。\n\n\n\n（1）    辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode ；（2）    在紧急情况下，可辅助恢复NameNode。\n\n\n\n\nHDFS 文件块大小（面试重点） ⭐⭐HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数( dfs.blocksize）来规定，默认大小在Hadoop2.x/3.x版本中是128M，1.x版本中是64M。\n1）集群中的block\n2）如果寻址时间约为10ms，即查找到目标block的时间为10ms。3）寻址时间为传输时间的1%时，则为最佳状态。（专家）因此，传输时间=10ms/0.01=1000ms=1s4）而目前磁盘的传输速率普遍为100MB/s。\n5） block大小=1s*100MB/s=100MB\n思考：为什么块的大小不能设置太小，也不能设置太大？\n（1）    HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；（2）    如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。\n总结：HDFS块的大小设置主要取决于==磁盘传输速率==。\nHDFS 的Shell 操作hadoop fs 具体命令  OR  hdfs dfs 具体命令 两个是完全相同的。 \n// 命令大全\n[-cat [-ignoreCrc] &lt;src&gt; ...]\n[-chgrp [-R] GROUP PATH...]\n[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]\n[-chown [-R] [OWNER][:[GROUP]] PATH...]\n[-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]\n[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]\n[-count [-q] &lt;path&gt; ...]\n[-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;]\n[-df [-h] [&lt;path&gt; ...]]\n[-du [-s] [-h] &lt;path&gt; ...]\n[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]\n[-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]\n[-help [cmd ...]]\n[-ls [-d] [-h] [-R] [&lt;path&gt; ...]]\n[-mkdir [-p] &lt;path&gt; ...]\n[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]\n[-moveToLocal &lt;src&gt; &lt;localdst&gt;]\n[-mv &lt;src&gt; ... &lt;dst&gt;]\n[-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]\n[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]\n[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]&lt;acl_spec&gt; &lt;path&gt;]]\n[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]\n[-stat [format] &lt;path&gt; ...]\n[-tail [-f] &lt;file&gt;]\n[-test -[defsz] &lt;path&gt;]\n[-text [-ignoreCrc] &lt;src&gt; ...]\n\n\n\n\n常用命令实操\n\n\n\n\n\nsbin/start-dfs.shsbin/start-yarn.sh\n启动Hadoop集群\n\n\nhadoop fs -help rm\n输出这个命令参数\n\n\nhadoop fs -mkdir /a\n创建/sanguo文件夹\n\n\nhadoop fs -moveFromLocal ./a.txt /a\n从本地剪切粘贴到HDFS\n\n\nhadoop fs -copyFromLocal a.txt /a\n从本地文件系统中拷贝文件到HDFS路径去\n\n\nhadoop fs -put ./a.txt /sangauo\n等同于copyFromLocal，生产环境更习惯用put\n\n\nhadoop fs -appendToFile a.txt /a/a.txt\n追加一个文件到已经存在的文件末尾\n\n\nhadoop fs -copyToLocal /a/a.txt ./\n从HDFS拷贝到本地\n\n\nhadoop fs -get /a/a.txt ./a.txt\n等同于copyToLocal，生产环境更习惯用get\n\n\nhadoop fs -ls /a\n显示目录信息\n\n\nhadoop fs -cat /a/a.txt\n显示文件内容\n\n\nhadoop fs -chmod 666\n-chgrp、-chmod、-chown：\n\n\nhadoop fs -chown yfr:yfr  /a/a.txt\nLinux文件系统中的用法一样，修改文件所属权限\n\n\nhadoop fs -mkdir /a\n创建路径\n\n\nhadoop fs -cp /a/a.txt /b\n从HDFS的一个路径拷贝到HDFS的另一个路径\n\n\nhadoop fs -mv /a/a.txt /b\n在HDFS目录中移动文件\n\n\nhadoop fs -tail /a/a.txt\n显示一个文件的末尾1kb的数据\n\n\nhadoop fs -rm /a/a.txt\n删除文件或文件夹\n\n\nhadoop fs -rm -r /a\n删除文件或文件夹\n\n\nhadoop fs -du -s -h /a\n递归删除目录及目录里面内容\n\n\nhadoop fs -du -h /a\n统计文件夹的大小信息\n\n\nhadoop fs -setrep 10 /a/a.txt\n设置HDFS中文件的副本数量\n\n\n\n\n\n\n\n\n\n\n\nHDFS的API操作//HDFS文件上传\n@Test\npublic void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException {\n\n    // 1 获取文件系统\n    Configuration configuration = new Configuration();\n    configuration.set(\"dfs.replication\", \"2\");\n    FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\");\n\n    // 2 上传文件\n    fs.copyFromLocalFile(new Path(\"d:/sunwukong.txt\"), new Path(\"/xiyou/huaguoshan\"));\n\n    // 3 关闭资源\n    fs.close();\n｝\n//HDFS文件下载\n@Test\npublic void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{\n\n    // 1 获取文件系统\n    Configuration configuration = new Configuration();\n    FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\");\n    \n    // 2 执行下载操作\n    // boolean delSrc 指是否将原文件删除\n    // Path src 指要下载的文件路径\n    // Path dst 指将文件下载到的路径\n    // boolean useRawLocalFileSystem 是否开启文件校验\n    fs.copyToLocalFile(false, new Path(\"/xiyou/huaguoshan/sunwukong.txt\"), new Path(\"d:/sunwukong2.txt\"), true);\n    \n    // 3 关闭资源\n    fs.close();\n}\n//HDFS更名和移动\n@Test\npublic void testRename() throws IOException, InterruptedException, URISyntaxException{\n\n\t// 1 获取文件系统\n\tConfiguration configuration = new Configuration();\n\tFileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\"); \n\t\t\n\t// 2 修改文件名称\n\tfs.rename(new Path(\"/xiyou/huaguoshan/sunwukong.txt\"), new Path(\"/xiyou/huaguoshan/meihouwang.txt\"));\n\t\t\n\t// 3 关闭资源\n\tfs.close();\n}\n\n//HDFS删除文件和目录\n@Test\npublic void testDelete() throws IOException, InterruptedException, URISyntaxException{\n\n\t// 1 获取文件系统\n\tConfiguration configuration = new Configuration();\n\tFileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\");\n\t\t\n\t// 2 执行删除\n\tfs.delete(new Path(\"/xiyou\"), true);\n\t\t\n\t// 3 关闭资源\n\tfs.close();\n}\n//查看文件名称、权限、长度、块信息\n@Test\npublic void testListFiles() throws IOException, InterruptedException, URISyntaxException {\n\n\t// 1获取文件系统\n\tConfiguration configuration = new Configuration();\n\tFileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\");\n\n\t// 2 获取文件详情\n\tRemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(\"/\"), true);\n\n\twhile (listFiles.hasNext()) {\n\t\tLocatedFileStatus fileStatus = listFiles.next();\n\n\t\tSystem.out.println(\"========\" + fileStatus.getPath() + \"=========\");\n\t\tSystem.out.println(fileStatus.getPermission());\n\t\tSystem.out.println(fileStatus.getOwner());\n\t\tSystem.out.println(fileStatus.getGroup());\n\t\tSystem.out.println(fileStatus.getLen());\n\t\tSystem.out.println(fileStatus.getModificationTime());\n\t\tSystem.out.println(fileStatus.getReplication());\n\t\tSystem.out.println(fileStatus.getBlockSize());\n\t\tSystem.out.println(fileStatus.getPath().getName());\n\n\t\t// 获取块信息\n\t\tBlockLocation[] blockLocations = fileStatus.getBlockLocations();\n\t\tSystem.out.println(Arrays.toString(blockLocations));\n\t}\n\t// 3 关闭资源\n\tfs.close();\n}\n//HDFS文件和文件夹判断\n@Test\npublic void testListStatus() throws IOException, InterruptedException, URISyntaxException{\n\n    // 1 获取文件配置信息\n    Configuration configuration = new Configuration();\n    FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\");\n\n    // 2 判断是文件还是文件夹\n    FileStatus[] listStatus = fs.listStatus(new Path(\"/\"));\n\n    for (FileStatus fileStatus : listStatus) {\n\n        // 如果是文件\n        if (fileStatus.isFile()) {\n            System.out.println(\"f:\"+fileStatus.getPath().getName());\n        }else {\n            System.out.println(\"d:\"+fileStatus.getPath().getName());\n        }\n    }\n\n    // 3 关闭资源\n    fs.close();\n}\nHDFS的读写流程（面试重点）⭐⭐HDFS写数据流程\n（1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。（2）NameNode返回是否可以上传。（3）客户端请求第一个 Block上传到哪几个DataNode服务器上。（4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。（5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。（6）dn1、dn2、dn3逐级应答客户端。（7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。（8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。\n网络拓扑-节点距离计算节点距离：两个节点到达最近的共同祖先的距离总和。 \n例如，假设有数据中心 d1 机架 r1 中的节点 n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。 \nDistance(/d1/r1/n0, /d1/r1/n0)=0（同一节点上的进程）Distance(/d1/r1/n1, /d1/r1/n2)=2（同一机架上的不同节点）Distance(/d1/r2/n0, /d1/r3/n2)=4（同一数据中心不同机架上的节点）Distance(/d1/r2/n1, /d2/r4/n1)=6（不同数据中心的节点）\nHadoop3.1.3副本节点选择第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。第二个副本在另一个机架的随机一个节点第三个副本在第二个副本所在机架的随机节点\nHDFS的读数据流程\n（1）客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。（2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。（3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。（4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。\nNameNode和SecondaryNameNode思考：NameNode中的元数据是存储在哪里的？        首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，==元数据需要存放在内存中==。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在==磁盘中备份元数据的FsImage==。        这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件（只进行追加操作，效率很高）。==每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中==。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。        但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要==定期进行FsImage和Edits的合并==，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点==SecondaryNamenode，专门用于FsImage和Edits的合并==。\nNN和2NN工作机制\n1）第一阶段：NameNode启动（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。（2）客户端对元数据进行增删改的请求。（3）NameNode记录操作日志，更新滚动日志。（4）NameNode在内存中对元数据进行增删改。\n2）第二阶段：Secondary NameNode工作（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。（2）Secondary NameNode请求执行CheckPoint。（3）NameNode滚动正在写的Edits日志。（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。（6）生成新的镜像文件fsimage.chkpoint。（7）拷贝fsimage.chkpoint到NameNode。（8）NameNode将fsimage.chkpoint重新命名成fsimage。\nFsimage和Edits概念\n（1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。（2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。（3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字（4）每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。\n查看oiv 和oev 命令\n查看fsimage：hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径 \n查看edits：hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径\nDataNodeDataNode 工作机制\n（1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。（2）DataNode启动后向NameNode注册，通过后，周期性（6小时）的向NameNode上报所有的块信息。\n\nDN向NN汇报当前解读信息的时间间隔，默认6小时；\nDN扫描自己节点块信息列表的时间，默认6小时\n\n（3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。（4）集群运行中可以安全加入和退出一些机器。\nDataNode节点保证数据完整性的方法：（1）当DataNode读取Block的时候，它会计算CheckSum。（2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。（3）Client读取其他DataNode上的Block。（4）常见的校验算法crc（32），md5（128），sha1（160）（5）DataNode在其文件创建后周期验证CheckSum。\nDataNode掉线时限参数设置1、DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信2、NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。3、HDFS默认的超时时长为10分钟+30秒。4、如果定义超时时间为TimeOut，则超时时长的计算公式为：TimeOut = 2  dfs.namenode.heartbeat.recheck-interval + 10  dfs.heartbeat.interval。而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。\n4. MapReduceMapReduce概述MapReduce定义​        MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。\n​        MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。\n优缺点 优点1）MapReduce易于编程        它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。2）良好的扩展性        当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。3）高容错性        MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。4）适合PB级以上海量数据的离线处理        可以实现上千台服务器集群并发工作，提供数据处理能力。缺点1）不擅长实时计算        MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。2）不擅长流式计算        流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。3）不擅长DAG（有向无环图）计算        多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。\nMapReduce核心思想\n（1）分布式的运算程序往往需要分成至少2个阶段。（2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。（3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。（4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。总结：分析WordCount数据流走向深入理解MapReduce核心思想。\nMapReduce进程一个完整的MapReduce程序在分布式运行时有三类实例进程：（1）MrAppMaster：负责整个程序的过程调度及状态协调。（2）MapTask：负责Map阶段的整个数据处理流程。（3）ReduceTask：负责Reduce阶段的整个数据处理流程。\n常用数据序列化类型\n\n\n\nJava类型\nHadoop Writable类型\n\n\n\n\nBoolean\nBooleanWritable\n\n\nByte\nByteWritable\n\n\nInt\nIntWritable\n\n\nFloat\nFloatWritable\n\n\nLong\nLongWritable\n\n\nDouble\nDoubleWritable\n\n\nString\nText\n\n\nMap\nMapWritable\n\n\nArray\nArrayWritable\n\n\nNull\nNullWritable\n\n\n\n\nMapReduce编程规范\nMapper阶段(1)用户自定义的Mapper要继承自己的父类(2) Mapper的输入数据是KV对的形式(KV的类型可自定义)(3) Mapper中的业务逻辑写在map()方法中(4) Mapper的输 出数据是KV对的形式(KV的类型可自定义)(5) map()方 法(MapTask进程) 对每一个 调用- -次\n\nReducer阶 段(1) 用户自定义的Reducer要继承自己的父类(2) Reducer的输入数据类型对应Mapper的输出数据类型，也是KV(3) Reducer的业 务逻辑写在reduce()方法中(4) ReduceTask进程对每- -组相同k的组调用- -次reduce()方法\n\nDriver阶段相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象\n\nHadoop序列化序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 \n反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。\n为什么要序列化?        一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。为什么不用Java的序列化?        Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）。Hadoop序列化特点：（1）紧凑 ：高效使用存储空间。（2）快速：读写数据的额外开销小。（3）互操作：支持多语言的交互\n自定义bean对象实现序列化接口（Writable）（1）必须实现Writable接口（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造\npublic FlowBean() {\n\tsuper();\n}\n（3）重写序列化方法\n@Override\npublic void write(DataOutput out) throws IOException {\n\tout.writeLong(upFlow);\n\tout.writeLong(downFlow);\n\tout.writeLong(sumFlow);\n}\n（4）重写反序列化方法\n@Override\npublic void readFields(DataInput in) throws IOException {\n\tupFlow = in.readLong();\n\tdownFlow = in.readLong();\n\tsumFlow = in.readLong();\n}\n（5）注意反序列化的顺序和序列化的顺序完全一致（6）要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用。（7）如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。详见后面排序案例。\n@Override\npublic int compareTo(FlowBean o) {\n\t// 倒序排列，从大到小\n\treturn this.sumFlow &gt; o.getSumFlow() ? -1 : 1;\n}\nMapReduce框架原理\nInputFormat数据输入MapTask并行度决定机制\n数据块：Block是HDFS物理上把数据分成一块一块。数据块是HDFS存储数据单位。\n数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是MapReduce程序计算输入数据的单位，一个切片会对应启动一个MapTask。\n1)一个Job的Map阶段并行度由 客户端在提交Job时的切片数决定2)每一个Spli切片分配一个MapTask并行实例处理3)默认情况下，切大小=BlockSize4)切片时不考虑数据集整体,而是逐个针对每一个文件单独切片\nJob提交流程源码详解\nwaitForCompletion()\n\nsubmit();\n\n// 1建立连接\nconnect();\t\n// 1）创建提交Job的代理\nnew Cluster(getConfiguration());\n// 2）判断是本地运行环境还是yarn集群运行环境\ninitialize(jobTrackAddr, conf); \n\n// 2 提交job\nsubmitter.submitJobInternal(Job.this, cluster)\n\n// 1）创建给集群提交数据的Stag路径\nPath jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);\n\n// 2）获取jobid ，并创建Job路径\nJobID jobId = submitClient.getNewJobID();\n\n// 3）拷贝jar包到集群\ncopyAndConfigureFiles(job, submitJobDir);\t\nrUploader.uploadFiles(job, jobSubmitDir);\n\n// 4）计算切片，生成切片规划文件\nwriteSplits(job, submitJobDir);\nmaps = writeNewSplits(job, jobSubmitDir);\ninput.getSplits(job);\n\n// 5）向Stag路径写XML配置文件\nwriteConf(conf, submitJobFile);\nconf.writeXml(out);\n\n// 6）提交Job,返回提交状态\nstatus = submitClient.submitJob(jobId, submitJobDir.toString(),job.getCredentials());\n\nFileInputFormat切片源码解析\n(1)程序先找到你数据存储的目录。(2)开始遍历处理(规划切片)目录下的每一个文件(3)遍历第-个文件ss.txt    a)获取文件大小fs.sizeOf(ss .txt)    b)计算切片大小.        computeSplitSize(Math. max(minSize,Math .min(maxSize,blocksize)))=blocksize= l 28M    c) 默认情况下，切片大小=blocksize    d)开始切，形成第1个切片: ss.txt- - -0:128M第2个切片ss.txt- 128:256M 第3个切片ss.txt一256M:300M        (每次切片时，都要判断切完剩下的部分是否大于块的1 .1倍，不大于1 .1倍就划分-块切片)    e)将切片信息写到一个切片规划文件中    f)整个切片的核心过程在getS$plit()方 法中完成    g) InputSplit只记录 了切片的元数据信息，比如起始位置、长度以及所在的节列表等。(4)提交切片规划文件到YARN上，YARN 上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数。\nFileInputFormat切片机制\n(1)简单地按照文件的内容长度进行切片(2) 切片大小，默认等于Block大小(3)切片时不考虑数据集整体，而是逐个针对每一个文件单独切片\nFileInputFormat切片大小的参数配置\n(1)源码中计算切片大小的公式.Math. max(ninSize, Math. min( maxSize, blockS ize));mapreduce. input. fileinputformat. split. minsize=l默认值为1mapreduce. input fileinputformat. split maxsize= Long MAXValue默认值Long .MAXValue因此，默认情况下，切片大小=blocksize。(2)切片大小设置maxsize (切片最大值) :参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。minsize (切片 最小值) :参数调的比blockSize大， 则可以让切片变得比blockSize还大。(3)获取切片信息API//获取切片的文件名称String name = inputSplit. getPath() . getName() ;//根据文件类型获取切片信息Fi leSplit inputSplit = (FileSplit) context. getInputSplit () ;\nTextInputFormat​        FileInputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。\n​        TextInputFormat是默认的FileInputFormat实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text类型。\n以下是一个示例，比如，一个分片包含了如下4条文本记录。\n\n\n\n\n\n\n\n\n\nRich learning form\nIntelligent learning engine\nLearning more convenient\nFrom the real demand for more close to the enterprise\n每条记录表示为以下键/值对：\n\n\n\n\n\n\n\n\n\n(0,Rich learning form)\n(20,Intelligent learning engine)\n(49,Learning more convenient)\n(74,From the real demand for more close to the enterprise)\nCombineTextInputFormat切片机制​        框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。1）应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。2）虚拟存储切片最大值设置CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。3）切片机制生成切片过程包括：虚拟存储过程和切片过程二部分。\n存储与切片过程\n（1）虚拟存储过程：        将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。        例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。（2）切片过程：    （a）判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。    （b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。    （c）测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个        文件块，大小分别为：1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M）最终会形成3个切片，大小分        别为：（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M\nMapReduce工作流程\n\n上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下：（1）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中（2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件（3）多个溢出文件会被合并成大的溢出文件（4）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序（5）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据（6）ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）（7）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）注意：（1）Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。（2）缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb默认100M。\nShuffle机制Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。\n\nPartition分区 ing ♂\n问题引出要求将统计结果按照条件输出到不同文件中(分区) .比如:将统计结果按照手机归属地不同省份输出到不同文件中(分区)\n默认Partitioner分区\n\npublic class HashRartitioner&lt;K, V&gt; extends Partitioner&lt;K, v&gt; {\n    public int getPartition(K key, v value, int numReduceTasks) {\n    \treturn (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;\n    }\n}\n默认分区是根据key的hashCode对Reduce Iasks个数取模得到的。用户没法控制哪个key存储到哪个分区。\n\n自定义Partitioner步骤(1) 自定义类继承Partitioner, 重写getPartition()方法\n\npublic class C ustomPar titioner extends Partitioner &lt;Text，FlowBe an&gt; {\n    @Override\n    public int getPartition (Text key, FlowBean value, int numPartitions) {\n    //控制分区代码逻辑\n    .. ..\n    return partition;\n    }\n}\n​    (2)在Job驱动中，设置自定义Partitioner\njob. setP artitionerClas s(CustomPartitioner.class);\n​    (3)自定义Partition后， 要根据自定义P artitioner的逻辑设置相应数量的ReduceTask\njob. setNumReduceTasks(5);\n\n分区总结(1)如果ReduceTask的数 量&gt; getPartition的结果数,则会多产生几个空的输出文件pat-r 000xx;(2)如果1&lt;ReduceTask的数 量&lt;getP artition的结果数，则有-部分分区数据无处安放，会Exception;(3)如果ReduceTask的数 量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件pat-00000;(4)分区号必须从零开始,逐一累加。\n素例分析例如:假设自定义分区数为5，则(1) job.setNumReduceTasks(1)， 会正常运行， 只不过会产生一个输出文件(2) job.setENumReduceTasks(2), 会报错(3) job.setNumReduceTasks(6), 大于5，程序会正常运行，会产生空文件\n\nWritableComparable排序​        排序是MapReduc e框架中最重要的操作之一。​        MapTask和ReduceTask均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。​        默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。\n​        对于MapTask,它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘.上，而当数据处理完毕后，它会对磁盘.上所有文件进行归并排序。​        对于ReduceTask，它从每个M ap Task上远程拷贝相应的数据文件，如果文件大小超过一 定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到-定阈值，则进行一次归并排序以生成一个更大文件;如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统-对内存和磁盘上的所有数据进行一次归并排序。\n排序分类\n(1)部分排序MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。(2)全排序.最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。 但该方法在处理大型文件时效率极低，因为-台机器处理所有文件，完全丧失了 MapRe duce所提供的并行架构。(3)辅助排序: (GroupingCompar ator分组)在Reduce端对key进行分组。应用于:在接收的key为bean对象时，想让一个或几个字段相同(全部.字段比较不相同)的k&lt;ey进 入到同一个reduce方法时，可以采用分组排序。(4)二次排序在自定义排序过程中，如果c ompareTo中的判断条件为两个即为二次排序。\n自定义排序WritableComparable原理分析\nbean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序。\n@Override\npublic int compareTo(FlowBean bean) {\n\n\tint result;\n\t\t\n\t// 按照总流量大小，倒序排列\n\tif (this.sumFlow &gt; bean.getSumFlow()) {\n\t\tresult = -1;\n\t}else if (this.sumFlow &lt; bean.getSumFlow()) {\n\t\tresult = 1;\n\t}else {\n\t\tresult = 0;\n\t}\n\n\treturn result;\n}\nCombiner合并(1) Combiner是MR程 序中Mapper和Reducer之外的一种组件。(2) Combiner组件的父类就是Reducer。(3) Combiner和Reducer的区别在 于运行的位置      Combiner是在每一个MapTask所在的节点运行;      Reducer是接收全局所有Mapper的输出结果;(4) Combiner的意义就是对每一个 MapTask的输出进行局部汇总，以减小网络传输量。(5) Combiner能够应 用的前提是不能影响最终的业务逻辑，而且，Combiner的输 出kv应该跟Reducer的输，入Iv类型要对应起来。\n自定义Combiner实现步骤\n（a）自定义一个Combiner继承Reducer，重写Reduce方法\npublic class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n\n    private IntWritable outV = new IntWritable();\n\n    @Override\n    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {\n\n        int sum = 0;\n        for (IntWritable value : values) {\n            sum += value.get();\n        }\n     \n        outV.set(sum);\n     \n        context.write(key,outV);\n    }\n}\n（b）在Job驱动类中设置： \njob.setCombinerClass(WordCountCombiner.class);\nOutputFormat数据输出OupltFoma是Mpduce输出的基类,所有实现MapReduce输出都实现了OupuFormat接口。下面我们介绍几种常见的OutputFormat实现类。\n\nOutputFormat实现类\n默认输出格式TextOutputFormat\n自定义OutputFormat\n\n应用场景:输出数据到MySQL/HBase/Elasticsearch等存储框架中。自定义OutputFormat步骤➢自定义一个类继承FileOutputF ormat。➢改写RecordWriter， 具体改写输出数据的方法write0。\nMapReduce内核源码解析\n1）Read阶段：MapTask通过InputFormat获得的RecordReader，从输入InputSplit中解析出一个个key/value。\n2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。\n3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。\n4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。\n溢写阶段详情：\n​    步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。\n​    步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。\n​    步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。\n 5）Merge阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。\n​    当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。\n​    在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并mapreduce.task.io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。\n​    让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。\nReduceTask工作机制\n 1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。\n 2）Sort阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。\n 3）Reduce阶段：reduce()函数将计算结果写到HDFS上。\nReduceTask并行度决定机制回顾：MapTask并行度由切片个数决定，切片个数由输入文件和切片规则决定。\n思考：ReduceTask并行度由谁决定？\n1）设置ReduceTask并行度（个数）\nReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置：\n// 默认值是1，手动设置为4\njob.setNumReduceTasks(4);\n注意事项\n(1) ReduceTask=0, 表示没有Reduce阶段，输出文件个数和Map个数-致。(2) ReduceTask默认值就是1 ，所以输出文件个数为一一个。(3)如果数据分布不均匀，就有可能在R educe阶段产生数据倾斜(4) ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask.(5)具体多少个ReduceTask,需要根据集群性能而定。(6)如果分区数不是1,但是ReduceTask为1，是否执行分区过程。答案是:不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1 .肯定不执行。.\nJoin应用Reduce Join​        Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。\n     Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志）分开，最后进行合并就ok了。\nMap Join1）使用场景\nMap Join适用于一张表十分小、一张表很大的场景。\n2）优点\n思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？\n在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。\n3）具体办法：采用DistributedCache\n​    （1）在Mapper的setup阶段，将文件读取到缓存集合中。\n​    （2）在Driver驱动类中加载缓存。\n//缓存普通文件到Task运行节点。\njob.addCacheFile(new URI(“file:///e:/cache/pd.txt”));\n//如果是集群运行,需要设置HDFS路径\njob.addCacheFile(new URI(“hdfs://hadoop102:8020/cache/pd.txt”));\n数据清洗（ETL）“ETL，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL一词较常用在数据仓库，但其对象并不限于数据仓库\n在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。\nMapReduce开发总结1）输入数据接口：InputFormat（1）默认使用的实现类是：TextInputFormat（2）TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。（3）CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。\n2）逻辑处理接口：Mapper用户根据业务需求实现其中三个方法：map()   setup()   cleanup () \n3）Partitioner分区（1）有默认实现 HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces（2）如果业务上有特别的需求，可以自定义分区。\n4）Comparable排序（1）当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法。（2）部分排序：对最终输出的每一个文件进行内部排序。（3）全排序：对所有数据进行排序，通常只有一个Reduce。（4）二次排序：排序的条件有两个。\n5）Combiner合并Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果。\n6）逻辑处理接口：Reducer用户根据业务需求实现其中三个方法：reduce()   setup()   cleanup () \n7）输出数据接口：OutputFormat（1）默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对，向目标文本文件输出一行。（2）用户还可以自定义OutputFormat。\nHadoop数据压缩1）压缩的好处和坏处压缩的优点：以减少磁盘IO、减少磁盘存储空间。压缩的缺点：增加CPU开销。2）压缩原则（1）运算密集型的Job，少用压缩（2）IO密集型的Job，多用压缩\nMR支持的压缩编码\n\n\n\n\n压缩格式\nHadoop自带？\n算法\n文件扩展名\n是否可切片\n换成压缩格式后，原来的程序是否需要修改\n\n\n\n\nDEFLATE\n是，直接使用\nDEFLATE\n.deflate\n否\n和文本处理一样，不需要修改\n\n\nGzip\n是，直接使用\nDEFLATE\n.gz\n否\n和文本处理一样，不需要修改\n\n\nbzip2\n是，直接使用\nbzip2\n.bz2\n是\n和文本处理一样，不需要修改\n\n\nLZO\n否，需要安装\nLZO\n.lzo\n是\n需要建索引，还需要指定输入格式\n\n\nSnappy\n是，直接使用\nSnappy\n.snappy\n否\n和文本处理一样，不需要修改\n\n\n\n\n压缩性能的比较\n\n\n\n\n压缩算法\n原始文件大小\n压缩文件大小\n压缩速度\n解压速度\n\n\n\n\ngzip\n8.3GB\n1.8GB\n17.5MB/s\n58MB/s\n\n\nbzip2\n8.3GB\n1.1GB\n2.4MB/s\n9.5MB/s\n\n\nLZO\n8.3GB\n2.9GB\n49.3MB/s\n74.6MB/s\n\n\n\n\n压缩方式选择压缩方式选择时重点考虑：压缩/解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片。\nGzip压缩\n优点：压缩率比较高； \n缺点：不支持Split；压缩/解压速度一般；\nBzip2压缩\n优点：压缩率高；支持Split； \n缺点：压缩/解压速度慢。\nLzo压缩\n优点：压缩/解压速度比较快；支持Split；\n缺点：压缩率一般；想支持切片需要额外创建索引。\nSnappy压缩\n优点：压缩和解压缩速度快； \n缺点：不支持Split；压缩率一般； \n压缩位置选择\n压缩可以在MapReduce作用的任意阶段启用。\n\n压缩参数配置1）为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器\n\n\n\n\n压缩格式\n对应的编码/解码器\n\n\n\n\nDEFLATE\norg.apache.hadoop.io.compress.DefaultCodec\n\n\ngzip\norg.apache.hadoop.io.compress.GzipCodec\n\n\nbzip2\norg.apache.hadoop.io.compress.BZip2Codec\n\n\nLZO\ncom.hadoop.compression.lzo.LzopCodec\n\n\nSnappy\norg.apache.hadoop.io.compress.SnappyCodec\n\n\n\n\n2）要在Hadoop中启用压缩，可以配置如下参数\n\n\n\n\n参数\n默认值\n阶段\n建议\n\n\n\n\nio.compression.codecs    （在core-site.xml中配置）\n无，这个需要在命令行输入hadoop checknative查看\n输入压缩\nHadoop使用文件扩展名判断是否支持某种编解码器\n\n\nmapreduce.map.output.compress（在mapred-site.xml中配置）\nfalse\nmapper输出\n这个参数设为true启用压缩\n\n\nmapreduce.map.output.compress.codec（在mapred-site.xml中配置）\norg.apache.hadoop.io.compress.DefaultCodec\nmapper输出\n企业多使用LZO或Snappy编解码器在此阶段压缩数据\n\n\nmapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）\nfalse\nreducer输出\n这个参数设为true启用压缩\n\n\nmapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）\norg.apache.hadoop.io.compress.DefaultCodec\nreducer输出\n使用标准工具或者编解码器，如gzip和bzip2\n\n\n\n\n常见错误及解决方案1）导包容易出错。尤其Text和CombineTextInputFormat。2）Mapper中第一个输入的参数必须是LongWritable或者NullWritable，不可以是IntWritable.  报的错误是类型转换异常。3）java.lang.Exception: java.io.IOException: Illegal partition for 13926435656 (4)，说明Partition和ReduceTask个数没对上，调整ReduceTask个数。4）如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。5）在Windows环境编译的jar包导入到Linux环境中运行，\nhadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/ /user/atguigu/output\n报如下错误：\nException in thread \"main\" java.lang.UnsupportedClassVersionError: com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version 52.0\n原因是Windows环境用的jdk1.7，Linux环境用的jdk1.8。解决方案：统一jdk版本。6）缓存pd.txt小文件案例中，报找不到pd.txt文件原因：大部分为路径书写错误。还有就是要检查pd.txt.txt的问题。还有个别电脑写相对路径找不到pd.txt，可以修改为绝对路径。7）报类型转换异常。通常都是在驱动函数中设置Map输出和最终输出时编写错误。Map输出的key如果没有排序，也会报类型转换异常。8）集群中运行wc.jar时出现了无法获得输入文件。原因：WordCount案例的输入文件不能放用HDFS集群的根目录。9）出现了如下相关异常\nException in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:609)\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)\njava.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n\tat org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:356)\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:371)\n\tat org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:364)\n解决方案：拷贝hadoop.dll文件到Windows目录C:\\Windows\\System32。个别同学电脑还需要修改Hadoop源码。方案二：创建如下包名，并将NativeIO.java拷贝到该包名下\n10）自定义Outputformat时，注意在RecordWirter中的close方法必须关闭流资源。否则输出的文件内容中数据为空。\n@Override\npublic void close(TaskAttemptContext context) throws IOException, InterruptedException {\n\t\tif (atguigufos != null) {\n\t\t\tatguigufos.close();\n\t\t}\n\t\tif (otherfos != null) {\n\t\t\totherfos.close();\n\t\t}\n}\n5. Yarn​        Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。\nYarn资源调度器Yarn基础架构YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。\n\nYarn工作机制\n1）MR程序提交到客户端所在的节点。\n2）YarnRunner向ResourceManager申请一个Application。\n3）RM将该应用程序的资源路径返回给YarnRunner。\n4）该程序将运行所需资源提交到HDFS上。\n5）程序资源提交完毕后，申请运行mrAppMaster。\n6）RM将用户的请求初始化成一个Task。\n7）其中一个NodeManager领取到Task任务。\n8）该NodeManager创建容器Container，并产生MRAppmaster。\n9）Container从HDFS上拷贝资源到本地。\n10）MRAppmaster向RM 申请运行MapTask资源。\n11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。\n12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。\n13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。\n14）ReduceTask向MapTask获取相应分区的数据。\n15）程序运行完毕后，MR会向RM申请注销自己。\n作业提交全过程\n\n\n作业提交全过程详解\n（1）作业提交\n第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。\n第2步：Client向RM申请一个作业id。\n第3步：RM给Client返回该job资源的提交路径和作业id。\n第4步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。\n第5步：Client提交完资源后，向RM申请运行MrAppMaster。\n（2）作业初始化\n第6步：当RM收到Client的请求后，将该job添加到容量调度器中。\n第7步：某一个空闲的NM领取到该Job。\n第8步：该NM创建Container，并产生MRAppmaster。\n第9步：下载Client提交的资源到本地。\n（3）任务分配\n第10步：MrAppMaster向RM申请运行多个MapTask任务资源。\n第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。\n（4）任务运行\n第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。\n第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。\n第14步：ReduceTask向MapTask获取相应分区的数据。\n第15步：程序运行完毕后，MR会向RM申请注销自己。\n（5）进度和状态更新\n​        YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。\n（6）作业完成\n​        除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。\nYarn调度器和调度算法目前，Hadoop作业调度器主要有三种：FIFO、容量（Capacity Scheduler）和公平（Fair Scheduler）。Apache Hadoop3.1.3默认的资源调度器是Capacity Scheduler。\nCDH框架默认调度器是Fair Scheduler。具体设置详见：yarn-default.xml文件\n&lt;property&gt;\n    &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt;\n    &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;\n&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;\n&lt;/property&gt;\n先进先出调度器（FIFO）FIFO调度器（First In First Out）：单队列，根据提交作业的先后顺序，先来先服务。\n\n优点：简单易懂；\n缺点：不支持多队列，生产环境很少使用；\n容量调度器（Capacity Scheduler）Capacity Scheduler是Yahoo开发的多用户调度器。\n\n1、多队列:每个队列可配置-定的资源量, 每个队列采用FIF0调度策略。2、容量保证:管理员可为每个队列设置资源最低保证和资源使用上限3、灵活性:如果一个队列中的资源有剩余,可以暂时共享给那些需要资源的队列，而- -旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。4、多租户:支持多用户共享集群和多应用程序同时运行。为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。\n\n公平调度器（Fair Scheduler）Fair Schedulere是Facebook开发的多用户调度器。\n\n1)与容量调度器相同点(1)多队列:支持多队列多作业(2)容量保证:管理员可为每个队列设置资源最低保证和资源使用上线(3)灵活性:如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。(4)多租户:支持多用户共享集群和多应用程序同时运行;为了防止同一个用户的作业独占队列中的资源，该调度器:会对同一用户提交的作业所占资源量进行限定。\n2)与容量调度器不同点(1)核心调度策略不同\n容量调度器:优先选择资源利用率低的队列\n公平调度器:优先选择对资源的缺额比例大的\n(2)每个队列可以单独设置资源分配方式\n容量调度器: FIFO、DRF公平调度器: FIFO、 FAIR、k DRF\n\n公平调度器设计目标是: 在时我尔度占。所有作业获得公平的资源。某一时刻一个作业应获资源际获取资的差叫’缺额”调度器会优先为缺额大的作业分配资源\n公平调度器队列资源分配方式\n1) FIFO策略公平调度器每个队列资源分配策略如果选择FIFO的话，此时公平调度器相当于上面讲过的容量调度器。2) Fair策略  Fair策略(默认)是一-种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资  源。这意味着，如果一-个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源;如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。  具体资源分配流程和容量调度器一致;  (l)选择队列  (2)选择作业  (3)选择容器  以上三步，每一一步都是按照公平策略分配资源  ➢实际最小资源份额: mindshare = Min (资源需求量，配置的最小资源)  ➢是否饥饿: isNeedy =资源使用量&lt; mindshare (实际最小资源份额)  ➢资源分配比: minShareRatio =资源使用量/ Max (mindshare, 1 )  ➢资源使用权重比: use ToWeightRatio=资源使用量/权重\n\n\n3)DRF策略\n​        DRF ( Dominant Resource Faimness)，我们之前说的资源，都是单一标准，例如只考虑内存(也是Yarn默认的情况)。但是很多时候我们资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。​        那么在YARN中，我们用DRF来决定如何调度:​        假设集群- :共有100 CPU和10T内存，而应用A需要(2 CPU, 300GB)，应用B需要(6 CPU，100GB) 。则两个应用分别需要A (2%CPU, 3%内存)和B (6%CPU, 1%内存)的资源，这就意味着A是内存主导的，B是CPU主导的，针对这种情况，我们可以选择DRF策略对不同应用进行不同资源(CPU和内存)的一个不同比例的限制。\nYarn常用命令yarn application查看任务Yarn状态的查询，除了可以在hadoop103:8088页面查看外，还可以通过命令操作。常见的命令操作如下所示：\n需求：执行WordCount案例，并用Yarn命令查看任务运行情况。\n[atguigu@hadoop102 hadoop-3.1.3]$ myhadoop.sh start\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output\nyarn application查看任务\nyarn application -list\n根据Application状态过滤：\nyarn application -list -appStates （所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）\nKill掉Application：\nyarn application -kill application_1612577921195_0001\nyarn logs查看日志查询Application日志：yarn logs -applicationId \nyarn logs -applicationId application_1612577921195_0001\n查询Container日志：yarn logs -applicationId  -containerId \nyarn logs -applicationId application_1612577921195_0001 -containerId container_1612577921195_0001_01_000001\nyarn applicationattempt查看尝试运行的任务列出所有Application尝试的列表：yarn applicationattempt -list \nyarn applicationattempt -list application_1612577921195_0001\n打印ApplicationAttemp状态：yarn applicationattempt -status \nyarn applicationattempt -status appattempt_1612577921195_0001_000001\nyarn container查看容器列出所有Container：yarn container -list \nyarn container -list appattempt_1612577921195_0001_000001\n打印Container状态： yarn container -status \nyarn container -status container_1612577921195_0001_01_000001\nyarn node查看节点状态列出所有节点：yarn node -list -all\nyarn rmadmin更新配置加载队列配置：yarn rmadmin -refreshQueues\nyarn queue查看队列打印队列信息：yarn queue -status \nYarn生产环境核心参数\n向Hive队列提交任务1）hadoop jar的方式\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output\n注: -D表示运行时改变参数值\n2）打jar包的方式\n默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：\npublic class WcDrvier {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        Configuration conf = new Configuration();\n\n        conf.set(\"mapreduce.job.queuename\",\"hive\");\n\n        //1. 获取一个Job实例\n        Job job = Job.getInstance(conf);\n\n        。。。 。。。\n\n        //6. 提交Job\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\n这样，这个任务在集群提交时，就会提交到hive队列：\n任务优先级容量调度器，支持任务优先级的配置，在资源紧张时，优先级高的任务将优先获取资源。默认情况，Yarn将所有任务的优先级限制为0，若想使用任务的优先级功能，须开放该限制。\n1）修改yarn-site.xml文件，增加以下参数\n&lt;property&gt;\n  &lt;name&gt;yarn.cluster.max-application-priority&lt;/name&gt;\n  &lt;value&gt;5&lt;/value&gt;\n&lt;/property&gt;\n2）分发配置，并重启Yarn\n[atguigu@hadoop102 hadoop]$ xsync yarn-site.xml\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh\n3）模拟资源紧张环境，可连续提交以下任务，直到新提交的任务申请不到资源为止。\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 2000000\n4）再次重新提交优先级高的任务\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi  -D mapreduce.job.priority=5 5 2000000\n5）也可以通过以下命令修改正在执行的任务的优先级。\nyarn application -appID  -updatePriority 优先级\n[atguigu@hadoop102 hadoop-3.1.3]$ yarn application -appID application_1611133087930_0009 -updatePriority 5\n","slug":"B2-Hadoop","date":"2021-11-12T11:13:22.000Z","categories_index":"大数据","tags_index":"Hadoop","author_index":"YFR718"},{"id":"9a9e7d03fd63b8b9dd7648918fd31ad5","title":"反射、新特性","content":"1. Java反射机制1.1 Java反射机制概述1.2 理解Class类并获取Class实例1.3 类的加载与ClassLoader的理解1.4 创建运行时类的对象1.5 获取运行时类的完整结构1.6 调用运行时类的指定结构1.7 反射的应用：动态代理2. Java8的其它新特性2.1 Lambda表达式2.2 函数式(Functional)接口2.3 方法引用与构造器引用2.4 强大的Stream API2.5 Optional类Java9&amp;Java10&amp; Java11新特性3.1 Java 9 的新特性3.2 Java 10 的新特性3.3 Java 11 的新特性","slug":"J6-反射、新特性","date":"2021-11-12T02:31:21.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"8bbd211a4ba9676d95abee1aaae0937f","title":"IO、网络编程","content":"1. IO流1.1 File类的使用1.2 IO流原理及流的分类1.3 节点流(或文件流)1.4 缓冲流1.5 转换流1.6 标准输入、输出流1.7 打印流1.8 数据流1.9 对象流1.10 随机存取文件流1.11 NIO.2中Path、Paths、 Files类的使用2. 网络编程2.1 网络编程概述2.2 网络通信要素概述2.3 通信要素1：IP和端口号2.4 通信要素2：网络协议2.5 TCP网络编程2.6 UDP网络编程2.7 URL编程","slug":"J5-IO、网络编程","date":"2021-11-12T02:31:03.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"a3bf914b929f60f3127d07c8144366a0","title":"集合、泛型","content":"1. Java集合1.1 Java集合框架概述1.2 Collection接口方法\n1.3 Iterator迭代器接口\n1.4 Collection子接口一：List\n1.5 Collection子接口二：Set\n1.6 Map接口\n1.7 Collections工具类\n2. 泛型2.1 为什么要有泛型\n2.2 在集合中使用泛型\n2.3 自定义泛型结构\n2.4 泛型在继承上的体现\n2.5 通配符的使用\n2.6 泛型应用举例\n","slug":"J4-集合、泛型","date":"2021-11-12T02:30:35.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"d0396f84e2a9cf009e1f2a35f0f836dd","title":"常用类、枚举、注解","content":"1. Java常用类1.1 字符串相关的类StringString的特性\n\nString类：代表字符串。Java 程序中的所有字符串字面值（如 “abc” ）都作为此类的实例实现。\nString是一个final类，代表不可变的字符序列。\n字符串是常量，用双引号引起来表示。它们的值在创建之后不能更改。\nString对象的字符内容是存储在一个字符数组value[]中的。\n\nString对象的创建\nString str = \"hello\";\n\n//本质上this.value = new char[0];\nString s1 = new String(); \n\n//this.value = original.value;\nString s2 = new String(String original); \n\n//this.value = Arrays.copyOf(value, value.length);\nString s3 = new String(char[] a); \nString s4 = new String(char[] a,int startIndex,int count);\n字符串对象是如何存储的结论：\n\n常量与常量的拼接结果在常量池。且常量池中不会存在相同内容的常量。\n只要其中有一个是变量，结果就在堆中\n如果拼接的结果调用intern()方法，返回值就在常量池中\n\nString使用陷阱\n\nString s1 = “a”;说明：在字符串常量池中创建了一个字面量为”a”的字符串。\ns1 = s1 + “b”;说明：实际上原来的“a”字符串对象已经丢弃了，现在在堆空间中产生了一个字符串s1+”b”（也就是”ab”)。如果多次执行这些改变串内容的操作，会导致大量副本字符串对象存留在内存中，降低效率。如果这样的操作放到循环中，会极大影响程序的性能。\nString s2 = “ab”;说明：直接在字符串常量池中创建一个字面量为”ab”的字符串。\nString s3 = “a” + “b”;说明：s3指向字符串常量池中已经创建的”ab”的字符串。String s4 = s1.intern();说明：堆空间的s1对象在调用intern()之后，会将常量池中已经存在的”ab”字符串赋值给s4。\n\nString常用方法\nint length()：返回字符串的长度： return value.length\nchar charAt(int index)： 返回某索引处的字符return value[index]\nboolean isEmpty()：判断是否是空字符串：return value.length == 0\nString toLowerCase()：使用默认语言环境，将 String 中的所有字符转换为小写\nString toUpperCase()：使用默认语言环境，将 String 中的所有字符转换为大写\nString trim()：返回字符串的副本，忽略前导空白和尾部空白\nboolean equals(Object obj)：比较字符串的内容是否相同\nboolean equalsIgnoreCase(String anotherString)：与equals方法类似，忽略大小写\nString concat(String str)：将指定字符串连接到此字符串的结尾。 等价于用“+”\nint compareTo(String anotherString)：比较两个字符串的大小\nString substring(int beginIndex)：返回一个新的字符串，它是此字符串的从beginIndex开始截取到最后的一个子字符串。\nString substring(int beginIndex, int endIndex) ：返回一个新字符串，它是此字符串从beginIndex开始截取到endIndex(不包含)的一个子字符串。\nboolean endsWith(String suffix)：测试此字符串是否以指定的后缀结束\nboolean startsWith(String prefix)：测试此字符串是否以指定的前缀开始\nboolean startsWith(String prefix, int toffset)：测试此字符串从指定索引开始的子字符串是否以指定前缀开始\nboolean contains(CharSequence s)：当且仅当此字符串包含指定的 char 值序列时，返回 true\nint indexOf(String str)：返回指定子字符串在此字符串中第一次出现处的索引\nint indexOf(String str, int fromIndex)：返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始\nint lastIndexOf(String str)：返回指定子字符串在此字符串中最右边出现处的索引\nint lastIndexOf(String str, int fromIndex)：返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索\nString replace(char oldChar, char newChar)：返回一个新的字符串，它是通过用newChar 替换此字符串中出现的所有oldChar 得到的。\nString replace(CharSequence target, CharSequence replacement)：使用指定的字面值替换序列替换此字符串所有匹配字面值目标序列的子字符串。\nString replaceAll(String regex, String replacement) ： 使用 给 定 的replacement 替换此字符串所有匹配给定的正则表达式的子字符串。\nString replaceFirst(String regex, String replacement) ：使用给定的replacement 替换此字符串匹配给定的正则表达式的第一个子字符串。\nboolean matches(String regex)：告知此字符串是否匹配给定的正则表达式。\nString[] split(String regex)：根据给定正则表达式的匹配拆分此字符串。\nString[] split(String regex, int limit)：根据匹配给定的正则表达式来拆分此字符串，最多不超过limit个，如果超过了，剩下的全部都放到最后一个元素中。\n\nString与字符数组转换\n字符数组-&gt;字符串    String 类的构造器：String(char[]) 和 String(char[]，int offset，int length) 分别用字符数组中的全部字符和部分字符创建字符串对象。\n\n字符串-&gt;字符数组    public char[] toCharArray()：将字符串中的全部字符存放在一个字符数组中的方法。    public void getChars(int srcBegin, int srcEnd, char[] dst,int dstBegin)：提供了将指定索引范围内的字符串存放到数组中的方法。\n\n字节数组-&gt;字符串    String(byte[])：通过使用平台的默认字符集解码指定的 byte 数组，构造一个新的 String。    String(byte[]，int offset，int length) ：用指定的字节数组的一部分，即从数组起始位置offset开始取length个字节构造一个字符串对象。\n\n字符串-&gt;字节数组    public byte[] getBytes() ：使用平台的默认字符集将此 String 编码为byte 序列，并将结果存储到一个新的 byte 数组中。    public byte[] getBytes(String charsetName) ：使用指定的字符集将此 String 编码到 byte 序列，并将结果存储到新的 byte 数组。\n\n\nStringBuffer类\njava.lang.StringBuffer代表可变的字符序列，JDK1.0中声明，可以对字符串内容进行增删，此时不会产生新的对象。\n很多方法与String相同。\n作为参数传递时，方法内部可以改变值。\n\nStringBuffer类不同于String，其对象必须使用构造器生成。有三个构造器：    StringBuffer()：初始容量为16的字符串缓冲区    StringBuffer(int size)：构造指定容量的字符串缓冲区    StringBuffer(String str)：将内容初始化为指定字符串内容\nStringBuffer类的常用方法\nStringBuffer append(xxx)：提供了很多的append()方法，用于进行字符串拼接\nStringBuffer delete(int start,int end)：删除指定位置的内容\nStringBuffer replace(int start, int end, String str)：把[start,end)位置替换为str \nStringBuffer insert(int offset, xxx)：在指定位置插入xxx\nStringBuffer reverse() ：把当前字符序列逆转\npublic int indexOf(String str)\npublic String substring(int start,int end) public int length()\npublic char charAt(int n )\npublic void setCharAt(int n ,char ch)\n\nStringBuilder类StringBuilder 和 StringBuffer 非常类似，均代表可变的字符序列，而且提供相关功能的方法也一样面试题：对比String、StringBuffer、StringBuilder⭐⭐\n    String(JDK1.0)：不可变字符序列    StringBuffer(JDK1.0)：可变字符序列、效率低、线程安全    StringBuilder(JDK 5.0)：可变字符序列、效率高、线程不安全\n注意：作为参数传递的话，方法内部String不会改变其值，StringBuffer和StringBuilder会改变其值。\n1.2 JDK 8之前的日期时间API1. java.lang.System类System类提供的public static long currentTimeMillis()用来返回当前时间与1970年1月1日0时0分0秒之间以毫秒为单位的时间差。\n此方法适于计算时间差。\n\n计算世界时间的主要标准有：    UTC(Coordinated Universal Time)    GMT(Greenwich Mean Time)    CST(Central Standard Time)\n\n2. java.util.Date类表示特定的瞬间，精确到毫秒\n\n构造器：    Date()：使用无参构造器创建的对象可以获取本地当前时间。    Date(long date)\n常用方法    getTime():返回自 1970 年 1 月 1 日 00:00:00 GMT 以来此 Date 对象表示的毫秒数。    toString():把此 Date 对象转换为以下形式的 String： dow mon dd hh:mm:ss zzz yyyy 其中： dow 是一周中的某一天 (Sun, Mon, Tue, Wed, Thu, Fri, Sat)，zzz是时间标准。    其它很多方法都过时了。\n\n3. java.text.SimpleDateFormat类\nDate类的API不易于国际化，大部分被废弃了，java.text.SimpleDateFormat类是一个不与语言环境有关的方式来格式化和解析日期的具体类。\n\n它允许进行格式化：日期文本、解析：文本日期\n\n格式化\n    SimpleDateFormat() ：默认的模式和语言环境创建对象    public SimpleDateFormat(String pattern)：该构造方法可以用参数pattern指定的格式创建一个对象，该对象调用：    public String format(Date date)：方法格式化时间对象date\n\n解析：public Date parse(String source)：从给定字符串的开始解析文本，以生成一个日期。\n\n\n\n4. java.util.Calendar(日历)类\nCalendar是一个抽象基类，主用用于完成日期字段之间相互操作的功能。\n获取Calendar实例的方法    使用Calendar.getInstance()方法    调用它的子类GregorianCalendar的构造器。\n一个Calendar的实例是系统时间的抽象表示，通过get(int field)方法来取得想要的时间信息。比如YEAR、MONTH、DAY_OF_WEEK、HOUR_OF_DAY 、 MINUTE、SECOND    public void set(int field,int value)    public void add(int field,int amount)    public final Date getTime()    public final void setTime(Date date)\n注意:    获取月份时：一月是0，二月是1，以此类推，12月是11    获取星期时：周日是1，周二是2 ， 。。。。周六是7\n\n1.3 JDK 8中新日期时间API新时间日期API\n    第三次引入的API是成功的，并且Java 8中引入的java.time API 已经纠正了过去的缺陷，将来很长一段时间内它都会为我们服务。    Java 8 吸收了 Joda-Time 的精华，以一个新的开始为 Java 创建优秀的API。新的 java.time 中包含了所有关于本地日期（LocalDate）、本地时间（LocalTime）、本地日期时间（LocalDateTime）、时区（ZonedDateTime）和持续时间（Duration）的类。历史悠久的 Date 类新增了 toInstant() 方法，用于把 Date 转换成新的表示形式。这些新增的本地化时间日期API 大大简化了日期时间和本地化的管理。\n\njava.time – 包含值对象的基础包\njava.time.chrono – 提供对不同的日历系统的访问\njava.time.format – 格式化和解析时间和日期\njava.time.temporal – 包括底层框架和扩展特性\njava.time.zone – 包含时区支持的类\n\n    LocalDate、LocalTime、LocalDateTime 类是其中较重要的几个类，它们的实例是不可变的对象，分别表示使用 ISO-8601日历系统的日期、时间、日期和时间。它们提供了简单的本地日期或时间，并不包含当前的时间信息，也不包含与时区相关的信息。    LocalDate代表IOS格式（yyyy-MM-dd）的日期,可以存储 生日、纪念日等日期。    LocalTime表示一个时间，而不是日期。    LocalDateTime是用来表示日期和时间的，这是一个最常用的类之一。\n\n\n\n\n方法\n描述\n\n\n\n\nnow() / *  now(ZoneId zone)\n静态方法，根据当前时间创建对象/指定时区的对象\n\n\nof()\n静态方法，根据指定日期/时间创建对象\n\n\ngetDayOfMonth()/getDayOfYear()\n获得月份天数(1-31) /获得年份天数(1-366)\n\n\ngetDayOfWeek()\n获得星期几(返回一个 DayOfWeek 枚举值)\n\n\ngetMonth()\n获得月份, 返回一个 Month 枚举值\n\n\ngetMonthValue() / getYear()\n获得月份(1-12) /获得年份\n\n\ngetHour()/getMinute()/getSecond()\n获得当前对象对应的小时、分钟、秒\n\n\nwithDayOfMonth()/withDayOfYear()/withMonth()/withYear()\n将月份天数、年份天数、月份、年份修改为指定的值并返回新的对象\n\n\nplusDays(), plusWeeks(), plusMonths(), plusYears(),plusHours()\n向当前对象添加几天、几周、几个月、几年、几小时\n\n\nminusMonths() / minusWeeks()/minusDays()/minusYears()/minusHours()\n从当前对象减去几月、几周、几天、几年、几小时\n\n\n\n\n瞬时：Instant    Instant：时间线上的一个瞬时点。 这可能被用来记录应用程序中的事件时间戳。    在处理时间和日期的时候，我们通常会想到年,月,日,时,分,秒。然而，这只是时间的一个模型，是面向人类的。第二种通用模型是面向机器的，或者说是连续的。在此模型中，时间线中的一个点表示为一个很大的数，这有利于计算机处理。在UNIX中，这个数从1970年开始，以秒为的单位；同样的，在Java中，也是从1970年开始，但以毫秒为单位。    java.time包通过值类型Instant提供机器视图，不提供处理人类意义上的时间单位。Instant表示时间线上的一点，而不需要任何上下文信息，例如，时区。概念上讲，它只是简单的表示自1970年1月1日0时0分0秒（UTC）开始的秒数。因为java.time包是基于纳秒计算的，所以Instant的精度可以达到纳秒级。    (1 ns = 10-9 s)   1秒 = 1000毫秒 =10^6微秒=10^9纳秒\n\n\n\n\n方法\n描述\n\n\n\n\nnow()\n静态方法，返回默认UTC时区的Instant类的对象\n\n\nofEpochMilli(long epochMilli)\n静态方法，返回在1970-01-01 00:00:00基础上加上指定毫秒数之后的Instant类的对象\n\n\natOffset(ZoneOffset offset)\n结合即时的偏移来创建一个OffsetDateTime\n\n\ntoEpochMilli()\n返回1970-01-01 00:00:00到当前时间的毫秒数，即为时间戳\n\n\n\n\n时间戳是指格林威治时间1970年01月01日00时00分00秒(北京时间1970年01月01日08时00分00秒)起至现在的总秒数。\n格式化与解析日期或时间java.time.format.DateTimeFormatter 类：该类提供了三种格式化方法：\n    预定义的标准格式。如：ISO_LOCAL_DATE_TIME;ISO_LOCAL_DATE;ISO_LOCAL_TIME    本地化相关的格式。如：ofLocalizedDateTime(FormatStyle.LONG)    自定义的格式。如：ofPattern(“yyyy-MM-dd hh:mm:ss”)\n\n\n\n\n方 法\n描 述\n\n\n\n\nofPattern(String pattern)\n静态方法 ， 返 回 一 个 指 定 字 符 串 格 式 的DateTimeFormatter\n\n\nformat(TemporalAccessor t)\n格式化一个日期、时间，返回字符串\n\n\nparse(CharSequence text)\n将指定格式的字符序列解析为一个日期、时间\n\n\n\n\n其它API\nZoneId：该类中包含了所有的时区信息，一个时区的ID，如 Europe/Paris\nZonedDateTime：一个在ISO-8601日历系统时区的日期时间，如 2007-12-03T10:15:30+01:00 Europe/Paris。\n其中每个时区都对应着ID，地区ID都为“{区域}/{城市}”的格式，例如：Asia/Shanghai等\nClock：使用时区提供对当前即时、日期和时间的访问的时钟。\n持续时间：Duration，用于计算两个“时间”间隔\n日期间隔：Period，用于计算两个“日期”间隔\nTemporalAdjuster : 时间校正器。有时我们可能需要获取例如：将日期调整到“下一个工作日”等操作。\nTemporalAdjusters : 该类通过静态方法(firstDayOfXxx()/lastDayOfXxx()/nextXxx())提供了大量的常用TemporalAdjuster 的实现。\n\n与传统日期处理的转换\n\n\n\n类\nTo 遗留类\nFrom 遗留类\n\n\n\n\njava.time.Instant与java.util.Date\nDate.from(instant)\ndate.toInstant()\n\n\njava.time.Instant与java.sql.Timestamp\nTimestamp.from(instant)\ntimestamp.toInstant()\n\n\njava.time.ZonedDateTime与java.util.GregorianCalendar\nGregorianCalendar.from(zonedDateTime)\ncal.toZonedDateTime()\n\n\njava.time.LocalDate与java.sql.Time\nDate.valueOf(localDate)\ndate.toLocalDate()\n\n\njava.time.LocalTime与java.sql.Time\nDate.valueOf(localDate)\ndate.toLocalTime()\n\n\njava.time.LocalDateTime与java.sql.Timestamp\nTimestamp.valueOf(localDateTime)\ntimestamp.toLocalDateTime()\n\n\njava.time.ZoneId与java.util.TimeZone\nTimezone.getTimeZone(id)\ntimeZone.toZoneId()\n\n\njava.time.format.DateTimeFormatter与java.text.DateFormat\nformatter.toFormat()\n无\n\n\n\n\n1.4 Java比较器在Java中经常会涉及到对象数组的排序问题，那么就涉及到对象之间的比较问题。Java实现对象排序的方式有两种：\n\n自然排序：java.lang.Comparable\n定制排序：java.util.Comparator\n\n自然排序：java.lang.Comparable    Comparable接口强行对实现它的每个类的对象进行整体排序。这种排序被称为类的自然排序。    实现 Comparable 的类必须实现 compareTo(Object obj) 方法，两个对象即通过 compareTo(Object obj) 方法的返回值来比较大小。如果当前对象this大于形参对象obj，则返回正整数，如果当前对象this小于形参对象obj，则返回负整数，如果当前对象this等于形参对象obj，则返回零。    实现Comparable接口的对象列表（和数组）可以通过 Collections.sort 或Arrays.sort进行自动排序。实现此接口的对象可以用作有序映射中的键或有序集合中的元素，无需指定比较器。    对于类 C 的每一个 e1 和 e2 来说，当且仅当 e1.compareTo(e2) == 0 与e1.equals(e2) 具有相同的 boolean 值时，类 C 的自然排序才叫做与 equals一致。建议（虽然不是必需的）最好使自然排序与 equals 一致。\nComparable 的典型实现：(默认都是从小到大排列的)    String：按照字符串中字符的Unicode值进行比较    Character：按照字符的Unicode值来进行比较    数值类型对应的包装类以及BigInteger、BigDecimal：按照它们对应的数值大小进行比较    Boolean：true 对应的包装类实例大于 false 对应的包装类实例    Date、Time等：后面的日期时间比前面的日期时间大\n定制排序：java.util.Comparator    当元素的类型没有实现java.lang.Comparable接口而又不方便修改代码，或者实现了java.lang.Comparable接口的排序规则不适合当前的操作，那么可以考虑使用 Comparator 的对象来排序，强行对多个对象进行整体排序的比较。    重写compare(Object o1,Object o2)方法，比较o1和o2的大小：如果方法返回正整数，则表示o1大于o2；如果返回0，表示相等；返回负整数，表示o1小于o2。    可以将 Comparator 传递给 sort 方法（如 Collections.sort 或Arrays.sort），从而允许在排序顺序上实现精确控制。    还可以使用 Comparator 来控制某些数据结构（如有序 set或有序映射）的顺序，或者为那些没有自然顺序的对象 collection 提供排序。\n1.5 System类    System类代表系统，系统级的很多属性和控制方法都放置在该类的内部。该类位于java.lang包。    由于该类的构造器是private的，所以无法创建该类的对象，也就是无法实例化该类。其内部的成员变量和成员方法都是static的，所以也可以很方便的进行调用。成员变量    System类内部包含in、out和err三个成员变量，分别代表标准输入流(键盘输入)，标准输出流(显示器)和标准错误输出流(显示器)。成员方法    native long currentTimeMillis()：该方法的作用是返回当前的计算机时间，时间的表达格式为当前计算机时间和GMT时间(格林威治时间)1970年1月1号0时0分0秒所差的毫秒数。    void exit(int status)：该方法的作用是退出程序。其中status的值为0代表正常退出，非零代表异常退出。使用该方法可以在图形界面编程中实现程序的退出功能等。\n\nvoid gc()：该方法的作用是请求系统进行垃圾回收。至于系统是否立刻回收，则取决于系统中垃圾回收算法的实现以及系统执行时的情况。\nString getProperty(String key)：该方法的作用是获得系统中属性名为key的属性对应的值。系统中常见的属性名以及属性的作用如下表所示：\n\n1.6 Math类ava.lang.Math提供了一系列静态方法用于科学计算。其方法的参数和返回值类型一般为double型。\n\n\n\n\n\n\n\n\n\n\nabs\n绝对值\n\n\nacos,asin,atan,cos,sin,tan\n三角函数\n\n\nsqrt\n平方根\n\n\npow(double a,doble b)\na的b次幂\n\n\nlog\n自然对数\n\n\nexp\ne为底指数\n\n\nmax(double a,double b)\n\n\n\nmin(double a,double b)\n\n\n\nrandom()\n返回0.0到1.0的随机数\n\n\nlong round(double a)\ndouble型数据a转换为long型（四舍五入）\n\n\ntoDegrees(double angrad)\n弧度—&gt;角度\n\n\ntoRadians(double angdeg)\n角度—&gt;弧度\n\n\n\n\n1.7 BigInteger与BigDecimalBigInteger类    Integer类作为int的包装类，能存储的最大整型值为231-1，Long类也是有限的，最大为263-1。如果要表示再大的整数，不管是基本数据类型还是他们的包装类都无能为力，更不用说进行运算了。    java.math包的BigInteger可以表示不可变的任意精度的整数。BigInteger 提供所有 Java 的基本整数操作符的对应物，并提供 java.lang.Math 的所有相关方法。另外，BigInteger 还提供以下运算：模算术、GCD 计算、质数测试、素数生成、位操作以及一些其他操作。    构造器：BigInteger(String val)：根据字符串构建BigInteger对象\n常用方法\n\npublic BigInteger abs()：返回此 BigInteger 的绝对值的 BigInteger。\nBigInteger add(BigInteger val) ：返回其值为 (this + val) 的 BigInteger\nBigInteger subtract(BigInteger val) ：返回其值为 (this - val) 的 BigInteger\nBigInteger multiply(BigInteger val) ：返回其值为 (this * val) 的 BigInteger\nBigInteger divide(BigInteger val) ：返回其值为 (this / val) 的 BigInteger。整数相除只保留整数部分。\nBigInteger remainder(BigInteger val) ：返回其值为 (this % val) 的 BigInteger。\nBigInteger[] divideAndRemainder(BigInteger val)：返回包含 (this / val) 后跟(this % val) 的两个 BigInteger 的数组。\nBigInteger pow(int exponent) ：返回其值为 (thisexponent) 的 BigInteger。\n\nBigDecimal类​        一般的Float类和Double类可以用来做科学计算或工程计算，但在商业计算中，要求数字精度比较高，故用到java.math.BigDecimal类。​        BigDecimal类支持不可变的、任意精度的有符号十进制定点数。构造器    public BigDecimal(double val)    public BigDecimal(String val)常用方法    public BigDecimal add(BigDecimal augend)    public BigDecimal subtract(BigDecimal subtrahend)    public BigDecimal multiply(BigDecimal multiplicand)    public BigDecimal divide(BigDecimal divisor, int scale, int roundingMode)\n2. 枚举类与注解枚举类的实现    JDK1.5之前需要自定义枚举类    JDK 1.5 新增的 enum 关键字用于定义枚举类\n\n若枚举只有一个对象, 则可以作为一种单例模式的实现方式。\n枚举类的属性    枚举类对象的属性不应允许被改动, 所以应该使用 private final 修饰    枚举类的使用 private final 修饰的属性应该在构造器中为其赋值    若枚举类显式的定义了带参数的构造器, 则在列出枚举值时也必须对应的传入参数\n\n2.1 枚举类的使用自定义枚举类\n私有化类的构造器，保证不能在类的外部创建其对象\n在类的内部创建枚举类的实例。声明为：public static final\n对象如果有实例变量，应该声明为private final，并在构造器中初始化\n\n使用关键字enum定义枚举类使用说明    使用 enum 定义的枚举类默认继承了 java.lang.Enum类，因此不能再继承其他类    枚举类的构造器只能使用 private 权限修饰符    枚举类的所有实例必须在枚举类中显式列出(, 分隔 ; 结尾)。列出的实例系统会自动添加 public static final 修饰    必须在枚举类的第一行声明枚举类对象\n\nJDK 1.5 中可以在 switch 表达式中使用Enum定义的枚举类的对象作为表达式, case 子句可以直接使用枚举值的名字, 无需添加枚举类作为限定。\n\nEnum类的主要方法    values()方法：返回枚举类型的对象数组。该方法可以很方便地遍历所有的枚举值。    valueOf(String str)：可以把一个字符串转为对应的枚举类对象。要求字符串必须是枚举类对象的“名字”。如不是，会有运行时异常：IllegalArgumentException。    toString()：返回当前枚举类对象常量的名称\n\n实现接口的枚举类\n和普通 Java 类一样，枚举类可以实现一个或多个接口\n若每个枚举值在调用实现的接口方法呈现相同的行为方式，则只要统一实现该方法即可。\n若需要每个枚举值在调用实现的接口方法呈现出不同的行为方式,则可以让每个枚举值分别来实现该方法\n\n2.2 注解的使用注解(Annotation)概述    从 JDK 5.0 开始, Java 增加了对元数据(MetaData) 的支持, 也就是Annotation(注解)    Annotation 其实就是代码里的特殊标记, 这些标记可以在编译, 类加载, 运行时被读取, 并执行相应的处理。通过使用Annotation, 程序员可以在不改变原有逻辑的情况下, 在源文件中嵌入一些补充信息。代码分析工具、开发工具和部署工具可以通过这些补充信息进行验证或者进行部署。    Annotation 可以像修饰符一样被使用, 可用于修饰包,类, 构造器, 方法, 成员变量, 参数, 局部变量的声明, 这些信息被保存在Annotation的 “name=value” 对中。\n    在JavaSE中，注解的使用目的比较简单，例如标记过时的功能，忽略警告等。在JavaEE/Android中注解占据了更重要的角色，例如用来配置应用程序的任何切面，代替JavaEE旧版中所遗留的繁冗代码和XML配置等。    未来的开发模式都是基于注解的，JPA是基于注解的，Spring2.5以上都是基于注解的，Hibernate3.x以后也是基于注解的，现在的Struts2有一部分也是基于注解的了，注解是一种趋势，一定程度上可以说：框架 = 注解 + 反射 + 设计模式。\n常见的Annotation示例    使用Annotation 时要在其前面增加 @ 符号, 并把该Annotation 当成一个修饰符使用。用于修饰它支持的程序元素    示例一：生成文档相关的注解\n@author 标明开发该类模块的作者，多个作者之间使用,分割\n@version 标明该类模块的版本\n@see 参考转向，也就是相关主题\n@since 从哪个版本开始增加的\n@param 对方法中某参数的说明，如果没有参数就不能写\n@return 对方法返回值的说明，如果方法的返回值类型是void就不能写\n@exception 对方法可能抛出的异常进行说明，如果方法没有用throws显式抛出的异常就不能写其中\n\n@param @return 和@exception 这三个标记都是只用于方法的。 @param的格式要求：@param 形参名形参类型 形参说明\n@return 的格式要求：@return 返回值类型返回值说明\n@exception的格式要求：@exception 异常类型异常说明\n@param和@exception可以并列多个\n\n示例二：在编译时进行格式检查(JDK内置的三个基本注解)    @Override: 限定重写父类方法, 该注解只能用于方法    @Deprecated: 用于表示所修饰的元素(类, 方法等)已过时。通常是因为所修饰的结构危险或存在更好的选择    @SuppressWarnings: 抑制编译器警告\n自定义Annotation\n定义新的Annotation 类型使用@interface 关键字\n自定义注解自动继承了java.lang.annotation.Annotation接口\nAnnotation 的成员变量在 Annotation 定义中以无参数方法的形式来声明。其方法名和返回值定义了该成员的名字和类型。我们称为配置参数。类型只能是八种基本数据类型、String类型、Class类型、enum类型、Annotation类型、以上所有类型的数组。\n可以在定义 Annotation 的成员变量时为其指定初始值, 指定成员变量的初始值可使用default 关键字\n如果只有一个参数成员，建议使用参数名为value\n如果定义的注解含有配置参数，那么使用时必须指定参数值，除非它有默认值。格式是“参数名 = 参数值”，如果只有一个参数成员，且名称为value，可以省略“value=”\n没有成员定义的 Annotation 称为标记; 包含成员变量的 Annotation 称为元数据Annotation\n\nJDK中的元注解JDK 的元Annotation 用于修饰其他Annotation 定义\nJDK5.0提供了4个标准的meta-annotation类型，分别是：    Retention    Target    Documented    Inherited\n\n@Retention: 只能用于修饰一个Annotation 定义, 用于指定该Annotation 的生命周期, @Rentention 包含一个 RetentionPolicy 类型的成员变量, 使用@Rentention 时必须为该 value 成员变量指定值:    RetentionPolicy.SOURCE:在源文件中有效（即源文件保留），编译器直接丢弃这种策略的注释    RetentionPolicy.CLASS:在class文件中有效（即class保留） ， 当运行 Java 程序时, JVM不会保留注解。 这是默认值    RetentionPolicy.RUNTIME:在运行时有效（即运行时保留），当运行 Java 程序时, JVM 会保留注释。程序可以通过反射获取该注释。\n\n@Target: 用于修饰Annotation 定义, 用于指定被修饰的Annotation 能用于修饰哪些程序元素。 @Target 也包含一个名为 value 的成员变量。\n\n\n@Documented: 用于指定被该元Annotation 修饰的Annotation 类将被javadoc 工具提取成文档。默认情况下，javadoc是不包括注解的。    定义为Documented的注解必须设置Retention值为RUNTIME。\n\n@Inherited: 被它修饰的Annotation 将具有继承性。如果某个类使用了被@Inherited 修饰的Annotation, 则其子类将自动具有该注解。    比如：如果把标有@Inherited注解的自定义的注解标注在类级别上，子类则可以继承父类类级别的注解    实际应用中，使用较少\n\n\n利用反射获取注解信息（在反射部分涉及）\nJDK 5.0 在 java.lang.reflect 包下新增了 AnnotatedElement 接口, 该接口代表程序中可以接受注解的程序元素\n当一个Annotation 类型被定义为运行时Annotation 后, 该注解才是运行时可见, 当 class 文件被载入时保存在 class 文件中的Annotation 才会被虚拟机读取\n程序可以调用AnnotatedElement对象的如下方法来访问Annotation 信息\n\nJDK 8中注解的新特性Java 8对注解处理提供了两点改进：可重复的注解及可用于类型的注解。此外，反射也得到了加强，在Java8中能够得到方法参数的名称。这会简化标注在方法参数上的注解。\n类型注解：\n\nJDK1.8之后，关于元注解@Target的参数类型ElementType枚举值多了两个：TYPE_PARAMETER,TYPE_USE。\n在Java 8之前，注解只能是在声明的地方所使用，Java8开始，注解可以应用在任何地方。    ElementType.TYPE_PARAMETER 表示该注解能写在类型变量的声明语句中（如：泛型声明）。    ElementType.TYPE_USE 表示该注解能写在使用类型的任何语句中。 \n\n","slug":"J3-常用类、枚举、注解","date":"2021-11-12T02:30:10.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"8dfa68efe4979ccc3a00434835ec3a9a","title":"异常与多线程","content":"1. 异常处理1.1 异常概述与异常体系结构异常：在Java语言中，将程序执行中发生的不正常情况\n异常事件可分为两类：\n\nError：Java虚拟机无法解决的严重问题。如：JVM系统内部错误、资源耗尽等严重情况。比如：StackOverflowError和OOM。一般不编写针对性的代码进行处理。\nException: 其它因编程错误或偶然的外在因素导致的一般性问题，可以使用针对性的代码进行处理。例如：    空指针访问    试图读取不存在的文件    网络连接中断    数组角标越界\n\n错误的处理方法\n\n遇到错误就终止程序的运行。\n由程序员在编写程序时，就考虑到错误的检测、错误消息的提示，以及错误的处理。\n\n\n捕获错误最理想的是在编译期间，但有的错误只有在运行时才会发生。比如：除数为0，数组下标越界等\n分类：编译时异常和运行时异常\n\n运行时异常    是指编译器不要求强制处置的异常。一般是指编程时的逻辑错误，是程序员应该积极避免其出现的异常。java.lang.RuntimeException类及它的子类都是运行时异常。    对于这类异常，可以不作处理，因为这类异常很普遍，若全处理可能会对程序的可读性和运行效率产生影响。\n编译时异常    是指编译器要求必须处置的异常。即程序在运行时由于外界因素造成的一般性异常。编译器要求Java程序必须捕获或声明所有编译时异常。    对于这类异常，如果程序不处理，可能会带来意想不到的结果。\n1.2 常见异常\n1.3 异常处理机制一：try-catch-finallyJava程序的执行过程中如出现异常，会生成一个异常类对象，该异常对象将被提交给Java运行时系统，这个过程称为抛出(throw)异常。\n异常对象的生成\n\n由虚拟机自动生成：程序运行过程中，虚拟机检测到程序发生了问题，如果在当前代码中没有找到相应的处理程序，就会在后台自动创建一个对应异常类的实例对象并抛出——自动抛出\n由开发人员手动创建：Exception exception = new ClassCastException();——创建好的异常对象不抛出对程序没有任何影响，和创建一个普通对象一样\n\n异常抛出流程\n\n如果一个方法内抛出异常，该异常对象会被抛给调用者方法中处理。如果异常没有在调用者方法中处理，它继续被抛给这个调用方法的上层方法。这个过程将一直继续下去，直到异常被处理。这一过程称为捕获(catch)异常。\n如果一个异常回到main()方法，并且main()也不处理，则程序运行终止。\n程序员通常只能处理Exception，而对Error无能为力。\n\n异常处理是通过try-catch-finally语句实现的。\ntry{\n//可能产生异常的代码\n}\ncatch( ExceptionName1 e ){\n//当产生ExceptionName1型异常时的处置措施\n}\ncatch( ExceptionName2 e ){\n//当产生ExceptionName2型异常时的处置措施\n}\n[ finally{\n.. //无论是否发生异常，都无条件执行的语句\n}]\n\ntry捕获异常的第一步是用try{…}语句块选定捕获异常的范围，将可能出现异常的代码放在try语句块中。catch (Exceptiontype e)在catch语句块中是对异常对象进行处理的代码。每个try语句块可以伴随一个或多个catch语句，用于处理可能产生的不同类型的异常对象。\n\n如果明确知道产生的是何种异常，可以用该异常类作为catch的参数；也可以用其父类作为catch的参数。\n\n捕获异常的有关信息：\ngetMessage() 获取异常信息，返回字符串printStackTrace()  获取异常类名和异常信息，以及异常出现在程序中的位置。返回值void。\nfinally\n\n捕获异常的最后一步是通过finally语句为异常处理提供一个统一的出口，使得在控制流转到程序的其它部分以前，能够对程序的状态作统一的管理。\n不论在try代码块中是否发生了异常事件，catch语句是否执行，catch语句是否有异常，catch语句中是否有return， finally块中的语句都会被执行。\nfinally语句和catch语句是任选的\n\n1.4 异常处理机制二：throws​        如果一个方法(中的语句执行时)可能生成某种异常，但是并不能确定如何处理这种异常，则此方法应显示地声明抛出异常，表明该方法将不对这些异常进行处理，而由该方法的调用者负责处理。在方法声明中用throws语句可以声明抛出异常的列表，throws后面的异常类型可以是方法中产生的异常类型，也可以是它的父类。\npublic void readFile(String file) throws FileNotFoundException {\n    //读文件的操作可能产生FileNotFoundException类型的异常\n    FilelnputStream fis = new FilelnputStream(file);\n}\n重写方法声明抛出异常的原则\n重写方法不能抛出比被重写方法范围更大的异常类型。在多态的情况下，对methodA()方法的调用-异常的捕获按父类声明的异常处理。\n1.5 手动抛出异常：throw​        Java异常类对象除在程序执行过程中出现异常时由系统自动生成并抛出，也可根据需要使用人工创建并抛出。首先要生成异常类对象，然后通过throw语句实现抛出操作(提交给Java运行环境)。\nIOException e = new IOException(); throw e;\n可以抛出的异常必须是Throwable或其子类的实例。下面的语句在编译时将会产生语法错误：\nthrow new String(\"want to throw\");\n1.6 用户自定义异常类\n一般地，用户自定义异常类都是RuntimeException的子类。\n自定义异常类通常需要编写几个重载的构造器。\n自定义异常需要提供serialVersionUID\n自定义的异常通过throw抛出。\n自定义异常最重要的是异常类的名字，当异常出现时，可以根据名字判断异常类型。\n\n2 多线程2.1 基本概念：程序、进程、线程程序(program)是为完成特定任务、用某种语言编写的一组指令的集合。即指一段静态的代码，静态对象。进程(process)是程序的一次执行过程，或是正在运行的一个程序。是一个动态的过程：有它自身的产生、存在和消亡的过程。——生命周期    如：运行中的QQ，运行中的MP3播放器    程序是静态的，进程是动态的    进程作为资源分配的单位，系统在运行时会为每个进程分配不同的内存区域线程(thread)，进程可进一步细化为线程，是一个程序内部的一条执行路径。    若一个进程同一时间并行执行多个线程，就是支持多线程的    线程作为调度和执行的单位，每个线程拥有独立的运行栈和程序计数器(pc)，线程切换的开销小    一个进程中的多个线程共享相同的内存单元/内存地址空间它们从同一堆中分配对象，可以访问相同的变量和对象。这就使得线程间通信更简便、高效。但多个线程操作共享的系统资源可能就会带来安全的隐患。\n并行与并发    并行：多个CPU同时执行多个任务。比如：多个人同时做不同的事。    并发：一个CPU(采用时间片)同时执行多个任务。比如：秒杀、多个人做同一件事。\n多线程程序的优点：\n\n提高应用程序的响应。对图形化界面更有意义，可增强用户体验。\n提高计算机系统CPU的利用率\n改善程序结构。将既长又复杂的进程分为多个线程，独立运行，利于理解和修改\n\n2.2 线程的创建和使用⭐⭐Thread类的特性    每个线程都是通过某个特定Thread对象的run()方法来完成操作的，经常把run()方法的主体称为线程体    通过该Thread对象的start()方法来启动这个线程，而非直接调用run()\nThread类构造器    Thread()：创建新的Thread对象    Thread(String threadname)：创建线程并指定线程实例名    Thread(Runnable target)：指定创建线程的目标对象，它实现了Runnable接口中的run方法    Thread(Runnable target, String name)：创建新的Thread对象\nAPI中创建线程的两种方式方式一：继承Thread类\n1)    定义子类继承Thread类。2)    子类中重写Thread类中的run方法。3)    创建Thread子类对象，即创建了线程对象。4)    调用线程对象start方法：启动线程，调用run方法。\n注意点：\n\n如果自己手动调用run()方法，那么就只是普通方法，没有启动多线程模式。\nrun()方法由JVM调用，什么时候调用，执行的过程控制都有操作系统的CPU调度决定。\n想要启动多线程，必须调用start方法。\n一个线程对象只能调用一次start()方法启动，如果重复调用了，则将抛出以上的异常“IllegalThreadStateException”。\n\n方式二：实现Runnable接口\n1)    定义子类，实现Runnable接口。2)    子类中重写Runnable接口中的run方法。3)    通过Thread类含参构造器创建线程对象。4)    将Runnable接口的子类对象作为实际参数传递给Thread类的构造器中。5)    调用Thread类的start方法：开启线程，调用Runnable子类接口的run方法。\n继承方式和实现方式的联系与区别\n区别    继承Thread：线程代码存放Thread子类run方法中。    实现Runnable：线程代码存在接口的子类的run方法。实现方式的好处    避免了单继承的局限性    多个线程可以共享同一个接口实现类的对象，非常适合多个相同线程来处理同一份资源。\nThread类的有关方法\nvoid start():  启动线程，并执行对象的run()方法\nrun():  线程在被调度时执行的操作\nString getName():  返回线程的名称\nvoid setName(String name):设置该线程名称\nstatic Thread currentThread(): 返回当前线程。在Thread子类中就是this，通常用于主线程和Runnable实类\nstatic  void  yield()：线程让步    暂停当前正在执行的线程，把执行机会让给优先级相同或更高的线程    若队列中没有同优先级的线程，忽略此方法\njoin() ：当某个程序执行流中调用其他线程的 join() 方法时，调用线程将被阻塞，直到 join() 方法加入的 join 线程执行完为止    低优先级的线程也可以获得执行\nstatic  void  sleep(long millis)：(指定时间:毫秒)    令当前活动线程在指定时间段内放弃对CPU控制,使其他线程有机会被执行,时间到后重排队。    抛出InterruptedException异常\nstop(): 强制线程生命期结束，不推荐使用\nboolean isAlive()：返回boolean，判断线程是否还活着\n\n线程的调度Java的调度方法    同优先级线程组成先进先出队列（先到先服务），使用时间片策略    对高优先级，使用优先调度的抢占式策略\n线程的优先级线程的优先级等级    MAX_PRIORITY：10    MIN _PRIORITY：1    NORM_PRIORITY：5涉及的方法    getPriority() ：返回线程优先值    setPriority(int newPriority) ：改变线程的优先级说明    线程创建时继承父线程的优先级    低优先级只是获得调度的概率低，并非一定是在高优先级线程之后才被调用\n线程的分类Java中的线程分为两类：一种是守护线程，一种是用户线程。\n\n它们在几乎每个方面都是相同的，唯一的区别是判断JVM何时离开。\n守护线程是用来服务用户线程的，通过在start()方法前调用thread.setDaemon(true)可以把一个用户线程变成一个守护线程。\nJava垃圾回收就是一个典型的守护线程。\n若JVM中都是守护线程，当前JVM将退出。\n形象理解：兔死狗烹，鸟尽弓藏\n\n2.3 线程的生命周期JDK中用Thread.State类定义了线程的几种状态\n新建： 当一个Thread类或其子类的对象被声明并创建时，新生的线程对象处于新建状态就绪：处于新建状态的线程被start()后，将进入线程队列等待CPU时间片，此时它已具备了运行的条件，只是没分配到CPU资源运行：当就绪的线程被调度并获得CPU资源时,便进入运行状态， run()方法定义了线程的操作和功能阻塞：在某种特殊情况下，被人为挂起或执行输入输出操作时，让出 CPU 并临时中止自己的执行，进入阻塞状态死亡：线程完成了它的全部工作或线程被提前强制性地中止或出现异常导致结束\n\n2.4 线程的同步问题的提出    多个线程执行的不确定性引起执行结果的不稳定    多个线程对账本的共享，会造成操作的不完整性，会破坏数据。\nSynchronized的使用方法对多条操作共享数据的语句，只能让一个线程都执行完，在执行过程中，其他线程不可以参与执行。\n\n同步代码块：\n\nsynchronized (对象){\n// 需要被同步的代码；\n}\n\nsynchronized还可以放在方法声明中，表示整个方法为同步方法。例如：\n\npublic synchronized void show (String name){ \n...\n}\n同步机制中的锁当资源被一个任务使用时，在其上加锁。第一个访问某项资源的任务必须锁定这项资源，使其他任务在其被解锁之前，就无法访问它了，而在其被解锁之时，另一个任务就可以锁定并使用它了。\nsynchronized的锁\n\n任意对象都可以作为同步锁。所有对象都自动含有单一的锁（监视器）。\n同步方法的锁：静态方法（类名.class）、非静态方法（this）\n同步代码块：自己指定，很多时候也是指定为this或类名.class\n必须确保使用同一个资源的多个线程共用一把锁，这个非常重要，否则就无法保证共享资源的安全\n一个线程类中的所有静态方法共用同一把锁（类名.class），所有非静态方法共用同一把锁（this），同步代码块（指定需谨慎）\n\n同步的范围1、如何找问题，即代码是否存在线程安全？（非常重要）\n（1）    明确哪些代码是多线程运行的代码（2）    明确多个线程是否有共享数据（3）    明确多线程运行代码中是否有多条语句操作共享数据\n2、如何解决呢？（非常重要）对多条操作共享数据的语句，只能让一个线程都执行完，在执行过程中，其他线程不可以参与执行。即所有操作共享数据的这些语句都要放在同步范围中3、切记：\n    范围太小：没锁住所有有安全问题的代码    范围太大：没发挥多线程的功能。\n释放锁的操作    当前线程的同步方法、同步代码块执行结束。    当前线程在同步代码块、同步方法中遇到break、return终止了该代码块、该方法的继续执行。    当前线程在同步代码块、同步方法中出现了未处理的Error或Exception，导致异常结束。    当前线程在同步代码块、同步方法中执行了线程对象的wait()方法，当前线程暂停，并释放锁。\n不会释放锁的操作\n线程执行同步代码块或同步方法时，程序调用Thread.sleep()、 Thread.yield()方法暂停当前线程的执行\n线程执行同步代码块时，其他线程调用了该线程的suspend()方法将该线程挂起，该线程不会释放锁（同步监视器）。    应尽量避免使用suspend()和resume()来控制线程\n\n线程的死锁问题死锁:不同的线程分别占用对方需要的同步资源不放弃，都在等待对方放弃自己需要的同步资源，就形成了线程的死锁,出现死锁后，不会出现异常，不会出现提示，只是所有的线程都处于阻塞状态，无法继续。解决方法    专门的算法、原则    尽量减少同步资源的定义    尽量避免嵌套同步\nLock(锁)    从JDK 5.0开始，Java提供了更强大的线程同步机制——通过显式定义同步锁对象来实现同步。同步锁使用Lock对象充当。    java.util.concurrent.locks.Lock接口是控制多个线程对共享资源进行访问的工具。锁提供了对共享资源的独占访问，每次只能有一个线程对Lock对象加锁，线程开始访问共享资源之前应先获得Lock对象。    ReentrantLock 类实现了 Lock ，它拥有与 synchronized 相同的并发性和内存语义，在实现线程安全的控制中，比较常用的是ReentrantLock，可以显式加锁、释放锁。\nclass A{\n    private final ReentrantLock lock = new ReenTrantLock();\n    public void m(){\n        lock.lock();\n        try{\n        \t11保证线程安全的代码;\n        }\n        finally{\n        \tlock.unlock();\n        }\n    }\n}\n//注意：如果同步代码有异常，要将unlock()写入finally语句块\nsynchronized 与 Lock 的对比\nLock是显式锁（手动开启和关闭锁，别忘记关闭锁），synchronized是隐式锁，出了作用域自动释放\nLock只有代码块锁，synchronized有代码块锁和方法锁\n使用Lock锁，JVM将花费较少的时间来调度线程，性能更好。并且具有更好的扩展性（提供更多的子类）\n\n2.5 线程的通信\nwait()：令当前线程挂起并放弃CPU、同步资源并等待，使别的线程可访问并修改共享资源，而当前线程排队等候其他线程调用notify()或notifyAll()方法唤醒，唤醒后等待重新获得对监视器的所有权后才能继续执行。\nnotify()：唤醒正在排队等待同步资源的线程中优先级最高者结束等待\nnotifyAll ()：唤醒正在排队等待资源的所有线程结束等待.\n\n    这三个方法只有在synchronized方法或synchronized代码块中才能使用，否则会报java.lang.IllegalMonitorStateException异常。因为这三个方法必须有锁对象调用，而任意对象都可以作为synchronized的同步锁，因此这三个方法只能在Object类中声明。\nwait() 方法    在当前线程中调用方法： 对象名.wait()    使当前线程进入等待（某对象）状态 ，直到另一线程对该对象发出 notify(或notifyAll) 为止。    调用方法的必要条件：当前线程必须具有对该对象的监控权（加锁）    调用此方法后，当前线程将释放对象监控权 ，然后进入等待    在当前线程被notify后，要重新获得监控权，然后从断点处继续代码的执行。\nnotify()/notifyAll()    在当前线程中调用方法： 对象名.notify()    功能：唤醒等待该对象监控权的一个/所有线程。    调用方法的必要条件：当前线程必须具有对该对象的监控权（加锁）\n2.6 JDK5.0新增线程创建方式新增方式一：实现Callable接口与使用Runnable相比， Callable功能更强大些    相比run()方法，可以有返回值    方法可以抛出异常    支持泛型的返回值    需要借助FutureTask类，比如获取返回结果\nFuture接口    可以对具体Runnable、Callable任务的执行结果进行取消、查询是否完成、获取结果等。    FutrueTask是Futrue接口的唯一的实现类    FutureTask 同时实现了Runnable, Future接口。它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值\n新增方式二：使用线程池背景：经常创建和销毁、使用量特别大的资源，比如并发情况下的线程，对性能影响很大。思路：提前创建好多个线程，放入线程池中，使用时直接获取，使用完放回池中。可以避免频繁创建销毁、实现重复利用。类似生活中的公共交通工具。好处：    提高响应速度（减少了创建新线程的时间）    降低资源消耗（重复利用线程池中线程，不需要每次都创建）    便于线程管理            corePoolSize：核心池的大小            maximumPoolSize：最大线程数            keepAliveTime：线程没有任务时最多保持多长时间后会终止\n线程池相关APIJDK 5.0起提供了线程池相关API：ExecutorService 和 ExecutorsExecutorService：真正的线程池接口。常见子类ThreadPoolExecutor    void execute(Runnable command) ：执行任务/命令，没有返回值，一般用来执行Runnable     Future submit(Callable task)：执行任务，有返回值，一般又来执行Callable    void shutdown() ：关闭连接池Executors：工具类、线程池的工厂类，用于创建并返回不同类型的线程池    Executors.newCachedThreadPool()：创建一个可根据需要创建新线程的线程池    Executors.newFixedThreadPool(n); 创建一个可重用固定线程数的线程池    Executors.newSingleThreadExecutor() ：创建一个只有一个线程的线程池    Executors.newScheduledThreadPool(n)：创建一个线程池，它可安排在给定延迟后运行命令或者定期地执行。\n","slug":"J2-异常与多线程","date":"2021-11-12T02:28:44.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"3c25c67d0781653798d002eeefd584b0","title":"环境与包管理","content":"1. Pip对 Python 包的查找、下载、安装、卸载的功能。\n1.1 pip常用命令pip的更新\npython -m pip install --upgrade pip\npip安装包\npip install 包名==版本号\npip更新包\npip install --upgrade 要更新的包名\npip删除包\npip uninstall 要卸载的包名\n1.2 pip镜像配置临时使用pip安装包的时候加参数-i 镜像源地址\npip install bs4 -i https://pypi.tuna.tsinghua.edu.cn/simple\n清华：https://pypi.tuna.tsinghua.edu.cn/simple阿里云：http://mirrors.aliyun.com/pypi/simple/中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/华中理工大学：http://pypi.hustunique.com/山东理工大学：http://pypi.sdutlinux.org/豆瓣：http://pypi.douban.com/simple/\n永久更改Linux：修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹)\n[global] \nindex-url = https://pypi.tuna.tsinghua.edu.cn/simple\n[install]\ntrusted-host=mirrors.aliyun.com\nwindows直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，然后新建文件pip.ini，即 %HOMEPATH%\\pip\\pip.ini，在pip.ini文件中输入以下内容（以豆瓣镜像为例）：\n[global]\nindex-url = http://pypi.douban.com/simple\n[install]\ntrusted-host = pypi.douban.com\n1.3 pip依赖迁移导出依赖\npip freeze &gt;  requirements.txt\n安装依赖\npip install -r requirements.txt\n2. Anacondaconda is a tool for managing and deploying applications, environments and packages.\n2.1 下载安装官网下载：Anaconda | The World’s Most Popular Data Science Platform\n下一步、下一步点点点，安装完成\n2.2 conda环境管理conda clean ：净化Anaconda\n# 删除索引缓存，锁定文件，未使用的缓存包，和包。\nconda clean -a\nconda compare：比较conda环境之间的包。\nconda config：配置环境的配置信息。\nconda create：创建新的conda环境。⭐⭐\n# 创建环境：conda create -n 环境名称 [python]\nconda create -n env_name python=3.8\n\n# 用 environment.yml 配置文件创建环境\nconda env create -f nvironment.yml\n\n# 导出environment.yml环境文件\nconda env_name export &gt; environment.yml \n\n\n\n\n\n\n\n\n\n坑爹：\n装新环境必须删除原镜像配置\nconda info：显示有关当前conda安装的信息。\nconda activate env_name：激活环境。⭐⭐\nconda deactivate env_name：停用环境。\nconda env remove —n env_name：删除环境\n构建相同的conda环境（不同机器间的环境复制）\n# 激活需要导出配置文件的环境\nconda list --explicit &gt; files.txt\n# 在同系统的不同机器执行\nconda create --name env_name -f files.txt\n克隆环境（同一台机器的环境复制\nconda create --name clone_env_name --clone env_name\nconda env list：查看环境列表\n2.3 conda包管理conda search [包名]：查看特定包\nconda install 包名[=版本号]：安装指定包\nconda update 包名：更新包\nconda update python：更新python\nconda list：查看当前环境的包\nconda remove 包名：删除包\n2.4 conda镜像修改conda config —remove-key channels恢复默认镜像\nconda config —add 镜像地址\n# 清华大学镜像\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge \nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/\nconda config —show：在channel里查看配置的镜像地址\n3. Jupyter notebook3.1 设置代码提示conda activate base\n\npip install jupyter_contrib_nbextensions\n\njupyter contrib nbextension install --user\n\npip install jupyter_nbextensions_configurator\n\njupyter nbextensions_configurator enable --user\n\n\n3.2 jupyter中添加conda环境查看JupyterNotebook的kernel及存放位置\nipython kernelspec list\n添加kernel\n# 首先安装ipykernel：\nconda install ipykernel\n\n# 在虚拟环境下创建kernel文件：\nconda install -n 环境名称 ipykernel\n\n# 激活conda环境，将环境写入notebook的kernel中\n\npython -m ipykernel install --user --name 环境名称 --display-name \"Python (环境名称)\"\n\n# 打开notebook服务器：jupyter notebook\n\n# 删除kernel环境：\n\njupyter kernelspec remove 环境名称\n","slug":"P3-Anaconda与jupyter notebook","date":"2021-11-11T10:25:42.000Z","categories_index":"Python","tags_index":"","author_index":"YFR718"},{"id":"4a207b7e885fdbcca8129599135e5f2e","title":"MySQL基础","content":"1 MySQL概述1.1 数据库概述数据库的好处持久化数据到本地可以实现结构化查询，方便管理\n数据库相关概念DB：数据库，保存一组有组织的数据的容器DBMS：数据库管理系统，又称为数据库软件（产品），用于管理DB中的数据SQL:结构化查询语言，用于和DBMS通信的语言\n数据库存储数据的特点\n将数据放到表中，表再放到库中\n一个数据库中可以有多个表，每个表都有一个的名字，用来标识自己。表名具有唯一性。\n表具有一些特性，这些特性定义了数据在表中如何存储，类似java中 “类”的设计。\n表由列组成，我们也称为字段。所有表都是由一个或多个列组成的，每一列类似java 中的”属性”\n表中的数据是按行存储的，每一行类似于java中的“对象”。\n\n1.2 MySQL的背景前身属于瑞典的一家公司，MySQL AB\n08年被sun公司收购\n09年sun被oracle收购\nMySQL优点\n1、开源、免费、成本低\n2、性能高、移植性也好\n3、体积小，便于安装\n1.3 MySQL产品的介绍和安装MySQL服务的启动和停止方式一：计算机——右击管理——服务方式二：通过管理员身份运行\nnet start 服务名（启动服务）\nnet stop 服务名（停止服务）\nMySQL服务的登录和退出  方式一：通过mysql自带的客户端（只限于root用户）\n  方式二：通过windows自带的客户端\n#登录：\nmysql 【-h主机名 -P端口号 】-u用户名 -p密码\n#退出：\nexit或ctrl+C\nMySQL的语法规范不区分大小写,但建议关键字大写，表名、列名小写\n每条命令最好用分号结尾\n每条命令根据需要，可以进行缩进 或换行\n注释\n    单行注释：#注释文字\n    单行注释：-- 注释文字\n    多行注释：/* 注释文字  */\n2. 基本操作存储过程：创建数据库=&gt;确认字段=&gt;创建数据表=&gt;插入数据\n创建数据库# 展示数据库\nmysql&gt; SHOW DATABASES;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| sakila             |\n| sys                |\n| world              |\n+--------------------+\n# 打开指定的库\nmysql&gt; use 库名\n# 创建数据库\nmysql&gt; CREATE DATABASE demo;\n# 删除数据库\nmysql&gt; DROP DATABASE demo;\n# 查看服务器的版本\n#方式一：登录到mysql服务端\nmysql&gt; select version();\n#方式二：没有登录到mysql服务端\nmysql --version\n或\nmysql --V\n●”information_schema”是MySQL系统自带的数据库,主要保存MySQL数据库服务器的系统信息，比如数据库的名称、数据表的名称、字段名称、存取权限、数据文件所在的文件夹和系统使用的文件夹，等等。●”performance_ schema”是MySQL系统自带的数据库，可以用来监控MySQL的各类性能指标。●“sys” 数据库是MySQL系统自带的数据库，主要作用是，以一种更容易被理解的方式展示MySQL数据库服务器的各类性能指标，帮助系统管理员和开发人员监控MySQL的技术性能。●”mysql”数据库保存了MySQL数据库服务器运行时需要的系统信息，比如数据文件夹、当前使用的字符集、约束检查信息，等等。\n确认字段MySQL数据表由行与列组成，一行就是一条数据记录， 每一条数据记录都被分成许多列，一列就叫一个字段。每个字段都需要定义数据类型，这个数据类型叫做字段类型。\n创建数据表#展示数据表\nmysql&gt; SHOW TABLES;\n+----------------+\n| Tables_in_demo |\n+----------------+\n| test           |\n+----------------+\n# 查看其它库的所有表\nmysql&gt; show tables from 库名;\n#创建数据表\nmysql&gt; CREATE TABLE demo.test(barcode text,goodname text,price int);\n# 查看表结构\nmysql&gt; desc 表名;\nmysql&gt; show columns from actor;\nmysql&gt; DESCRIBE demo.test;\n+----------+---------+------+-----+---------+-------+\n| Field    | Type    | Null | Key | Default | Extra |\n+----------+---------+------+-----+---------+-------+\n| barcode  | text    | YES  |     | NULL    |       |\n| goodname | text    | YES  |     | NULL    |       |\n| price    | int(11) | YES  |     | NULL    |       |\n+----------+---------+------+-----+---------+-------+\n●Field: 表示字段名称。●Type: 表示字段类型。●Null: 表示这个字段是否允许是空值(NULL) 。在MySQL里面,空值不等于空字符串。一个空字符串的长度是0,而一个空值的长度是空。而且，在MySQL里面，空值是占用空间的。●Key:我们暂时把它叫做键。●Default: 表示默认值。我们导入的表的所有的字段都允许是空，默认值都是NULL。●Extra:表示附加信息。\n#其他show语句\n❑ SHOW STATUS，用于显示广泛的服务器状态信息；\n❑ SHOW CREATE DATABASE和SHOW CREATE TABLE，分别用来显示创建特定数据库或表的MySQL语句；\n❑ SHOW GRANTS，用来显示授予用户（所有用户或特定用户）的安全权限；\n❑ SHOW ERRORS 和 SHOW WARNINGS，用来显示服务器错误或警告消息。\n插入数据#添加主键\nALTER TABLE demo. test\nADD COLUMN itemnumber int PRIMARY KEY AUTO_INCREMENT ;\n#向表中添加数据\nINSERT INTO demo.test\n(barcode, goodsname,price)\nVALUES ('0001','本' ,3);\n\nSQL的语言分类DQL（Data Query Language）：数据查询语言\n    select \nDML(Data Manipulate Language):数据操作语言\n    insert 、update、delete\nDDL（Data Define Languge）：数据定义语言\n    create、drop、alter\nTCL（Transaction Control Language）：事务控制语言\n    commit、rollback\n3. DQL语言数据查询语言：select \n/*\n类似于Java中 :System.out.println(要打印的东西);\n特点：\n①通过select查询完的结果 ，是一个虚拟的表格，不是真实存在\n② 要查询的东西 可以是常量值、可以是表达式、可以是字段、可以是函数*/\n\n#检索单个列\nmysql&gt; SELECT 列名 FROM 表名;\n#检索多个列\nmysql&gt; SELECT 列名,列名... FROM 表名;\n#检索所有列\nmysql&gt; SELECT * FROM 表名;\n#使用完全限定的名字\nmysql&gt; SELECT 表名.列名 FROM 数据库.表名;\n#拼接计算字段\nmysql&gt; SELECT CONCAT(列名，列名) FROM 表名;\n#计算字段运算\nmysql&gt; SELECT 列名*5 FROM 表名;\n\n\n\n\n子句\n功能\n示例\n\n\n\n\nDISTINCT\n去重\nSELECT DISTINCT 列名 FROM 表名;\n\n\nLIMIT\n限制行数\nSELECT 列名 FROM 表名 LIMIT 10;SELECT 列名 FROM 表名 LIMIT 10,3; #从行10开始取3行\n\n\nORDER BY\n排序\nSELECT 列名 FROM 表名 ORDER BY 列名 LIMIT 10;   //先排序再取10SELECT 列名 FROM 表名 ORDER BY 列名,列名..;     //按多个列排序\n\n\nDESC\n降序排\nSELECT 列名 FROM 表名 ORDER BY 列名 DESC;\n\n\nASC\n升序排\n与不加相同\n\n\nWHERE\n数据过滤\nSELECT 列名 FROM 表名 WHERE 条件;\n\n\nGROUP BY\n分组数据\nSELECT 列名 FROM 表名 GROUP BY 列名;\n\n\nHAVING\n过滤基于分组值\nSELECT 列名 FROM 表名 GROUP BY 列名 HAVING COUNT(*)&gt;=2;;\n\n\nUNION (ALL)\n组合查询\n执行多条SELECT语句，并将结果作为单个查询结果集返回\n\n\n\n\nSELECT子句顺序\n\n\n\n子句\n说明\n是否必须使用\n\n\n\n\nSELECT\n要返回的列\n是\n\n\nFROM\n要检索的表\n仅在表中选择数据\n\n\nWHERE\n行过滤\n否\n\n\nGROUP BY\n分组说明\n仅在按组计算聚集时\n\n\nHAVING\n组过滤\n否\n\n\nORDER BY\n排序\n否\n\n\nLIMIT\n要检索的行数\n否\n\n\n\n\nWHERE子句操作符\n\n\n\n操作符\n说明\n\n\n\n\n=\n等于\n\n\n&lt;&gt;或!=\n不等于\n\n\n&lt;,&lt;=,&gt;,&gt;=\n小于大于\n\n\nBETWEEN A AND B\n[A,B]\n\n\nIS NULL\n空值检查\n\n\nAND OR NOT\n与 或 非\n\n\nIN (A,B)\n集合范围匹配\n\n\n()\n优先运算括号里的\n\n\nLIKE\n通配符\n\n\n\n\n优先级：AND&gt;OR\n通配符LIKE用法\n\n\n\n通配符\n含义\n示例\n\n\n\n\n%\n任何字符出现任意次数\nWHERE 列名 LIKE ‘yfr%’：\n\n\n_\n只匹配单个字符\nWHERE 列名 LIKE ‘yfr_’：\n\n\n\n\n正则表达式\n\n\n\n关键字\n含义\n示例\n\n\n\n\nREGEXP\n包含\nwhere 列名 REGEXP ‘400’;\n\n\nBINARY\n不区分大小匹配\nwhere 列名 REGEXP BINARY  ‘AS’;\n\n\n\\\n\nOR匹配\nwhere 列名 REGEXP ‘400\\\n500’;\n\n\n[123]\n匹配几个字符之一\nwhere 列名 REGEXP ‘[123]400’;\n\n\n123\n匹配几个字符之外所有\nwhere 列名 REGEXP ‘[\\^123]400’;\n\n\n[0-9]\n范围匹配\nwhere 列名 REGEXP ‘[0-9]400’;\n\n\n\\\\\n匹配特殊字符．、[]、\n和-where 列名 REGEXP ‘\\400’;\n\n\n字符类\n匹配字符类\n[ :alpha: ] [:blank:]等等\n\n\n{n,m}\n匹配数目范围\nwhere 列名 REGEXP ‘[0-9]{1,}’;匹配一次以上\n\n\n[[:&lt;:]]、[[:&gt;:]]、$、^\n定位符：词的开始/尾，文本的开始/尾\nwhere 列名 REGEXP ‘^[0-9]’;\n\n\n\n\n函数文本处理函数\n\n\n\n函数\n\n\n\n\n\nConcat()\n拼接\n\n\nUpper()\n转换成大写\n\n\nLower()\n转换成小写\n\n\nTrim()\n去前后指定的空格和字符\n\n\nLeft()\n返回串左边的字符\n\n\nLength()\n返回串的长度\n\n\nLocate()\n找出串的一个子串\n\n\nLower()\n将串转换为小写\n\n\nLTrim()\n去掉串左边的空格\n\n\nRight()\n返回串右边的字符\n\n\nRTrim()\n去掉串右边的空格\n\n\nSoundex()\n返回串的SOUNDEX值\n\n\nSubString()\n返回子串的字符\n\n\nUpper()\n将串转换为大写\n\n\nReplace()\n替换\n\n\nLpad()\n左填充\n\n\nRpad()\n右填充\n\n\nInstr()\n返回子串第一次出现的索引\n\n\n\n\n数值和日期时间处理函数\n\n\n\n数值函数\n日期函数\n\n\n\n\nAbs()\nAddData()\n\n\nCos()\nAddTime()\n\n\nExp()\nCurData() 当前系统日期\n\n\nMod() 取余\nDate()\n\n\nPi()\nDay()\n\n\nRand()  随机数\nHour()\n\n\nSin()\nMinute()\n\n\nSqrt()\nNow() 当前系统日期+时间\n\n\nTan()\nYear()\n\n\nRound() 四舍五入\nCurtime() 当前系统时间\n\n\nFloor() 向下取整\nStr_to_date() 将字符转换成日期\n\n\nCeil() 向上取整\nDate_format() 将日期转换成字符\n\n\nTruncate() 截断\n\n\n\n\nSQL聚集函数\n\n\n\n函数\n\n\n\n\n\nAVG()\nmysql&gt; SELECT AVG(列名) from 表名;\n\n\nCOUNT()\nmysql&gt; SELECT COUNT(列名) from 表名;\n\n\nMAX()\nmysql&gt; SELECT MAX(列名) from 表名;\n\n\nMIN()\nmysql&gt; SELECT MIN(列名) from 表名;\n\n\nSUM()\nmysql&gt; SELECT SUM(列名) from 表名;\n\n\n\n\n特点：\n1、以上五个分组函数都忽略null值，除了count(*)\n2、sum和avg一般用于处理数值型\n    max、min、count可以处理任何数据类型\n3、都可以搭配distinct使用，用于统计去重后的结果\n4、count的参数可以支持：\n    字段、*、常量值，一般放1\n\n   建议使用 count(*)\n流程控制函数\n\n\n\n函数\n\n\n\n\n\nIF(判断条件，返回1，返回2)\n\n\n\ncase语句\n处理多分支：处理等值判断、处理条件判断\n\n\n\n\n\n\n\n其他函数\n\n\n\n函数\n\n\n\n\n\nversion() 版本\n\n\n\ndatabase() 当前库\n\n\n\nuser() 当前连接用户\n\n\n\npassword(‘字符’)：返回该字符的密码形式\n\n\n\nmd5(‘字符’):返回该字符的md5加密形式\n\n\n\n\n子查询嵌套在其他查询中的查询,在外面的查询语句，称为主查询或外查询。\nSELECT cust_id\nFROM orders\nWHERE order_num IN (SELECT order_num\n                     FROM orderitems\n                     WHERE prod_id = 'TNT2');\n/*\n1、子查询都放在小括号内\n2、子查询可以放在from后面、select后面、where后面、having后面，但一般放在条件的右侧\n3、子查询优先于主查询执行，主查询使用了子查询的执行结果\n4、子查询根据查询结果的行数不同分为以下两类：\n① 单行子查询\n\t结果集只有一行\n\t一般搭配单行操作符使用：&gt; &lt; = &lt;&gt; &gt;= &lt;= \n\t非法使用子查询的情况：\n\ta、子查询的结果为一组值\n\tb、子查询的结果为空\n\t\n② 多行子查询\n\t结果集有多行\n\t一般搭配多行操作符使用：any、all、in、not in\n\tin： 属于子查询结果中的任意一个就行\n\tany和all往往可以用其他查询代替\n\t*/\n连接表jion联结是一种机制，用来在一条SELECT语句中关联表，因此称之为联结。使用特殊的语法，可以联结多个表返回一组输出，联结在运行时关联表中正确的行。\n#等值联结（equijoin）\nSELECT *\nFROM A,B\nWHERE A.c=B.c;\n\n1.等值连接的结果 = 多个表的交集\n2.n表连接，至少需要n-1个连接条件\n3.多个表不分主次，没有顺序要求\n4.一般为表起别名，提高阅读性和性能\n\n#内部联结\nSELECT *\nFROM A INNER JOIN B\nWHERE A.c=B.c;\n#自然联结\nSELECT *\nFROM A AS a,A AS b\nWHERE a.c=b.c;\n#外部联结\nSELECT *\nFROM A OUTER JOIN B\nWHERE A.c=B.c;\n性能考虑 MySQL在运行时关联指定的每个表以处理联结。这种处理可能是非常耗费资源的，因此应该仔细，不要联结不必要的表。联结的表越多，性能下降越厉害。\n\n自然联结：无论何时对表进行联结，应该至少有一个列出现在不止一个表中（被联结的列）。自然联结排除多次出现，使每个列只返回一次。\n外部联结：许多联结将一个表中的行与另一个表中的行相关联。但有时候会需要包含没有关联行的那些行。\n\nsql99语法：通过join关键字实现连接含义：1999年推出的sql语法\n支持：\n等值连接、非等值连接 （内连接）\n外连接\n交叉连接\n\n语法：\n\nselect 字段，...\nfrom 表1\n【inner|left outer|right outer|cross】join 表2 on  连接条件\n【inner|left outer|right outer|cross】join 表3 on  连接条件\n【where 筛选条件】\n【group by 分组字段】\n【having 分组后的筛选条件】\n【order by 排序的字段或表达式】\n\n好处：语句上，连接条件和筛选条件实现了分离，简洁明了！\n别名AS:别名简化编码\nSELECT name as n\nFROM papa as p,Baba as b\nWHERE p.c=b.c;\n分页查询应用场景：实际的web项目中需要根据用户的需求提交对应的分页查询的sql语句\nselect 字段|表达式,...\nfrom 表\n【where 条件】\n【group by 分组字段】\n【having 条件】\n【order by 排序的字段】\nlimit 【起始的条目索引，】条目数;\n特点：\n1.起始条目索引从0开始\n\n2.limit子句放在查询语句的最后\n\n3.公式：select * from  表 limit （page-1）*sizePerPage,sizePerPage\n假如:\n每页显示条目数sizePerPage\n要显示的页数 page\n联合查询union 联合、合并\nselect 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】\nselect 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】\nselect 字段|常量|表达式|函数 【from 表】 【where 条件】 union  【all】\n.....\nselect 字段|常量|表达式|函数 【from 表】 【where 条件】\n特点：\n1、多条查询语句的查询的列数必须是一致的\n2、多条查询语句的查询的列的类型几乎相同\n3、union代表去重，union all代表不去重\n4. DML语言数据操作语言：insert、update、delete\n4.1 插入语法：\ninsert into 表名(字段名，...)\nvalues(值1，...);\n\n# 支持一次插入多行，语法如下：\ninsert into 表名【(字段名,..)】 values(值，..),(值，...),...;\n\n#支持子查询，语法如下：\ninsert into 表名 查询语句;\n\ninsert into 表名 set 字段=值,字段=值,...;\n特点：\n\n字段类型和值类型一致或兼容，而且一一对应\n可以为空的字段，可以不用插入值，或用null填充\n不可以为空的字段，必须插入值\n字段个数和值的个数必须一致\n字段可以省略，但默认所有字段，并且顺序和表中的存储顺序一致\n\n4.2 修改修改单表语法：\nupdate 表名 set 字段=新值,字段=新值\n【where 条件】\n修改多表语法：\nupdate 表1 别名1,表2 别名2\nset 字段=新值，字段=新值\nwhere 连接条件\nand 筛选条件\n4.3 删除方式1：delete语句单表的删除： ★\ndelete from 表名 【where 筛选条件】\n多表的删除：    delete 别名1，别名2    from 表1 别名1，表2 别名2    where 连接条件    and 筛选条件;\n方式2：truncate语句truncate table 表名\n两种方式的区别【面试题】\n#1.truncate不能加where条件，而delete可以加where条件\n\n#2.truncate的效率高一丢丢\n\n#3.truncate 删除带自增长的列的表后，如果再插入数据，数据从1开始\n#delete 删除带自增长列的表后，如果再插入数据，数据从上一次的断点处开始\n\n#4.truncate删除不能回滚，delete删除可以回滚\n5. DDL语言数据定义语言：create、drop、alter\n5.1 库和表的管理一、创建库\ncreate database 库名\n二、删除库\ndrop database 库名\n5.2 创建表 createcreate table 表名(\n  字段名 字段类型 not null,#非空\n  字段名 字段类型 primary key,#主键\n  字段名 字段类型 unique,#唯一\n  字段名 字段类型 default 值,#默认\n  constraint 约束名 foreign key(字段名) references 主表（被引用列）\n)\nCREATE TABLE IF NOT EXISTS stuinfo(\n\tstuId INT,\n\tstuName VARCHAR(20),\n\tgender CHAR,\n\tbornDate DATETIME\n);\nDESC studentinfo;\n\n\n\n\n\n支持类型\n可以起约束名\n\n\n\n\n列级约束\n除了外键\n不可以\n\n\n表级约束\n除了非空和默认\n可以，但对主键无效\n\n\n\n\n列级约束可以在一个字段上追加多个，中间用空格隔开，没有顺序要求\n常见类型\n\n常见约束\nNOT NULL：非空，该字段的值必填\nUNIQUE：唯一，该字段的值不可重复\nDEFAULT：默认，该字段的值不用手动插入有默认值\nCHECK：检查，mysql不支持\nPRIMARY KEY：主键，该字段的值不可重复并且非空 unique+not null\nFOREIGN KEY：外键，该字段的值引用了另外的表的字段\n主键和唯一\n1、区别：\n①、一个表至多有一个主键，但可以有多个唯一\n②、主键不允许为空，唯一可以为空\n2、相同点\n都具有唯一性，都支持组合键，但不推荐\n外键：\n\n用于限制两个表的关系，从表的字段值引用了主表的某字段值\n外键列和主表的被引用列要求类型一致，意义一样，名称无要求\n主表的被引用列要求是一个key（一般就是主键）\n插入数据，先插入主表，删除数据，先删除从表\n\n可以通过以下两种方式来删除主表的记录\n#方式一：级联删除\nALTER TABLE stuinfo ADD CONSTRAINT fk_stu_major FOREIGN KEY(majorid) REFERENCES major(id) ON DELETE CASCADE;\n\n#方式二：级联置空\nALTER TABLE stuinfo ADD CONSTRAINT fk_stu_major FOREIGN KEY(majorid) REFERENCES major(id) ON DELETE SET NULL;\n自增长列特点：\n\n不用手动插入值，可以自动提供序列值，默认从1开始，步长为1\n\n如果要更改起始值：手动插入值\n如果要更改步长：更改系统变量\nset auto_increment_increment=值;\n\n一个表至多有一个自增长列\n\n自增长列只能支持数值型\n\n自增长列必须为一个key\n\n\n创建表时设置自增长列\ncreate table 表(\n\t字段名 字段类型 约束 auto_increment\n)\n修改表时设置自增长列\nalter table 表 modify column 字段名 字段类型 约束 auto_increment\n删除自增长列\nalter table 表 modify column 字段名 字段类型 约束 \n5.3 修改表 alter语法：ALTER TABLE 表名 ADD|MODIFY|DROP|CHANGE COLUMN 字段名 【字段类型】;\n\n#修改字段名\nALTER TABLE studentinfo CHANGE  COLUMN sex gender CHAR;\n\n#修改表名\nALTER TABLE stuinfo RENAME [TO]  studentinfo;\n\n#修改字段类型和列级约束\nALTER TABLE studentinfo MODIFY COLUMN borndate DATE ;\n\n#添加字段\nALTER TABLE studentinfo ADD COLUMN email VARCHAR(20) first;\n\n#删除字段\nALTER TABLE studentinfo DROP COLUMN email;\n\n#添加列\nalter table 表名 add column 列名 类型 【first|after 字段名】;\n\n#修改列名\nalter table 表名 change column 旧列名 新列名 类型;\n\n#删除列\nalter table 表名 drop column 列名;\n\n修改表时添加或删除约束\n1、非空\n添加非空\nalter table 表名 modify column 字段名 字段类型 not null;\n删除非空\nalter table 表名 modify column 字段名 字段类型 ;\n\n2、默认\n添加默认\nalter table 表名 modify column 字段名 字段类型 default 值;\n删除默认\nalter table 表名 modify column 字段名 字段类型 ;\n3、主键\n添加主键\nalter table 表名 add【 constraint 约束名】 primary key(字段名);\n删除主键\nalter table 表名 drop primary key;\n\n4、唯一\n添加唯一\nalter table 表名 add【 constraint 约束名】 unique(字段名);\n删除唯一\nalter table 表名 drop index 索引名;\n5、外键\n添加外键\nalter table 表名 add【 constraint 约束名】 foreign key(字段名) references 主表（被引用列）;\n删除外键\nalter table 表名 drop foreign key 约束名;\n\n5.4 删除表DROP TABLE [IF EXISTS] studentinfo;\n复制表\n\n复制表的结构create table 表名 like 旧表;\n复制表的结构+数据create table 表名select 查询列表 from 旧表【where 筛选】;\n\n6. TCL语言事务控制语言：commit、rollback\n6.1 数据库事务含义：通过一组逻辑操作单元（一组DML——sql语句），将数据从一种状态切换到另外一种状态\n特点（ACID）：\n\n原子性：要么都执行，要么都回滚\n一致性：保证数据的状态操作前和操作后保持一致\n隔离性：多个事务同时操作相同数据库的同一个数据时，一个事务的执行不受另外一个事务的干扰\n持久性：一个事务一旦提交，则数据将持久化到本地，除非其他事务对其进行修改\n\n相关步骤：\n开启事务\n编写事务的一组逻辑操作单元（多条sql语句）\n提交事务或回滚事务\n\n事务的分类：隐式事务，没有明显的开启和结束事务的标志，比如 insert、update、delete语句本身就是一个事务\n显式事务，具有明显的开启和结束事务的标志\n1、开启事务\n取消自动提交事务的功能\n\n2、编写事务的一组逻辑操作单元（多条sql语句）\ninsert\nupdate\ndelete\n\n3、提交事务或回滚事务\n使用到的关键字set autocommit=0;\nstart transaction;\ncommit;\nrollback;\n\nsavepoint  断点\ncommit to 断点\nrollback to 断点\n事务的隔离级别:事务并发问题如何发生？\n\n\n\n\n\n\n\n\n\n当多个事务同时操作同一个数据库的相同数据时\n事务的并发问题有哪些？\n\n\n\n\n\n\n\n\n\n脏读：一个事务读取到了另外一个事务未提交的数据不可重复读：同一个事务中，多次读取到的数据不一致幻读：一个事务读取数据时，另外一个事务进行更新，导致第一个事务读取到了没有更新的数据\n如何避免事务的并发问题？\n\n\n\n\n\n\n\n\n\n通过设置事务的隔离级别1、READ UNCOMMITTED2、READ COMMITTED 可以避免脏读3、REPEATABLE READ 可以避免脏读、不可重复读和一部分幻读4、SERIALIZABLE可以避免脏读、不可重复读和幻读\n设置隔离级别：\nset session|global  transaction isolation level 隔离级别名;\n查看隔离级别：\nselect @@tx_isolation;\n7. 视图含义：理解成一张虚拟的表\n视图和表的区别：\n\n\n\n\n\n使用方式\n占用物理空间\n\n\n\n\n视图\n完全相同\n占用物理空间\n\n\n表\n完全相同\n占用\n\n\n\n\n\n\n\n\n视图的好处：\n\nsql语句提高重用性，效率高\n和表实现了分离，提高了安全性\n\n7.1 视图的创建、删除、查看CREATE VIEW  视图名\nAS\n查询语句;\n\n#视图的删除\nDROP VIEW test_v1,test_v2,test_v3;\n\n###视图结构的查看\t\nDESC test_v7;\nSHOW CREATE VIEW test_v7;\n7.2 视图的增删改查1、查看视图的数据 ★\nSELECT * FROM my_v4;\nSELECT * FROM my_v1 WHERE last_name='Partners';\n\n2、插入视图的数据\nINSERT INTO my_v4(last_name,department_id) VALUES('虚竹',90);\n\n3、修改视图的数据\nUPDATE my_v4 SET last_name ='梦姑' WHERE last_name='虚竹';\n\n4、删除视图的数据\nDELETE FROM my_v4;\n​    \n7.3 某些视图不能更新​    包含以下关键字的sql语句：\n\n分组函数、distinct、group  by、having、union或者union all\n常量视图\nSelect中包含子查询\njoin\nfrom一个不能更新的视图\nwhere子句的子查询引用了from子句中的表\n\n\n7.4 视图逻辑的更新#方式一：\nCREATE OR REPLACE VIEW test_v7\nAS\nSELECT last_name FROM employees\nWHERE employee_id&gt;100;\n\n#方式二:\nALTER VIEW test_v7\nAS\nSELECT employee_id FROM employees;\n\nSELECT * FROM test_v7;\n8 存储过程含义：一组经过预先编译的sql语句的集合好处：\n\n提高了sql语句的重用性，减少了开发程序员的压力\n提高了效率\n减少了传输次数\n\n分类：\n1、无返回无参2、仅仅带in类型，无返回有参3、仅仅带out类型，有返回无参4、既带in又带out，有返回有参5、带inout，有返回有参注意：in、out、inout都可以在一个存储过程中带多个\n8.1 创建存储过程语法：\ncreate procedure 存储过程名(in|out|inout 参数名  参数类型,...)\nbegin\n\t存储过程体\nend\n类似于方法：\n修饰符 返回类型 方法名(参数类型 参数名,...){\n\n\t方法体;\n}\n注意\n\n需要设置新的结束标记delimiter 新的结束标记\n\n示例：\ndelimiter $\n\nCREATE PROCEDURE 存储过程名(IN|OUT|INOUT 参数名  参数类型,...)\nBEGIN\n\tsql语句1;\n\tsql语句2;\n\nEND $\n\n\n存储过程体中可以有多条sql语句，如果仅仅一条sql语句，则可以省略begin end\n\n参数前面的符号的意思in:该参数只能作为输入 （该参数不能做返回值）out：该参数只能作为输出（该参数只能做返回值）inout：既能做输入又能做输出\n\n\n8.2 调用存储过程call 存储过程名(实参列表)\n学过的函数：LENGTH、SUBSTR、CONCAT等语法：\nCREATE FUNCTION 函数名(参数名 参数类型,...) RETURNS 返回类型\nBEGIN\n\t函数体\n\nEND\n调用函数：SELECT 函数名（实参列表）\n函数和存储过程的区别\n\n\n\n\n\n关键字\n调用语法\n返回值\n应用场景\n\n\n\n\n函数\nFUNCTION\nSELECT\n只能是一个\n一般用于查询结果为一个值并返回时，当有返回值而且仅仅一个\n\n\n存储过程\nPROCEDURE\nCALL\n可以有0个或多个\n一般用于更新\n\n\n\n\n8.3 查看存储过程show create procedure 存储过程名;\n8.4 删除drop procedure 存储过程名;\n9 流程控制结构9.1 系统变量全局变量\n作用域：针对于所有会话（连接）有效，但不能跨重启\n查看所有全局变量\nSHOW GLOBAL VARIABLES;\n查看满足条件的部分系统变量\nSHOW GLOBAL VARIABLES LIKE '%char%';\n查看指定的系统变量的值\nSELECT @@global.autocommit;\n为某个系统变量赋值\nSET @@global.autocommit=0;\nSET GLOBAL autocommit=0;\n会话变量\n作用域：针对于当前会话（连接）有效\n查看所有会话变量\nSHOW SESSION VARIABLES;\n查看满足条件的部分会话变量\nSHOW SESSION VARIABLES LIKE '%char%';\n查看指定的会话变量的值\nSELECT @@autocommit;\nSELECT @@session.tx_isolation;\n为某个会话变量赋值\nSET @@session.tx_isolation='read-uncommitted';\nSET SESSION tx_isolation='read-committed';\n9.2 自定义变量用户变量\n声明并初始化：\nSET @变量名=值;\nSET @变量名:=值;\nSELECT @变量名:=值;\n赋值：\n方式一：一般用于赋简单的值\nSET 变量名=值;\nSET 变量名:=值;\nSELECT 变量名:=值;\n方式二：一般用于赋表 中的字段值\nSELECT 字段名或表达式 INTO 变量\nFROM 表;\n使用：\nselect @变量名;\n局部变量\n声明：\ndeclare 变量名 类型 【default 值】;\n赋值：\n方式一：一般用于赋简单的值\nSET 变量名=值;\nSET 变量名:=值;\nSELECT 变量名:=值;\n方式二：一般用于赋表 中的字段值\nSELECT 字段名或表达式 INTO 变量\nFROM 表;\n使用：\nselect 变量名\n二者的区别：\n\n\n\n\n\n作用域\n定义位置\n语法\n\n\n\n\n语法\n当前会话\n会话的任何地方\n加@符号，不用指定类型\n\n\n局部变量\n定义它的BEGIN END中\nBEGIN END的第一句话\n一般不用加@,需要指定类型\n\n\n\n\n9.3 分支if函数    语法：if(条件，值1，值2)    特点：可以用在任何位置\ncase语句\n情况一：类似于switch\ncase 表达式\nwhen 值1 then 结果1或语句1(如果是语句，需要加分号) \nwhen 值2 then 结果2或语句2(如果是语句，需要加分号)\n...\nelse 结果n或语句n(如果是语句，需要加分号)\nend 【case】（如果是放在begin end中需要加上case，如果放在select后面不需要）\n\n情况二：类似于多重if\ncase \nwhen 条件1 then 结果1或语句1(如果是语句，需要加分号) \nwhen 条件2 then 结果2或语句2(如果是语句，需要加分号)\n...\nelse 结果n或语句n(如果是语句，需要加分号)\nend 【case】（如果是放在begin end中需要加上case，如果放在select后面不需要）\n特点：可以用在任何位置\nif elseif语句\nif 情况1 then 语句1;\nelseif 情况2 then 语句2;\n...\nelse 语句n;\nend if;\n特点：只能用在begin end中！！！！！！！！！！！！！！！应用场合\n\nif函数：简单双分支\ncase结构：等值判断 的多分支\nif结构：区间判断 的多分支\n\n9.4 循环【标签：】WHILE 循环条件  DO\n    循环体\nEND WHILE 【标签】;\n特点：\n只能放在BEGIN END里面\n如果要搭配leave跳转语句，需要使用标签，否则可以不用标签\nleave类似于java中的break语句，跳出所在循环！！！\n","slug":"B1-MySQL基础","date":"2021-11-10T12:30:04.000Z","categories_index":"数据库","tags_index":"MySQL","author_index":"YFR718"},{"id":"31135c47a98e6bfd7ef6f55be399b5e8","title":"PyMySQL","content":"1. 连接数据库import pymysql\n\n# 创建连接对象\nconn = pymysql.connect(host='localhost', port=3306, user='root', password='mysql',database='python', charset='utf8')\n\n# 获取游标对象\ncursor = conn.cursor()\n\n# 关闭游标\ncursor.close()\n\n# 关闭连接\nconn.close()\n2. 数据库操作2.1 DQL语言selectselect选择\n# 查询 SQL 语句\nsql = \"select * from students;\"\n# 执行 SQL 语句 返回值就是 SQL 语句在执行过程中影响的行数\nrow_count = cursor.execute(sql)\n\n# 取出结果集中一行数据,　例如:(1, '张三')\nprint(cursor.fetchone())\n\n# 取出结果集中的所有数据, 例如:((1, '张三'), (2, '李四'), (3, '王五'))\nfor line in cursor.fetchall():\n    print(line)\n        \nPython查询Mysql使用 fetchone() 方法获取单条数据, 使用fetchall() 方法获取多条数据。\n\nfetchone(): 该方法获取下一个查询结果集。结果集是一个对象\n\nfetchall(): 接收全部的返回结果行.\n\nrowcount: 这是一个只读属性，并返回执行execute()方法后影响的行数。\n\n\n2.2 DML数据操作语言insert 、update、deleteinsert插入\n# SQL 插入语句\nsql = \"\"\"INSERT INTO EMPLOYEE(FIRST_NAME,\n         LAST_NAME, AGE, SEX, INCOME)\n         VALUES ('Mac', 'Mohan', 20, 'M', 2000)\"\"\"\ntry:\n   # 执行sql语句\n   cursor.execute(sql)\n   # 提交到数据库执行\n   db.commit()\nexcept:\n   # 如果发生错误则回滚\n   db.rollback()\n插入多行数据时，数据是list，其中元素是元组。\n# 一次插入多行数据\n# print(value)\nsql = 'insert into summary(id,dataname,herf,rel) values(%s,%s,%s,%s)'\nvalue = []\nfor rep in root.findall('repository'):\n    id = rep.find('id').text\n    dataname = rep.find('name').text\n    herf = rep.find('link').attrib['href']\n    rel = rep.find('link').attrib['rel']\n    value.append((id,dataname,herf,rel))\n    \ntry:\n    cursor.executemany(sql,value)\n    db.commit()\n    print(\"summary写入成功\")\nexcept Exception as e:\n    print(e)\n    db.rollback()\n    print(\"summary写入失败\")\nfinally:\n    db.close()\n\nupdate更新\n# SQL 更新语句\nsql = \"UPDATE EMPLOYEE SET AGE = AGE + 1 WHERE SEX = '%c'\" % ('M')\ntry:\n   # 执行SQL语句\n   cursor.execute(sql)\n   # 提交到数据库执行\n   db.commit()\nexcept:\n   # 发生错误时回滚\n   db.rollback()\ndelete删除\n# SQL 删除语句\nsql = \"DELETE FROM EMPLOYEE WHERE AGE &gt; %s\" % (20)\ntry:\n   # 执行SQL语句\n   cursor.execute(sql)\n   # 提交修改\n   db.commit()\nexcept:\n   # 发生错误时回滚\n   db.rollback()\n2.3 DDL数据定义语言create、drop、altercreate创建数据表\n# 使用 cursor() 方法创建一个游标对象 cursor\ncursor = db.cursor()\n \n\n# 使用预处理语句创建表\nsql = \"\"\"CREATE TABLE EMPLOYEE (\n         FIRST_NAME  CHAR(20) NOT NULL,\n         LAST_NAME  CHAR(20),\n         AGE INT,  \n         SEX CHAR(1),\n         INCOME FLOAT )\"\"\"\n \ncursor.execute(sql)\ndrop删除数据表\n# 使用 execute() 方法执行 SQL，如果表存在则删除\ncursor.execute(\"DROP TABLE IF EXISTS EMPLOYEE\")\n \nalter更改数据表\n2.4 TCL事务控制语言commit、rollback# SQL删除记录语句\nsql = \"DELETE FROM EMPLOYEE WHERE AGE &gt; %s\" % (20)\ntry:\n   # 执行SQL语句\n   cursor.execute(sql)\n   # 向数据库提交\n   db.commit()\nexcept:\n   # 发生错误时回滚\n   db.rollback()\n","slug":"P2-PyMySQL","date":"2021-11-10T02:06:01.000Z","categories_index":"Python","tags_index":"MySQL","author_index":"YFR718"},{"id":"446bbf5d6e6c49aba6d56562199f8a0a","title":"XML解析","content":"1 xml.etree.ElementTree对Python的轻量级XML支持。\nXML是一种固有的分层数据格式，最自然的表示方法是使用树。这个模块有两个类用于此目的:\n\nElementTree将整个XML文档表示为树。\n元素表示此树中的单个节点。\n\n​        与整个文档的交互(对文件的读写)通常在ElementTree级别完成。与单个XML元素及其子元素的交互是在element级别上完成的。\n​        元素是一种灵活的容器对象，用于在内存中存储分层数据结构。它可以被描述为介于列表和字典之间的混合体。每个元素都有许多与之相关的属性:\n\n‘tag’——包含元素名称的字符串。\n‘attributes’——一个存储元素属性的Python字典。\n‘text’ -一个包含元素文本内容的字符串。\n‘tail’ -一个可选字符串，在元素的结束标签之后包含文本。以及一系列存储在Python序列中的子元素。\n\n​        要创建元素实例，请使用element构造函数或SubElement工厂函数。还可以使用ElementTree类包装元素结构，并将其与XML进行转换。\n1.1 导入数据从xml文件导入\nimport xml.etree.ElementTree as ET\ntree = ET.parse('country_data.xml')\nroot = tree.getroot()\n从html导入\nimport xml.etree.ElementTree as ET\n# 要爬取的地址\nurl = \"https://www.re3data.org/api/v1/repositories\"\nres = requests.get(url)\nroot = ET.fromstring(res.text)\n1.2 数据查询1.2.1 获取四个属性print(root.tag)\nprint(root.attrib)\nprint(root.tail)\nprint(res.text)\n1.2.2 访问子节点用 len(Element) 检查子节点个数\n用 Element[0] 访问第0个子节点，Element[1] 访问第1个子节点…\n用 for child in Element 遍历所有子节点\n用 Element.remove(child) 删除某个子节点\nfor i in root:\n    print(i.tag)\n1.2.3 查询指定标签\nElementt.iter() #递归迭代xml文件中所有节点（包含子节点，以及子节点的子节点），返回一个包含所有节点的迭代器\nElement.find(tag) #查找第一个节点为tag的直接子元素，返回一个节点对象\nElement.findall(tag) #查找节点为tag的所有直接子元素’，返回一个节点列表直接子元素的意思：只会查找当前节点的子节点那一级目录\n\nfor rep in root.findall('repository'):\n    # print(rep.find('id').text)\n    # print(rep.find('name').text)\n    print(rep.find('link').attrib['href'],rep.find('link').attrib['rel'])\n处理含namespace的文件\nnamespace = \"{http://www.re3data.org/schema/2-2}\"\nfor element in root.iter(namespace + \"re3data.orgIdentifier\"):  # ID\n    print(\"1#   \", element.text)\n    ID = element.text\n1.2.4 树结构与 XML 字符串的相互转换使用 xml.etree.ElementTree 包中的 tostring() 和 fromstring() 函数：\n\n如果 tostring() 参数不指定 encoding=\"unicode\"，函数将返回 byte 序列。\n\nfrom xml.etree.ElementTree import Element, tostring\n\ntree_str = tostring(root, encoding=\"unicode\")\n\nnew_root = fromstring(tree_str)\nprint(new_root.tag, new_root[0].tag, new_root[1].tag)\n1.3 数据创建1.3.1 创建树节点创建树节点时，一定要指定节点名：\ntree_node = Element(\"node1\")\nprint(tree_node.tag)    # 输出 node1\nprint(tree_node.text)   # 输出 None\nprint(tree_node.tail)   # 输出 None\nprint(tree_node.attrib) # 输出 {}\n也可在创建时指定属性（Element.attrib）：\ntree_node = Element(\"node2\", {\"attr1\": 1, \"attr2\": 2})\nprint(tree_node.tag)    # 输出 node2\nprint(tree_node.text)   # 输出 None\nprint(tree_node.tail)   # 输出 None\nprint(tree_node.attrib) # 输出 {'attr1': 1, 'attr2': 2}\n1.3.2 设置文本（Element.text）或附加文本（Element.tail）创建节点后，可以设置 text, tail 等类成员。这些成员的初始值为 None。\ntree_node = Element(\"node1\")\ntree_node.text = \"Hello world\"\ntree_node.tail = \"Bye\"\n1.3.3 添加子节点可以用 Element.append() 成员函数添加子节点：\nroot = Element(\"root\")\nchild1 = Element(\"child1\")\nchild2 = Element(\"child2\")\n\nroot.append(child1)\nroot.append(child2)\n","slug":"P1-XML解析","date":"2021-11-10T01:23:54.000Z","categories_index":"Python","tags_index":"XML","author_index":"YFR718"},{"id":"0e809b380a45c732473fb3fa1636bd4c","title":"ASCII码","content":"ASCII码表格：\n\n\n\n\nASCII值\n控制字符\nASCII值\n字符\nASCII值\n字符\nASCII值\n字符\n\n\n\n\n0\nNUT\n32\n(space)\n64\n@\n96\n、\n\n\n1\nSOH\n33\n!\n65\nA\n97\na\n\n\n2\nSTX\n34\n“\n66\nB\n98\nb\n\n\n3\nETX\n35\n#\n67\nC\n99\nc\n\n\n4\nEOT\n36\n$\n68\nD\n100\nd\n\n\n5\nENQ\n37\n%\n69\nE\n101\ne\n\n\n6\nACK\n38\n&amp;\n70\nF\n102\nf\n\n\n7\nBEL\n39\n,\n71\nG\n103\ng\n\n\n8\nBS\n40\n(\n72\nH\n104\nh\n\n\n9\nHT\n41\n)\n73\nI\n105\ni\n\n\n10\nLF\n42\n*\n74\nJ\n106\nj\n\n\n11\nVT\n43\n+\n75\nK\n107\nk\n\n\n12\nFF\n44\n,\n76\nL\n108\nl\n\n\n13\nCR\n45\n-\n77\nM\n109\nm\n\n\n14\nSO\n46\n.\n78\nN\n110\nn\n\n\n15\nSI\n47\n/\n79\nO\n111\no\n\n\n16\nDLE\n48\n0\n80\nP\n112\np\n\n\n17\nDCI\n49\n1\n81\nQ\n113\nq\n\n\n18\nDC2\n50\n2\n82\nR\n114\nr\n\n\n19\nDC3\n51\n3\n83\nS\n115\ns\n\n\n20\nDC4\n52\n4\n84\nT\n116\nt\n\n\n21\nNAK\n53\n5\n85\nU\n117\nu\n\n\n22\nSYN\n54\n6\n86\nV\n118\nv\n\n\n23\nTB\n55\n7\n87\nW\n119\nw\n\n\n24\nCAN\n56\n8\n88\nX\n120\nx\n\n\n25\nEM\n57\n9\n89\nY\n121\ny\n\n\n26\nSUB\n58\n:\n90\nZ\n122\nz\n\n\n27\nESC\n59\n;\n91\n[\n123\n{\n\n\n28\nFS\n60\n&lt;\n92\n/\n124\n\\\n\n\n\n29\nGS\n61\n=\n93\n]\n125\n}\n\n\n30\nRS\n62\n&gt;\n94\n^\n126\n`\n\n\n31\nUS\n63\n?\n95\n_\n127\nDEL\n\n\n\n\nASCII 码大致由以下两部分组成：\n\nASCII 非打印控制字符： ASCII 表上的数字 0-31 分配给了控制字符，用于控制像打印机等一些外围设备。\nASCII 打印字符：数字 32-126 分配给了能在键盘上找到的字符，当查看或打印文档时就会出现。\n\n","slug":"Z0-计算机基础","date":"2021-11-06T11:52:59.000Z","categories_index":"计算机基础","tags_index":"","author_index":"YFR718"},{"id":"f8534e8f12d0bc66698739d3b7153c65","title":"J5.对象与类","content":"\n面向对象编程OOP类(Class)和对象(Object)是面向对象的核心概念。\n\n类是对一类事物的描述，是抽象的、概念上的定义\n对象是实际存在的该类事物的每个个体，因而也称为实例(instance)。\n属 性：对应类中的成员变量\n行 为：对应类中的成员方法\n由类构造（construct）对象的过程称为创建类的实例（instance）。\n\n创建Java自定义类\n\n定义类（考虑修饰符、类名）\n编写类的属性（考虑修饰符、属性类型、属性名、初始化值）\n编写类的方法（考虑修饰符、返回值类型、方法名、形参等）\n\n对象的创建和使用\n创建对象语法： 类名 对象名 = new 类名();使用“对象名.对象成员”的方式访问对象成员（包括属性和方法）\n属性修饰符 数据类型 属性名 = 初始化值 ; \n\n修饰符：常用的权限修饰符有：private、缺省、protected、public，其他修饰符：static、final (暂不考虑)\n数据类型：任何基本数据类型(如int、Boolean) 或 任何引用数据类型。\n属性名：属于标识符，符合命名规则和规范即可。\n\n变量的分类：成员变量与局部变量\n\n\n\n\n\n\n成员变量\n局部变量\n\n\n\n\n声明的位置\n直接声明在类中\n方法形参或内部、代码块内、构造器内等\n\n\n修饰符\nprivate、public、static、final等\n不能用权限修饰符修饰，可以用final修饰\n\n\n初始化值\n有默认初始化值\n没有默认初始化值，必须显式赋值，方可使用\n\n\n内存加载位置\n堆空间 或 静态域内\n栈空间\n\n\n\n\n方法\n方法是类或对象行为特征的抽象，用来完成某个功能操作。在某些语言中也称为函数或过程。\n将功能封装为方法的目的是，可以实现代码重用，简化代码\nJava里的方法不能独立存在，所有的方法必须定义在类里。\n\n// 方法的声明格式：\n修饰符  返回值类型  方法名（参数类型形参1, 参数类型形参2, ….）｛\n    方法体程序代码\n    return 返回值;\n}\n修饰符：public,缺省,private, protected等返回值类型：\n返回值类型：没有返回值：void，有返回值，声明出返回值的类型。与方法体中“return 返回值”搭配使用\n方法名：属于标识符，命名时遵循标识符命名规则和规范，“见名知意” \n形参列表：可以包含零个，一个或多个参数。多个参数时，中间用“,”隔开返回值：方法在执行完毕后返还给调用它的程序的数据。\nmain方法\n\n\n\nmain方法\n\n\n\n\n\npublic\n由于Java虚拟机需要调用类的main()方法，所以该方法的访问权限必须是public\n\n\nstatic\n因为Java虚拟机在执行main()方法时不必创建对象，所以该方法必须是static的\n\n\nString[] args\n该方法接收一个String类型的数组参数，该数组中保存执行Java命令时传递给所运行的类的参数\n\n\n\n\n代码块代码块(或初始化块)的作用： 对Java类或对象进行初始化代码块(或初始化块)的分类：一个类中代码块若有修饰符，则只能被static修饰，称为静态代码块(static block)，没有使用static修饰的，为非静态代码块。\n// static代码块通常用于初始化static的属性\nclass Person {\n    public static int total; \n    static {\n        total = 100;//为total赋初值\n    }\n}\n\n静态代码块：用static 修饰的代码块\n\n\n可以有输出语句。\n可以对类的属性、类的声明进行初始化操作。\n不可以对非静态的属性初始化。即：不可以调用非静态的属性和方法。\n若有多个静态的代码块，那么按照从上到下的顺序依次执行。\n静态代码块的执行要先于非静态代码块。\n静态代码块随着类的加载而加载，且只执行一次。\n\n\n非静态代码块：没有static修饰的代码块\n\n\n可以有输出语句。\n可以对类的属性、类的声明进行初始化操作。\n除了调用非静态的结构外，还可以调用静态的变量或方法。\n若有多个非静态的代码块，那么按照从上到下的顺序依次执行。\n每次创建对象的时候，都会执行一次。且先于构造器执行。\n\n⭐程序中成员变量赋值的执行顺序\n\n声明成员变量的默认初始化\n显式初始化、多个初始化块依次被执行（同级别下按先后顺序执行）\n构造器再对成员进行初始化操作\n通过”对象.属性”或”对象.方法”的方式，可多次给属性赋值\n\n重载 overload在同一个类中，允许存在一个以上的==同名方法==，只要它们的==参数个数或者参数类型==不同即可。\n==与返回值类型无关==，只看参数列表，且参数列表必须不同。(参数个数或参数类型)。调用时，根据方法参数列表的不同来区别。\n//返回两个整数的和\nint add(int x,int y){return x+y;}\n//返回三个整数的和\nint add(int x,int y,int z){return x+y+z;} \n//返回两个小数的和\ndouble add(double x,double y){return x+y;}\n可变个数的形参public static void test(int a ,String…books);\n\n可变参数：方法参数部分指定类型的参数个数是可变多个：0个，1个或多个\n可变个数形参的方法与同名的方法之间，彼此构成重载\n可变参数方法的使用与方法参数部分使用数组是一致的\n方法的参数部分有可变形参，需要放在形参声明的最后\n在一个方法的形参位置，最多只能声明一个可变个数形参\n\n方法参数的值传递机制    形参：方法声明时的参数    实参：方法调用时实际传给形参的参数值\nJava里方法的参数传递方式只有一种：值传递。 即将实际参数值的副本（复制品）传入方法内，而参数本身不受影响。\n    形参是基本数据类型：将实参基本数据类型变量的“数据值”传递给形参    形参是引用数据类型：将实参引用数据类型变量的“地址值”传递给形参\nint[] arr = new int[10];\nSystem.out.println(arr);//地址值\nchar[] arr1 = new char[10]; System.out.println(arr1); //内容\n//源码解析\npublic void println(Object x) {\n    String s = String.valueOf(x);\n    synchronized (this) {\n        print(s);\n        newLine();\n    }\n}\npublic void println(char x[]) {\n    synchronized (this) {\n        print(x);\n        newLine();\n    }\n}\n构造器 (或构造方法)构造器的特征\n\n它具有与类相同的名称\n它不声明返回值类型。（与声明为void不同）\n不能被static、final、synchronized、abstract、native修饰，不能有return语句返回值\n\n构造器的作用：创建对象；给对象进行初始化\n\n如：Order o = new Order();    Person p = new Person(“Peter”,15);\n如同我们规定每个“人”一出生就必须先洗澡，我们就可以在“人”的构造器中加入完成“洗澡”的程序代码，于是每个“人”一出生就会自动完成“洗澡”，程序就不必再在每个人刚出生时一个一个地告诉他们要“洗澡”了。\n\n// 语法格式：\n修饰符 类名 (参数列表) {\n    初始化语句;\n}\n根据参数不同，构造器可以分为如下两类：    隐式无参构造器（系统默认提供）    显式定义一个或多个构造器（无参、有参）注 意：    Java语言中，每个类都至少有一个构造器    默认构造器的修饰符与所属类的修饰符一致    一旦显式定义了构造器，则系统不再提供默认构造器    一个类可以创建多个重载的构造器    父类的构造器不可被子类继承\n构造器重载\n构造器重载使得对象的创建更加灵活，方便创建各种不同的对象。\n// 构造器重载举例：\npublic class Person{\n    public Person(String name, int age, Date d) {this(name,age);…} \n    public Person(String name, int age) {…}\n    public Person(String name, Date d) {…}\n    public Person(){…}\n}\n关键字—this    它在方法内部使用，即这个方法所属对象的引用；    它在构造器内部使用，表示该构造器正在初始化的对象。\nthis 可以调用类的属性、方法和构造器\n什么时候使用this关键字呢？    当在方法内需要用到调用该方法的对象时，就用this。具体的：我们可以用this来区分属性和局部变量。比如：this.name = name;\n⭐this可以作为一个类中构造器相互调用的特殊格式\nclass Person{\n    //定义Person类\n    private String name ;\n    private int age ;\n    public Person(){\n        //无参构造器\n        System.out.println(\"新对象实例化\");\n    }\n    public Person(String name ){\n        this(); //调用本类中的无参构造器\n        this.name = name ;\n    }\n    public Person(String name,int age){\n        this(name); // 调用有一个参数的构造器\n        this.age = age;\n    }\n    public String getInfo(){\n    return \"姓名: \"+name+\"，年龄:\"+age;\n    }\n}\n\n    可以在类的构造器中使用”this(形参列表)”的方式，调用本类中重载的其他的构造器！    明确：构造器中不能通过”this(形参列表)”的方式调用自身构造器    如果一个类中声明了n个构造器，则最多有 n - 1个构造器中使用了”this(形参列表)”    “this(形参列表)”必须声明在类的构造器的首行！    在类的一个构造器中，最多只能声明一个”this(形参列表)”\n封装与隐藏    高内聚 ：类的内部数据操作细节自己完成，不允许外部干涉；    低耦合 ：仅对外暴露少量的方法用于使用。\n隐藏对象内部的复杂性，只对外公开简单的接口。便于外界调用，从而提高系统的可扩展性、可维护性。通俗的说，把该隐藏的隐藏起来，该暴露的暴露出来。这就是封装性的设计思想。\nJava中通过将数据声明为私有的(private)，再提供公共的（public）方法:getXxx()和setXxx()实现对该属性的操作，以实现下述目的：\n    隐藏一个类中不需要对外提供的实现细节；    使用者只能通过事先定制好的方法来访问数据，可以方便地加入控制逻辑，限制对属性的不合理操作；    便于修改，增强代码的可维护性；\n\n\n\n\n修饰符\n类内部\n同一个包\n不同包的子类\n同一个工程\n\n\n\n\nprivate\n√\n\n\n\n\n\n(缺省)\n√\n√\n\n\n\n\nprotected\n√\n√\n√\n\n\n\npublic\n√\n√\n√\n√\n\n\n\n\n包(package)的管理与作用package语句作为Java源文件的第一条语句，指明该文件中定义的类所在的包。(若缺省该语句，则指定为无名包)。它的格式为：package 顶层包名.子包名 ;\n//举例：pack1\\pack2\\PackageTest.java\npackage pack1.pack2;    //指定类PackageTest属于包pack1.pack2 \n\npublic class PackageTest{\n    public void display(){\n        System.out.println(\"in  method display()\");\n    }\n}\n    包对应于文件系统的目录，package语句中，用 “.” 来指明包(目录)的层次；    包通常用小写单词标识。通常使用所在公司域名的倒置：com.atguigu.xxx\n包的作用：\n    包帮助管理大型软件系统：将功能相近的类划分到同一个包中。比如：MVC的设计模式    包可以包含类和子包，划分项目层次，便于管理    解决类命名冲突的问题    控制访问权限\n\n\n\n\n\nJDK中主要的包介绍\n\n\n\n\njava.lang\n包含一些Java语言的核心类，如String、Math、Integer、 System和Thread，提供常用功能\n\n\njava.net\n包含执行与网络相关的操作的类和接口\n\n\njava.io\n包含能提供多种输入/输出功能的类\n\n\njava.util\n包含一些实用工具类，如定义系统特性、接口的集合框架类、使用与日期日历相关的函数\n\n\njava.text\n包含了一些java格式化相关的类\n\n\njava.sql\n包含了java进行JDBC数据库编程的相关类/接口\n\n\njava.awt\n包含了构成抽象窗口工具集（abstract window toolkits）的多个类，这些类被用来构建和管理应用程序的图形用户界面(GUI)\n\n\n\n\n继承 (inheritance)多个类中存在相同属性和行为时，将这些内容抽取到单独一个类中，那么多个类无需再定义这些属性和行为，只要继承那个类即可。\n    此处的多个类称为子类(派生类)，单独的这个类称为父类(基类或超类)。可以理解为:“子类 is a 父类”\n// 类继承语法规则:\nclass Subclass extends SuperClass{ }\n作用：    继承的出现减少了代码冗余，提高了代码的复用性。    继承的出现，更有利于功能的扩展。    继承的出现让类与类之间产生了关系，提供了多态的前提。\n\n子类继承了父类，就继承了父类的方法和属性。\n在子类中，可以使用父类中定义的方法和属性，也可以创建新的数据和方法。\n在Java 中，继承的关键字用的是“extends”，即子类不是父类的子集，而是对父类的“扩展”。\n\n关于继承的规则：\n\n子类不能直接访问父类中私有的(private)的成员变量和方法。\n⭐Java只支持单继承和多层继承，不允许多重继承\n\n方法的重写 (override/overwrite)定义：在子类中可以根据需要对从父类中继承来的方法进行改造，也称为方法的重置、覆盖。在程序执行时，子类的方法将覆盖父类的方法。\n要求：\n\n子类重写的方法必须和父类被重写的方法具有相同的方法名称、参数列表\n子类重写的方法的返回值类型不能大于父类被重写的方法的返回值类型\n子类重写的方法使用的访问权限不能小于父类被重写的方法的访问权限    子类不能重写父类中声明为private权限的方法\n子类方法抛出的异常不能大于父类被重写方法的异常\n子类与父类中同名同参数的方法必须同时声明为非static的(即为重写)，或者同时声明为static的（不是重写）。因为static方法是属于类的，子类无法覆盖父类的方法。\n\n子类继承父类\n    若子类重写了父类方法，就意味着子类里定义的方法彻底覆盖了父类里的同名方法，系统将不可能把父类里的方法转移到子类中。    对于实例变量则不存在这样的现象，即使子类里定义了与父类完全相同的实例变量，这个实例变量依然不可能覆盖父类中定义的实例变量\n关键字—super在Java类中使用super来调用父类中的指定操作：\n\nsuper可用于访问父类中定义的属性\nsuper可用于调用父类中定义的成员方法\nsuper可用于在子类构造器中调用父类的构造器\n尤其当子父类出现同名成员时，可以用super表明调用的是父类中的成员\nsuper的追溯不仅限于直接父类\nsuper和this的用法相像，this代表本类对象的引用，super代表父类的内存空间的标识\n\n调用父类的构造器\n    子类中所有的构造器默认都会访问父类中空参数的构造器    当父类中没有空参数的构造器时，子类的构造器必须通过this(参数列表)或者super(参数列表)语句指定调用本类或者父类中相应的构造器。同时，只能”二选一”，且必须放在构造器的首行    如果子类构造器中既未显式调用父类或本类的构造器，且父类中又没有无参的构造器，则编译出错\n\n\n\n\nNo.\n区别点\nthis\nsuper\n\n\n\n\n1\n访问属性\n访问本类中的属性，如果本类没有此属性则从父类中继续查找\n直接访问父类中的属性\n\n\n2\n调用方法\n访问本类中的方法，如果本类没有此方法则从父类中继续查找\n直接访问父类中的方法\n\n\n3\n调用构造器\n调用本类构造器，必须放在构造器的首行\n调用父类构造器，必须放在子类构造器的首行\n\n\n\n\n子类对象的实例化过程\n多态性对象的多态性：父类的引用指向子类的对象\nJava引用变量有两个类型：\n\n编译时类型：由声明该变量时使用的类型决定。\n运行时类型：由实际赋给该变量的对象决定。\n\n\n编译时，看左边；运行时，看右边。\n若编译时类型和运行时类型不一致，就出现了对象的多态性(Polymorphism)\n\n\n\n\n\n\n\n\n\n\n\n多态作用\n    提高了代码的通用性，常称作接口重用\n\n\n前提\n    需要存在继承或者实现关系    有方法的重写\n\n\n成员方法\n    编译时：要查看引用变量所声明的类中是否有所调用的方法。    运行时：调用实际new的对象所属的类中的重写方法。\n\n\n成员变量\n不具备多态性，只看引用变量所声明的类。\n\n\n\n\n对象的多态：在Java中,子类的对象可以替代父类的对象使用    一个变量只能有一种确定的数据类型    一个引用类型变量可能指向(引用)多种不同类型的对象\n\n子类可看做是特殊的父类，所以父类类型的引用可以指向子类的对象：向上转型(upcasting)。\n\n一个引用类型变量如果声明为父类的类型，但实际引用的是子类对象，那么该变量就==不能==再访问子类中添加的属性和方法\n虚拟方法调用\n子类中定义了与父类同名同参数的方法，在多态情况下，将此时父类的方法称为虚拟方法，父类根据赋给它的不同子类对象，动态调用属于子类的该方法。这样的方法调用在编译期是无法确定的。\ninstanceof运算符x instanceof A：检验x是否为类A的对象，返回值为boolean型。\n对象类型转换\n    从子类到父类的类型转换可以自动进行    从父类到子类的类型转换必须通过造型(强制类型转换)实现    无继承关系的引用类型间的转换是非法的    在造型前可以使用instanceof操作符测试一个对象的类型\n对象类型转换 (Casting )基本数据类型的Casting：\n\n自动类型转换：小的数据类型可以自动转换成大的数据类型如long g=20; double d=12.0f\n强制类型转换：可以把大的数据类型强制转换(casting)成小的数据类型如 float f=(float)12.0; int a=(int)1200L    对Java对象的强制类型转换称为造型    从子类到父类的类型转换可以自动进行    从父类到子类的类型转换必须通过造型(强制类型转换)实现    无继承关系的引用类型间的转换是非法的    在造型前可以使用instanceof操作符测试一个对象的类型\n\n方法的重载与重写从编译和运行的角度看：重载，是指允许存在多个同名方法，而这些方法的参数不同。编译器根据方法不同的参数表，对同名方法的名称做修饰。对于编译器而言，这些同名方法就成了不同的方法。它们的调用地址在编译期就绑定了。Java的重载是可以包括父类和子类的，即子类可以重载父类的同名不同参数的方法。所以：对于重载而言，在方法调用之前，编译器就已经确定了所要调用的方法，这称为“早绑定”或“静态绑定”；而对于多态，只有等到方法调用的那一刻，解释运行器才会确定所要调用的具体方法，这称为“晚绑定”或“动态绑定”。引用一句Bruce Eckel的话：“不要犯傻，如果它不是晚绑定，它就不是多态。”\n Object类的结构与方法Object类是所有Java类的根父类    如果在类的声明中未使用extends关键字指明其父类，则默认父类为java.lang.Object类\npublic class Person { ...\n}\n//等价于\npublic class Person extends Object { ...\n}\n\n\n\n\n方法名称\n类型\n描述\n\n\n\n\npublic Object()\n构造\n构造器\n\n\npublic boolean equals(Object obj)\n普通\n对象比较\n\n\npublic int hashCode()\n普通\n取得Hash码\n\n\npublic String toString()\n普通\n对象打印时调用\n\n\n\n\n==操作符与equals方法基本类型比较值:只要两个变量的值相等，即为true。\n引用类型比较引用(是否指向同一个对象)：只有指向同一个对象时，==才返回true。\n用“==”进行比较时，符号两边的数据类型必须兼容(可自动转换的基本数据类型除外)，否则编译出错\nequals()：所有类都继承了Object，也就获得了equals()方法。还可以重写。    只能比较引用类型，其作用与“==”相同,比较是否指向同一个对象。    格式:obj1.equals(obj2)    特例：当用equals()方法进行比较时，对类File、String、Date及包装类（Wrapper Class）来说，是比较类型及内容而不考虑引用的是否是同一个对象；    原因：在这些类中重写了Object类的equals()方法。    当自定义使用equals()时，可以重写。用于比较两个对象的“内容”是否都相等\n重写equals()方法的原则    对称性：如果x.equals(y)返回是“true”，那么y.equals(x)也应该返回是 “true”。    自反性：x.equals(x)必须返回是“true”。    传递性：如果x.equals(y)返回是“true”，而且y.equals(z)返回是“true”，那么z.equals(x)也应该返回是“true”。    一致性：如果x.equals(y)返回是“true”，只要x和y内容一直不变，不管你重复x.equals(y)多少次，返回都是“true”。    任何情况下，x.equals(null)，永远返回是“false”；x.equals(和x不同类型的对象)永远返回是“false”。\n⭐面试题：==和equals的区别\n== 既可以比较基本类型也可以比较引用类型。对于基本类型就是比较值，对于引用类型就是比较内存地址\nequals的话，它是属于java.lang.Object类里面的方法，如果该方法没有被重写过默认也是==;我们可以看到String等类的equals方法是被重写过的，而且String类在日常开发中用的比较多，久而久之，形成了equals是比较值的错误观点。\n具体要看自定义类里有没有重写Object的equals方法来判断。\n通常情况下，重写equals方法，会比较类中的相应属性是否都相等。\n\ntoString() 方法toString()方法在Object类中定义，其返回值是String类型，返回类名和它的引用地址。在进行String与其它类型数据的连接操作时，自动调用toString()方法Date now=new Date();\nSystem.out.println(“now=”+now); //相当于\nSystem.out.println(“now=”+now.toString());\n可以根据需要在用户自定义类型中重写toString()方法\n//如String 类重写了toString()方法，返回字符串的值。\ns1=“hello”;\nSystem.out.println(s1);//相当于System.out.println(s1.toString());\n    基本类型数据转换为String类型时，调用了对应包装类的toString()方法    int a=10; System.out.println(“a=”+a);\n包装类的使用    针对八种基本数据类型定义相应的引用类型—包装类（封装类）\nByte、Short、Integer、Long、Float、Double、Boolean、Character\n基本数据类型包装成包装类的实例 —-装箱通过包装类的构造器实现：\nint i = 500;   Integer t = new Integer(i);\nint i = new Integer(“12”);\n通过字符串参数构造包装类对象：\nFloat f = new Float(“4.56”);\nLong l = new Long(“asdf”);  //NumberFormatException\n获得包装类对象中包装的基本类型变量 —-拆箱调用包装类的.xxxValue()方法：\nboolean b = bObj.booleanValue();\n通过包装类的parseXxx(String s)静态方法：\n// 字符串转包装类\nFloat f = Float.parseFloat(“12.1”);\n\n// 基本数据类型包装成包装类的实例 ---装箱\n// 通过包装类的构造器实现：\nint i = 500;   Integer t = new Integer(i);\n// 通过字符串参数构造包装类对象：\nFloat f = new Float(“4.56”);\nLong l = new Long(“asdf”);  //NumberFormatException\n\n//获得包装类对象中包装的基本类型变量 ---拆箱\n// 调用包装类的.xxxValue()方法：\nboolean b = bObj.booleanValue();\n\n// 字符串转换成基本数据类型\n// 通过包装类的构造器实现：\nint i = new Integer(“12”);\n// 通过包装类的parseXxx(String s)静态方法：\nFloat f = Float.parseFloat(“12.1”);\n\n// 基本数据类型转换成字符串\n// 调用字符串重载的valueOf()方法：\nString fstr = String.valueOf(2.34f);\n//更直接的方式：\nString intStr = 5 + “”\n关键字—static当我们编写一个类时，其实就是在描述其对象的属性和行为，而并没有产生实质上的对象，==只有通过new关键字才会产生出对象==，这时系统才会分配内存空间给对象，其方法才可以供外部调用。我们有时候希望无论是否产生了对象或无论产生了多少对象的情况下，==某些特定的数据在内存空间里只有一份==。\n\n类属性作为该类==各个对象之间共享的变量==。在设计类时,分析哪些属性不因对象的不同而改变，将这些属性设置为类属性。相应的方法设置为类方法。\n如果方法与调用者无关，则这样的方法通常被声明为类方法，由于不需要创建对象就可以调用类方法，从而简化了方法的调用。\n\n    使用范围：在Java类中，可用static修饰属性、方法、代码块、内部类\n被修饰后的成员具备以下特点：    随着类的加载而加载    优先于对象存在    修饰的成员，被所有对象所共享    访问权限允许时，可不创建对象，直接被类调用\n匿名对象\n不定义对象的句柄，而直接调用这个对象的方法。这样的对象叫做匿名对象。\nnew Person().shout();\n类变量(class Variable)类变量（类属性）由该类的所有实例共享\npublic static int total = 0;\n\nPerson.total = 100; // 不用创建对象就可以访问静态成员 \n//访问方式：类名.类属性，类名.类方法\n类方法(class method)没有对象的实例时，可以用类名.方法名()的形式访问由static修饰的类方法。    在static方法内部只能访问类的static修饰的属性或方法，不能访问类的非static的结构。    在static方法中不能有this，也不能有super\n理解main方法的语法​        由于Java虚拟机需要调用类的main()方法，所以该方法的访问权限必须是public，又因为Java虚拟机在执行main()方法时不必创建对象，所以该方法必须是static的，该方法接收一个String类型的数组参数，该数组中保存执行Java命令时传递给所运行的类的参数。\n​        又因为main() 方法是静态的，我们不能直接访问该类中的非静态成员，必须创建该类的一个实例对象后，才能通过这个对象去访问类中的非静态成员，这种情况，我们在之前的例子中多次碰到。\n关键字—final\nfinal标记的类不能被继承。提高安全性，提高程序的可读性。String类、System类、StringBuffer类\nfinal标记的方法不能被子类重写。Object类中的getClass()。\n    final标记的变量(成员变量或局部变量)即称为常量。名称大写，且只能被赋值一次。\n\n\n\n\n\nfinal\n在类、变量和方法时表示“最终的”\n\n\n\n\n类不能被继承\n提高安全性，提高程序的可读性。\n\n\n方法不能被子类重写\n    比如：Object类中的getClass()\n\n\nfinal标记的变量\n名称大写，且只能被赋值一次\n\n\n\n\n// final修饰类\nfinal class A{\n}\n// final修饰方法\nclass A {\n    public final void print() {\n        System . out . println(\"A\");\n    }\n}\n//final修饰变量——常量\nprivate final String INFO = \"atguigu\";  //声明常量\n抽象类与抽象方法—abstract类的设计应该保证父类和子类能够共享特征。有时将一个父类设计得非常抽象，以至于它没有具体的实例，这样的类叫做抽象类。\n用abstract关键字来修饰一个类，这个类叫做==抽象类==。用abstract来修饰一个方法，该方法叫做==抽象方法==。    抽象方法：只有方法的声明，没有方法的实现。以分号结束：比如：public abstract void talk();    含有抽象方法的类必须被声明为抽象类。    抽象类不能被实例化。抽象类是用来被继承的，抽象类的子类必须重写父类的抽象方法，并提供方法体。若没有重写全部的抽象方法，仍为抽象类。    不能用abstract修饰变量、代码块、构造器；    不能用abstract修饰私有方法、静态方法、final的方法、final的类。\n关键字—interface接口(interface)是抽象方法和常量值定义的集合。接口的特点：    用interface来定义。    接口中的所有成员变量都默认是由public static final修饰的。    接口中的所有抽象方法都默认是由public abstract修饰的。    接口中没有构造器。    接口采用多继承机制。\npublic interface Runner {\n    public static final int ID = 1; \n    public abstract void start(); \n    public abstract void run(); \n    public abstract void stop();\n}\n定义Java类的语法格式：先写extends，后写implements    class SubClass extends SuperClass implements InterfaceA{ }一个类可以实现多个接口，接口也可以继承其它接口。    实现接口的类中必须提供接口中所有方法的具体实现内容，方可实例化。否则，仍为抽象类。    接口的主要用途就是被实现类实现。（面向接口编程）    与继承关系类似，接口与实现类之间存在多态性    接口和类是并列关系，或者可以理解为一种特殊的类。从本质上讲，接口是一种特殊的抽象类，这种抽象类中只包含常量和方法的定义，(JDK7.0及之前)，而没有变量和方法的实现。\n\n一个类可以实现多个无关的接口\n与继承关系类似，接口与实现类之间存在多态性\n\n\n\n\n\n区别点\n抽象类\n接口\n\n\n\n\n定义\n包含抽象方法的类\n主要是抽象方法和全局常量的集合\n\n\n组成\n构造方法、抽象方法、普通方法、常量、变量\n常量、抽象方法、(jdk8.0:默认方法、静态方法)\n\n\n使用\n子类继承抽象类(extends)\n子类实现接口(implements)\n\n\n关系\n抽象类可以实现多个接口\n接口不能继承抽象类，但允许继承多个接口\n\n\n常见设计模式\n模板方法\n简单工厂、工厂方法、代理模式\n\n\n对象\n都通过对象的多态性产生实例化对象\n\n\n\n局限\n抽象类有单继承的局限\n接口没有此局限\n\n\n实际\n作为一个模板\n是作为一个标准或是表示一种能力\n\n\n选择\n如果抽象类和接口都可以使用的话，\n优先使用接口，因为避免单继承的局限\n\n\n\n\n在开发中，常看到一个类不是去继承一个已经实现好的类，而是要么继承抽象类，要么实现接口。\nJava 8中关于接口的改进Java 8中，你可以为接口添加静态方法和默认方法。从技术角度来说，这是完全合法的，只是它看起来违反了接口作为一个抽象定义的理念。静态方法：使用 static 关键字修饰。可以通过接口直接调用静态方法，并执行其方法体。我们经常在相互一起使用的类中使用静态方法。你可以在标准库中找到像Collection/Collections或者Path/Paths这样成对的接口和类。默认方法：默认方法使用 default 关键字修饰。可以通过实现类对象来调用。我们在已有的接口中提供新方法的同时，还保持了与旧版本代码的兼容性。比如：java 8 API中对Collection、List、Comparator等接口提供了丰富的默认方法。\n接口中的默认方法\n    若一个接口中定义了一个默认方法，而另外一个接口中也定义了一个同名同参数的方法（不管此方法是否是默认方法），在实现类同时实现了这两个接口时，会出现：接口冲突。    解决办法：实现类必须覆盖接口中同名同参数的方法，来解决冲突。\n若一个接口中定义了一个默认方法，而父类中也定义了一个同名同参数的非抽象方法，则不会出现冲突问题。因为此时遵守：类优先原则。接口中具有相同名称和参数的默认方法会被忽略。\n内部类当一个事物的内部，还有一个部分需要一个完整的结构进行描述，而这个内部的完整的结构又只为外部事物提供服务，那么整个内部的完整结构最好使用内部类。\n    Inner class一般用在定义它的类或语句块之内，在外部引用它时必须给出完整的名称。    Inner class的名字不能与包含它的外部类类名相同；\n\n成员内部类（static成员内部类和非static成员内部类）\n\n局部内部类（不谈修饰符）、匿名内部类\n\n\n成员内部类作为类的成员的角色：    和外部类不同，Inner class还可以声明为private或protected；    可以调用外部类的结构    Inner class 可以声明为static的，但此时就不能再使用外层类的非static的成员变量；成员内部类作为类的角色：    可以在内部定义属性、方法、构造器等结构    可以声明为abstract类 ，因此可以被其它的内部类继承    可以声明为final的    编译以后生成OuterClass$InnerClass.class字节码文件（也适用于局部内部类）\n注意：\n\n非static的成员内部类中的成员不能声明为static的，只有在外部类或static的成员内部类中才可声明static成员。\n外部类访问成员内部类的成员，需要“内部类.成员”或“内部类对象.成员”的方式\n成员内部类可以直接使用外部类的所有成员，包括私有的数据\n当想要在外部类的静态成员部分使用内部类时，可以考虑内部类声明为静态的\n\nclass外部类{\n    方法(){\n    \tclass局部内部类{\n    \t}\n\t}\n    {\n        class局部内部类{\n\t\t}\n    }\n}\n\n如何使用局部内部类\n\n    只能在声明它的方法或代码块中使用，而且是先声明后使用。除此之外的任何地方都不能使用该类    但是它的对象可以通过外部方法的返回值返回使用，返回值类型只能是局部内部类的父类或父接口类型\n\n局部内部类的特点\n\n    内部类仍然是一个独立的类，在编译之后内部类会被编译成独立的.class文件，但是前面冠以外部类的类名和$符号，以及数字编号。    只能在声明它的方法或代码块中使用，而且是先声明后使用。除此之外的任何地方都不能使用该类。    局部内部类可以使用外部类的成员，包括私有的。    局部内部类可以使用外部方法的局部变量，但是必须是final的。由局部内部类和局部变量的声明周期不同所致。    局部内部类和局部变量地位类似，不能使用public,protected,缺省,private    局部内部类不能使用static修饰，因此也不能包含静态成员\n匿名内部类\n    匿名内部类不能定义任何静态成员、方法和类，只能创建匿名内部类的一个实例。一个匿名内部类一定是在new的后面，用其隐含实现一个接口或实现一个类。\nnew 父类构造器（实参列表）|实现接口(){\n    //匿名内部类的类体部分\n}\n匿名内部类的特点    匿名内部类必须继承父类或实现接口    匿名内部类只能有一个对象    匿名内部类对象只能使用多态形式引用\n关键字—native使用 native 关键字说明这个方法是==原生函数==，也就是这个方法是用 ==C/C++==等非Java 语言实现的，并且被编译成了 DLL，由 java 去调用。 \n（1）为什么要用 native 方法java 使用起来非常方便，然而有些层次的任务用 java 实现起来不容易，或者我们对程序的效率很在意时，问题就来了。例如：有时 java 应用需要与 java 外面的环境交互。这是本地方法存在的主要原因，你可以想想 java 需要与一些底层系统如操作系统或某些硬件交换信息时的情况。本地方法正是这样一种交流机制：它为我们提供了一个非常简洁的接口，而且我们无需去了解 java 应用之外的繁琐的细节。（2）native 声明的方法，对于调用者，可以当做和其他 Java 方法一样使用一个native method 方法可以返回任何 java 类型，包括非基本类型，而且同样可以进行异常控制。native method 的存在并不会对其他类调用这些本地方法产生任何影响，实际上调用这些方法的其他类甚至不知道它所调用的是一个本地方法。JVM 将控制调用本地方法的所有细节。如果一个含有本地方法的类被继承，子类会继承这个本地方法并且可以用 java语言重写这个方法（如果需要的话）。 \n","slug":"J1-对象与类","date":"2021-11-05T06:23:36.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"30c2d1a91e3f3491d3662bdaaf39ffa5","title":"Leetcode：1~100","content":"1. 两数之和给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。\n你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。\n你可以按任意顺序返回答案。\n\n\n\n\n\n\n\n\n\n输入：nums = [2,7,11,15], target = 9输出：[0,1]解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。\n\n\n\n\n\n\n\n暴力解法\n双重for循环\n\n\n// Java\nclass Solution {\n    public int[] twoSum(int[] nums, int target) {\n        for(int i = 0;i&lt;nums.length;i++) { \n            for(int j = i+1;j&lt;nums.length;j++) {\n                if(nums[i] + nums[j] == target) {\n                    return new int[] {i,j};\n                }\n            }\n        }\n        return new int[] {0};\n    }\n}\n\n\n\n\n\n\n\n 哈希表\n\n\n// Java\nclass Solution {\n    public int[] twoSum(int[] nums, int target) {\n        Map&lt;Integer, Integer&gt; hashTable = new HashMap&lt;Integer, Integer&gt;();\n        for (int i = 0; i &lt; nums.length; i++) {\n            if(hashTable.containsKey(target - nums[i])) {\n                return new int[]{hashTable.get(target - nums[i]),i};\n            }\n            hashTable.put(nums[i],i);\n        }\n        return new int[] {0};\n\n    }\n}\n# Python\nclass Solution(object):\n    def twoSum(self, nums, target):\n        \"\"\"\n        :type nums: List[int]\n        :type target: int\n        :rtype: List[int]\n        \"\"\"\n        d = {}\n        for i in range(len(nums)):\n            r = target-nums[i]\n            if r in d:\n                return [d[r],i]\n            else:\n                d[nums[i]]=i\n2. 两数相加给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。\n请你将两个数相加，并以相同形式返回一个表示和的链表。\n你可以假设除了数字 0 之外，这两个数都不会以 0 开头。\n\n\n\n\n\n\n\n\n\n输入：l1 = [2,4,3], l2 = [5,6,4]输出：[7,0,8]解释：342 + 465 = 807.\n\n\n\n\n\n\n\n模拟\n\n\n/**\n * Definition for singly-linked list.\n * public class ListNode {\n *     int val;\n *     ListNode next;\n *     ListNode() {}\n *     ListNode(int val) { this.val = val; }\n *     ListNode(int val, ListNode next) { this.val = val; this.next = next; }\n * }\n */\nclass Solution {\n    public ListNode addTwoNumbers(ListNode l1, ListNode l2) {\n        ListNode ans = new ListNode();\n        ListNode a = ans;\n        int t = 0;\n        while(l1 != null || l2 != null){\n            if(l1 != null) {\n                t += l1.val;\n                l1 = l1.next;\n            }\n            if(l2 != null) {\n                t += l2.val;\n                l2 = l2.next;\n            }\n            a.next = new ListNode(t%10);\n            a = a.next;\n            t = t/10;\n        }\n        if(t != 0) a.next = new ListNode(t);\n        return ans.next;\n\n    }\n}\n3. 无重复字符的最长子串给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。\n\n\n\n\n\n\n\n\n\n输入: s = “abcabcbb”输出: 3解释: 因为无重复字符的最长子串是 “abc”，所以其长度为 3。\n\n\n\n\n\n\n\n双指针 \n\n设置左指针初值-1，右指针初值0，int数组对区间(l,r]出现过的字母计数\nfor循环对右指针指向的字母计数\n如果计数&lt;=1,更新答案\n如果计数&gt;1,左指针向右移，直到计数=1\n\n\n\n// 数组计数\nclass Solution {\n    public int lengthOfLongestSubstring(String s) {\n        int a[] = new int[1000];\n        int l = -1, r = 0;\n        int maxl = 0;\n        for(r = 0; r&lt; s.length();r++){\n            a[(int)s.charAt(r)]+=1;     // 计数\n            if(a[(int)s.charAt(r)]&lt;=1){ // 不重复更新答案\n                maxl = maxl&gt; (r-l) ? maxl: r-l; \n            }else{                      //重复左指针向右移，直到不重复\n                while(a[(int)s.charAt(r)]&gt;1){\n                    l+=1;\n                    a[(int)s.charAt(l)]-=1;\n                }\n            }\n        }\n        return maxl;\n    }\n    \n}\n// 哈希集合\nclass Solution {\n    public int lengthOfLongestSubstring(String s) {\n        Set&lt;Character&gt; occ = new HashSet&lt;Character&gt;();\n        int len = s.length();\n        int l = -1, r = 0;\n        int maxl = 0;\n        for(r = 0; r&lt; s.length();r++){\n            while(occ.contains(s.charAt(r))) {\n                occ.remove(s.charAt(++l));\n            }\n            occ.add(s.charAt(r));\n            maxl = Math.max(maxl,r-l);\n        }\n        return maxl;\n    }\n    \n}\n4. 寻找两个正序数组的中位数给定两个大小分别为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。请你找出并返回这两个正序数组的 中位数 。\n\n\n\n\n\n\n\n\n\n输入：nums1 = [1,3], nums2 = [2]输出：2.00000解释：合并数组 = [1,2,3] ，中位数 2\n\n\n\n\n\n\n\nsort\n将两数组合在一起，排序后直接输出中位数。\n\n\nclass Solution {\n    public double findMedianSortedArrays(int[] nums1, int[] nums2) {\n        int l1 = nums1.length, l2 = nums2.length;\n        int[] nums = new int[l1+l2];\n        for(int i = 0;i &lt; l1; i++) {\n            nums[i] = nums1[i];\n        }\n        for(int i = 0; i &lt; l2; i++) {\n            nums[l1+i] = nums2[i];\n        }\n        Arrays.sort(nums);\n\n        if((l1 + l2) % 2 == 1) {\n            return (double)nums[(l1 + l2) / 2];\n        }else{\n            return (double)(nums[(l1 + l2) / 2]+nums[(l1 + l2) / 2 - 1]) / 2.0;\n        }\n    }\n}\n\n\n\n\n\n\n\n 递归\n🚹\n\n\n:heart\n","slug":"l01","date":"2021-11-04T12:06:25.000Z","categories_index":"LeetCode","tags_index":"哈希","author_index":"YFR718"},{"id":"07213b6ca5497f88ac351468e0325797","title":"Hexo 个人博客搭建","content":"Hexo 个人博客搭建安装步骤\ngithub创建yfr718.github.io项目\n\n安装git、设置用户名和邮箱、ssh连接github\ngit config --global user.name &quot;你的GitHub用户名&quot;\ngit config --global user.email &quot;你的GitHub注册邮箱&quot;\nssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;\n#打开GitHub_Settings_keys 页面，新建new SSH Key\n#Title为标题，任意填即可，将刚刚复制的id_rsa.pub内容粘贴进去，最后点击Add SSH key。在Git Bash中检测GitHub公钥设置是否成功，输入 \nssh git@github.com\n\n\n\n安装Node.js\n# 安装后，检测Node.js是否安装成功，在命令行中输入 \nnode -v\n\n安装Hexo\n# 在电脑常里创建一个文件夹，可以命名为Blog，Hexo框架与以后你自己发布的网页都在这个文件夹中。\n# 在相应文件夹安装Hexo\nnpm install -g hexo-cli \n# 初始化博客\nhexo init blog\n# 查看博客网页\nhexo g #生成网页\nhexo s #运行网页\n# 完成后，打开浏览器输入地址：\nlocalhost:4000\n\n\nHexo 常用命令博客管理npm install hexo -g # 安装Hexo\nnpm update hexo -g # 升级\nhexo init # 初始化博客\nhexo clean # 清除缓存文件 db.json 和已生成的静态文件 public\nhexo g # 自动生成网站静态文件，并部署到设定的仓库\nhexo s # 启动本地服务器，用于预览主题。默认地址： http:&#x2F;&#x2F;localhost:4000&#x2F;\nhexo d # 自动生成网站静态文件，并部署到设定的仓库\nhexo clean &amp;&amp; hexo g &amp;&amp; hexo d # 本地更新后重新部署到github上\n文章管理# 新建文章\nhexo new 文章文件名\n一些markdown格式\n\n\n\n\n\n\n\n\n123456\n\n\n\n\n\n\n提示\nNormal Tips Container\n\n\n\n\n\n\n\n\n注意\nWarning!!!\n\n\n\n\n\n\n\n\naaa\nWarning!!!\n\n\n\n\n\n\n\n\n特别注意\nDanger!!!\n\n\nClick to see more\n\n隐藏内容\n\n\n\n引用块\n\n","slug":"0.Hexo个人博客搭建","date":"2021-11-02T09:47:00.000Z","categories_index":"前端","tags_index":"hexo","author_index":"YFR718"},{"id":"9b69dffe0262764ac691bc95bc415c22","title":"Python基础","content":"\n\n\n\n\n\n\n\n\n天池Python训练营\nPython基础1.变量、运算符与数据类型1.1 注释# 表示注释，作用于整行。\n‘’’ ‘’’ 或者 “”” “”” 表示区间注释，在三引号之间的所有内容被注释\n# 这是一个注释\nprint(\"Hello world\")\n'''\n这是多行注释，用三个单引号\n'''\n\"\"\"\n这是多行注释，用三个双引号\n\"\"\"\n1.2 运算符\n\n\n\n运算符\n\n\n\n\n\n\n算术运算符\n+, -, , /, //, %, *\n\n\n\n比较运算符\n&gt;, &gt;=, &lt;, &lt;=, !=\n\n\n\n逻辑运算符\nand, or, not\n\n\n\n位运算符\n~, &amp;, \\\n, ^, &lt;&lt;, &gt;&gt;\n\n\n\n三元运算符\nx if x&lt;y else y\n\n\n\n其他运算符\nis, not is, in, not in\n\n\n\n\n注意\n\nis, is not 对比的是两个变量的内存地址：假如比较的两个变量，指向的都是地址不可变的类型（str等），那么is，is not 和 ==，！= 是完全等价的。\n==, != 对比的是两个变量的值：假如对比的两个变量，指向的是地址可变的类型（list，dict，tuple等），则两者是有区别的。\n\n运算符的优先级\n\n一元运算符优于二元运算符。如正负号。\n先算术运算，后移位运算，最后位运算。例如 1 &lt;&lt; 3 + 2 &amp; 7等价于 (1 &lt;&lt; (3 + 2)) &amp; 7\n逻辑运算最后结合\n\n位运算原码、反码和补码\n二进制有三种不同的表示形式：原码、反码和补码，计算机内部使用补码来表示。\n原码：就是其二进制表示（注意，有一位符号位）。\n反码：正数的反码就是原码，负数的反码是符号位不变，其余位取反（对应正数按位取反）。\n补码：正数的补码就是原码，负数的补码是反码+1。\n符号位：最高位为符号位，0表示正数，1表示负数。在位运算中符号位也参与运算。利用位运算实现快速计算\n\n通过 &lt;&lt; ， &gt;&gt; 快速计算2的倍数问题。\n\n通过 ^ 快速交换两个整数。\na ^= b\nb ^= a\na ^= b\n\n通过 a &amp; (-a) 快速获取a 的最后为 1 位置的整数。\n\n\n利用位运算实现整数集合\n\n一个数的二进制表示可以看作是一个集合（0 表示不在集合中，1 表示在集合中）。比如集合 {1, 3, 4, 8} ，可以表示成 01 00 01 10 10 而对应的位运算也就可以看作是对集合进行的操作。\n\n元素与集合的操作：\na | (1&lt;&lt;i) -> 把 i 插入到集合中\na &amp; ~(1&lt;&lt;i) -> 把 i 从集合中删除\na &amp; (1&lt;&lt;i) -> 判断 i 是否属于该集合（零不属于，非零属于）\n\n集合之间的操作：\na 补 -> ~a\na 交 b -> a &amp; b\na 并 b -> a | b\na 差 b -> a &amp; (~b)\n\n\n1.3 变量和赋值\n在使用变量之前，需要对其先赋值。\n变量名可以包括字母、数字、下划线、但变量名不能以数字开头。\nPython 变量名是大小写敏感的，foo != Foo。\n\nfirst = 2\nsecond = 3\nthird = first + second\nprint(third) # 5\n1.4 数据类型与转换Python3 中有六个标准的数据类型：\n\nNumber（数字）：int、float、bool、complex（复数）。\nString（字符串）\nList（列表）\nTuple（元组）\nSet（集合）\nDictionary（字典）\n\nPython3 的六个标准数据类型中：\n\n不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）；\n可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。\n\n简单数据类型\n\n整型\n浮点型\n布尔型\n\n容器数据类型\n\n列表\n元组\n字典\n集合\n字符串\n\n1.5 print() 函数print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False)\n\n将对象以字符串表示的方式格式化输出到流文件对象file里。其中所有非关键字参数都按str() 方式进行转换为字符串输出；\n关键字参数sep 是实现分隔符，比如多个参数输出时想要输出中间的分隔字符；\n关键字参数end 是输出结束时的字符，默认是换行符\\n ；\n关键字参数file 是定义流输出的文件，可以是标准的系统输出sys.stdout ，也可以重定义为别的文件；\n关键字参数flush 是立即把内容输出到流文件，不作缓存。\n\n【例子】， item 值与’another string’ 两个值之间用sep 设置的参数&amp; 分割。由于end 参数没有设置，因此默认是输出解释后换行，即end 参数的默认值为\\n 。\nshoplist = ['apple', 'mango', 'carrot', 'banana']\nprint(\"This is printed with 'sep='&amp;''.\")\nfor item in shoplist:\nprint(item, 'another string', sep='&amp;')\n# This is printed with 'sep='&amp;''.\n# apple&amp;another string\n# mango&amp;another string\n# carrot&amp;another string\n# banana&amp;another string\n2 逻辑语句\n\n\n\n语句类型\n\n\n\n\n\n\n判断\nif…elif…else\n\n\n\nwhile循环\nwhile…else…\n\n\n\nfor循环\nfor 迭代变量 in 可迭代对象…else…\n\n\n\n其他\nbreak、continue、pass\n\n\n\n\n# 当while 循环正常执行完的情况下，执行else 输出，如果while 循环中执行了跳出循环的语句，比如 break ，将不执行else 代码块的内容。\ncount = 0\nwhile count &lt; 5:\n    print(\"%d is less than 5\" % count)\n    count = count + 1\nelse:\n\tprint(\"%d is not less than 5\" % count)\nfor i in 'ILoveLSGO':\n\tprint(i, end=' ') # 不换行输出\n# I L o v e L S G O\n\nfor i in range(len(member)):\n\tprint(member[i])\n\t\ndic = &#123;'a': 1, 'b': 2, 'c': 3, 'd': 4&#125;\nfor key, value in dic.items():\n\tprint(key, value, sep=':', end=' ')\n# a:1 b:2 c:3 d:4\n\ndic = &#123;'a': 1, 'b': 2, 'c': 3, 'd': 4&#125;\nfor key in dic.keys():\n\tprint(key, end=' ')\n# a b c d\n\nfor num in range(10, 20): # 迭代 10 到 20 之间的数字\n    for i in range(2, num): # 根据因子迭代\n        if num % i == 0: # 确定第一个因子\n            j = num / i # 计算第二个因子\n            print('%d 等于 %d * %d' % (num, i, j))\n            break # 跳出当前循环\n    else: # 循环的 else 部分\n    \tprint(num, '是一个质数')\n# 10 等于 2 * 5\n# 11 是一个质数\n# 12 等于 2 * 6\n# 13 是一个质数\n# 14 等于 2 * 7\n# 15 等于 3 * 5\n# 16 等于 2 * 8\n# 17 是一个质数\n# 18 等于 2 * 9\n# 19 是一个质数\n\nbreak 语句可以跳出当前所在层的循环。\ncontinue 终止本轮循环并开始下一轮循环。\npass 语句的意思是“不做任何事”，如果你在需要有语句的地方不写任何语句，那么解释器会提示出错，而 pass 语句就是用来解决这些问题的。\n\n2.1assert 关键词assert 这个关键词我们称之为“断言”，当这个关键词后边的条件为 False 时，程序自动崩溃并抛出AssertionError 的异常。\nmy_list = ['lsgogroup']\nmy_list.pop(0)\nassert len(my_list) > 0\n2.2 range() 函数range([start,] stop[, step=1])\n\nfor i in range(2, 9): # 不包含9\n\tprint(i)\n\n这个BIF（Built-in functions）有三个参数，其中用中括号括起来的两个表示这两个参数是可选的。\nstep=1 表示第三个参数的默认值是1。\nrange 这个BIF的作用是生成一个从start 参数的值开始到stop 参数的值结束的数字序列，该序列包含start 的值但不包含stop 的值。\n\n2.3 enumerate()函数enumerate(sequence, [start=0])\n\nsequence — 一个序列、迭代器或其他支持迭代对象。\nstart — 下标起始位置。\n返回 enumerate(枚举) 对象\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\nlst = list(enumerate(seasons))\nprint(lst)\n# [(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]\n\nlst = list(enumerate(seasons, start=1)) # 下标从 1 开始\nprint(lst)\n# [(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]\nenumerate() 与 for 循环的结合使用\nfor i, a in enumerate(A)\n\tdo something with a\n用 enumerate(A) 不仅返回了 A 中的元素，还顺便给该元素一个索引值 (默认从 0 开始)。此外，用enumerate(A, j) 还可以确定索引起始值为 j 。\n2.4 推导式列表推导式[ expr for value in collection [if condition] ]\n\nx = [-4, -2, 0, 2, 4]\ny = [a * 2 for a in x]\nprint(y)\n# [-8, -4, 0, 4, 8]\n\nx = [i ** 2 for i in range(1, 10)]\nprint(x)\n# [1, 4, 9, 16, 25, 36, 49, 64, 81]\n\nx = [(i, i ** 2) for i in range(6)]\nprint(x)\n# [(0, 0), (1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n\nx = [i for i in range(100) if (i % 2) != 0 and (i % 3) == 0]\nprint(x)\n# [3, 9, 15, 21, 27, 33, 39, 45, 51, 57, 63, 69, 75, 81, 87, 93, 99]\n\na = [(i, j) for i in range(0, 3) for j in range(0, 3)]\nprint(a)\n# [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n\nx = [[i, j] for i in range(0, 3) for j in range(0, 3)]\nprint(x)\n# [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]]\nx[0][0] = 10\nprint(x)\n# [[10, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]]\n\na = [(i, j) for i in range(0, 3) if i &lt; 1 for j in range(0, 3) if j > 1]\nprint(a)\n# [(0, 2)]\n元组推导式( expr for value in collection [if condition] )\n\na = (x for x in range(10))\nprint(a)\n# &lt;generator object &lt;genexpr> at 0x0000025BE511CC48>\nprint(tuple(a))\n# (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n字典推导式&#123; key_expr: value_expr for value in collection [if condition] &#125;\n\nb = &#123;i: i % 2 == 0 for i in range(10) if i % 3 == 0&#125;\nprint(b)\n# &#123;0: True, 3: False, 6: True, 9: False&#125;\n集合推导式&#123; expr for value in collection [if condition] &#125;\n\nc = &#123;i for i in [1, 2, 3, 4, 5, 5, 6, 4, 3, 2, 1]&#125;\nprint(c)\n# &#123;1, 2, 3, 4, 5, 6&#125;\n3 异常处理异常就是运行期检测到的错误。计算机语言针对可能出现的错误定义了异常类型，某种错误引发对应的异常时，异常处理程序将被启动，从而恢复程序的正常运行。\n3.1 Python 标准异常总结\nBaseException：所有异常的 基类\nException：常规异常的 基类\nStandardError：所有的内建标准异常的基类\nArithmeticError：所有数值计算异常的基类\nFloatingPointError：浮点计算异常\nOverflowError：数值运算超出最大限制\nZeroDivisionError：除数为零\nAssertionError：断言语句（assert）失败\nAttributeError：尝试访问未知的对象属性\nEOFError：没有内建输入，到达EOF标记\nEnvironmentError：操作系统异常的基类\nIOError：输入/输出操作失败\nOSError：操作系统产生的异常（例如打开一个不存在的文件）\nWindowsError：系统调用失败\nImportError：导入模块失败的时候\nKeyboardInterrupt：用户中断执行\nLookupError：无效数据查询的基类\nIndexError：索引超出序列的范围\nKeyError：字典中查找一个不存在的关键字\nMemoryError：内存溢出（可通过删除对象释放内存）\nNameError：尝试访问一个不存在的变量\nUnboundLocalError：访问未初始化的本地变量\nReferenceError：弱引用试图访问已经垃圾回收了的对象\nRuntimeError：一般的运行时异常\nNotImplementedError：尚未实现的方法\nSyntaxError：语法错误导致的异常\nIndentationError：缩进错误导致的异常\nTabError：Tab和空格混用\nSystemError：一般的解释器系统异常\nTypeError：不同类型间的无效操作\nValueError：传入无效的参数\nUnicodeError：Unicode相关的异常\nUnicodeDecodeError：Unicode解码时的异常\nUnicodeEncodeError：Unicode编码错误导致的异常\nUnicodeTranslateError：Unicode转换错误导致的异常\n\n异常体系内部有层次关系，Python异常体系中的部分关系如下所示：\n\n3.2 Python标准警告总结\nWarning：警告的基类\nDeprecationWarning：关于被弃用的特征的警告\nFutureWarning：关于构造将来语义会有改变的警告\nUserWarning：用户代码生成的警告\nPendingDeprecationWarning：关于特性将会被废弃的警告\nRuntimeWarning：可疑的运行时行为(runtime behavior)的警告\nSyntaxWarning：可疑语法的警告\nImportWarning：用于在导入模块过程中触发的警告\nUnicodeWarning：与Unicode相关的警告\nBytesWarning：与字节或字节码相关的警告\nResourceWarning：与资源使用相关的警告\n\n3.3 异常处理语句try - excepttry:\n\t检测范围\nexcept Exception[as reason]:\n\t出现异常后的处理代码\ntry 语句按照如下方式工作：\n\n首先，执行try 子句（在关键字try 和关键字except 之间的语句）\n如果没有异常发生，忽略except 子句， try 子句执行后结束。\n如果在执行try 子句的过程中发生了异常，那么try 子句余下的部分将被忽略。如果异常的类型和except 之后的名称相符，那么对应的except 子句将被执行。最后执行try 语句之后的代码。\n如果一个异常没有与任何的except 匹配，那么这个异常将会传递给上层的try 中。\n\ntry:\n    int(\"abc\")\n    s = 1 + '1'\n    f = open('test.txt')\n    print(f.read())\n    f.close()\nexcept OSError as error:\n\tprint('打开文件出错\\n原因是：' + str(error))\nexcept TypeError as error:\n\tprint('类型出错\\n原因是：' + str(error))\nexcept ValueError as error:\n\tprint('数值出错\\n原因是：' + str(error))\n# 数值出错\n# 原因是：invalid literal for int() with base 10: 'abc'\n\ndict1 = &#123;'a': 1, 'b': 2, 'v': 22&#125;\ntry:\n\tx = dict1['y']\nexcept KeyError:\n\tprint('键错误')\nexcept LookupError:\n\tprint('查询错误')\nelse:\n\tprint(x)\n# 键错误\ntry-except-else 语句尝试查询不在dict 中的键值对，从而引发了异常。这一异常准确地说应属于KeyError ，但由于KeyError 是LookupError 的子类，且将LookupError 置于KeyError 之前，因此程序优先执行该except 代码块。所以，使用多个except 代码块时，必须坚持对其规范排序，要从最具针对性的异常到最通用的异常。\ntry:\n    s = 1 + '1'\n    int(\"abc\")\n    f = open('test.txt')\n    print(f.read())\n    f.close()\nexcept (OSError, TypeError, ValueError) as error:\n\tprint('出错了！\\n原因是：' + str(error))\n# 出错了！\n# 原因是：unsupported operand type(s) for +: 'int' and 'str'\n一个 except 子句可以同时处理多个异常，这些异常将被放在一个括号里成为一个元组。\ntry - except - finallytry:\n\t检测范围\nexcept Exception[as reason]:\n\t出现异常后的处理代码\nfinally:\n\t无论如何都会被执行的代码\n不管try 子句里面有没有发生异常， finally 子句都会执行。如果一个异常在try 子句里被抛出，而又没有任何的except 把它截住，那么这个异常会在finally 子句执行后被抛出。\ntry - except - else如果在try 子句执行时没有发生异常，Python将执行else 语句后的语句。\ntry:\n\t检测范围\nexcept(Exception1[, Exception2[,...ExceptionN]]]):\n\t发生以上多个异常中的一个，执行这块代码\nelse:\n\t如果没有异常执行这块代码\nraise语句Python 使用raise 语句抛出一个指定的异常。\ntry:\n\traise NameError('HiThere')\nexcept NameError:\n\tprint('An exception flew by!')\n# An exception flew by!\n4 列表 list4.1 列表的定义列表是有序集合，没有固定大小，能够保存任意数量任意类型的 Python 对象，语法为 [元素1, 元素2, …, 元素n] 。\n\n关键点是「中括号 []」和「逗号 ,」\n中括号 把所有元素绑在一起\n逗号 将每个元素一一分开\n\n列表的创建\n\n直接定义\n利用range() 创建列表\n利用推导式创建列表\n\nx = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n\nx = list(range(10))\n\nx = [i for i in range(100) if (i % 2) != 0 and (i % 3) == 0]\n\nx = [[0 for col in range(3)] for row in range(4)]\n由于list的元素可以是任何对象，因此列表中所保存的是对象的指针。即使保存一个简单的[1,2,3] ，也有3个指针和3个整数对象。x = [a] * 4 操作中，只是创建4个指向list的引用，所以一旦a 改变， x 中4个a 也会随之改变。\n4.2 列表的操作添加元素\nlist.append(obj) 在列表末尾添加新的对象，只接受一个参数，参数可以是任何数据类型，被追加的元素在 list中保持着原结构类型。\nlist.extend(seq) 在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）\nlist.insert(index, obj) 在编号 index 位置前插入 obj 。\n\nx = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\nx.append('Thursday')\nx.extend(['Thursday', 'Sunday'])\nx.insert(2, 'Sunday')\n删除元素\nlist.remove(obj) 移除列表中某个值的第一个匹配项\nlist.pop([index=-1]) 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值\ndel var1[, var2 ……] 删除单个或多个对象。\n\nx = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\nx.remove('Monday')\ny = x.pop()\ndel x[0:2]\n获取列表中的元素\n通过元素的索引值，从列表获取单个元素，注意，列表索引值是从0开始的。\n通过将索引指定为-1，可让Python返回最后一个列表元素，索引 -2 返回倒数第二个列表元素，以此类推。\n切片的通用写法是 start : stop : step\n\nx = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\nprint(x[3:]) # ['Thursday', 'Friday']\nprint(x[-3:]) # ['Wednesday', 'Thursday', 'Friday']\n列表的常用操作符\n等号操作符： ==\n连接操作符 +\n重复操作符 *\n成员关系操作符 in 、not in\n\n「等号 ==」，只有成员、成员位置都相同时才返回True。和元组拼接一样， 列表拼接也有两种方式，用「加号 +」和「乘号 *」，前者首尾拼接，后者复制拼接。\nlist1 = [123, 456]\nlist2 = [456, 123]\nlist3 = [123, 456]\nprint(list1 == list2) # False\nprint(list1 == list3) # True\n\nlist4 = list1 + list2 # extend()\nprint(list4) # [123, 456, 456, 123]\n\nlist5 = list3 * 3\nprint(list5) # [123, 456, 123, 456, 123, 456]\n\nlist3 *= 3\nprint(list3) # [123, 456, 123, 456, 123, 456]\n\nprint(123 in list3) # True\nprint(456 not in list3) # False\n前面三种方法（ append , extend , insert ）可对列表增加元素，它们没有返回值，是直接修改了原数据对象。将两个list相加，需要创建新的 list 对象，从而需要消耗额外的内存，特别是当 list 较大时，尽量不要使用 “+” 来添加list。\n列表的其它方法\n\n\n\nlist方法\n\n\n\n\n\nlist.count(obj)\n统计某个元素在列表中出现的次数\n\n\nlist.index(x[, start[, end]])\n从列表中找出某个值第一个匹配项的索引位置\n\n\nlist.reverse()\n反向列表中元素\n\n\nlist.sort(key=None, reverse=False)\n对原列表进行排序。\n\n\n\n\n\n\n\n# 获取列表的第二个元素\ndef takeSecond(elem):\nreturn elem[1]\nx = [(2, 2), (3, 4), (4, 1), (1, 3)]\nx.sort(key=takeSecond)\nprint(x)\n# [(4, 1), (2, 2), (1, 3), (3, 4)]\nx.sort(key=lambda a: a[0])\nprint(x)\n# [(1, 3), (2, 2), (3, 4), (4, 1)]\n5 元组「元组」定义语法为： (元素1, 元素2, …, 元素n)\n\n小括号把所有元素绑在一起\n逗号将每个元素一一分开\n\n5.1 创建和访问一个元组\nPython 的元组与列表类似，不同之处在于tuple被创建后就不能对其进行修改，类似字符串。\n元组使用小括号，列表使用方括号。\n\nt1 = (1, 10.31, 'python')\nt2 = 1, 10.31, 'python'\nprint(t1, type(t1))\n# (1, 10.31, 'python') &lt;class 'tuple'>\nprint(t2, type(t2))\n# (1, 10.31, 'python') &lt;class 'tuple'>\n\ntuple1 = (1, 2, 3, 4, 5, 6, 7, 8)\nprint(tuple1[1]) # 2\nprint(tuple1[5:]) # (6, 7, 8)\nprint(tuple1[:5]) # (1, 2, 3, 4, 5)\ntuple2 = tuple1[:]\nprint(tuple2) # (1, 2, 3, 4, 5, 6, 7, 8)\n\n创建元组可以用小括号 ()，也可以什么都不用，为了可读性，建议还是用 ()。\n元组中只包含一个元素时，需要在元素后面添加逗号，否则括号会被当作运算符使用：\n\n5.2 更新和删除一个元组元组有不可更改 (immutable) 的性质，因此不能直接给元组的元素赋值，但是只要元组中的元素可更改 (mutable)，那么我们可以直接更改其元素，注意这跟赋值其元素不同。\nweek = ('Monday', 'Tuesday', 'Thursday', 'Friday')\nweek = week[:2] + ('Wednesday',) + week[2:]\nprint(week) # ('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday')\n\nt1 = (1, 2, 3, [4, 5, 6])\nprint(t1) # (1, 2, 3, [4, 5, 6])\nt1[3][0] = 9\nprint(t1) # (1, 2, 3, [9, 5, 6])\n\n5.3 元组相关的操作符\n比较操作符\n逻辑操作符\n连接操作符 +\n重复操作符 *\n成员关系操作符 in 、not in\n\nt1 = (2, 3, 4, 5)\nt2 = ('老马的程序人生', '小马的程序人生')\nt3 = t1 + t2\nprint(t3)\n# (2, 3, 4, 5, '老马的程序人生', '小马的程序人生')\nt4 = t2 * 2\nprint(t4)\n# ('老马的程序人生', '小马的程序人生', '老马的程序人生', '小马的程序人生')\n5.4 内置方法\n.count() 是记录在元组  中该元素出现几次\n.index() 是找到该元素在元组的索引\n\nt = (1, 10.31, 'python')\nprint(t.count('python')) # 1\nprint(t.index(10.31)) # 1\n5.5 解压元组t = (1, 10.31, 'python')\n(a, b, c) = t\nprint(a, b, c)\n# 1 10.31 python\n\nt = (1, 10.31, ('OK', 'python'))\n(a, b, (c, d)) = t\nprint(a, b, c, d)\n# 1 10.31 OK python\n\n# 如果你只想要元组其中几个元素，用通配符「*」，英文叫 wildcard，在计算机语言中代表一个或多个元素。下例就是把多个元素丢给了 rest 变量。\nt = 1, 2, 3, 4, 5\na, b, *rest, c = t\nprint(a, b, c) # 1 2 5\nprint(rest) # [3, 4]\n\n# 如果你根本不在乎 rest 变量，那么就用通配符「*」加上下划线「_」。\na, b, *_ = t\nprint(a, b) # 1 2\n6 字符串6.1 字符串的定义\nPython 中字符串被定义为引号之间的字符集合。\nPython 支持使用成对的 单引号 或 双引号\n\nt1 = 'i love Python!'\nprint(t1, type(t1))\n# i love Python! &lt;class 'str'>\nt2 = \"I love Python!\"\nprint(t2, type(t2))\n# I love Python! &lt;class 'str'>\nprint(5 + 8) # 13\nprint('5' + '8') # 58\nPython 的常用转义字符\n\n\n\n\n转移字符\n描述\n\n\n\n\n\\\\\\\\\n反斜杠\n\n\n\\\\’\n单引号\n\n\n\\\\”\n双引号\n\n\n\\\\n\n换行\n\n\n\\\\t\nTAB\n\n\n\\\\r\n回车\n\n\n\n\n原始字符串只需要在字符串前边加一个英文字母 r 即可。\n6.2 字符串的切片与拼接\n类似于元组具有不可修改性\n从 0 开始 (和 C 一样)\n切片通常写成 start:end 这种形式，包括「start 索引」对应的元素，不包括「end 索引」对应的元素。\n索引值可正可负，正索引从 0 开始，从左往右；负索引从 -1 开始，从右往左。使用负数索引时，会从最后一个元素开始计数。最后一个元素的位置编号是 -1。\n\nstr1 = 'I Love LsgoGroup'\nprint(str1[:6]) # I Love\nprint(str1[5]) # e\nprint(str1[:6] + \" 插入的字符串 \" + str1[6:])\n# I Love 插入的字符串 LsgoGroup\ns = 'Python'\nprint(s) # Python\nprint(s[2:4]) # th\nprint(s[-5:-2]) # yth\nprint(s[2]) # t\nprint(s[-1]) # n\n6.3 字符串的常用内置方法\ncapitalize() 将字符串的第一个字符转换为大写。\n\nstr2 = 'xiaoxie'\nprint(str2.capitalize()) # Xiaoxie\n\nlower() 转换字符串中所有大写字符为小写。\nupper() 转换字符串中的小写字母为大写。\nswapcase() 将字符串中大写转换为小写，小写转换为大写。\n\nstr2 = \"DAXIExiaoxie\"\nprint(str2.lower()) # daxiexiaoxie\nprint(str2.upper()) # DAXIEXIAOXIE\nprint(str2.swapcase()) # daxieXIAOXIE\n\ncount(str, beg= 0,end=len(string)) 返回str 在 string 里面出现的次数，如果beg 或者end 指定则返回指定范围内str 出现的次数。\n\nstr2 = \"DAXIExiaoxie\"\nprint(str2.count('xi')) # 2\n\nendswith(suffix, beg=0, end=len(string)) 检查字符串是否以指定子字符串 suffix 结束，如果是，返回True,否则返回 False。如果 beg 和 end 指定值，则在指定范围内检查。\nstartswith(substr, beg=0,end=len(string)) 检查字符串是否以指定子字符串 substr 开头，如果是，返回True，否则返回 False。如果 beg 和 end 指定值，则在指定范围内检查。\n\nstr2 = \"DAXIExiaoxie\"\nprint(str2.endswith('ie')) # True\nprint(str2.endswith('xi')) # False\nprint(str2.startswith('Da')) # False\nprint(str2.startswith('DA')) # True\n\nfind(str, beg=0, end=len(string)) 检测 str 是否包含在字符串中，如果指定范围 beg 和 end ，则检查是否包含在指定范围内，如果包含，返回开始的索引值，否则返回 -1。\nrfind(str, beg=0,end=len(string)) 类似于 find() 函数，不过是从右边开始查找。\n\nstr2 = \"DAXIExiaoxie\"\nprint(str2.find('xi')) # 5\nprint(str2.find('ix')) # -1\nprint(str2.rfind('xi')) # 9\n\nisnumeric() 如果字符串中只包含数字字符，则返回 True，否则返回 False。\n\nstr3 = '12345'\nprint(str3.isnumeric()) # True\nstr3 += 'a'\nprint(str3.isnumeric()) # False\n\nljust(width[, fillchar]) 返回一个原字符串左对齐，并使用fillchar （默认空格）填充至长度width 的新字符串。\nrjust(width[, fillchar]) 返回一个原字符串右对齐，并使用fillchar （默认空格）填充至长度width 的新字符串。\n\nstr4 = '1101'\nprint(str4.ljust(8, '0')) # 11010000\nprint(str4.rjust(8, '0')) # 00001101\n\nlstrip([chars]) 截掉字符串左边的空格或指定字符。\nrstrip([chars]) 删除字符串末尾的空格或指定字符。\nstrip([chars]) 在字符串上执行lstrip() 和rstrip() 。\n\nstr5 = ' I Love LsgoGroup '\nprint(str5.lstrip()) # 'I Love LsgoGroup '\nprint(str5.lstrip().strip('I')) # ' Love LsgoGroup '\nprint(str5.rstrip()) # ' I Love LsgoGroup'\nprint(str5.strip()) # 'I Love LsgoGroup'\nprint(str5.strip().strip('p')) # 'I Love LsgoGrou'\n\npartition(sub) 找到子字符串sub，把字符串分为一个三元组(pre_sub,sub,fol_sub) ，如果字符串中不包含sub则返回(‘原字符串’,’’,’’) 。\nrpartition(sub) 类似于partition() 方法，不过是从右边开始查找。\n\nstr5 = ' I Love LsgoGroup '\nprint(str5.strip().partition('o')) # ('I L', 'o', 've LsgoGroup')\nprint(str5.strip().partition('m')) # ('I Love LsgoGroup', '', '')\nprint(str5.strip().rpartition('o')) # ('I Love LsgoGr', 'o', 'up')\n\nreplace(old, new [, max]) 把 将字符串中的old 替换成new ，如果max 指定，则替换不超过max 次。\n\nstr5 = ' I Love LsgoGroup '\nprint(str5.strip().replace('I', 'We')) # We Love LsgoGroup\n\nsplit(str=””, num) 不带参数默认是以空格为分隔符切片字符串，如果num 参数有设置，则仅分隔num 个子字符串，返回切片后的子字符串拼接的列表。\n\nstr5 = ' I Love LsgoGroup '\nprint(str5.strip().split()) # ['I', 'Love', 'LsgoGroup']\nprint(str5.strip().split('o')) # ['I L', 've Lsg', 'Gr', 'up']\n\nu = \"www.baidu.com.cn\"\n# 分割两次\nprint(u.split(\".\", 2)) # ['www', 'baidu', 'com.cn']\n\nsplitlines([keepends]) 按照行(‘\\r’, ‘\\r\\n’, \\n’)分隔，返回一个包含各行作为元素的列表，如果参数keepends 为False，不包含换行符，如果为 True，则保留换行符。\n\nstr6 = 'I \\n Love \\n LsgoGroup'\nprint(str6.splitlines()) # ['I ', ' Love ', ' LsgoGroup']\nprint(str6.splitlines(True)) # ['I \\n', ' Love \\n', ' LsgoGroup']\n\nmaketrans(intab, outtab) 创建字符映射的转换表，第一个参数是字符串，表示需要转换的字符，第二个参数也是字符串表示转换的目标。\ntranslate(table, deletechars=””) 根据参数table 给出的表，转换字符串的字符，要过滤掉的字符放到deletechars 参数中。\n\nstr = 'this is string example....wow!!!'\nintab = 'aeiou'\nouttab = '12345'\ntrantab = str.maketrans(intab, outtab)\nprint(trantab) # &#123;97: 49, 111: 52, 117: 53, 101: 50, 105: 51&#125;\nprint(str.translate(trantab)) # th3s 3s str3ng 2x1mpl2....w4w!!!\n6.4 字符串格式化format 格式化函数\nstr = \"&#123;0&#125; Love &#123;1&#125;\".format('I', 'Lsgogroup') # 位置参数\nprint(str) # I Love Lsgogroup\nstr = \"&#123;a&#125; Love &#123;b&#125;\".format(a='I', b='Lsgogroup') # 关键字参数\nprint(str) # I Love Lsgogroup\nstr = \"&#123;0&#125; Love &#123;b&#125;\".format('I', b='Lsgogroup') # 位置参数要在关键字参数之前\nprint(str) # I Love Lsgogroup\nstr = '&#123;0:.2f&#125;&#123;1&#125;'.format(27.658, 'GB') # 保留小数点后两位\nprint(str) # 27.66GB\nPython 字符串格式化符号\n\n\n\n\n符号\n\n\n\n\n\n%c\n格式化字符及其ASCII码\n\n\n%s\n格式化字符串，用str()方法处理对象\n\n\n%r\n格式化字符串，用rper()方法处理对象\n\n\n%d\n格式化整数\n\n\n%o\n格式化无符号八进制数\n\n\n%x\n格式化无符号十六进制数\n\n\n%X\n格式化无符号十六进制数（大写）\n\n\n%f\n格式化浮点数字，可指定小数点后的精度\n\n\n%e\n用科学计数法格式化浮点数\n\n\n%E\n作用同%e，用科学计数法格式化浮点数\n\n\n%g\n根据值的大小决定使用%f或%e\n\n\n%G\n作用同%g，根据值的大小决定使用%f或%E\n\n\n\n\nprint('%c' % 97) # a\nprint('%c %c %c' % (97, 98, 99)) # a b c\nprint('%d + %d = %d' % (4, 5, 9)) # 4 + 5 = 9\nprint(\"我叫 %s 今年 %d 岁!\" % ('小明', 10)) # 我叫 小明 今年 10 岁!\nprint('%o' % 10) # 12\nprint('%x' % 10) # a\nprint('%X' % 10) # A\nprint('%f' % 27.658) # 27.658000\nprint('%e' % 27.658) # 2.765800e+01\nprint('%E' % 27.658) # 2.765800E+01\nprint('%g' % 27.658) # 27.658\ntext = \"I am %d years old.\" % 22\nprint(\"I said: %s.\" % text) # I said: I am 22 years old..\nprint(\"I said: %r.\" % text) # I said: 'I am 22 years old.'\n格式化操作符辅助指令\n\n\n\n\n符号\n功能\n\n\n\n\nm.n\nm 是显示的最小总宽度,n 是小数点后的位数(如果可用的话)\n\n\n-\n用做左对齐\n\n\n+\n在正数前面显示加号( + )\n\n\n#\n在八进制数前面显示零(‘0’)，在十六进制前面显示’0x’或者’0X’(取决于用的是’x’还是’X’)\n\n\n0\n显示的数字前面填充’0’而不是默认的空格\n\n\n\n\nprint('%5.1f' % 27.658) # ' 27.7'\nprint('%.2e' % 27.658) # 2.77e+01\nprint('%10d' % 10) # ' 10'\nprint('%-10d' % 10) # '10 '\nprint('%+d' % 10) # +10\nprint('%#o' % 10) # 0o12\nprint('%#x' % 108) # 0x6c\nprint('%010d' % 5) # 0000000005\n7 字典7.1 可变类型与不可变类型\n序列是以连续的整数为索引，与此不同的是，字典以”关键字”为索引，关键字可以是任意==不可变类型==，通常用字符串或数值。\n字典是 Python 唯一的一个 映射类型，字符串、元组、列表属于序列类型。\n\n那么如何快速判断一个数据类型 X 是不是可变类型的呢？两种方法：\n\n麻烦方法：用 id(X) 函数，对 X 进行某种操作，比较操作前后的 id ，如果不一样，则 X 不可变，如果一样，则X 可变。\n便捷方法：用 hash(X) ，只要不报错，证明 X 可被哈希，即不可变，反过来不可被哈希，即可变。\n\ni &#x3D; 1\nprint(id(i)) # 140732167000896\ni &#x3D; i + 2\nprint(id(i)) # 140732167000960\nl &#x3D; [1, 2]\nprint(id(l)) # 4300825160\nl.append(&#39;Python&#39;)\nprint(id(l)) # 4300825160\n\n整数 i 在加 1 之后的 id 和之前不一样，因此加完之后的这个 i (虽然名字没变)，但不是加之前的那个 i 了，因此整数是不可变类型。\n列表 l 在附加 ‘Python’ 之后的 id 和之前一样，因此列表是可变类型。\n\nprint(hash('Name')) # -9215951442099718823\nprint(hash((1, 2, 'Python'))) # 823362308207799471\nprint(hash([1, 2, 'Python']))\n# TypeError: unhashable type: 'list'\nprint(hash(&#123;1, 2, 3&#125;))\n# TypeError: unhashable type: 'set'\n\n数值、字符和元组 都能被哈希，因此它们是不可变类型。\n列表、集合、字典不能被哈希，因此它是可变类型。\n\n7.2 字典的定义字典 是无序的 键:值（ key:value ）对集合，键必须是互不相同的（在同一个字典之内）。\n\ndict 内部存放的顺序和 key 放入的顺序是没有关系的。\ndict 查找和插入的速度极快，不会随着 key 的增加而增加，但是需要占用大量的内存。\n\n字典 定义语法为 {元素1, 元素2, …, 元素n}\n\n其中每一个元素是一个「键值对」— 键:值 ( key:value )\n关键点是「大括号 {}」,「逗号 ,」和「冒号 :」\n大括号 — 把所有元素绑在一起\n逗号 — 将每个键值对分开\n冒号 — 将键和值分开\n\n7.3 创建和访问字典通过字符串或数值作为key 来创建字典。\ndic = &#123;'李宁': '一切皆有可能', '耐克': 'Just do it', '阿迪达斯': 'Impossible is nothing'&#125;\nprint('耐克的口号是:', dic['耐克'])\n# 耐克的口号是: Just do it\n注意：如果我们取的键在字典中不存在，会直接报错KeyError 。\n通过元组作为key 来创建字典，但一般不这样使用。\ndic = &#123;(1, 2, 3): \"Tom\", \"Age\": 12, 3: [3, 5, 7]&#125;\nprint(dic) # &#123;(1, 2, 3): 'Tom', 'Age': 12, 3: [3, 5, 7]&#125;\nprint(type(dic)) # &lt;class 'dict'>\n通过构造函数dict 来创建字典。\ndict() -&gt; 创建一个空的字典。\ndic = dict()\ndic['a'] = 1\ndic['b'] = 2\ndic['c'] = 3\nprint(dic)\n# &#123;'a': 1, 'b': 2, 'c': 3&#125;\n7.4 字典的内置方法dict.fromkeys(seq[, value]) 用于创建一个新字典，以序列 seq 中元素做字典的键， value 为字典所有键对应的初始值。\nseq = ('name', 'age', 'sex')\ndic1 = dict.fromkeys(seq)\nprint(\"新的字典为 : %s\" % str(dic1))\n# 新的字典为 : &#123;'name': None, 'age': None, 'sex': None&#125;\ndic2 = dict.fromkeys(seq, 10)\nprint(\"新的字典为 : %s\" % str(dic2))\n# 新的字典为 : &#123;'name': 10, 'age': 10, 'sex': 10&#125;\ndic3 = dict.fromkeys(seq, ('小马', '8', '男'))\nprint(\"新的字典为 : %s\" % str(dic3))\n# 新的字典为 : &#123;'name': ('小马', '8', '男'), 'age': ('小马', '8', '男'), 'sex': ('小马', '8', '男')&#125;\ndict.keys() 返回一个可迭代对象，可以使用 list() 来转换为列表，列表为字典中的所有键。\ndic = &#123;'Name': 'lsgogroup', 'Age': 7&#125;\nprint(dic.keys()) # dict_keys(['Name', 'Age'])\nlst = list(dic.keys()) # 转换为列表\nprint(lst) # ['Name', 'Age']\ndict.values() 返回一个迭代器，可以使用 list() 来转换为列表，列表为字典中的所有值。\ndic = &#123;'Sex': 'female', 'Age': 7, 'Name': 'Zara'&#125;\nprint(\"字典所有值为 : \", list(dic.values()))\n# 字典所有值为 : [7, 'female', 'Zara']\ndict.items() 以列表返回可遍历的 (键, 值) 元组数组。\ndic = &#123;'Name': 'Lsgogroup', 'Age': 7&#125;\nprint(\"Value : %s\" % dic.items())\n# Value : dict_items([('Name', 'Lsgogroup'), ('Age', 7)])\nprint(tuple(dic.items()))\n# (('Name', 'Lsgogroup'), ('Age', 7))\ndict.get(key, default=None) 返回指定键的值，如果值不在字典中返回默认值。\ndic = &#123;'Name': 'Lsgogroup', 'Age': 27&#125;\nprint(\"Age 值为 : %s\" % dic.get('Age')) # Age 值为 : 27\nprint(\"Sex 值为 : %s\" % dic.get('Sex', \"NA\")) # Sex 值为 : NA\ndict.setdefault(key, default=None) 和get() 方法 类似, 如果键不存在于字典中，将会添加键并将值设为默认值。\ndic = &#123;'Name': 'Lsgogroup', 'Age': 7&#125;\nprint(\"Age 键的值为 : %s\" % dic.setdefault('Age', None)) # Age 键的值为 : 7\nprint(\"Sex 键的值为 : %s\" % dic.setdefault('Sex', None)) # Sex 键的值为 : None\nprint(\"新字典为：\", dic)\n# 新字典为： &#123;'Age': 7, 'Name': 'Lsgogroup', 'Sex': None&#125;\nkey in dict  in操作符用于判断键是否存在于字典中，如果键在字典 dict 里返回true ，否则返回false 。而not in 操作符刚好相反，如果键在字典 dict 里返回false ，否则返回true 。\ndic = &#123;'Name': 'Lsgogroup', 'Age': 7&#125;\n# in 检测键 Age 是否存在\nif 'Age' in dic:\n\tprint(\"键 Age 存在\")\nelse:\n\tprint(\"键 Age 不存在\")\n# 检测键 Sex 是否存在 \nif 'Sex' in dic:\n\tprint(\"键 Sex 存在\")\nelse:\n\tprint(\"键 Sex 不存在\")\n# not in 检测键 Age 是否存在\nif 'Age' not in dic:\n\tprint(\"键 Age 不存在\")\nelse:\n\tprint(\"键 Age 存在\")\n# 键 Age 存在\n# 键 Sex 不存在\n# 键 Age 存在\ndict.pop(key[,default]) 删除字典给定键 key 所对应的值，返回值为被删除的值。key 值必须给出。若key不存在，则返回 default 值。\ndel dict[key] 删除字典给定键 key 所对应的值。\ndic1 = &#123;1: \"a\", 2: [1, 2]&#125;\nprint(dic1.pop(1), dic1) # a &#123;2: [1, 2]&#125;\n# 设置默认值，必须添加，否则报错\nprint(dic1.pop(3, \"nokey\"), dic1) # nokey &#123;2: [1, 2]&#125;\ndel dic1[2]\nprint(dic1) # &#123;&#125; \ndict.popitem() 随机返回并删除字典中的一对键和值，如果字典已经为空，却调用了此方法，就报出KeyError异常。\ndic1 = &#123;1: \"a\", 2: [1, 2]&#125;\nprint(dic1.popitem()) # (1, 'a')\nprint(dic1) # &#123;2: [1, 2]&#125;\ndict.clear() 用于删除字典内所有元素。\ndic = &#123;'Name': 'Zara', 'Age': 7&#125;\nprint(\"字典长度 : %d\" % len(dic)) # 字典长度 : 2\ndict.clear()\nprint(\"字典删除后长度 : %d\" % len(dic)) # 字典删除后长度 : 0\ndict.copy() 返回一个字典的浅复制。\ndic1 = &#123;'Name': 'Lsgogroup', 'Age': 7, 'Class': 'First'&#125;\ndic2 = dic1.copy()\nprint(\"新复制的字典为 : \", dic2)\n# 新复制的字典为 : &#123;'Age': 7, 'Name': 'Lsgogroup', 'Class': 'First'&#125;\ndict.update(dict2) 把字典参数 dict2 的 key:value 对 更新到字典 dict 里。\ndic = &#123;'Name': 'Lsgogroup', 'Age': 7&#125;\ndic2 = &#123;'Sex': 'female', 'Age': 8&#125;\ndic.update(dic2)\nprint(\"更新字典 dict : \", dic)\n# 更新字典 dict : &#123;'Sex': 'female', 'Age': 8, 'Name': 'Lsgogroup'&#125;\n8 集合python 中set 与dict 类似，也是一组key 的集合，但不存储value 。由于key 不能重复，所以，在set 中，没有重复的key 。\n注意， key 为不可变类型，即可哈希的值。\n8.1 集合的创建\n先创建对象再加入元素。\n在创建空集合的时候只能使用s = set() ，因为s = {} 创建的是空字典。\n\nbasket = set()\nbasket.add('apple')\nbasket.add('banana')\nprint(basket) # &#123;'banana', 'apple'&#125;\n\n直接把一堆元素用花括号括起来{元素1, 元素2, …, 元素n} 。\n重复元素在set 中会被自动被过滤。\n\nbasket = &#123;'apple', 'orange', 'apple', 'pear', 'orange', 'banana'&#125;\nprint(basket) # &#123;'banana', 'apple', 'pear', 'orange'&#125;\n\n使用set(value) 工厂函数，把列表或元组转换成集合。\n\na = set('abracadabra')\nprint(a)\n# &#123;'r', 'b', 'd', 'c', 'a'&#125;\nb = set((\"Google\", \"Lsgogroup\", \"Taobao\", \"Taobao\"))\nprint(b)\n# &#123;'Taobao', 'Lsgogroup', 'Google'&#125;\nc = set([\"Google\", \"Lsgogroup\", \"Taobao\", \"Google\"])\nprint(c)\n# &#123;'Taobao', 'Lsgogroup', 'Google'&#125;\n由于 set 存储的是无序集合，所以我们不可以为集合创建索引或执行切片(slice)操作，也没有键(keys)可用来获取集合中元素的值，但是可以判断一个元素是否在集合中。\n8.2 访问集合中的值\n可以使用len() 內建函数得到集合的大小。\n\nthisset = set(['Google', 'Baidu', 'Taobao'])\nprint(len(thisset)) # 3\n\n可以使用for 把集合中的数据一个个读取出来。\n\nthisset = set(['Google', 'Baidu', 'Taobao'])\nfor item in thisset:\nprint(item)\n# Baidu\n# Google\n# Taobao\n\n可以通过in 或not in 判断一个元素是否在集合中已经存在\n\nthisset = set(['Google', 'Baidu', 'Taobao'])\nprint('Taobao' in thisset) # True\nprint('Facebook' not in thisset) # True\n8.3 集合的内置方法\nset.add(elmnt) 用于给集合添加元素，如果添加的元素在集合中已存在，则不执行任何操作。\n\nfruits = &#123;\"apple\", \"banana\", \"cherry\"&#125;\nfruits.add(\"orange\")\nprint(fruits)\n# &#123;'orange', 'cherry', 'banana', 'apple'&#125;\nfruits.add(\"apple\")\nprint(fruits)\n# &#123;'orange', 'cherry', 'banana', 'apple'&#125;\n\nset.update(set) 用于修改当前集合，可以添加新的元素或集合到当前集合中，如果添加的元素在集合中已存在，则该元素只会出现一次，重复的会忽略。\n\nx = &#123;\"apple\", \"banana\", \"cherry\"&#125;\ny = &#123;\"google\", \"baidu\", \"apple\"&#125;\nx.update(y)\nprint(x)\n# &#123;'cherry', 'banana', 'apple', 'google', 'baidu'&#125;\ny.update([\"lsgo\", \"dreamtech\"])\nprint(y)\n# &#123;'lsgo', 'baidu', 'dreamtech', 'apple', 'google'&#125;\n\nset.remove(item) 用于移除集合中的指定元素。如果元素不存在，则会发生错误。\n\nfruits = &#123;\"apple\", \"banana\", \"cherry\"&#125;\nfruits.remove(\"banana\")\nprint(fruits) # &#123;'apple', 'cherry'&#125;\n\nset.discard(value) 用于移除指定的集合元素。remove() 方法在移除一个不存在的元素时会发生错误，而discard() 方法不会。\n\nfruits = &#123;\"apple\", \"banana\", \"cherry\"&#125;\nfruits.discard(\"banana\")\nprint(fruits) # &#123;'apple', 'cherry'&#125;\n\nset.pop() 用于随机移除一个元素。\n\nfruits = &#123;\"apple\", \"banana\", \"cherry\"&#125;\nx = fruits.pop()\nprint(fruits) # &#123;'cherry', 'apple'&#125;\nprint(x) # banana\n由于 set 是无序和无重复元素的集合，所以两个或多个 set 可以做数学意义上的集合操作。\n\nset.intersection(set1, set2 …) 返回两个集合的交集。\nset1 &amp; set2 返回两个集合的交集。\nset.intersection_update(set1, set2 …) 交集，在原始的集合上移除不重叠的元素。\n\na = set('abracadabra')\nb = set('alacazam')\nprint(a) # &#123;'r', 'a', 'c', 'b', 'd'&#125;\nprint(b) # &#123;'c', 'a', 'l', 'm', 'z'&#125;\nc = a.intersection(b)\nprint(c) # &#123;'a', 'c'&#125;\nprint(a &amp; b) # &#123;'c', 'a'&#125;\nprint(a) # &#123;'a', 'r', 'c', 'b', 'd'&#125;\na.intersection_update(b)\nprint(a) # &#123;'a', 'c'&#125;\n\nset.union(set1, set2…) 返回两个集合的并集。\nset1 | set2 返回两个集合的并集。\n\na = set('abracadabra')\nb = set('alacazam')\nprint(a) # &#123;'r', 'a', 'c', 'b', 'd'&#125;\nprint(b) # &#123;'c', 'a', 'l', 'm', 'z'&#125;\nprint(a | b) # &#123;'l', 'd', 'm', 'b', 'a', 'r', 'z', 'c'&#125;\nc = a.union(b)\nprint(c) # &#123;'c', 'a', 'd', 'm', 'r', 'b', 'z', 'l'&#125;\n\nset.difference(set) 返回集合的差集。\nset1 - set2 返回集合的差集。\nset.difference_update(set) 集合的差集，直接在原来的集合中移除元素，没有返回值。\n\na = set('abracadabra')\nb = set('alacazam')\nprint(a) # &#123;'r', 'a', 'c', 'b', 'd'&#125;\nprint(b) # &#123;'c', 'a', 'l', 'm', 'z'&#125;\nc = a.difference(b)\nprint(c) # &#123;'b', 'd', 'r'&#125;\nprint(a - b) # &#123;'d', 'b', 'r'&#125;\nprint(a) # &#123;'r', 'd', 'c', 'a', 'b'&#125;\na.difference_update(b)\nprint(a) # &#123;'d', 'r', 'b'&#125;\n\nset.symmetric_difference(set) 返回集合的异或。\nset1 ^ set2 返回集合的异或。\nset.symmetric_difference_update(set) 移除当前集合中在另外一个指定集合相同的元素，并将另外一个指定集合中不同的元素插入到当前集合中。\n\na = set('abracadabra')\nb = set('alacazam')\nprint(a) # &#123;'r', 'a', 'c', 'b', 'd'&#125;\nprint(b) # &#123;'c', 'a', 'l', 'm', 'z'&#125;\nc = a.symmetric_difference(b)\nprint(c) # &#123;'m', 'r', 'l', 'b', 'z', 'd'&#125;\nprint(a ^ b) # &#123;'m', 'r', 'l', 'b', 'z', 'd'&#125;\nprint(a) # &#123;'r', 'd', 'c', 'a', 'b'&#125;\na.symmetric_difference_update(b)\nprint(a) # &#123;'r', 'b', 'm', 'l', 'z', 'd'&#125;\n\nset.issubset(set) 判断集合是不是被其他集合包含，如果是则返回 True，否则返回 False。\nset1 &lt;= set2 判断集合是不是被其他集合包含，如果是则返回 True，否则返回 False。\n\nx = &#123;\"a\", \"b\", \"c\"&#125;\ny = &#123;\"f\", \"e\", \"d\", \"c\", \"b\", \"a\"&#125;\nz = x.issubset(y)\nprint(z) # True\nprint(x &lt;= y) # True\nx = &#123;\"a\", \"b\", \"c\"&#125;\ny = &#123;\"f\", \"e\", \"d\", \"c\", \"b\"&#125;\nz = x.issubset(y)\nprint(z) # False\nprint(x &lt;= y) # False\n\nset.issuperset(set) 用于判断集合是不是包含其他集合，如果是则返回 True，否则返回 False。\nset1 &gt;= set2 判断集合是不是包含其他集合，如果是则返回 True，否则返回 False。\n\nx = &#123;\"f\", \"e\", \"d\", \"c\", \"b\", \"a\"&#125;\ny = &#123;\"a\", \"b\", \"c\"&#125;\nz = x.issuperset(y)\nprint(z) # True\nprint(x >= y) # True\nx = &#123;\"f\", \"e\", \"d\", \"c\", \"b\"&#125;\ny = &#123;\"a\", \"b\", \"c\"&#125;\nz = x.issuperset(y)\nprint(z) # False\nprint(x >= y) # False\n\nset.isdisjoint(set) 用于判断两个集合是不是不相交，如果是返回 True，否则返回 False。\n\nx = &#123;\"f\", \"e\", \"d\", \"c\", \"b\"&#125;\ny = &#123;\"a\", \"b\", \"c\"&#125;\nz = x.isdisjoint(y)\nprint(z) # False\nx = &#123;\"f\", \"e\", \"d\", \"m\", \"g\"&#125;\ny = &#123;\"a\", \"b\", \"c\"&#125;\nz = x.isdisjoint(y)\nprint(z) # True\n8.4 集合的转换se = set(range(4))\nli = list(se)\ntu = tuple(se)\nprint(se, type(se)) # &#123;0, 1, 2, 3&#125; &lt;class 'set'>\nprint(li, type(li)) # [0, 1, 2, 3] &lt;class 'list'>\nprint(tu, type(tu)) # (0, 1, 2, 3) &lt;class 'tuple'>\n8.5 不可变集合Python 提供了不能改变元素的集合的实现版本，即不能增加或删除元素，类型名叫frozenset 。需要注意的是frozenset 仍然可以进行集合操作，只是不能用带有update 的方法。\n\nfrozenset([iterable]) 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。\n\na = frozenset(range(10)) # 生成一个新的不可变集合\nprint(a)\n# frozenset(&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;)\nb = frozenset('lsgogroup')\nprint(b)\n# frozenset(&#123;'g', 's', 'p', 'r', 'u', 'o', 'l'&#125;)\n9 序列针对序列的内置函数\nlist(sub) 把一个可迭代对象转换为列表。\n\na = list()\nprint(a) # []\nb = 'I Love LsgoGroup'\nb = list(b)\nprint(b)\n# ['I', ' ', 'L', 'o', 'v', 'e', ' ', 'L', 's', 'g', 'o', 'G', 'r', 'o', 'u', 'p']\nc = (1, 1, 2, 3, 5, 8)\nc = list(c)\nprint(c) # [1, 1, 2, 3, 5, 8]\n\ntuple(sub) 把一个可迭代对象转换为元组。\n\na = tuple()\nprint(a) # ()\nb = 'I Love LsgoGroup'\nb = tuple(b)\nprint(b)\n# ('I', ' ', 'L', 'o', 'v', 'e', ' ', 'L', 's', 'g', 'o', 'G', 'r', 'o', 'u', 'p')\nc = [1, 1, 2, 3, 5, 8]\nc = tuple(c)\nprint(c) # (1, 1, 2, 3, 5, 8)\n\nstr(obj) 把obj对象转换为字符串\n\na = 123\na = str(a)\nprint(a) # 123\n\nlen(s) 返回对象（字符、列表、元组等）长度或元素个数。a. s — 对象。\n\na = list()\nprint(len(a)) # 0\nb = ('I', ' ', 'L', 'o', 'v', 'e', ' ', 'L', 's', 'g', 'o', 'G', 'r', 'o', 'u', 'p')\nprint(len(b)) # 16\nc = 'I Love LsgoGroup'\nprint(len(c)) # 16\n\nmax(sub) 返回序列或者参数集合中的最大值\n\nprint(max(1, 2, 3, 4, 5)) # 5\nprint(max([-8, 99, 3, 7, 83])) # 99\nprint(max('IloveLsgoGroup')) # v\n\nmin(sub) 返回序列或参数集合中的最小值\n\nprint(min(1, 2, 3, 4, 5)) # 1\nprint(min([-8, 99, 3, 7, 83])) # -8\nprint(min('IloveLsgoGroup')) # G\n\nsum(iterable[, start=0]) 返回序列iterable 与可选参数start 的总和。\n\nprint(sum([1, 3, 5, 7, 9])) # 25\nprint(sum([1, 3, 5, 7, 9], 10)) # 35\nprint(sum((1, 3, 5, 7, 9))) # 25\nprint(sum((1, 3, 5, 7, 9), 20)) # 45\n\nsorted(iterable, key=None, reverse=False) 对所有可迭代的对象进行排序操作。a. iterable — 可迭代对象。  b. key — 主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代  对象中的一个元素来进行排序。  c. reverse — 排序规则， reverse = True 降序 ， reverse = False 升序（默认）。\nd. 返回重新排序的列表。\n\n\nx = [-8, 99, 3, 7, 83]\nprint(sorted(x)) # [-8, 3, 7, 83, 99]\nprint(sorted(x, reverse=True)) # [99, 83, 7, 3, -8]\nt = (&#123;\"age\": 20, \"name\": \"a\"&#125;, &#123;\"age\": 25, \"name\": \"b\"&#125;, &#123;\"age\": 10, \"name\": \"c\"&#125;)\nx = sorted(t, key=lambda a: a[\"age\"])\nprint(x)\n# [&#123;'age': 10, 'name': 'c'&#125;, &#123;'age': 20, 'name': 'a'&#125;, &#123;'age': 25, 'name': 'b'&#125;]\n\nreversed(seq) 函数返回一个反转的迭代器。a. seq — 要转换的序列，可以是 tuple, string, list 或 range。\n\ns = 'lsgogroup'\nx = reversed(s)\nprint(type(x)) # &lt;class 'reversed'>\nprint(x) # &lt;reversed object at 0x000002507E8EC2C8>\nprint(list(x))\n# ['p', 'u', 'o', 'r', 'g', 'o', 'g', 's', 'l']\nt = ('l', 's', 'g', 'o', 'g', 'r', 'o', 'u', 'p')\nprint(list(reversed(t)))\n# ['p', 'u', 'o', 'r', 'g', 'o', 'g', 's', 'l']\nr = range(5, 9)\nprint(list(reversed(r)))\n# [8, 7, 6, 5]\nx = [-8, 99, 3, 7, 83]\nprint(list(reversed(x)))\n# [83, 7, 3, 99, -8]\n\nenumerate(sequence, [start=0])\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\na = list(enumerate(seasons))\nprint(a)\n# [(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]\nb = list(enumerate(seasons, 1))\nprint(b)\n# [(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]\nfor i, element in a:\nprint('&#123;0&#125;,&#123;1&#125;'.format(i, element))\n\nzip(iter1 [,iter2 […]])a. 用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象，这样做的好处是节约了不少的内存。b. 我们可以使用 list() 转换来输出列表。c. 如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。\n\na = [1, 2, 3]\nb = [4, 5, 6]\nc = [4, 5, 6, 7, 8]\nzipped = zip(a, b)\nprint(zipped) # &lt;zip object at 0x000000C5D89EDD88>\nprint(list(zipped)) # [(1, 4), (2, 5), (3, 6)]\nzipped = zip(a, c)\nprint(list(zipped)) # [(1, 4), (2, 5), (3, 6)]\na1, a2 = zip(*zip(a, b))\nprint(list(a1)) # [1, 2, 3]\nprint(list(a2)) # [4, 5, 6]\n10 函数与Lambda表达式10.1 函数函数的定义\n函数以def 关键词开头，后接函数名和圆括号()。\n函数执行的代码以冒号起始，并且缩进。\nreturn [表达式] 结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None 。\n\ndef functionname(parameters):\n\"函数_文档字符串\"\nfunction_suite\nreturn [expression]\n函数参数Python 的函数具有非常灵活多样的参数形态，既可以实现简单的调用，又可以传入非常复杂的参数。从简到繁的参数形态如下：\n\n位置参数 (positional argument)\ndef functionname(arg1):\n    \"函数_文档字符串\"\n    function_suite\nreturn [expression]\n\n默认参数 (default argument)\ndef functionname(arg1, arg2=v):\n    \"函数_文档字符串\"\n    function_suite\nreturn [expression]\n# 默认参数一定要放在位置参数 后面，不然程序会报错。\n\n可变参数 (variable argument)\ndef functionname(arg1, arg2=v, *args):\n    for var in args:\n\t\tprint(var)\nreturn [expression]\n\n*args - 可变参数，可以是从零个到任意个，自动组装成元组。\n加了星号（*）的变量名会存放所有未命名的变量参数。\n\n\n关键字参数 (keyword argument)\ndef functionname(arg1, arg2=v, *args, **kw):\n    \"函数_文档字符串\"\n    function_suite\nreturn [expression]\n\n**kw - 关键字参数，可以是从零个到任意个，自动组装成字典。\n\ndef printinfo(arg1, *args, **kwargs):\nprint(arg1)\nprint(args)\nprint(kwargs)\nprintinfo(70, 60, 50)\n# 70\n# (60, 50)\n# &#123;&#125;\nprintinfo(70, 60, 50, a=1, b=2)\n# 70\n# (60, 50)\n# &#123;'a': 1, 'b': 2&#125;\n「可变参数」和「关键字参数」的同异总结如下：\n\n可变参数允许传入零个到任意个参数，它们在函数调用时自动组装为一个元组 (tuple)。\n关键字参数允许传入零个到任意个参数，它们在函数内部自动组装为一个字典 (dict)。\n\n\n命名关键字参数 (name keyword argument)\ndef functionname(arg1, arg2=v, *args, *, nkw, **kw):\n    \"函数_文档字符串\"\n    function_suite\nreturn [expression]\n\n, nkw - 命名关键字参数，用户想要输入的关键字参数，定义方式是在nkw 前面加个分隔符 。\n如果要限制关键字参数的名字，就可以用「命名关键字参数」\n使用命名关键字参数时，要特别注意不能缺少参数名。\n\ndef printinfo(arg1, *, nkw, **kwargs):\n    print(arg1)\n    print(nkw)\n    print(kwargs)\nprintinfo(70, nkw=10, a=1, b=2)\n# 70\n# 10\n# &#123;'a': 1, 'b': 2&#125;\nprintinfo(70, 10, a=1, b=2)\n# TypeError: printinfo() takes 1 positional argument but 2 were given\n\n没有写参数名nwk ，因此 10 被当成「位置参数」，而原函数只有 1 个位置函数，现在调用了 2 个，因此程序会报错。\n\n\n参数组合\n在 Python 中定义函数，可以用位置参数、默认参数、可变参数、命名关键字参数和关键字参数，这 5 种参数中的 4 个都可以一起使用，但是注意，参数定义的顺序必须是：\n\n位置参数、默认参数、可变参数和关键字参数。\n位置参数、默认参数、命名关键字参数和关键字参数。要注意定义可变参数和关键字参数的语法：\n*args 是可变参数， args 接收的是一个 tuple\n*kw 是关键字参数， kw 接收的是一个 dict命名关键字参数是为了限制调用者可以传入的参数名，同时可以提供默认值。定义命名关键字参数不要忘了写分隔符，否则定义的是位置参数。\n\n警告：虽然可以组合多达 5 种参数，但不要同时使用太多的组合，否则函数很难懂。\n\n\n变量作用域\nPython 中，程序的变量并不是在哪个位置都可以访问的，访问权限决定于这个变量是在哪里赋值的。\n定义在函数内部的变量拥有局部作用域，该变量称为局部变量。\n定义在函数外部的变量拥有全局作用域，该变量称为全局变量。\n局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。\n当内部作用域想修改外部作用域的变量时，就要用到global 和nonlocal 关键字了。\n\nnum = 1\ndef fun1():\n    global num # 需要使用 global 关键字声明\n    print(num) # 1\n    num = 123\n    print(num) # 123\nfun1()\nprint(num) # 123\n10.2 Lambda 表达式匿名函数的定义lambda argument_list: expression\n\nlambda - 定义匿名函数的关键词。\nargument_list - 函数参数，它们可以是位置参数、默认参数、关键字参数，和正规函数里的参数类型一样。\n: - 冒号，在函数参数和表达式中间要加个冒号。\nexpression - 只是一个表达式，输入函数参数，输出一些值。\n\n注意：\n\nexpression 中没有 return 语句，因为 lambda 不需要它来返回，表达式本身结果就是返回值。\n匿名函数拥有自己的命名空间，且不能访问自己参数列表之外或全局命名空间里的参数。\n\ndef sqr(x):\nreturn x ** 2\nprint(sqr)\n# &lt;function sqr at 0x000000BABD3A4400>\n\ny = [sqr(x) for x in range(10)]\nprint(y)\n# [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\nlbd_sqr = lambda x: x ** 2\nprint(lbd_sqr)\n# &lt;function &lt;lambda> at 0x000000BABB6AC1E0>\n\ny = [lbd_sqr(x) for x in range(10)]\nprint(y)\n# [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\nsumary = lambda arg1, arg2: arg1 + arg2\nprint(sumary(10, 20)) # 30\nfunc = lambda *args: sum(args)\nprint(func(1, 2, 3, 4, 5)) # 15\n匿名函数的应用函数式编程 是指代码中每一块都是不可变的，都由纯函数的形式组成。这里的纯函数，是指函数本身相互独立、互不影响，对于相同的输入，总会有相同的输出，没有任何副作用。\n匿名函数 常常应用于函数式编程的高阶函数 (high-order function)中，主要有两种形式：\n\n参数是函数 (filter, map)\n返回值是函数 (closure)如，在 filter 和map 函数中的应用：\nfilter(function, iterable) 过滤序列，过滤掉不符合条件的元素，返回一个迭代器对象，如果要转换为列表，可以使用 list() 来转换。\n\nodd = lambda x: x % 2 == 1\ntemplist = filter(odd, [1, 2, 3, 4, 5, 6, 7, 8, 9])\nprint(list(templist)) # [1, 3, 5, 7, 9]\n\nmap(function, *iterables) 根据提供的函数对指定序列做映射。\n\nm1 = map(lambda x: x ** 2, [1, 2, 3, 4, 5])\nprint(list(m1))\n# [1, 4, 9, 16, 25]\nm2 = map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])\nprint(list(m2))\n# [3, 7, 11, 15, 19]\n11 类与对象11.1 对象 = 属性 + 方法对象是类的实例。换句话说，类主要定义对象的结构，然后我们以类为模板创建对象。类不但包含方法定义，而且还包含所有实例共享的数据。\n封装：信息隐蔽技术我们可以使用关键字 class 定义 Python 类，关键字后面紧跟类的名称、分号和类的实现。\nclass Turtle: # Python中的类名约定以大写字母开头\n\"\"\"关于类的一个简单例子\"\"\"\n    # 属性\n    color = 'green'\n    mouth = '大嘴'\n    # 方法\n    def climb(self):\n    \tprint('我正在很努力的向前爬...')\n    \t\n    \t\ntt = Turtle()\ntt.climb()\n# 我正在很努力的向前爬...\n继承：子类自动共享父类之间数据和方法的机制\nclass MyList(list):\n\tpass\nlst = MyList([1, 5, 2, 7, 8])\nlst.append(9)\nlst.sort()\nprint(lst)\n# [1, 2, 5, 7, 8, 9]\n多态：不同对象对同一方法响应不同的行动\nclass Animal:\n\tdef run(self):\n\t\traise AttributeError('子类必须实现这个方法')\nclass People(Animal):\n\tdef run(self):\n\t\tprint('人正在走')\nclass Pig(Animal):\n\tdef run(self):\n\t\tprint('pig is walking')\n11.2 self 是什么？Python 的 self 相当于 C++ 的 this 指针。\nclass Test:\n    def prt(self):\n        print(self)\n        print(self.__class__)\nt &#x3D; Test()\nt.prt()\n# &lt;__main__.Test object at 0x000000BC5A351208&gt;\n# &lt;class &#39;__main__.Test&#39;&gt;\n类的方法与普通的函数只有一个特别的区别 —— 它们必须有一个额外的第一个参数名称（对应于该实例，即该对象本身），按照惯例它的名称是 self 。在调用方法时，我们无需明确提供与参数 self 相对应的参数。\nclass Ball:\n    def setName(self, name):\n    \tself.name = name\n    def kick(self):\n    \tprint(\"我叫%s,该死的，谁踢我...\" % self.name)\n11.3 Python 的魔法方法类有一个名为__init__(self[, param1, param2…]) 的魔法方法，该方法在类实例化时会自动调用。\nclass Ball:\n    def __init__(self, name):\n    \tself.name = name\n11.4 公有和私有在 Python 中定义私有变量只需要在变量名或函数名前加上“__”两个下划线，那么这个函数或变量就会为私有的了。\nclass JustCounter:\n    __secretCount &#x3D; 0 # 私有变量\n    publicCount &#x3D; 0 # 公开变量\n    \n    def __foo(self): # 私有方法\n\t\tprint(&#39;这是私有方法&#39;)\n11.5 继承Python 同样支持类的继承，派生类的定义如下所示：\nclass DerivedClassName(BaseClassName):\n    &lt;statement-1>\n    .\n    .\n    .\n    &lt;statement-N>\nBaseClassName （示例中的基类名）必须与派生类定义在一个作用域内。除了类，还可以用表达式，基类定义在另一个模块中时这一点非常有用：\nclass DerivedClassName(modname.BaseClassName):\n    &lt;statement-1>\n    .\n    .\n    .\n    &lt;statement-N>\n\n如果子类中定义与父类同名的方法或属性，则会自动覆盖父类对应的方法或属性。\n\nclass Fish:\n    def __init__(self):\n        self.x = r.randint(0, 10)\n        self.y = r.randint(0, 10)\n    def move(self):\n    \tself.x -= 1\n    \tprint(\"我的位置\", self.x, self.y)\nclass Shark(Fish): # 鲨鱼\n    def __init__(self):\n        Fish.__init__(self)\n        self.hungry = True\n\n使用super函数super().init()\n\nclass Shark(Fish): # 鲨鱼\n    def __init__(self):\n        super().__init__()\n        self.hungry = True\nPython 虽然支持多继承的形式，但我们一般不使用多继承，因为容易引起混乱。\n需要注意圆括号中父类的顺序，若是父类中有相同的方法名，而在子类使用时未指定，Python 从左至右搜索，即方法在子类中未找到时，从左到右查找父类中是否包含方法。\n11.6 类、类对象和实例对象类对象：创建一个类，其实也是一个对象也在内存开辟了一块空间，称为类对象，类对象只有一个。\n实例对象：就是通过实例化类创建的对象，称为实例对象，实例对象可以有多个。\n# 类对象\nclass A(object):\n\tpass\n# 实例化对象 a、b、c都属于实例对象。\na = A()\nb = A()\nc = A()\n类属性：类里面方法外面定义的变量称为类属性。类属性所属于类对象并且多个实例对象之间共享同一个类属性，说白了就是类属性所有的通过该类实例化的对象都能共享。\nclass A():\n    a = xx #类属性\n    def __init__(self):\n    \tA.a = xx #使用类属性可以通过 （类名.类属性）调用。\n实例属性：实例属性和具体的某个实例对象有关系，并且一个实例对象和另外一个实例对象是不共享属性的，说白了实例属性只能在自己的对象里面使用，其他的对象不能直接使用，因为self 是谁调用，它的值就属于该对象。\nclass 类名():\n    __init__(self)：\n    \tself.name = xx #实例属性\n类属性和实例属性区别\n\n类属性：类外面，可以通过实例对象.类属性和类名.类属性进行调用。类里面，通过self.类属性和类名.类属性进行调用。\n实例属性 ：类外面，可以通过实例对象.实例属性调用。类里面，通过self.实例属性调用。\n实例属性就相当于局部变量。出了这个类或者这个类的实例对象，就没有作用了。\n类属性就相当于类里面的全局变量，可以和这个类的所有实例对象共享。\n\n11.8 什么是绑定？Python 严格要求方法需要有实例才能被调用，这种限制其实就是 Python 所谓的绑定概念。Python 对象的数据属性通常存储在名为. dict 的字典中，我们可以直接访问dict ，或利用 Python 的内置函数vars() 获取. dict 。\n11.9 一些相关的内置函数（BIF）\nissubclass(class, classinfo) 方法用于判断参数 class 是否是类型参数 classinfo 的子类。\n一个类被认为是其自身的子类。\nclassinfo 可以是类对象的元组，只要class是其中任何一个候选类的子类，则返回True 。\n\nclass A:\n\tpass\nclass B(A):\n\tpass\nprint(issubclass(B, A)) # True\nprint(issubclass(B, B)) # True\nprint(issubclass(A, B)) # False\nprint(issubclass(B, object)) # True\n\nisinstance(object, classinfo) 方法用于判断一个对象是否是一个已知的类型，类似type() 。\ntype() 不会认为子类是一种父类类型，不考虑继承关系。\nisinstance() 会认为子类是一种父类类型，考虑继承关系。\n如果第一个参数不是对象，则永远返回False 。\n如果第二个参数不是类或者由类对象组成的元组，会抛出一个TypeError 异常。\n\na = 2\nprint(isinstance(a, int)) # True\nprint(isinstance(a, str)) # False\nprint(isinstance(a, (str, int, list))) # True\nclass A:\npass\nclass B(A):\npass\nprint(isinstance(A(), A)) # True\nprint(type(A()) == A) # True\nprint(isinstance(B(), A)) # True\nprint(type(B()) == A) # False\n\nhasattr(object, name) 用于判断对象是否包含对应的属性。\n\nclass Coordinate:\n    x = 10\n    y = -5\n    z = 0\npoint1 = Coordinate()\nprint(hasattr(point1, 'x')) # True\nprint(hasattr(point1, 'y')) # True\nprint(hasattr(point1, 'z')) # True\nprint(hasattr(point1, 'no')) # False\n\ngetattr(object, name[, default]) 用于返回一个对象属性值。\n\nclass A(object):\n\tbar = 1\na = A()\nprint(getattr(a, 'bar')) # 1\nprint(getattr(a, 'bar2', 3)) # 3\nprint(getattr(a, 'bar2'))\n# AttributeError: 'A' object has no attribute 'bar2'\n\nsetattr(object, name, value) 对应函数 getattr() ，用于设置属性值，该属性不一定是存在的。\n\nclass A(object):\nbar = 1\na = A()\nprint(getattr(a, 'bar')) # 1\nsetattr(a, 'bar', 5)\nprint(a.bar) # 5\nsetattr(a, \"age\", 28)\nprint(a.age) # 28\n\ndelattr(object, name) 用于删除属性。\n\nclass property([fget[, fset[, fdel[, doc]]]]) 用于在新式类中返回属性值。a. fget — 获取属性值的函数b. fset — 设置属性值的函数c. fdel — 删除属性值函数d. doc — 属性描述信息\n\n\nclass C(object):\n    def __init__(self):\n    \tself.__x = None\n    def getx(self):\n    \treturn self.__x\n    def setx(self, value):\n    \tself.__x = value\n    def delx(self):\n    \tdel self.__x\n    x = property(getx, setx, delx, \"I'm the 'x' property.\")\ncc = C()\ncc.x = 2\nprint(cc.x) # 2\n12 魔法方法魔法方法总是被双下划线包围，例如init 。魔法方法是面向对象的 Python 的一切，如果你不知道魔法方法，说明你还没能意识到面向对象的 Python 的强大。魔法方法的“魔力”体现在它们总能够在适当的时候被自动调用。魔法方法的第一个参数应为cls （类方法） 或者self （实例方法）。\n\ncls ：代表一个类的名称\nself ：代表一个实例对象的名称\n\n12.1 基本的魔法方法init(self[, …]):构造器，当一个实例被创建的时候调用的初始化方法\nclass Rectangle:\n    def __init__(self, x, y):\n    \tself.x = x\n    \tself.y = y\n__new__(cls[, …])\n\nnew 是在一个对象实例化的时候所调用的第一个方法，在调用init 初始化前，先调用new 。\nnew 至少要有一个参数cls ，代表要实例化的类，此参数在实例化时由 Python 解释器自动提供，后面的参数直接传递给init 。\nnew 对当前类进行了实例化，并将实例返回，传给init 的self 。但是，执行了new ，并不一定会进入init ，只有new 返回了，当前类cls 的实例，当前类的init 才会进入。\n若new 没有正确返回当前类cls 的实例，那init 是不会被调用的，即使是父类的实例也不行，将没有init 被调用。\n可利用new 实现单例模式。\nnew 方法主要是当你继承一些不可变的 class 时（比如int, str, tuple ）， 提供给你一个自定义这些类的实例化过程的途径。\n\n__del__(self)\n析构器，当一个对象将要被系统回收之时调用的方法。        Python 采用自动引用计数（ARC）方式来回收对象所占用的空间，当程序中有一个变量引用该 Python 对象时，Python会自动保证该对象引用计数为 1；当程序中有两个变量引用该 Python 对象时，Python 会自动保证该对象引用计数为 2，依此类推，如果一个对象的引用计数变成了 0，则说明程序中不再有变量引用该对象，表明程序不再需要该对象，因此Python 就会回收该对象。        大部分时候，Python 的 ARC 都能准确、高效地回收系统中的每个对象。但如果系统中出现循环引用的情况，比如对象a 持有一个实例变量引用对象 b，而对象 b 又持有一个实例变量引用对象 a，此时两个对象的引用计数都是 1，而实际上程序已经不再有变量引用它们，系统应该回收它们，此时 Python 的垃圾回收器就可能没那么快，要等专门的循环垃圾回收器（Cyclic Garbage Collector）来检测并回收这种引用循环。\n__str__(self) :\n\n当你打印一个对象的时候，触发str\n当你使用%s 格式化的时候，触发str\nstr 强转数据类型的时候，触发str\n\n__repr__(self):\n\nrepr 是str 的备胎\n有str 的时候执行str ,没有实现str 的时候，执行repr\nrepr(obj) 内置函数对应的结果是repr 的返回值\n当你使用%r 格式化的时候 触发repr\n\nstr(self) 的返回结果可读性强。也就是说， str 的意义是得到便于人们阅读的信息，就像下面的 ‘2019-10-11’ 一样。repr(self) 的返回结果应更准确。怎么说， repr 存在的目的在于调试，便于开发者使用。\n12.2 算术运算符类型工厂函数，指的是不通过类而是通过函数来创建对象。\n\n__add__(self, other) 定义加法的行为： +\n__sub__(self, other) 定义减法的行为： -\n__mul__(self, other) 定义乘法的行为： *\n__truediv__(self, other) 定义真除法的行为： /\n__floordiv__(self, other) 定义整数除法的行为： //\n__mod__(self, other) 定义取模算法的行为： %\n__divmod__(self, other) 定义当被 divmod() 调用时的行为\ndivmod(a, b) 把除数和余数运算结果结合起来，返回一个包含商和余数的元组(a // b, a % b) 。\n__mul__(self, other) 定义乘法的行为： *\n__truediv__(self, other) 定义真除法的行为： /\n__floordiv__(self, other) 定义整数除法的行为： //\n__mod__(self, other) 定义取模算法的行为： %\n__divmod__(self, other) 定义当被 divmod() 调用时的行为\ndivmod(a, b) 把除数和余数运算结果结合起来，返回一个包含商和余数的元组(a // b, a % b) 。\n\n12.3 反算术运算符反运算魔方方法，与算术运算符保持一一对应，不同之处就是反运算的魔法方法多了一个“r”。当文件左操作不支持相应的操作时被调用。\n\n__radd__(self, other) 定义加法的行为： +\n__rsub__(self, other) 定义减法的行为： -\n__rmul__(self, other) 定义乘法的行为： *\n__rtruediv__(self, other) 定义真除法的行为： /\n__rfloordiv__(self, other) 定义整数除法的行为： //\n__rmod__(self, other) 定义取模算法的行为： %\n__rdivmod__(self, other) 定义当被 divmod() 调用时的行为\n__rpow__(self, other[, module]) 定义当被 power() 调用或 ** 运算时的行为\n__rlshift__(self, other) 定义按位左移位的行为： &lt;&lt;\n__rrshift__(self, other) 定义按位右移位的行为： &gt;&gt;\n__rand__(self, other) 定义按位与操作的行为： &amp;\n__rxor__(self, other) 定义按位异或操作的行为： ^\n__ror__(self, other) 定义按位或操作的行为： |\n\na + b这里加数是a ，被加数是b ，因此是a 主动，反运算就是如果a 对象的__add() 方法没有实现或者不支持相应的操作，那么 Python 就会调用b 的\\radd__() 方法。\nclass Nint(int):\n    def __radd__(self, other):\n    \treturn int.__sub__(other, self) # 注意 self 在后面\na &#x3D; Nint(5)\nb &#x3D; Nint(3)\nprint(a + b) # 8\nprint(1 + b) # -2\n12.4 增量赋值运算符\n__iadd__(self, other) 定义赋值加法的行为： +=\n__isub__(self, other) 定义赋值减法的行为： -=\n__imul__(self, other) 定义赋值乘法的行为： *=\n__itruediv__(self, other) 定义赋值真除法的行为： /=\n__ifloordiv__(self, other) 定义赋值整数除法的行为： //=\n__imod__(self, other) 定义赋值取模算法的行为： %=\n__ipow__(self, other[, modulo]) 定义赋值幂运算的行为： **=\n__ilshift__(self, other) 定义赋值按位左移位的行为： &lt;&lt;=\n__irshift__(self, other) 定义赋值按位右移位的行为： &gt;&gt;=\n__iand__(self, other) 定义赋值按位与操作的行为： &amp;=\n__ixor__(self, other) 定义赋值按位异或操作的行为： ^=\n__ior__(self, other) 定义赋值按位或操作的行为： |=\n\n12.5 一元运算符\n__neg__(self) 定义正号的行为： +x\n__pos__(self) 定义负号的行为： -x\n__abs__(self) 定义当被abs() 调用时的行为\n__invert__(self) 定义按位求反的行为： ~x\n\n12.6 属性访问__getattr(self, name) : 定义当用户试图获取一个不存在的属性时的行为。__getattribute__(self, name) ：定义当该类的属性被访问时的行为（先调用该方法，查看是否存在该属性，若不存在，接着去调用\\getattr__ ）。__setattr__(self, name, value) ：定义当一个属性被设置时的行为。__delattr__(self, name) ：定义当一个属性被删除时的行为。\n12.7 描述符描述符就是将某种特殊类型的类的实例指派给另一个类的属性。\n\n__get__(self, instance, owner) 用于访问属性，它返回属性的值。\n\n__set__(self, instance, value) 将在属性分配操作中调用，不返回任何内容。\n\n__del__(self, instance) 控制删除操作，不返回任何内容。\n\n\n12.8 定制序列协议（Protocols）与其它编程语言中的接口很相似，它规定你哪些方法必须要定义。然而，在 Python 中的协议就显得不那么正式。事实上，在 Python 中，协议更像是一种指南。容器类型的协议\n\n如果说你希望定制的容器是不可变的话，你只需要定义__len() 和\\getitem__() 方法。\n如果你希望定制的容器是可变的话，除了__len() 和\\getitem() 方法，你还需要定义\\setitem()和\\delitem__() 两个方法。\n\n__len__(self) 定义当被len() 调用时的行为（返回容器中元素的个数）。\n\n__getitem__(self, key) 定义获取容器中元素的行为，相当于self[key] 。\n__setitem__(self, key, value) 定义设置容器中指定元素的行为，相当于self[key] = value 。\n__delitem__(self, key) 定义删除容器中指定元素的行为，相当于del self[key] 。\n\n12.9 迭代器\n迭代是 Python 最强大的功能之一，是访问集合元素的一种方式。\n迭代器是一个可以记住遍历的位置的对象。\n迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。\n迭代器只能往前不会后退。\n字符串，列表或元组对象都可用于创建迭代器：\n\nstring = 'lsgogroup'\nfor c in string:\n\tprint(c)\n\n迭代器有两个基本的方法： iter() 和 next() 。\niter(object) 函数用来生成迭代器。\nnext(iterator[, default]) 返回迭代器的下一个项目。\niterator — 可迭代对象\ndefault — 可选，用于设置在没有下一个元素时返回该默认值，如果不设置，又没有下一个元素则会触发StopIteration 异常。\n\nlinks = &#123;'B': '百度', 'A': '阿里', 'T': '腾讯'&#125;\nit = iter(links)\nprint(next(it)) # B\nprint(next(it)) # A\nprint(next(it)) # T\nprint(next(it)) # StopIteration\n\nit = iter(links)\nwhile True:\n    try:\n    \teach = next(it)\n    except StopIteration:\n    \tbreak\n    print(each)\n# B\n# A\n# T\n把一个类作为一个迭代器使用需要在类中实现两个魔法方法 iter() 与 next() 。\n\n__iter(self) 定义当迭代容器中的元素的行为，返回一个特殊的迭代器对象， 这个迭代器对象实现了next__() 方法并通过 StopIteration 异常标识迭代的完成。\n__next__() 返回下一个迭代器对象。\nStopIteration 异常用于标识迭代的完成，防止出现无限循环的情况，在 next() 方法中我们可以设置在完成指定循环次数后触发 StopIteration 异常来结束迭代。\n\n12.10 生成器\n在 Python 中，使用了 yield 的函数被称为生成器（generator）。\n跟普通函数不同的是，生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。\n在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行。\n调用一个生成器函数，返回的是一个迭代器对象。\n\n13 模块​        在前面我们脚本是用 Python 解释器来编程，如果你从 Python 解释器退出再进入，那么你定义的所有的方法和变量就都消失了。​        为此 Python 提供了一个办法，把这些定义存放在文件中，为一些脚本或者交互式的解释器实例使用，这个文件被称为模块（Module）。​        模块是一个包含所有你定义的函数和变量的文件，其后缀名是.py 。模块可以被别的程序引入，以使用该模块中的函数等功能。这也是使用 Python 标准库的方法。\n13.1 什么是模块\n容器 -&gt; 数据的封装\n函数 -&gt; 语句的封装\n类 -&gt; 方法和属性的封装\n模块 -&gt; 程序文件\n\n13.2 命名空间命名空间因为对象的不同，也有所区别，可以分为如下几种：\n\n内置命名空间（Built-in Namespaces）：Python 运行起来，它们就存在了。内置函数的命名空间都属于内置命名空间，所以，我们可以在任何程序中直接运行它们，比如id() ,不需要做什么操作，拿过来就直接使用了。\n全局命名空间（Module：Global Namespaces）：每个模块创建它自己所拥有的全局命名空间，不同模块的全局命名空间彼此独立，不同模块中相同名称的命名空间，也会因为模块的不同而不相互干扰。\n本地命名空间（Function &amp; Class：Local Namespaces）：模块中有函数或者类，每个函数或者类所定义的命名空间就是本地命名空间。如果函数返回了结果或者抛出异常，则本地命名空间也结束了。\n\n程序在查询上述三种命名空间的时候，就按照从里到外的顺序，即：Local Namespaces —&gt; Global Namesspaces —&gt; Built-inNamesspaces。\nimport hello\nhello.hi() # Hi everyone, I love lsgogroup!\nhi() # NameError: name 'hi' is not defined\n13.3 导入模块\n第一种：import 模块名\n第二种：from 模块名 import 函数名\n第三种：import 模块名 as 新名字\n\n13.4 if __name == ‘\\main__’​        对于很多编程语言来说，程序都必须要有一个入口，而 Python 则不同，它属于脚本语言，不像编译型语言那样先将程序编译成二进制再运行，而是动态的逐行解释运行。也就是从脚本第一行开始运行，没有统一的入口。假设我们有一个 const.py 文件，内容如下：\n13.5 搜索路径当解释器遇到 import 语句，如果模块在当前的搜索路径就会被导入。\nimport sys\nprint(sys.path)\n# ['C:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib',\n'C:\\\\ProgramData\\\\Anaconda3', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages',...]\n​        我们使用 import 语句的时候，Python 解释器是怎样找到对应的文件的呢？​        这就涉及到 Python 的搜索路径，搜索路径是由一系列目录名组成的，Python 解释器就依次从这些目录中去寻找所引入的模块。​        这看起来很像环境变量，事实上，也可以通过定义环境变量的方式来确定搜索路径。搜索路径是在 Python 编译或安装的时候确定的，安装新的库应该也会修改。搜索路径被存储在 sys 模块中的 path 变量中。\n13.6 包（package）包是一种管理 Python 模块命名空间的形式，采用”点模块名称”。创建包分为三个步骤：\n\n创建一个文件夹，用于存放相关的模块，文件夹的名字即包的名字。\n在文件夹中创建一个 init.py 的模块文件，内容可以为空。\n将相关的模块放入文件夹中。\n\n不妨假设你想设计一套统一处理声音文件和数据的模块（或者称之为一个”包”）。\n现存很多种不同的音频文件格式（基本上都是通过后缀名区分的，例如：.wav，.aiff，.au），所以你需要有一组不断增加的模块，用来在不同的格式之间转换。并且针对这些音频数据，还有很多不同的操作（比如混音，添加回声，增加均衡器功能，创建人造立体声效果），所以你还需要一组怎么也写不完的模块来处理这些操作。\n14 datetime模块14.1 datetime类class datetime(date):\n    def __init__(self, year, month, day, hour, minute, second, microsecond, tzinfo)\n    \tpass\n    def now(cls, tz=None):\n    \tpass\n    def timestamp(self):\n    \tpass\n    def fromtimestamp(cls, t, tz=None):\n    \tpass\n    def date(self):\n    \tpass\n    def time(self):\n    \tpass\n    def year(self):\n    \tpass\n    def month(self):\n    \tpass\n    def day(self):\n    \tpass\n    def hour(self):\n    \tpass\n    def minute(self):\n    \tpass\n    def second(self):\n    \tpass\n    def isoweekday(self):\n    \tpass\n    def strftime(self, fmt):\n    \tpass\n    def combine(cls, date, time, tzinfo=True):\n    \tpass\n\ndatetime.now(tz=None) 获取当前的日期时间，输出顺序为：年、月、日、时、分、秒、微秒。\ndatetime.timestamp() 获取以 1970年1月1日为起点记录的秒数。\ndatetime.fromtimestamp(tz=None) 使用 unixtimestamp 创建一个 datetime。\n\nimport datetime\ndt = datetime.datetime(year=2020, month=6, day=25, hour=11, minute=23, second=59)\nprint(dt) # 2020-06-25 11:23:59\nprint(dt.timestamp()) # 1593055439.0\ndt = datetime.datetime.fromtimestamp(1593055439.0)\nprint(dt) # 2020-06-25 11:23:59\nprint(type(dt)) # &lt;class 'datetime.datetime'>\ndt = datetime.datetime.now()\nprint(dt) # 2020-06-25 11:11:03.877853\nprint(type(dt)) # &lt;class 'datetime.datetime'>\n\ndatetime.strftime(fmt) 格式化 datetime 对象。\n\n%a 本地简化星期名称（如星期一，返回 Mon）%A 本地完整星期名称（如星期一，返回 Monday）%b 本地简化的月份名称（如一月，返回 Jan）%B 本地完整的月份名称（如一月，返回 January）%c 本地相应的日期表示和时间表示%d 月内中的一天（0-31）%H 24小时制小时数（0-23）%I 12小时制小时数（01-12）%j 年内的一天（001-366）%m 月份（01-12）%M 分钟数（00-59）%p 本地A.M.或P.M.的等价符%S 秒（00-59）%U 一年中的星期数（00-53）星期天为星期的开始%w 星期（0-6），星期天为星期的开始%W 一年中的星期数（00-53）星期一为星期的开始%x 本地相应的日期表示%X 本地相应的时间表示%y 两位数的年份表示（00-99）%Y 四位数的年份表示（0000-9999）%Z 当前时区的名称（如果是本地时间，返回空字符串）%% %号本身\nimport datetime\ndt = datetime.datetime(year=2020, month=6, day=25, hour=11, minute=51, second=49)\ns = dt.strftime(\"'%Y/%m/%d %H:%M:%S\")\nprint(s) # '2020/06/25 11:51:49\ns = dt.strftime('%d %B, %Y, %A')\nprint(s) # 25 June, 2020, Thursday\n\ndatetime.date() Return the date part.\ndatetime.time() Return the time part, with tzinfo None.\ndatetime.year 年\ndatetime.month 月\ndatetime.day 日\ndatetime.hour 小时\ndatetime.minute 分钟\ndatetime.second 秒\ndatetime.isoweekday 星期几\n\n在处理含有字符串日期的数据集或表格时，我们需要一种自动解析字符串的方法，无论它是什么格式的，都可以将其转化为 datetime 对象。这时，就要使用到 dateutil 中的 parser 模块。\n\nparser.parse(timestr, parserinfo=None, **kwargs)\n\nfrom dateutil import parser\ns = '2020-06-25'\ndt = parser.parse(s)\nprint(dt) # 2020-06-25 00:00:00\nprint(type(dt)) # &lt;class 'datetime.datetime'>\ns = 'March 31, 2010, 10:51pm'\ndt = parser.parse(s)\nprint(dt) # 2010-03-31 22:51:00\nprint(type(dt)) # &lt;class 'datetime.datetime'>\n14.2 date类class date:\n    def __init__(self, year, month, day):\n    \tpass\n    def today(cls):\n    \tpass\n\ndate.today() 获取当前日期信息。\n\nimport datetime\nd = datetime.date(2020, 6, 25)\nprint(d) # 2020-06-25\nprint(type(d)) # &lt;class 'datetime.date'>\nd = datetime.date.today()\nprint(d) # 2020-06-25\nprint(type(d)) # &lt;class 'datetime.date'>\n14.3 time类class time:\n    def __init__(self, hour, minute, second, microsecond, tzinfo):\n    \tpass\nimport datetime\nt = datetime.time(12, 9, 23, 12980)\nprint(t) # 12:09:23.012980\nprint(type(t)) # &lt;class 'datetime.time'>\n\n# 输入\ndate = datetime.date(2019, 10, 2)\n# 输出\n2019-10-02 00:00:00\n14.4 timedelta类timedelta 表示具体时间实例中的一段时间。你可以把它们简单想象成两个日期或时间之间的间隔。它常常被用来从 datetime 对象中添加或移除一段特定的时间。\nclass timedelta(SupportsAbs[timedelta]):\n    def __init__(self, days, seconds, microseconds, milliseconds, minutes, hours, weeks,):\n    \tpass\n    def days(self):\n    \tpass\n    def total_seconds(self):\n    \tpass\nimport datetime\n\ntd = datetime.timedelta(days=30)\nprint(td) # 30 days, 0:00:00\nprint(type(td)) # &lt;class 'datetime.timedelta'>\nprint(datetime.date.today()) # 2020-07-01\nprint(datetime.date.today() + td) # 2020-07-31\n\ndt1 = datetime.datetime(2020, 1, 31, 10, 10, 0)\ndt2 = datetime.datetime(2019, 1, 31, 10, 10, 0)\ntd = dt1 - dt2\nprint(td) # 365 days, 0:00:00\nprint(type(td)) # &lt;class 'datetime.timedelta'>\n\ntd1 = datetime.timedelta(days=30) # 30 days\ntd2 = datetime.timedelta(weeks=1) # 1 week\ntd = td1 - td2\nprint(td) # 23 days, 0:00:00\nprint(type(td)) # &lt;class 'datetime.timedelta'>\n如果将两个 datetime 对象相减，就会得到表示该时间间隔的 timedelta 对象。同样地，将两个时间间隔相减，可以得到另一个 timedelta 对象。\n15 文件与文件系统15.1 打开文件open(file, mode=’r’, buffering=None, encoding=None, errors=None, newline=None, closefd=True)Open file and return a stream. Raise OSError upon failure.a. file : 必需，文件路径（相对或者绝对路径）。b. mode : 可选，文件打开模式c. buffering : 设置缓冲d. encoding : 一般使用utf8e. errors : 报错级别f. newline : 区分换行符常见的mode 如下表所示：\n\n\n\n\n打开模式\n执行操作\n\n\n\n\n‘r’\n以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。\n\n\n‘w’\n打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑。即原有内容会被删除。如果该文件不存在，创建新文件。\n\n\n‘x’\n写模式，新建一个文件，如果该文件已存在则会报错。\n\n\n‘a’\n追加模式，打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。\n\n\n‘b’\n以二进制模式打开文件。一般用于非文本文件，如：图片。\n\n\n‘t’\n以文本模式打开（默认）。一般用于文本文件，如：txt。\n\n\n‘+’\n可读写模式（可添加到其它模式中使用）\n\n\n\n\nf = open('将进酒.txt')\nprint(f)\n# &lt;_io.TextIOWrapper name='将进酒.txt' mode='r' encoding='cp936'>\nfor each in f:\nprint(each)\n15.2 文件对象方法fileObject.close() 用于关闭一个已打开的文件。关闭后的文件不能再进行读写操作， 否则会触发ValueError错误。\nf = open(\"将进酒.txt\")\nprint('FileName:', f.name) # FileName: 将进酒.txt\nf.close()\nfileObject.read([size]) 用于从文件读取指定的字符数，如果未给定或为负则读取所有。\nf = open('将进酒.txt', 'r')\nline = f.read(20)\nprint(\"读取的字符串: %s\" % line)\n# 读取的字符串: 君不见，黄河之水天上来，奔流到海不复回。\nf.close()\nfileObject.readline() 读取整行，包括 “\\n” 字符。\nf = open('将进酒.txt', 'r')\nline = f.readline()\nprint(\"读取的字符串: %s\" % line)\n# 读取的字符串: 君不见，黄河之水天上来，奔流到海不复回。\nf.close()\nfileObject.readlines() 用于读取所有行(直到结束符 EOF)并返回列表，该列表可以由 Python 的for… in … 结构进行处理。\nf = open('将进酒.txt', 'r')\nline = f.readline()\nprint(\"读取的字符串: %s\" % line)\n# 读取的字符串: 君不见，黄河之水天上来，奔流到海不复回。\nf.close()\nfileObject.tell() 返回文件的当前位置，即文件指针当前位置。\nf = open('将进酒.txt', 'r')\nline = f.readline()\nprint(line)\n# 君不见，黄河之水天上来，奔流到海不复回。\npos = f.tell()\nprint(pos) # 42\nf.close()\nfileObject.seek(offset[, whence]) 用于移动文件读取指针到指定位置。a. offset ：开始的偏移量，也就是代表需要移动偏移的字节数，如果是负数表示从倒数第几位开始。b. whence ：可选，默认值为 0。给 offset 定义一个参数，表示要从哪个位置开始偏移；0 代表从文件开头开始算起，1 代表从当前位置开始算起，2 代表从文件末尾算起。\nf = open('将进酒.txt', 'r')\nline = f.readline()\nprint(line)\n# 君不见，黄河之水天上来，奔流到海不复回。\nline = f.readline()\nprint(line)\n# 君不见，高堂明镜悲白发，朝如青丝暮成雪。\nf.seek(0, 0)\nline = f.readline()\nprint(line)\n# 君不见，黄河之水天上来，奔流到海不复回。\nf.close()\nfileObject.write(str) 用于向文件中写入指定字符串，返回的是写入的字符长度。\nf = open('workfile.txt', 'wb+')\nprint(f.write(b'0123456789abcdef')) # 16\nprint(f.seek(5)) # 5\nprint(f.read(1)) # b'5'\nprint(f.seek(-3, 2)) # 13\nprint(f.read(1)) # b'd'\n在文件关闭前或缓冲区刷新前，字符串内容存储在缓冲区中，这时你在文件中是看不到写入的内容的。如果文件打开模式带b ，那写入文件内容时， str （参数）要用encode 方法转为bytes 形式，否则报错： TypeError: a bytes-like object is required, not ‘str’ 。\nstr = '...'\n# 文本 = Unicode字符序列\n# 相当于 string 类型\nstr = b'...'\n# 文本 = 八位序列(0到255之间的整数)\n# 字节文字总是以‘b’或‘B’作为前缀；它们产生一个字节类型的实例，而不是str类型。\n# 相当于 byte[]\nfileObject.writelines(sequence) 向文件写入一个序列字符串列表，如果需要换行则要自己加入每行的换行符\\n 。\nf = open('test.txt', 'w+')\nseq = ['小马的程序人生\\n', '老马的程序人生']\nf.writelines(seq)\nf.seek(0, 0)\nfor each in f:\nprint(each)\n# 小马的程序人生\n# 老马的程序人生\nf.close()\n15.3 简洁的 with 语句一些对象定义了标准的清理行为，无论系统是否成功的使用了它，一旦不需要它了，那么这个标准的清理行为就会执行。关键词 with 语句就可以保证诸如文件之类的对象在使用完之后一定会正确的执行它的清理方法。\ntry:\nf = open('myfile.txt', 'w')\nfor line in f:\nprint(line)\nexcept OSError as error:\nprint('出错啦!%s' % str(error))\nfinally:\nf.close()\n# 出错啦!not readable\ntry:\nwith open('myfile.txt', 'w') as f:\nfor line in f:\nprint(line)\nexcept OSError as error:\nprint('出错啦!%s' % str(error))\n# 出错啦!not readable\n16 OS我们所知道常用的操作系统就有：Windows，Mac OS，Linu，Unix等，这些操作系统底层对于文件系统的访问工作原理是不一样的，因此你可能就要针对不同的系统来考虑使用哪些文件系统模块……，这样的做法是非常不友好且麻烦的，因为这样就意味着当你的程序运行环境一改变，你就要相应的去修改大量的代码来应对。有了OS（Operation System）模块，我们不需要关心什么操作系统下使用什么模块，OS模块会帮你选择正确的模块并调用。\n\nos.getcwd() 用于返回当前工作目录。\nos.chdir(path) 用于改变当前工作目录到指定的路径。\n\nimport os\npath = 'C:\\\\'\nprint(\"当前工作目录 : %s\" % os.getcwd())\n# 当前工作目录 : C:\\Users\\Administrator\\PycharmProjects\\untitled1\nos.chdir(path)\nprint(\"目录修改成功 : %s\" % os.getcwd())\n# 目录修改成功 : C:\\\nlistdir (path=’.’) 返回path 指定的文件夹包含的文件或文件夹的名字的列表。\nimport os\ndirs = os.listdir()\nfor item in dirs:\nprint(item)\nos.mkdir(path) 创建单层目录，如果该目录已存在抛出异常。\nimport os\nif os.path.isdir(r'.\\b') is False:\n    os.mkdir(r'.\\B')\n    os.mkdir(r'.\\B\\A')\nos.mkdir(r'.\\C\\A') # FileNotFoundError\nos.makedirs(path) 用于递归创建多层目录，如果该目录已存在抛出异常。\nimport os\nos.makedirs(r'.\\E\\A')\nos.remove(path) 用于删除指定路径的文件。如果指定的路径是一个目录，将抛出 OSError 。\nimport os\nprint(\"目录为: %s\" % os.listdir(r'.\\E\\A'))\nos.remove(r'.\\E\\A\\test.txt')\nprint(\"目录为: %s\" % os.listdir(r'.\\E\\A'))\nos.rmdir(path) 用于删除单层目录。仅当这文件夹是空的才可以, 否则, 抛出 OSError 。\nimport os\nprint(\"目录为: %s\" % os.listdir(r'.\\E'))\nos.rmdir(r'.\\E\\A')\nprint(\"目录为: %s\" % os.listdir(r'.\\E'))\nos.removedirs(path) 递归删除目录，从子目录到父目录逐层尝试删除，遇到目录非空则抛出异常。\nimport os\nprint(\"目录为: %s\" % os.listdir(os.getcwd()))\nos.removedirs(r'.\\E\\A') # 先删除A 然后删除E\nprint(\"目录为: %s\" % os.listdir(os.getcwd()))\nos.rename(src, dst) 方法用于命名文件或目录，从 src 到 dst ，如果 dst 是一个存在的目录, 将抛出OSError \nimport os\nprint(\"目录为: %s\" % os.listdir(os.getcwd()))\nos.rename(\"test.txt\", \"test2.txt\")\nprint(\"重命名成功。\")\nprint(\"目录为: %s\" % os.listdir(os.getcwd()))\nos.system(command) 运行系统的shell命令（将字符串转化成命令）\nimport os\npath = os.getcwd() + '\\\\a.py'\na = os.system(r'python %s' % path)\nos.system('calc') # 打开计算器\n\nos.curdir 指代当前目录（ . ）\nos.pardir 指代上一级目录（ .. ）\nos.sep 输出操作系统特定的路径分隔符（win下为\\\\ ，Linux下为/ ）\nos.linesep 当前平台使用的行终止符（win下为\\r\\n ，Linux下为\\n ）\nos.name 指代当前使用的操作系统（包括：’mac’，’nt’）\n\nimport os\nprint(os.curdir) # .\nprint(os.pardir) # ..\nprint(os.sep) # \\\nprint(os.linesep)\nprint(os.name) # nt\n\nos.path.basename(path) 去掉目录路径，单独返回文件名\nos.path.dirname(path) 去掉文件名，单独返回目录路径\nos.path.join(path1[, path2[, …]]) 将 path1 ， path2 各部分组合成一个路径名\nos.path.split(path) 分割文件名与路径，返回(f_path,f_name) 元组。如果完全使用目录，它会将最后一个目录作为文件名分离，且不会判断文件或者目录是否存在。\nos.path.splitext(path) 分离文件名与扩展名，返回(f_path,f_name) 元组。\n\nimport os\n# 返回文件名\nprint(os.path.basename(r'C:\\test\\lsgo.txt')) # lsgo.txt\n# 返回目录路径\nprint(os.path.dirname(r'C:\\test\\lsgo.txt')) # C:\\test\n# 将目录和文件名合成一个路径\nprint(os.path.join('C:\\\\', 'test', 'lsgo.txt')) # C:\\test\\lsgo.txt\n# 分割文件名与路径\nprint(os.path.split(r'C:\\test\\lsgo.txt')) # ('C:\\\\test', 'lsgo.txt')\n# 分离文件名与扩展名\nprint(os.path.splitext(r'C:\\test\\lsgo.txt')) # ('C:\\\\test\\\\lsgo', '.txt')\n\nos.path.getsize(file) 返回指定文件大小，单位是字节。\nos.path.getatime(file) 返回指定文件最近的访问时间\nos.path.getctime(file) 返回指定文件的创建时间\nos.path.getmtime(file) 返回指定文件的最新的修改时间\n浮点型秒数，可用time模块的gmtime() 或localtime() 函数换算\n\nimport os\nimport time\nfile = r'.\\lsgo.txt'\nprint(os.path.getsize(file)) # 30\nprint(os.path.getatime(file)) # 1565593737.347196\nprint(os.path.getctime(file)) # 1565593737.347196\nprint(os.path.getmtime(file)) # 1565593797.9298275\nprint(time.gmtime(os.path.getctime(file)))\n# time.struct_time(tm_year=2019, tm_mon=8, tm_mday=12, tm_hour=7, tm_min=8, tm_sec=57, tm_wday=0,\ntm_yday=224, tm_isdst=0)\nprint(time.localtime(os.path.getctime(file)))\n# time.struct_time(tm_year=2019, tm_mon=8, tm_mday=12, tm_hour=15, tm_min=8, tm_sec=57, tm_wday=0,\ntm_yday=224, tm_isdst=0)\n\nos.path.exists(path) 判断指定路径（目录或文件）是否存在\nos.path.isabs(path) 判断指定路径是否为绝对路径\nos.path.isdir(path) 判断指定路径是否存在且是一个目录\nos.path.isfile(path) 判断指定路径是否存在且是一个文件\nos.path.islink(path) 判断指定路径是否存在且是一个符号链接\nos.path.ismount(path) 判断指定路径是否存在且是一个悬挂点\nos.path.samefile(path1,path2) 判断path1和path2两个路径是否指向同一个文件\n\nimport os\nprint(os.path.ismount('D:\\\\')) # True\nprint(os.path.ismount('D:\\\\Test')) # False\n17 序列化与反序列化Python 的 pickle 模块实现了基本的数据序列和反序列化。\n\n通过 pickle 模块的序列化操作我们能够将程序中运行的对象信息保存到文件中去，永久存储。\n通过 pickle 模块的反序列化操作，我们能够从文件中创建上一次程序保存的对象。\n\npickle模块中最常用的函数为：pickle.dump(obj, file, [,protocol]) 将obj 对象序列化存入已经打开的file 中。\n\nobj ：想要序列化的obj 对象。\nfile :文件名称。\nprotocol ：序列化使用的协议。如果该项省略，则默认为0。如果为负值或HIGHEST_PROTOCOL ，则使用最高的协议版本。\n\npickle.load(file) 将file 中的对象序列化读出。\nfile ：文件名称。\nimport pickle\ndataList = [[1, 1, 'yes'],\n            [1, 1, 'yes'],\n            [1, 0, 'no'],\n            [0, 1, 'no'],\n            [0, 1, 'no']]\ndataDic = &#123;0: [1, 2, 3, 4],\n            1: ('a', 'b'),\n            2: &#123;'c': 'yes', 'd': 'no'&#125;&#125;\n# 使用dump()将数据序列化到文件中\nfw = open(r'.\\dataFile.pkl', 'wb')\n# Pickle the list using the highest protocol available.\npickle.dump(dataList, fw, -1)\n# Pickle dictionary using protocol 0.\npickle.dump(dataDic, fw)\nfw.close()\n# 使用load()将数据从文件中序列化读出\nfr = open('dataFile.pkl', 'rb')\ndata1 = pickle.load(fr)\nprint(data1)\ndata2 = pickle.load(fr)\nprint(data2)\nfr.close()\n# [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n# &#123;0: [1, 2, 3, 4], 1: ('a', 'b'), 2: &#123;'c': 'yes', 'd': 'no'&#125;&#125;\n","slug":"P0-Python","date":"2021-11-01T08:38:34.000Z","categories_index":"Python","tags_index":"","author_index":"YFR718"}]