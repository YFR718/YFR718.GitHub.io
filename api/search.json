[{"id":"3a0647bbf7ec80551437039f438fd197","title":"Sqoop","content":"1111111. Sqoop简介​        Sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。​        Sqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使用者能够快速部署，也为了让开发人员能够更快速的迭代开发，Sqoop独立成为一个Apache项目。​        Sqoop2的最新版本是1.99.7。请注意，2与1不兼容，且特征不完整，它并不打算用于生产部署。Sqoop原理：\n\n将导入或导出命令翻译成mapreduce程序来实现。\n在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。\n\n2. Sqoop安装安装Sqoop的前提是已经具备Java和Hadoop的环境。\n2.1 下载并解压\n下载地址：http://mirrors.hust.edu.cn/apache/sqoop/1.4.6/\n\n上传安装包sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz到虚拟机中\n\n解压sqoop安装包到指定目录，如：\n$ tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/\n\n\n2.2 修改配置文件Sqoop的配置文件与大多数大数据框架类似，在sqoop根目录下的conf目录中。\n\n重命名配置文件\n$ mv sqoop-env-template.sh sqoop-env.sh\n\n修改配置文件 sqoop-env.sh\nexport HADOOP_COMMON_HOME=/opt/module/hadoop-2.7.2\nexport HADOOP_MAPRED_HOME=/opt/module/hadoop-2.7.2\nexport HIVE_HOME=/opt/module/hive\nexport ZOOKEEPER_HOME=/opt/module/zookeeper-3.4.10\nexport ZOOCFGDIR=/opt/module/zookeeper-3.4.10\nexport HBASE_HOME=/opt/module/hbase\n\n\n2.3 拷贝JDBC驱动拷贝jdbc驱动到sqoop的lib目录下，如：\n$ cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop-1.4.6.bin__hadoop-2.0.4-alpha/lib/\n2.4 验证Sqoop我们可以通过某一个command来验证sqoop配置是否正确：\n$ bin/sqoop help\n出现一些Warning警告（警告信息已省略），并伴随着帮助命令的输出：\nAvailable commands:\n  codegen            Generate code to interact with database records\n  create-hive-table     Import a table definition into Hive\n  eval               Evaluate a SQL statement and display the results\n  export             Export an HDFS directory to a database table\n  help               List available commands\n  import             Import a table from a database to HDFS\n  import-all-tables     Import tables from a database to HDFS\n  import-mainframe    Import datasets from a mainframe server to HDFS\n  job                Work with saved jobs\n  list-databases        List available databases on a server\n  list-tables           List available tables in a database\n  merge              Merge results of incremental imports\n  metastore           Run a standalone Sqoop metastore\n  version            Display version information\n2.5 测试Sqoop是否能够成功连接数据库$ bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password 000000\n出现如下输出：\ninformation_schema\nmetastore\nmysql\noozie\nperformance_schema\n3. Sqoop的简单使用案例3.1 导入数据​        在Sqoop中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，HBASE）中传输数据，叫做：导入，即使用import关键字。RDBMS到HDFS\n\n确定Mysql服务开启正常\n\n在Mysql中新建一张表并插入一些数据\n$ mysql -uroot -p000000\nmysql&gt; create database company;\nmysql&gt; create table company.staff(id int(4) primary key not null auto_increment, name varchar(255), sex varchar(255));\nmysql&gt; insert into company.staff(name, sex) values('Thomas', 'Male');\nmysql&gt; insert into company.staff(name, sex) values('Catalina', 'FeMale');\n\n导入数据 （1）全部导入\n$ bin/sqoop import \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table staff \\\n--target-dir /user/company \\\n--delete-target-dir \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\"\n（2）查询导入\n$ bin/sqoop import \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--target-dir /user/company \\\n--delete-target-dir \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\" \\\n--query 'select name,sex from staff where id &lt;=1 and $CONDITIONS;'\n提示：must contain ‘如果后使用的是双引号，则CONDITIONS前必须加转移符，防止shell识别为自己的变量。（3）导入指定列\n\n\n$ bin/sqoop import \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--target-dir /user/company \\\n--delete-target-dir \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\" \\\n--columns id,sex \\\n--table staff\n  提示：columns中如果涉及到多列，用逗号分隔，分隔时不要添加空格  （4）使用sqoop关键字筛选查询导入数据\n$ bin/sqoop import \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--target-dir /user/company \\\n--delete-target-dir \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\" \\\n--table staff \\\n--where \"id=1\"\nRDBMS到Hive\n$ bin/sqoop import \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table staff \\\n--num-mappers 1 \\\n--hive-import \\\n--fields-terminated-by \"\\t\" \\\n--hive-overwrite \\\n--hive-table staff_hive\n  提示：该过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库，第一步默认的临时目录是/user/atguigu/表名RDBMS到Hbase\n$ bin/sqoop import \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table company \\\n--columns \"id,name,sex\" \\\n--column-family \"info\" \\\n--hbase-create-table \\\n--hbase-row-key \"id\" \\\n--hbase-table \"hbase_company\" \\\n--num-mappers 1 \\\n--split-by id\n  提示：sqoop1.4.6只支持HBase1.0.1之前的版本的自动创建HBase表的功能  解决方案：手动创建HBase表\nhbase&gt; create 'hbase_company,'info'\n  (5) 在HBase中scan这张表得到如下内容\nhbase&gt; scan ‘hbase_company’\n3.2 导出数据​        在Sqoop中，“导出”概念指：从大数据集群（HDFS，HIVE，HBASE）向非大数据集群（RDBMS）中传输数据，叫做：导出，即使用export关键字。HIVE/HDFS到RDBMS\n$ bin/sqoop export \\\n  --connect jdbc:mysql://hadoop102:3306/company \\\n  --username root \\\n  --password 000000 \\\n  --table staff \\\n  --num-mappers 1 \\\n  --export-dir /user/hive/warehouse/staff_hive \\\n  --input-fields-terminated-by \"\\t\"\n  提示：Mysql中如果表不存在，不会自动创建\n3.3 脚本打包  使用opt格式的文件打包sqoop命令，然后执行\n\n创建一个.opt文件\n$ mkdir opt\n$ touch opt/job_HDFS2RDBMS.opt\n\n编写sqoop脚本\n$ vi opt/job_HDFS2RDBMS.opt\n\nexport\n--connect jdbc:mysql://hadoop102:3306/company\n--username root\n--password 000000\n--table staff\n--num-mappers 1\n--export-dir /user/hive/warehouse/staff_hive\n--input-fields-terminated-by \"\\t\"\n\n执行该脚本\n$ bin/sqoop --options-file opt/job_HDFS2RDBMS.opt\n\n\n4. Sqoop一些常用命令及参数4.1 常用命令列举这里给大家列出来了一部分Sqoop操作时的常用参数，以供参考，需要深入学习的可以参看对应类的源代码。\n\n\n\n\n序号\n命令\n类\n说明\n\n\n\n\n1\nimport\nImportTool\n将数据导入到集群\n\n\n2\nexport\nExportTool\n将集群数据导出\n\n\n3\ncodegen\nCodeGenTool\n获取数据库中某张表数据生成Java并打包Jar\n\n\n4\ncreate-hive-table\nCreateHiveTableTool\n创建Hive表\n\n\n5\neval\nEvalSqlTool\n查看SQL执行结果\n\n\n6\nimport-all-tables\nImportAllTablesTool\n导入某个数据库下所有表到HDFS中\n\n\n7\njob\nJobTool\n用来生成一个sqoop的任务，生成后，该任务并不执行，除非使用命令执行该任务。\n\n\n8\nlist-databases\nListDatabasesTool\n列出所有数据库名\n\n\n9\nlist-tables\nListTablesTool\n列出某个数据库下所有表\n\n\n10\nmerge\nMergeTool\n将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中\n\n\n11\nmetastore\nMetastoreTool\n记录sqoop job的元数据信息，如果不启动metastore实例，则默认的元数据存储目录为：~/.sqoop，如果要更改存储目录，可以在配置文件sqoop-site.xml中进行更改。\n\n\n12\nhelp\nHelpTool\n打印sqoop帮助信息\n\n\n13\nversion\nVersionTool\n打印sqoop版本信息\n\n\n\n\n4.2 命令&amp;参数详解刚才列举了一些Sqoop的常用命令，对于不同的命令，有不同的参数，让我们来一一列举说明。\n首先来我们来介绍一下公用的参数，所谓公用参数，就是大多数命令都支持的参数。\n4.2.1 公用参数：数据库连接\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—connect\n连接关系型数据库的URL\n\n\n2\n—connection-manager\n指定要使用的连接管理类\n\n\n3\n—driver\nHadoop根目录\n\n\n4\n—help\n打印帮助信息\n\n\n5\n—password\n连接数据库的密码\n\n\n6\n—username\n连接数据库的用户名\n\n\n7\n—verbose\n在控制台打印出详细信息\n\n\n\n\n4.2.2 公用参数：import\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—enclosed-by \n给字段值前加上指定的字符\n\n\n2\n—escaped-by \n对字段中的双引号加转义符\n\n\n3\n—fields-terminated-by \n设定每个字段是以什么符号作为结束，默认为逗号\n\n\n4\n—lines-terminated-by \n设定每行记录之间的分隔符，默认是\\n\n\n\n5\n—mysql-delimiters\nMysql默认的分隔符设置，字段之间以逗号分隔，行之间以\\n分隔，默认转义符是\\，字段值以单引号包裹。\n\n\n6\n—optionally-enclosed-by \n给带有双引号或单引号的字段值前后加上指定字符。\n\n\n\n\n4.2.3 公用参数：export\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—input-enclosed-by \n对字段值前后加上指定字符\n\n\n2\n—input-escaped-by \n对含有转移符的字段做转义处理\n\n\n3\n—input-fields-terminated-by \n字段之间的分隔符\n\n\n4\n—input-lines-terminated-by \n行之间的分隔符\n\n\n5\n—input-optionally-enclosed-by \n给带有双引号或单引号的字段前后加上指定字符\n\n\n\n\n4.2.4 公用参数：hive\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—hive-delims-replacement \n用自定义的字符串替换掉数据中的\\r\\n和\\013 \\010等字符\n\n\n2\n—hive-drop-import-delims\n在导入数据到hive时，去掉数据中的\\r\\n\\013\\010这样的字符\n\n\n3\n—map-column-hive \n生成hive表时，可以更改生成字段的数据类型\n\n\n4\n—hive-partition-key\n创建分区，后面直接跟分区名，分区字段的默认类型为string\n\n\n5\n—hive-partition-value \n导入数据时，指定某个分区的值\n\n\n6\n—hive-home \nhive的安装目录，可以通过该参数覆盖之前默认配置的目录\n\n\n7\n—hive-import\n将数据从关系数据库中导入到hive表中\n\n\n8\n—hive-overwrite\n覆盖掉在hive表中已经存在的数据\n\n\n9\n—create-hive-table\n默认是false，即，如果目标表已经存在了，那么创建任务失败。\n\n\n10\n—hive-table\n后面接要创建的hive表,默认使用MySQL的表名\n\n\n11\n—table\n指定关系数据库的表名\n\n\n\n\n公用参数介绍完之后，我们来按照命令介绍命令对应的特有参数。\n4.2.5 命令&amp;参数：import将关系型数据库中的数据导入到HDFS（包括Hive，HBase）中，如果导入的是Hive，那么当Hive中没有对应表时，则自动创建。\n1) 命令：\n如：导入数据到hive中\n$ bin/sqoop import \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table staff \\\n--hive-import\n如：增量导入数据到hive中，mode=append\nappend导入：\n$ bin/sqoop import \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table staff \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\" \\\n--target-dir /user/hive/warehouse/staff_hive \\\n--check-column id \\\n--incremental append \\\n--last-value 3\n尖叫提示：append不能与—hive-等参数同时使用（Append mode for hive imports is not yet supported. Please remove the parameter —append-mode）\n如：增量导入数据到hdfs中，mode=lastmodified\n先在mysql中建表并插入几条数据：\nmysql&gt; create table company.staff_timestamp(id int(4), name varchar(255), sex varchar(255), last_modified timestamp DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP);\nmysql&gt; insert into company.staff_timestamp (id, name, sex) values(1, 'AAA', 'female');mysql&gt; insert into company.staff_timestamp (id, name, sex) values(2, 'BBB', 'female');\n#先导入一部分数据：\n$ bin/sqoop import \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table staff_timestamp \\\n--delete-target-dir \\\n--m 1\n# 再增量导入一部分数据：\nmysql&gt; insert into company.staff_timestamp (id, name, sex) values(3, 'CCC', 'female');\n$ bin/sqoop import \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table staff_timestamp \\\n--check-column last_modified \\\n--incremental lastmodified \\\n--last-value \"2017-09-28 22:20:38\" \\\n--m 1 \\\n--append\n尖叫提示：使用lastmodified方式导入数据要指定增量数据是要—append（追加）还是要—merge-key（合并）\n尖叫提示：last-value指定的值是会包含于增量导入的数据中\n2) 参数：\n\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—append\n将数据追加到HDFS中已经存在的DataSet中，如果使用该参数，sqoop会把数据先导入到临时文件目录，再合并。\n\n\n2\n—as-avrodatafile\n将数据导入到一个Avro数据文件中\n\n\n3\n—as-sequencefile\n将数据导入到一个sequence文件中\n\n\n4\n—as-textfile\n将数据导入到一个普通文本文件中\n\n\n5\n—boundary-query \n边界查询，导入的数据为该参数的值（一条sql语句）所执行的结果区间内的数据。\n\n\n6\n—columns \n指定要导入的字段\n\n\n7\n—direct\n直接导入模式，使用的是关系数据库自带的导入导出工具，以便加快导入导出过程。\n\n\n8\n—direct-split-size\n在使用上面direct直接导入的基础上，对导入的流按字节分块，即达到该阈值就产生一个新的文件\n\n\n9\n—inline-lob-limit\n设定大对象数据类型的最大值\n\n\n10\n—m或–num-mappers\n启动N个map来并行导入数据，默认4个。\n\n\n11\n—query或—e \n将查询结果的数据导入，使用时必须伴随参—target-dir，—hive-table，如果查询中有where条件，则条件后必须加上$CONDITIONS关键字\n\n\n12\n—split-by \n按照某一列来切分表的工作单元，不能与—autoreset-to-one-mapper连用（请参考官方文档）\n\n\n13\n—table \n关系数据库的表名\n\n\n14\n—target-dir \n指定HDFS路径\n\n\n15\n—warehouse-dir \n与14参数不能同时使用，导入数据到HDFS时指定的目录\n\n\n16\n—where\n从关系数据库导入数据时的查询条件\n\n\n17\n—z或—compress\n允许压缩\n\n\n18\n—compression-codec\n指定hadoop压缩编码类，默认为gzip(Use Hadoop codec default gzip)\n\n\n19\n—null-string \nstring类型的列如果null，替换为指定字符串\n\n\n20\n—null-non-string \n非string类型的列如果null，替换为指定字符串\n\n\n21\n—check-column \n作为增量导入判断的列名\n\n\n22\n—incremental \nmode：append或lastmodified\n\n\n23\n—last-value \n指定某一个值，用于标记增量导入的位置\n\n\n\n\n4.2.6 命令&amp;参数：export从HDFS（包括Hive和HBase）中奖数据导出到关系型数据库中。\n1) 命令：\n如：\n$ bin/sqoop export \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table staff \\\n--export-dir /user/company \\\n--input-fields-terminated-by \"\\t\" \\\n--num-mappers 1 \n2) 参数：\n\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—direct\n利用数据库自带的导入导出工具，以便于提高效率\n\n\n2\n—export-dir \n存放数据的HDFS的源目录\n\n\n3\n-m或—num-mappers \n启动N个map来并行导入数据，默认4个\n\n\n4\n—table \n指定导出到哪个RDBMS中的表\n\n\n5\n—update-key \n对某一列的字段进行更新操作\n\n\n6\n—update-mode \nupdateonlyallowinsert(默认)\n\n\n7\n—input-null-string \n请参考import该类似参数说明\n\n\n8\n—input-null-non-string \n请参考import该类似参数说明\n\n\n9\n—staging-table \n创建一张临时表，用于存放所有事务的结果，然后将所有事务结果一次性导入到目标表中，防止错误。\n\n\n10\n—clear-staging-table\n如果第9个参数非空，则可以在导出操作执行前，清空临时事务结果表\n\n\n\n\n4.2.7 命令&amp;参数：codegen将关系型数据库中的表映射为一个Java类，在该类中有各列对应的各个字段。\n如：\n$ bin/sqoop codegen \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table staff \\\n--bindir /home/admin/Desktop/staff \\\n--class-name Staff \\\n--fields-terminated-by \"\\t\"\n\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—bindir \n指定生成的Java文件、编译成的class文件及将生成文件打包为jar的文件输出路径\n\n\n2\n—class-name \n设定生成的Java文件指定的名称\n\n\n3\n—outdir \n生成Java文件存放的路径\n\n\n4\n—package-name \n包名，如com.z，就会生成com和z两级目录\n\n\n5\n—input-null-non-string \n在生成的Java文件中，可以将null字符串或者不存在的字符串设置为想要设定的值（例如空字符串）\n\n\n6\n—input-null-string \n将null字符串替换成想要替换的值（一般与5同时使用）\n\n\n7\n—map-column-java \n数据库字段在生成的Java文件中会映射成各种属性，且默认的数据类型与数据库类型保持对应关系。该参数可以改变默认类型，例如：—map-column-java id=long, name=String\n\n\n8\n—null-non-string \n在生成Java文件时，可以将不存在或者null的字符串设置为其他值\n\n\n9\n—null-string \n在生成Java文件时，将null字符串设置为其他值（一般与8同时使用）\n\n\n10\n—table \n对应关系数据库中的表名，生成的Java文件中的各个属性与该表的各个字段一一对应\n\n\n\n\n4.2.8 命令&amp;参数：create-hive-table生成与关系数据库表结构对应的hive表结构。\n命令：\n$ bin/sqoop create-hive-table \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table staff \\\n--hive-table hive_staff\n参数：\n\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—hive-home \nHive的安装目录，可以通过该参数覆盖掉默认的Hive目录\n\n\n2\n—hive-overwrite\n覆盖掉在Hive表中已经存在的数据\n\n\n3\n—create-hive-table\n默认是false，如果目标表已经存在了，那么创建任务会失败\n\n\n4\n—hive-table\n后面接要创建的hive表\n\n\n5\n—table\n指定关系数据库的表名\n\n\n\n\n4.2.9 命令&amp;参数：eval可以快速的使用SQL语句对关系型数据库进行操作，经常用于在import数据之前，了解一下SQL语句是否正确，数据是否正常，并可以将结果显示在控制台。\n命令：\n如：\n$ bin/sqoop eval \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--query \"SELECT * FROM staff\"\n参数：\n\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—query或—e\n后跟查询的SQL语句\n\n\n\n\n4.2.10 命令&amp;参数：import-all-tables可以将RDBMS中的所有表导入到HDFS中，每一个表都对应一个HDFS目录\n命令：\n$ bin/sqoop import-all-tables \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--warehouse-dir /all_tables\n参数：\n\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—as-avrodatafile\n这些参数的含义均和import对应的含义一致\n\n\n2\n—as-sequencefile\n\n\n\n3\n—as-textfile\n\n\n\n4\n—direct\n\n\n\n5\n—direct-split-size \n\n\n\n6\n—inline-lob-limit \n\n\n\n7\n—m或—num-mappers \n\n\n\n8\n—warehouse-dir \n\n\n\n9\n-z或—compress\n\n\n\n10\n—compression-codec\n\n\n\n\n4.2.11 命令&amp;参数：job用来生成一个sqoop任务，生成后不会立即执行，需要手动执行。\n命令：\n$ bin/sqoop job \\ \n--create myjob -- import-all-tables \\ \n--connect jdbc:mysql://hadoop102:3306/company \\ \n--username root \\ \n--password 000000$ bin/sqoop job \\\n--list$ bin/sqoop job \\\n--exec myjob\n尖叫提示：注意import-all-tables和它左边的—之间有一个空格\n尖叫提示：如果需要连接metastore，则—meta-connect jdbc:hsqldb:hsql://linux01:16000/sqoop\n参数：\n\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—create \n创建job参数\n\n\n2\n—delete \n删除一个job\n\n\n3\n—exec \n执行一个job\n\n\n4\n—help\n显示job帮助\n\n\n5\n—list\n显示job列表\n\n\n6\n—meta-connect \n用来连接metastore服务\n\n\n7\n—show \n显示一个job的信息\n\n\n8\n—verbose\n打印命令运行时的详细信息\n\n\n\n\n尖叫提示：在执行一个job时，如果需要手动输入数据库密码，可以做如下优化\n&lt;property&gt;\t\n    &lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt;\t\n    &lt;value&gt;true&lt;/value&gt;\t\n    &lt;description&gt;If true, allow saved passwords in the metastore.&lt;/description&gt;&lt;/property&gt;\n4.2.12 命令&amp;参数：list-databases命令：\n$ bin/sqoop list-databases \\\n--connect jdbc:mysql://hadoop102:3306/ \\\n--username root \\\n--password 000000\n参数：与公用参数一样\n4.2.13 命令&amp;参数：list-tables命令：\n$ bin/sqoop list-tables \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000\n参数：与公用参数一样\n4.2.14 命令&amp;参数：merge将HDFS中不同目录下面的数据合并在一起并放入指定目录中\n数据环境：\nnew_staff\n1    AAA   male\n2    BBB   male\n3    CCC   male\n4    DDD   male\n\nold_staff\n1    AAA   female\n2    CCC   female\n3    BBB   female\n6    DDD   female\n尖叫提示：上边数据的列之间的分隔符应该为\\t，行与行之间的分割符为\\n，如果直接复制，请检查之。\n命令：\n# 创建JavaBean：\n$ bin/sqoop codegen \\\n--connect jdbc:mysql://hadoop102:3306/company \\\n--username root \\\n--password 000000 \\\n--table staff \\\n--bindir /home/admin/Desktop/staff \\\n--class-name Staff \\\n--fields-terminated-by \"\\t\" \n\n# 开始合并：\n$ bin/sqoop merge \\\n--new-data /test/new/ \\\n--onto /test/old/ \\\n--target-dir /test/merged \\\n--jar-file /home/admin/Desktop/staff/Staff.jar \\\n--class-name Staff \\\n--merge-key id\n# 结果：\n1\tAAA\tMALE\n2\tBBB\tMALE\n3\tCCC\tMALE\n4\tDDD\tMALE\n6\tDDD\tFEMALE\n参数：\n\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—new-data \nHDFS 待合并的数据目录，合并后在新的数据集中保留\n\n\n2\n—onto \nHDFS合并后，重复的部分在新的数据集中被覆盖\n\n\n3\n—merge-key \n合并键，一般是主键ID\n\n\n4\n—jar-file \n合并时引入的jar包，该jar包是通过Codegen工具生成的jar包\n\n\n5\n—class-name \n对应的表名或对象名，该class类是包含在jar包中的\n\n\n6\n—target-dir \n合并后的数据在HDFS里存放的目录\n\n\n\n\n4.2.15 命令&amp;参数：metastore记录了Sqoop job的元数据信息，如果不启动该服务，那么默认job元数据的存储目录为~/.sqoop，可在sqoop-site.xml中修改。\n命令：\n如：启动sqoop的metastore服务\n$ bin/sqoop metastore\n参数：\n\n\n\n\n序号\n参数\n说明\n\n\n\n\n1\n—shutdown\n关闭metastore\n\n\n\n\n","slug":"B15-Sqoop","date":"2021-12-14T06:01:37.000Z","categories_index":"大数据","tags_index":"Sqoop","author_index":"YFR718"},{"id":"a3f5b417ccee185425acbc3a549133c3","title":"LeetCode做题记录","content":"1111111111111111111\n题型汇总\n\n\n\n算法\n细分\n题目\n\n\n\n\n哈希\n\n1\n\n\n双指针\n\n3\n\n\n动态规划\n\n5\n\n\n二分\n\n4\n\n\n\n\n刷题汇总\n\n\n\n题号\n题目\n算法\n最优解\n时间复杂度\n它解\n独立解决\n难度\nbest\n\n\n\n\n1\n两数之和\n哈希\n💚\n\n\n🤩\n😘\n💖\n\n\n2\n两数相加\n模拟\n💚\n\n\n🤩\n🤔\n💖\n\n\n3\n无重复字符的最长子串\n双指针\n💚\n\n\n🤩\n🤔\n\n\n\n4\n寻找两个正序数组的中位数\n二分\n💚\n\n🤣\n😭\n🥵\n💖\n\n\n5\n最长回文子串\n中心扩散\n🤎\n\n😒马拉车二分+哈希\n😭\n🤔\n\n\n\n6\nZ 字形变换\n数学规律\n💚\n\n\n🤩\n🤔\n\n\n\n7\n整数反转\n初等数学\n💚\n\n\n🤩\n😘\n💖\n\n\n8\n字符串转换整数 (atoi)\n模拟\n💚\n\n\n🤩\n🤔\n💖\n\n\n9\n回文数\n模拟\n💚\n\n\n🤩\n😘\n\n\n\n10\n正则表达式匹配\ndp\n💔\nacwing2h13m\n\n😭\n🥵\n\n\n\n11\n盛最多水的容器\n贪心+双指针\n💚\n\n\n🤔\n🤔\n\n\n\n12\n整数转罗马数字\n贪心\n💚\n\n\n🤩\n🤔\n💖\n\n\n13\n罗马数字转整数\n模拟\n💚\n\n\n🤩\n😘\n\n\n\n14\n最长公共前缀\n模拟\n💚\n\n\n🤔\n😘\n\n\n\n15\n三数之和\n排序+双指针\n💚\n\n\n🤔\n🤔\n\n\n\n16\n最接近的三数之和\n排序+双指针\n🤎\n\n\n😭\n🤔\n\n\n\n17\n电话号码的字母组合\ndfs\n💚\n\n\n🤔\n🤔\n\n\n\n18\n四数之和\n排序+双指针\n💚\n\n\n🤩\n🤔\n\n\n\n19\n删除链表的倒数第 N 个结点\n双指针\n💚\n\n栈√\n🤩\n🤔\n💖\n\n\n20\n有效的括号\n栈\n💚\n\n\n🤩\n😘\n💖\n\n\n21\n合并两个有序链表\n模拟\n💚\n\n递归√\n🤩\n😘\n💖\n\n\n22\n括号生成\ndfs\n💔💚\n\n\n😭\n🤔\n💖\n\n\n23\n合并K个升序链表\n归并+合并\n💚\n\n堆排序\n🤩\n🥵\n\n\n\n24\n两两交换链表中的节点\n递归\n💚\n\n\n🤩\n🤔\n💖\n\n\n25\n k个一组翻转链表\n递归\n💚\n\n\n🤔\n🥵\n💖\n\n\n26\n删除有序数组中的重复项\n双指针\n💚\n\n\n🤩\n😘\n💖\n\n\n27\n移除元素\n双指针\n💚\n\n\n🤩\n😘\n💖\n\n\n28\n实现 strStr()\n暴力\n🤎\n\nKMP\n🤔\n😘\n\n\n\n29\n两数相除\n倍减法\n🤎\n\n二分\n🤔\n🤔\n坑太多了\n\n\n30\n串联所有单词的子串\n\n💔\nACwing1h42m\n\n😭\n🥵\n\n\n\n31\n下一个排列\n思维\n💔💚\n\n\n😭\n🤔\n💖\n\n\n32\n最长有效括号\n\n💔\n\n\n😭\n🥵\n\n\n\n33\n搜索旋转排序数组\n两次二分\n🤎\n\n一次二分\n🤔\n🤔\n💖\n\n\n34\n有序数组查找第一最后一个位置\n二分\n💚\n\n\n🤩\n🤔\n💖\n\n\n35\n搜索插入位置\n二分\n💚\n\n\n🤩\n😘\n💖\n\n\n36\n有效的数独\n暴力模拟\n🤎\n\n一次遍历\n🤔\n🤔\n\n\n\n37\n解数独\ndfs\n💔💚\n\n\n😭\n🥵\n\n\n\n38\n外观数列\n模拟\n🤎\n\n打表√\n🤔\n🤔\n待优化\n\n\n39\n组合总和\ndfs\n💚\n\n\n🤩\n🤔\n\n\n\n40\n组合总和 II\ndfs\n💔💚\n\n\n😭\n🤔\n去重方法\n\n\n41\n缺失的第一个正数\n置换\n🤎💚\n\n哈希\n🤔\n🤔\n💖\n\n\n42\n接雨水\n贪心\n🤎\n\n单调栈\n🤔\n🥵\n\n\n\n43\n字符串相乘\n大数\n🤎\n\n还没做\n🤔\n🤔\n\n\n\n44\n通配符匹配\nDp\n💔💚\n\n\n😭\n🥵\n\n\n\n45\n跳跃游戏 II\n贪心\n💚\n\ndp值得一看\n🤩\n🤔\n💖\n\n\n46\n全排列\ndfs\n💚\n\n\n🤩\n🤔\n💖\n\n\n47\n全排列 II\ndfs\n💚\n\n\n🤔\n🤔\n\n\n\n48\n旋转图像\n数学规律\n💚\n\nACwing’1h40m\n🤔\n🤔\n💖\n\n\n49\n字母异位词分组\n\n\n\n\n\n\n\n\n\n50\nPow(x, n)\n\n\n\n\n\n\n\n\n\n51\n\n\n\n\n\n\n\n\n\n\n52\n\n\n\n\n\n\n\n\n\n\n53\n\n\n\n\n\n\n\n\n\n\n54\n\n\n\n\n\n\n\n\n\n\n55\n\n\n\n\n\n\n\n\n\n\n56\n\n\n\n\n\n\n\n\n\n\n57\n\n\n\n\n\n\n\n\n\n\n58\n\n\n\n\n\n\n\n\n\n\n59\n\n\n\n\n\n\n\n\n\n\n60\n\n\n\n\n\n\n\n\n\n\n61\n\n\n\n\n\n\n\n\n\n\n62\n\n\n\n\n\n\n\n\n\n\n63\n\n\n\n\n\n\n\n\n\n\n64\n\n\n\n\n\n\n\n\n\n\n65\n\n\n\n\n\n\n\n\n\n\n66\n\n\n\n\n\n\n\n\n\n\n67\n\n\n\n\n\n\n\n\n\n\n68\n\n\n\n\n\n\n\n\n\n\n69\n\n\n\n\n\n\n\n\n\n\n70\n\n\n\n\n\n\n\n\n\n\n71\n\n\n\n\n\n\n\n\n\n\n72\n\n\n\n\n\n\n\n\n\n\n73\n\n\n\n\n\n\n\n\n\n\n74\n\n\n\n\n\n\n\n\n\n\n75\n\n\n\n\n\n\n\n\n\n\n76\n\n\n\n\n\n\n\n\n\n\n77\n\n\n\n\n\n\n\n\n\n\n78\n\n\n\n\n\n\n\n\n\n\n79\n\n\n\n\n\n\n\n\n\n\n80\n\n\n\n\n\n\n\n\n\n\n81\n\n\n\n\n\n\n\n\n\n\n82\n\n\n\n\n\n\n\n\n\n\n83\n\n\n\n\n\n\n\n\n\n\n84\n\n\n\n\n\n\n\n\n\n\n85\n\n\n\n\n\n\n\n\n\n\n86\n\n\n\n\n\n\n\n\n\n\n87\n\n\n\n\n\n\n\n\n\n\n88\n\n\n\n\n\n\n\n\n\n\n89\n\n\n\n\n\n\n\n\n\n\n90\n\n\n\n\n\n\n\n\n\n\n91\n\n\n\n\n\n\n\n\n\n\n92\n\n\n\n\n\n\n\n\n\n\n93\n\n\n\n\n\n\n\n\n\n\n94\n\n\n\n\n\n\n\n\n\n\n95\n\n\n\n\n\n\n\n\n\n\n96\n\n\n\n\n\n\n\n\n\n\n97\n\n\n\n\n\n\n\n\n\n\n98\n\n\n\n\n\n\n\n\n\n\n\n算法分类对应题目分类","slug":"l00","date":"2021-11-05T13:51:29.000Z","categories_index":"LeetCode","tags_index":"","author_index":"YFR718"},{"id":"71dcb878b30b2fb16e5265f7515744d5","title":"疑难杂症队列","content":"Java\n整理笔记，等待发布\n尚硅谷整理的Java学习\n书5.7跳过\n\nLinux\n笔记整理，待发布\nshell编程笔记整理，待发布\n\n算法\nleetcode每日三道题\nACwing课程学习\n\nHadoop\nP88 job提交源码\nP89 切片源码\nP97 、98自定义分区\nP100~102\nP104 Combiner\nP107 OutPutFormat\nP101 源码解析\n\n\nHive\nP12 hive启动脚本\n\nHbase\nP15 root-\n\n","slug":"1-疑难杂症队列","date":"2021-11-04T05:27:30.000Z","categories_index":"学习规划","tags_index":"","author_index":"YFR718"},{"id":"f73a8e23e6f6f669cf99c7dba8fa0722","title":"","content":"title: Z1.算法刷题总结date: 2021-12-10 13:18:02comments: true #是否可评论toc: true  #是否显示文章目录cover: https://img-blog.csdnimg.cn/694ad945ebe7403ea1f05e05ab6938f5.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_centercategories: \n\n“LeetCode” #分类tags: \n算法\n\n题目的坑null值问题\n\n\n\nnull出现情况\n\n\n\n\n\n链表为null\n\n\n\n字符串长度为0\n\n\n\n数组长度为0\n\n\n\n\n多情况讨论\nn为奇数，n为偶数\n为正数、为负数\n\nint溢出\nint范围：[-pow(2,31),pow(2,31)-1]\nint范围：[-2147483648，2147483647]\n\n溢出的判断\n//数学推公式\nif( x &lt; 0 &amp;&amp; ans &lt; (-Math.pow(2,31) - x %10) / 10) return 0;\nif( x > 0 &amp;&amp; ans > (Math.pow(2,31) - x %10 -1) / 10) return 0;\n\n//小技巧：看除10与原来的值是否相等\nint temp = result;\nresult = (x % 10) + (result * 10);  \nif (result / 10 != temp) return 0;\n\n基础n数之和\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n无序数组，两数之和\n遍历，哈希查找另一个数，不存在就存在哈希表里\n$O(n)$\n1\n\n\n排序好的数组，两数之和\n双指针\n$O(n)$\n\n\n\n三数之和\n排序，for循环加双数之和\n$O(n^2)$\n1516\n\n\n四数之和\n双重for循环+双数之和，去重方法：和前一个数一样就跳过\n$O(n^3)$\n18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n反转\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n整数反转\n不断取最后一位加到新结果的后面\n$O(n)$\n7\n\n\n判断回文数\n对数字反转，看是否相等\n$O(n)$\n9\n\n\n\n\n\n\n\n\n\n双指针\n好马不吃回头草，双指针都朝单一方向移动\n\n同向移动\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n最长不重复子串\n不断移动右指针，维护左指针的位置和更新答案\n$O(n)$\n3\n\n\n删除链表的倒数第 N 个结点\n右指针在左指针前n-1个，两指针同时向右走，右指针到头，左指针就是答案\n$O(n)$\n19\n\n\n删除有序数组中的重复项\n左指针是正确的位置，右指针是当前遍历的位置\n$O(n)$\n26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n反向移动\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n有序数组，两数之和\n左右和大于t，右向中间移动，反之左向中间移动\n$O(n)$\n1\n\n\n盛水最多的容器\n不断将短的指针向中间移动，更新答案\n$O(n)$\n11\n\n\n\n\n\n\n\n\n\n栈\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n有效的括号\n左括号入栈，右括号出栈，进行判断\n$O(n)$\n20\n\n\n最长有效括号\n有点难啊\n$O(n)$\n32\n\n\n\n\n\n\n\n\n\n单调栈\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n接雨水\n单调递减栈，高于栈顶，取数加雨水\n$O(n)$\n42\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n二分查找\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n搜索旋转排序数组\n可以两次二分，也可以一次解决\n$O(logn)$\n33\n\n\n排序数组查找第一和最后位置\n两次二分，或二分一次再扩散·\n$O(logn)$\n34\n\n\n搜索插入位置\n简单的二分\n$O(logn)$\n35\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n递归\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n合并K个升序链表\n归并+合并\n$O(nlogn)$\n4\n\n\n合并两个有序链表\n大化小，小化了\n$O(n)$\n21\n\n\n两两交换链表中的节点\n大化小，小化了\n$O(n)$\n24\n\n\nK 个一组翻转链表\n大化小，小化了\n$O(n)$\n25、27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n归并\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n两个有序数组的中位数\n分别看两数组的中位数，小的那个左边的数一定在整体中位数前面\n$O(logn)$\n23\n\n\n合并两个有序链表\n大化小，小化了\n$O(n)$\n21\n\n\n\n\n\n\n\n\n\nDFS\n\n\n\n题目\n思路\n复杂度\n题目\n\n\n\n\n枚举所有组合\ndfs暴搜\n$O(logn)$\n17\n\n\n括号生成\n控制所有前缀左括号数量&gt;=右括号数量\n$O(C^n_{2n})$\n22\n\n\n解数独\ndfs暴搜，注意该停就停\n\n37\n\n\n组合总和\ndfs暴搜，注意不重复\n\n39\n\n\n全排列\ndfs暴搜，注意不重复\n\n46、47\n\n\n\n\n\n\n\n\n\nDp\n\n\n\n题目\n思路\n复杂度\n\n\n\n\n\n通配符匹配\ndp\n$O(n^2)$\n44\n\n\n跳跃游戏 II\ndp\n$O(n)$\n45\n\n\n\n\n\n\n\n\n\n\n数学数学规律\n\n\n\n题目\n思路\n复杂度\n\n\n\n\n\nZ 字形变换\n分别看两数组的中位数，小的那个左边的数一定在整体中位数前面\n$O(logn)$\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n指数变化计算快速幂倍减\n\n\n\n题目\n思路\n复杂度\n\n\n\n\n\n两数相除\n从大到小倍减\n$O(logn)$\n29\n\n\n旋转图像\n找到旋转的规律\n$O(n^2)$\n48\n\n\n\n\n\n\n\n\n\n字符串马拉车\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n最长回文子串\n\n$O()$\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKMP\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n查找子串的位置\n标准KMP\n$O(n)$\n28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n字典序\n\n\n\n题目\n思路\n复杂度\n题号\n\n\n\n\n下一个排列\n从右向左找到第一个递减的数字，将其与右边第一个比他大的交换，然后对刚才那个数右边的逆序\n$O(n)$\n31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Z1-算法刷题总结","date":"2021-12-10T05:18:02.659Z","categories_index":"","tags_index":"","author_index":"YFR718"},{"id":"bdaca94a32610d2aa675f6770452dae7","title":"J9.JAVA程序设计常用","content":"数组//数组创建\nint[] a = new int[100];\nint[] a = new int[]{0,1,2};\n\n//数组长度\nint len = a.length;\n\n//取数\nfor(int i = 0; i &lt; a.length; i++){\n    System.out.println(a[i]);\n}\nListStringBuilderStringBuilder s = new StringBuilder();\ns.append(')');\ns.deleteCharAt(s.length() - 1);\nMapHashMap// 创建\nHashMap&lt;Integer,Integer&gt; ha = new HashMap&lt;Integer,Integer&gt;();\n//添加键值对put(k,v)\nha.put(1,1);\n//访问元素get(k)\nha.get(1);\n//删除元素remove(k)\nha.remove(k);\n//清空数据clear()\nha.clear();\n//判断k是否存在containsKey()\nha.containsKey(1);\n// 输出 key 和 value\nfor (Integer i : Sites.keySet()) {\nSystem.out.println(\"key: \" + i + \" value: \" + Sites.get(i));\n}\n// 返回所有 value 值\nfor(String value: Sites.values()) {\n// 输出每一个value\nSystem.out.print(value + \", \");\n}\n\n\n\n\n\n\n\n\n\nJava HashMap 常用方法列表如下：\n\n\n\n\n方法\n描述\n\n\n\n\nclear()\n删除 hashMap 中的所有键/值对\n\n\nclone()\n复制一份 hashMap\n\n\nisEmpty()\n判断 hashMap 是否为空\n\n\nsize()\n计算 hashMap 中键/值对的数量\n\n\nput()\n将键/值对添加到 hashMap 中\n\n\nputAll()\n将所有键/值对添加到 hashMap 中\n\n\nputIfAbsent()\n如果 hashMap 中不存在指定的键，则将指定的键/值对插入到 hashMap 中。\n\n\nremove()\n删除 hashMap 中指定键 key 的映射关系\n\n\ncontainsKey()\n检查 hashMap 中是否存在指定的 key 对应的映射关系。\n\n\ncontainsValue()\n检查 hashMap 中是否存在指定的 value 对应的映射关系。\n\n\nreplace()\n替换 hashMap 中是指定的 key 对应的 value。\n\n\nreplaceAll()\n将 hashMap 中的所有映射关系替换成给定的函数所执行的结果。\n\n\nget()\n获取指定 key 对应对 value\n\n\ngetOrDefault()\n获取指定 key 对应对 value，如果找不到 key ，则返回设置的默认值\n\n\nforEach()\n对 hashMap 中的每个映射执行指定的操作。\n\n\nentrySet()\n返回 hashMap 中所有映射项的集合集合视图。\n\n\nkeySet()\n返回 hashMap 中所有 key 组成的集合视图。\n\n\nvalues()\n返回 hashMap 中存在的所有 value 值。\n\n\nmerge()\n添加键值对到 hashMap 中\n\n\ncompute()\n对 hashMap 中指定 key 的值进行重新计算\n\n\ncomputeIfAbsent()\n对 hashMap 中指定 key 的值进行重新计算，如果不存在这个 key，则添加到 hasMap 中\n\n\ncomputeIfPresent()\n对 hashMap 中指定 key 的值进行重新计算，前提是该 key 存在于 hashMap 中。\n\n\n\n\n ","slug":"J9-JAVA程序设计常用","date":"2021-12-09T02:07:42.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"1ef108aa9ecabbdf8ffcf18de47743c9","title":"B14.Flink","content":"","slug":"B14-Flink","date":"2021-11-26T06:26:46.000Z","categories_index":"","tags_index":"","author_index":"YFR718"},{"id":"5ba67c79c316c64fb891710085fcb117","title":"B13.Spark","content":"","slug":"B13-Spark","date":"2021-11-26T06:26:30.000Z","categories_index":"","tags_index":"","author_index":"YFR718"},{"id":"6d3c7a83d596d0745187b745b2ec60dc","title":"B12.scala","content":"","slug":"B12-scala","date":"2021-11-26T06:26:22.000Z","categories_index":"","tags_index":"","author_index":"YFR718"},{"id":"b244e3b85104863ef08f66c0cb6a98b0","title":"B11.kafka","content":"","slug":"B11-kafka","date":"2021-11-26T06:25:49.000Z","categories_index":"","tags_index":"","author_index":"YFR718"},{"id":"f6c5ab269c53087419eb1c0be5e917d0","title":"Flume","content":"1. Flume 概述​        Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单。\n\nFlume 组成架构如下图所示：\n\n\nAgentAgent 是一个 JVM 进程，它以事件的形式将数据从源头送至目的。Agent 主要有 3 个部分组成，Source、Channel、Sink。\nSourceSource 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、taildir、sequence generator、syslog、http、legacy。\nSinkSink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。\nChannelChannel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上。Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个Sink 的读取操作。Flume 自带两种 Channel：Memory Channel 和 File Channel。\nMemory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。\nFile Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。\n\n\nEvent传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。Event 由 Header 和 Body 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构，Body 用来存放该条数据，形式为字节数组。\n\n2. Flume 入门2.1 Flume 安装部署安装地址：\n（1）Flume 官网地址：http://flume.apache.org/（2）文档查看地址：http://flume.apache.org/FlumeUserGuide.html（3）下载地址：http://archive.apache.org/dist/flume/、\n安装部署：\n（1）将 apache-flume-1.9.0-bin.tar.gz 上传到 linux 的/opt/software 目录下（2）解压 apache-flume-1.9.0-bin.tar.gz 到/opt/module/目录下\n[atguigu@hadoop102 software]$ tar -zxf /opt/software/apache flume-1.9.0-bin.tar.gz -C /opt/module/\n（3）修改 apache-flume-1.9.0-bin 的名称为 flume\n[atguigu@hadoop102 module]$ mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume\n（4）将 lib 文件夹下的 guava-11.0.2.jar 删除以兼容 Hadoop 3.1.3\n[atguigu@hadoop102 lib]$ rm /opt/module/flume/lib/guava-11.0.2.jar\n2.2 入门案例1） 监控端口数据官方案例\n案例需求：使用 Flume 监听一个端口，收集该端口数据，并打印到控制台。\n需求分析：\n\n\n\n实现步骤：（1）安装 netcat 工具\n[atguigu@hadoop102 software]$ sudo yum install -y nc\n（2）判断 44444 端口是否被占用\n[atguigu@hadoop102 flume-telnet]$ sudo netstat -nlp | grep 44444\n（3）创建 Flume Agent 配置文件 flume-netcat-logger.conf（4）在 flume 目录下创建 job 文件夹并进入 job 文件夹。\n[atguigu@hadoop102 flume]$ mkdir job\n[atguigu@hadoop102 flume]$ cd job/\n（5）在 job 文件夹下创建 Flume Agent 配置文件 flume-netcat-logger.conf。\n[atguigu@hadoop102 job]$ vim flume-netcat-logger.conf\n（6）在 flume-netcat-logger.conf 文件中添加如下内容。添加内容如下：\n# Name the components on this agent\n# a1:表示agent的名称\n# r1:表示a1的Source的名称\n# k1:表示a1的Sink的名称\n# c1:表示a1的Channel的名称\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\n# 表示a1的输入源类型为netcat端口类型\n# 表示a1的监听的主机\n# 表示a1的监听的端口号\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\n# 表示a1的输出目的地是控制台logger类型\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\n# 表示a1的channel类型是memory内存型\n# 表示a1的channel总容量1000个event\n# 表示a1的channel传输时收集到了100条event以后再去提交事务\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\n# 表示将r1和c1连接起来\n# 表示将k1和c1连接起来\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n（7）先开启 flume 监听端口第一种写法：\n[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console\n第二种写法：\n[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console\n参数说明：—conf/-c：表示配置文件存储在 conf/目录—name/-n：表示给 agent 起名为 a1—conf-file/-f：flume 本次启动读取的配置文件是在 job 文件夹下的 flume-telnet.conf文件。-Dflume.root.logger=INFO,console ：-D 表示 flume 运行时动态修改 flume.root.logger参数属性值，并将控制台日志打印级别设置为 INFO 级别。日志级别包括:log、info、warn、error。\n（8）使用 netcat 工具向本机的 44444 端口发送内容\n[atguigu@hadoop102 ~]$ nc localhost 44444\nhello \natguigu\n（9）在 Flume 监听页面观察接收数据情况\n\n\n\n2） 实时监控单个追加文件\n案例需求：实时监控 Hive 日志，并上传到 HDFS 中 \n\n需求分析：\n\n\n\n\n实现步骤：（1)Flume 要想将数据输出到 HDFS，依赖 Hadoop 相关 jar 包检查/etc/profile.d/my_env.sh 文件，确认 Hadoop 和 Java 环境变量配置正确\nJAVA_HOME=/opt/module/jdk1.8.0_212\nHADOOP_HOME=/opt/module/ha/hadoop-3.1.3\nPATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\nexport PATH JAVA_HOME HADOOP_HOME\n（2)创建 flume-file-hdfs.conf 文件\n[atguigu@hadoop102 job]$ vim flume-file-hdfs.conf\n注：要想读取 Linux 系统中的文件，就得按照 Linux 命令的规则执行命令。由于 Hive日志在 Linux 系统中所以读取文件的类型选择：exec 即 execute 执行的意思。表示执行Linux 命令来读取文件。添加如下内容:\n# Name the components on this agent\n# 定义source\n# 定义sink\n# 定义channe\na2.sources = r2\na2.sinks = k2\na2.channels = c2\n\n# Describe/configure the source\n#定义source类型为exec可执行命令的\na2.sources.r2.type = exec\n#执行shell脚本的绝对路径\na2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log\na2.sources.r2.shell = /bin/bash -c\n\n# Describe the sink\na2.sinks.k2.type = hdfs\na2.sinks.k2.hdfs.path = hdfs://hadoop102:9820/flume/%Y%m%d/%H\n#上传文件的前缀\na2.sinks.k2.hdfs.filePrefix = logs- \n#是否按照时间滚动文件夹\na2.sinks.k2.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na2.sinks.k2.hdfs.roundValue = 1\n#重新定义时间单位\na2.sinks.k2.hdfs.roundUnit = hour\n#是否使用本地时间戳\na2.sinks.k2.hdfs.useLocalTimeStamp = true\n#积攒多少个 Event 才 flush 到 HDFS 一次\na2.sinks.k2.hdfs.batchSize = 100\n#设置文件类型，可支持压缩\na2.sinks.k2.hdfs.fileType = DataStream\n#多久生成一个新的文件\na2.sinks.k2.hdfs.rollInterval = 60\n#设置每个文件的滚动大小\na2.sinks.k2.hdfs.rollSize = 134217700\n#文件的滚动与 Event 数量无关\na2.sinks.k2.hdfs.rollCount = 0\n\n# Use a channel which buffers events in memory\na2.channels.c2.type = memory\na2.channels.c2.capacity = 1000\na2.channels.c2.transactionCapacity = 100\n# Bind the source and sink to the channel\na2.sources.r2.channels = c2\na2.sinks.k2.channel = c2\n注意：对于所有与时间相关的转义序列，Event Header 中必须存在以 “timestamp”的key（除非 hdfs.useLocalTimeStamp 设置为 true，此方法会使用 TimestampInterceptor 自动添加 timestamp）。a3.sinks.k3.hdfs.useLocalTimeStamp = true\n（3）运行 Flume\n[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf\n（4）开启 Hadoop 和 Hive 并操作 Hive 产生日志\n[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh\n[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh\n[atguigu@hadoop102 hive]$ bin/hive\nhive (default)&gt;\n（5）在 HDFS 上查看文件。\n\n\n3）实时监控目录下多个新文件\n案例需求：使用 Flume 监听整个目录的文件，并上传至 HDFS \n\n需求分析：\n\n\n实现步骤：（1）创建配置文件 flume-dir-hdfs.conf\n[atguigu@hadoop102 job]$ vim flume-dir-hdfs.conf\n #定义source\n#定义sink\n#定义channel\na3.sources = r3\na3.sinks = k3\na3.channels = c3\n\n# Describe/configure the source\n#定义source类型为目录\na3.sources.r3.type = spooldir\n#定义监控目录\na3.sources.r3.spoolDir = /opt/module/flume/upload\n#定义文件上传完，后缀\na3.sources.r3.fileSuffix = .COMPLETED\n#是否有文件头\na3.sources.r3.fileHeader = true\n#忽略所有以.tmp 结尾的文件，不上传\na3.sources.r3.ignorePattern = ([^ ]*\\.tmp)\n\n# Describe the sink\n#sink类型为hdfs\na3.sinks.k3.type = hdfs\na3.sinks.k3.hdfs.path = hdfs://hadoop102:9820/flume/upload/%Y%m%d/%H\n#上传文件的前缀\na3.sinks.k3.hdfs.filePrefix = upload- \n#是否按照时间滚动文件夹\na3.sinks.k3.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na3.sinks.k3.hdfs.roundValue = 1\n#重新定义时间单位\na3.sinks.k3.hdfs.roundUnit = hour\n#是否使用本地时间戳\na3.sinks.k3.hdfs.useLocalTimeStamp = true\n#积攒多少个 Event 才 flush 到 HDFS 一次\na3.sinks.k3.hdfs.batchSize = 100\n#设置文件类型，可支持压缩\na3.sinks.k3.hdfs.fileType = DataStream\n#多久生成一个新的文件\na3.sinks.k3.hdfs.rollInterval = 60\n#设置每个文件的滚动大小大概是 128M\na3.sinks.k3.hdfs.rollSize = 134217700\n#文件的滚动与 Event 数量无关\na3.sinks.k3.hdfs.rollCount = 0\n\n# Use a channel which buffers events in memory\na3.channels.c3.type = memory\na3.channels.c3.capacity = 1000\na3.channels.c3.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na3.sources.r3.channels = c3\na3.sinks.k3.channel = c3\n （2）启动监控文件夹命令\n[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf\n 说明：在使用 Spooling Directory Source 时，不要在监控目录中创建并持续修改文 件；上传完成的文件会以.COMPLETED 结尾；被监控文件夹每 500 毫秒扫描一次文件变动。 \n（3）向 upload 文件夹中添加文件 在/opt/module/flume 目录下创建 upload 文件夹\n[atguigu@hadoop102 flume]$ mkdir upload\n 向 upload 文件夹中添加文件\n[atguigu@hadoop102 upload]$ touch atguigu.txt\n[atguigu@hadoop102 upload]$ touch atguigu.tmp\n[atguigu@hadoop102 upload]$ touch atguigu.log\n （4）查看 HDFS 上的数据\n\n\n4）实时监控目录下的多个追加文件\nExec source 适用于监控一个实时追加的文件，不能实现断点续传；\nSpooldir Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；\nTaildir Source适合用于监听多个实时追加的文件，并且能够实现断点续传。\n\n\n案例需求:使用 Flume 监听整个目录的实时追加文件，并上传至 HDFS \n\n需求分析:\n\n\n实现步骤：（1）创建配置文件 flume-taildir-hdfs.conf\n[atguigu@hadoop102 job]$ vim flume-taildir-hdfs.conf\n#定义source\n#定义sink\n#定义channel\na3.sources = r3\na3.sinks = k3\na3.channels = c3\n\n# Describe/configure the source\n#定义source类型\na3.sources.r3.type = TAILDIR\na3.sources.r3.positionFile = /opt/module/flume/tail_dir.json\n#指定position_file位置\na3.sources.r3.filegroups = f1 f2\n#定义监控目录文件\na3.sources.r3.filegroups.f1 = /opt/module/flume/files/.*file.*\na3.sources.r3.filegroups.f2 = /opt/module/flume/files2/.*log.*\n\n# Describe the sink\n#sink类型为hdfs\na3.sinks.k3.type = hdfs\na3.sinks.k3.hdfs.path = hdfs://hadoop102:9820/flume/upload2/%Y%m%d/%H\n#上传文件的前缀\na3.sinks.k3.hdfs.filePrefix = upload-\n#是否按照时间滚动文件夹\na3.sinks.k3.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na3.sinks.k3.hdfs.roundValue = 1\n#重新定义时间单位\na3.sinks.k3.hdfs.roundUnit = hour\n#是否使用本地时间戳\na3.sinks.k3.hdfs.useLocalTimeStamp = true\n#积攒多少个 Event 才 flush 到 HDFS 一次\na3.sinks.k3.hdfs.batchSize = 100\n#设置文件类型，可支持压缩\na3.sinks.k3.hdfs.fileType = DataStream\n#多久生成一个新的文件\na3.sinks.k3.hdfs.rollInterval = 60\n#设置每个文件的滚动大小大概是 128M\na3.sinks.k3.hdfs.rollSize = 134217700\n#文件的滚动与 Event 数量无关\na3.sinks.k3.hdfs.rollCount = 0\n\n# Use a channel which buffers events in memory\na3.channels.c3.type = memory\na3.channels.c3.capacity = 1000\na3.channels.c3.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na3.sources.r3.channels = c3\na3.sinks.k3.channel = c3\n （2）启动监控文件夹命令\n[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-taildir-hdfs.conf\n （3）向 files 文件夹中追加内容 在/opt/module/flume 目录下创建 files 文件夹\n[atguigu@hadoop102 flume]$ mkdir files向 upload 文件夹中添加文件\n[atguigu@hadoop102 files]$ echo hello &gt;&gt; file1.txt\n[atguigu@hadoop102 files]$ echo atguigu &gt;&gt; file2.txt\n （4）查看 HDFS 上的数据Taildir 说明： \n    Taildir Source 维护了一个 json 格式的 position File，其会定期的往 position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。Position File 的格式如下：\n{\"inode\":2496272,\"pos\":12,\"file\":\"/opt/module/flume/files/file1.txt\"}\n{\"inode\":2496275,\"pos\":12,\"file\":\"/opt/module/flume/files/file2.txt\"}\n 注：Linux 中储存文件元数据的区域就叫做 inode，每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件，Unix/Linux 系统内部不使用文件名，而使用 inode 号码来识别文件。\n\n\n3. Flume 进阶3.1 Flume 事务Put事务流程\n\ndoPut:将批数据先写入临时缓冲区putList\ndoCommit:检查channel内存队列是否足够合并。\ndoRolback:channel内存队列空间不足，回滚数据\n\nTake事务\n\ndoTake:将数据取到临时缓冲区takeList,并将数据发送到HDFS\ndoCommit:如果数据全部发送成功,则清除临时缓冲区takeList\ndoRollback:数据发送过程中如果出现异常, rollback将临时缓冲区takeList中的数据归还给channel内存队列。\n\n3.2 Flume Agent 内部原理\n重要组件： \n\nChannelSelector \n\nChannelSelector 的作用就是选出 Event 将要被发往哪个 Channel。其共有两种类型，分别是 Replicating（复制）和 Multiplexing（多路复用）。\nReplicatingSelector 会将同一个 Event 发往所有的 Channel，Multiplexing 会根据相应的原则，将不同的 Event 发往不同的 Channel。 \n\n\nSinkProcessorSinkProcessor 共 有 三 种 类 型 \n\nDefaultSinkProcessor 对 应 的 是 单 个 的 Sink \nLoadBalancingSinkProcessor 和FailoverSinkProcessor 对应的是 Sink Group，LoadBalancingSinkProcessor 可以实现负载均衡的功能，FailoverSinkProcessor 可以错误恢复的功能。\n\n\n\n3.3 Flume 拓扑结构1）简单串联\n​        这种模式是将多个 flume 顺序连接起来了，从最初的 source 开始到最终 sink 传送的目的存储系统。此模式不建议桥接过多的 flume 数量， flume 数量过多不仅会影响传输速率，而且一旦传输过程中某个节点 flume 宕机，会影响整个传输系统。\n2）复制和多路复用\n​        Flume 支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel 中，或者将不同数据分发到不同的 channel 中，sink 可以选择传送到不同的目的地。\n3）负载均衡和故障转移\n​        Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能。\n4）聚合\n​        这种模式是我们最常见的，也非常实用，日常 web 应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用 flume 的这种组合方式能很好的解决这一问题，每台服务器部署一个 flume 采集日志，传送到一个集中收集日志的flume，再由此 flume 上传到 hdfs、hive、hbase 等，进行日志分析。\n3.4 Flume 企业开发案例learning。。。。。。。。\n3.5 自定义 Interceptor3.6 自定义 Source3.7 自定义 Sink3.8 Flume 数据流监控4. 企业真实面试题（重点）1）你是如何实现 Flume 数据传输的监控的\n使用第三方框架 Ganglia 实时监控 Flume。 \n2）Flume 的 Source，Sink，Channel 的作用？你们 Source 是什么类型？\n\n作用 \nSource 组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy\nChannel 组件对采集到的数据进行缓存，可以存放在 Memory 或 File 中。\nSink 组件是用于把数据发送到目的地的组件，目的地包括 Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。\n\n\n我公司采用的 Source 类型为：\n监控后台日志：exec\n监控后台产生日志的端口：netcat\n\n\n\n3）Flume 的 Channel Selectors\n\n​        Channel Selectors，可以让不同的项目日志通过不同的Channel到不同的Sink中去。官方文档上Channel Selectors 有两种类型:Replicating Channel Selector (default)和Multiplexing Channel Selector​        这两种Selector的区别是:Replicating 会 将source过来的events发往所有channel,而Multiplexing可以选择该发往哪些Channel。 \n4） Flume 参数调优\n\nSource \n    增加 Source 个数（使用 Tair Dir Source 时可增加 FileGroups 个数）可以增大 Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个 Source 以保证 Source 有足够的能力获取到新产生的数据。\n    batchSize 参数决定 Source 一次批量运输到 Channel 的 event 条数，适当调大这个参数可以提高 Source 搬运 Event 到 Channel 时的性能。\n\nChannel \n    type 选择 memory 时 Channel 的性能最好，但是如果 Flume 进程意外挂掉可能会丢失数据。type 选择 file 时 Channel 的容错性更好，但是性能上会比 memory channel 差。\n    使用 file Channel 时 dataDirs 配置多个不同盘下的目录可以提高性能。\n    Capacity 参数决定 Channel 可容纳最大的 event 条数。transactionCapacity 参数决定每次 Source 往 channel 里面写的最大 event 条数和每次 Sink 从 channel 里面读的最大event 条数。transactionCapacity 需要大于 Source 和 Sink 的 batchSize 参数。\n\nSink 增加 Sink 的个数可以增加 Sink 消费 event 的能力。Sink 也不是越多越好够用就行过多的 Sink 会占用系统资源，造成系统资源不必要的浪费。\n    batchSize 参数决定 Sink 一次批量从 Channel 读取的 event 条数，适当调大这个参数可以提高 Sink 从 Channel 搬出 event 的性能。\n\n\n\n\n5）Flume 的事务机制        Flume 的事务机制（类似数据库的事务机制）：Flume 使用两个独立的事务分别负责从Soucrce 到 Channel，以及从 Channel 到 Sink 的事件传递。        比如 spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到 Channel 且提交成功，那么 Soucrce 就将该文件标记为完成。        同理，事务以类似的方式处理从 Channel 到 Sink 的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到 Channel 中，等待重新传递。6） Flume 采集数据会丢失吗?        根据 Flume 的架构原理，Flume 是不可能丢失数据的，其内部有完善的事务机制，Source 到 Channel 是事务性的，Channel 到 Sink 是事务性的，因此这两个环节不会出现数据的丢失，唯一可能丢失数据的情况是 Channel 采用 memoryChannel，agent 宕机导致数据丢失，或者 Channel 存储数据已满，导致 Source 不再写入，未写入的数据丢失。\n​        Flume 不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由 Sink 发出，但是没有接收到响应，Sink 会再次发送数据，此时可能会导致数据的重复。\n","slug":"B10-Flume","date":"2021-11-26T06:25:34.000Z","categories_index":"大数据","tags_index":"Flume","author_index":"YFR718"},{"id":"a21b04ca115a88f58bf8270c3847c249","title":"Zookeeper","content":"Zookeeper简介​        Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目。\n​        Zookeeper从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然 后接受观察者的注 册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应。\n​        Zookeeper=文件系统+通知机制\nZookeeper特点\n1）Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群。 \n2）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。所以Zookeeper适合安装奇数台服务器。 \n3）全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。 \n4）更新请求顺序执行，来自同一个Client的更新请求按其发送顺序依次执行。 \n5）数据更新原子性，一次数据更新要么成功，要么失败。 \n6）实时性，在一定时间范围内，Client能读到最新数据。\n数据结构\n​        ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个节点称做一个 ZNode。每一个 ZNode 默认能够存储 1MB 的数据，每个 ZNode 都可以通过其路径唯一标识。\n应用场景\n提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。\n安装与配置本地安装\n（1）安装 JDK（2）拷贝 apache-zookeeper-3.5.7-bin.tar.gz 安装包到 Linux 系统下（3）解压到指定目录，修改名称，修改配置\n操作 Zookeeper\n#启动 Zookeeper\n$ bin/zkServer.sh start\n#查看进程是否启动\n$ jps\n4020 Jps\n4001 QuorumPeerMain\n#查看状态\n$ bin/zkServer.sh status\nZooKeeper JMX enabled by default\nUsing config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg\nMode: standalone\n#启动客户端\n$ bin/zkCli.sh\n#退出客户端：\nquit\n#停止 Zookeeper\n$ bin/zkServer.sh stop\n配置参数解读\nZookeeper中的配置文件zoo.cfg中参数含义解读如下： \n1）tickTime = 2000：通信心跳时间，Zookeeper服务器与客户端心跳时间，单位毫秒\n2）initLimit = 10：LF初始通信时限        Leader和Follower初始连接时能容忍的最多心跳数（tickTime的数量）3）syncLimit = 5：LF同步通信时限        Leader和Follower之间通信时间如果超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。 \n4）dataDir：保存Zookeeper中的数据注意：默认的tmp目录，容易被Linux系统定期删除，所以一般不用默认的tmp目录。 \n5）clientPort = 2181：客户端连接端口，通常不做修改。\nZookeeper 集群操作选举机制（面试重点）Zookeeper选举机制——第一次启动\nSID：服务器ID。用来唯一标识一台ZooKeeper集群中的机器，每台机器不能重复，和myid一致。\nZXID：事务ID。ZXID是一个事务ID，用来标识一次服务器状态的变更。在某一时刻，集群中的每台机器的ZXID值不一定完全一致，这和ZooKeeper服务器对于客户端“更新请求”的处理逻辑有关。\nEpoch：每个Leader任期的代号。没有Leader时同一轮投票过程中的逻辑时钟值是相同的。每投完一次票这个数据就会增加\n（1）服务器1启 动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING； \n（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的myid比自己目前投票推举的（服务器1） 大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOOKING\n（3）服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；\n（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为 1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING； \n（5）服务器5启动，同4一样当小弟。\nZookeeper选举机制——非第一次启动\n（1）当ZooKeeper集群中的一台服务器出现以下两种情况之一时，就会开始进入Leader选举：\n• 服务器初始化启动。\n • 服务器运行期间无法和Leader保持连接。 \n（2）而当一台机器进入Leader选举流程时，当前集群也可能会处于以下两种状态：• 集群中本来就已经存在一个Leader。        对于第一种已经存在Leader的情况，机器试图去选举Leader时，会被告知当前服务器的Leader信息，对于该机器来说，仅仅需要和Leader机器建立连接，并进行状态同步即可。• 集群中确实不存在Leader。        假设ZooKeeper由5台服务器组成，SID分别为1、2、3、4、5，ZXID分别为8、8、8、7、7，并且此时SID为3的服务器是Leader。某一时刻，3和5服务器出现故障，因此开始进行Leader选举。\n客户端命令行操作\n\n\n\n命令基本语法\n功能描述\n\n\n\n\nhelp\n显示所有操作命令\n\n\nls path\n使用 ls 命令来查看当前 znode 的子节点 [可监听] -w 监听子节点变化-s 附加次级信息\n\n\ncreate\n普通创建-s 含有序列-e 临时（重启或者超时消失）\n\n\nget path\n获得节点的值 [可监听] -w 监听节点内容变化-s 附加次级信息\n\n\nset\n设置节点的具体值\n\n\nstat\n查看节点状态\n\n\ndelete\n删除节点\n\n\ndeletall\n递归删除节点\n\n\n\n\nznode 节点数据信息ls -s /\n（1）czxid：创建节点的事务 zxid        每次修改 ZooKeeper 状态都会产生一个 ZooKeeper 事务 ID。事务 ID 是 ZooKeeper 中所有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1 在 zxid2 之前发生。（2）ctime：znode 被创建的毫秒数（从 1970 年开始）（3）mzxid：znode 最后更新的事务 zxid（4）mtime：znode 最后修改的毫秒数（从 1970 年开始）（5）pZxid：znode 最后更新的子节点 zxid（6）cversion：znode 子节点变化号，znode 子节点修改次数（7）dataversion：znode 数据变化号（8）aclVersion：znode 访问控制列表的变化号（9）ephemeralOwner：如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0。（10）dataLength：znode 的数据长度（11）numChildren：znode 子节点数量\n","slug":"B9-zookeeper","date":"2021-11-26T06:25:15.000Z","categories_index":"大数据","tags_index":"Zookeeper","author_index":"YFR718"},{"id":"4d4adb38002129eb79b23e1030e48e77","title":"mysql刷题","content":"笔记空值输出nullselect ifnull((select ... from ...),null) as ans;\n获取去重的第k名select ... from \ngroup by ...\norder by ... desc\nlimit k-1,1 ;\n删除操作，保留一部分delect from tablename\nwhere id not in (子查询)#要保留的\n先select再delect会报错，中间要再加一层selectdelete\nfrom contacts\nwhere id not in (\n    select * from (\n    select min(id) \n    from contacts \n    group by name) t\n);\nfrom 子查询 ，子查询要起别名\nselect * from (\n    min(id)             \n    from contacts             \n    group by name) as t\n)\nin子查询可以 in 一对数据#查询每位同学第一次登录平台听课的设备ID (device_id)。\nselect o.student_id,o.device_id\nfrom online_class_situations  as o\nwhere (o.student_id,o.date) in (\n    select distinct student_id,min(date)\n    from online_class_situations\n    where course_number&gt;0\n    group by student_id\n);\n 结果不包含null，如果需要null要特判\">安全不等&lt;&gt; 结果不包含null，如果需要null要特判select name \nfrom patients\nwhere infected_by_id &lt;&gt; 2 or infected_by_id is null;\ngroup by 不能直接用maxselect t.student_id\nfrom (\n    select student_id, count(*) as c\n    from exams e\n    where e.is_pass=0\n    group by e.student_id\n    ) as t\n,(\n    select count(*) cc\n    from exams ee\n    where ee.is_pass=0\n    group by ee.student_id\n) f \nwhere t.c = max(f.cc) #错误\nselect t.student_id\nfrom (\n    select student_id, count(*) as c\n    from exams e\n    where e.is_pass=0\n    group by e.student_id\n    ) as t\n,(select max(cc) acount from (\n    select count(*) cc\n    from exams ee\n    where ee.is_pass=0\n    group by ee.student_id\n) f ) ff\nwhere t.c = ff.acount;\n寻找连续两行select distinct a.id \nfrom boxes a,boxes b \nwhere abs(a.id-b.id) = 1 and (a.is_empty=1 and b.is_empty=1)\norder by a.id ;\n空值就不输出having ... is not null\n保留小数点后2位round(score,2)\n","slug":"B8-mysql刷题","date":"2021-11-26T03:03:44.000Z","categories_index":"数据库","tags_index":"MySQL","author_index":"YFR718"},{"id":"62ef56c51053b490f9a79e2b56de5f36","title":"J8.JDBC","content":"JDBC核心技术第1章：JDBC概述1.1 数据的持久化\n持久化(persistence)：把数据保存到可掉电式存储设备中以供之后使用。大多数情况下，特别是企业级应用，数据持久化意味着将内存中的数据保存到硬盘上加以”固化”，而持久化的实现过程大多通过各种关系数据库来完成。\n\n持久化的主要应用是将内存中的数据存储在关系型数据库中，当然也可以存储在磁盘文件、XML数据文件中。\n \n\n\n1.2 Java中的数据存储技术\n在Java中，数据库存取技术可分为如下几类：\n\nJDBC直接访问数据库\nJDO (Java Data Object )技术\n\n第三方O/R工具，如Hibernate, Mybatis 等\n\n\n\nJDBC是java访问数据库的基石，JDO、Hibernate、MyBatis等只是更好的封装了JDBC。\n\n\n1.3 JDBC介绍\nJDBC(Java Database Connectivity)是一个独立于特定数据库管理系统、通用的SQL数据库存取和操作的公共接口（一组API），定义了用来访问数据库的标准Java类库，（java.sql,javax.sql）使用这些类库可以以一种标准的方法、方便地访问数据库资源。\nJDBC为访问不同的数据库提供了一种统一的途径，为开发者屏蔽了一些细节问题。\nJDBC的目标是使Java程序员使用JDBC可以连接任何提供了JDBC驱动程序的数据库系统，这样就使得程序员无需对特定的数据库系统的特点有过多的了解，从而大大简化和加快了开发过程。\n如果没有JDBC，那么Java程序访问数据库时是这样的：\n\n\n\n\n有了JDBC，Java程序访问数据库时是这样的：\n\n\n\n\n总结如下：\n\n\n1.4 JDBC体系结构\nJDBC接口（API）包括两个层次：\n面向应用的API：Java API，抽象接口，供应用程序开发人员使用（连接数据库，执行SQL语句，获得结果）。\n面向数据库的API：Java Driver API，供开发商开发数据库驱动程序用。\n\n\n\n\n\n\n\n\n\n\n\n\nJDBC是sun公司提供一套用于数据库操作的接口，java程序员只需要面向这套接口编程即可。\n不同的数据库厂商，需要针对这套接口，提供不同实现。不同的实现的集合，即为不同数据库的驱动。                                                                ————面向接口编程\n1.5 JDBC程序编写步骤\n\n\n\n\n\n\n\n\n\n补充：ODBC(Open Database Connectivity，开放式数据库连接)，是微软在Windows平台下推出的。使用者在程序中只需要调用ODBC API，由 ODBC 驱动程序将调用转换成为对特定的数据库的调用请求。\n第2章：获取数据库连接2.1 要素一：Driver接口实现类2.1.1 Driver接口介绍\njava.sql.Driver 接口是所有 JDBC 驱动程序需要实现的接口。这个接口是提供给数据库厂商使用的，不同数据库厂商提供不同的实现。\n\n在程序中不需要直接去访问实现了 Driver 接口的类，而是由驱动程序管理器类(java.sql.DriverManager)去调用这些Driver实现。\n\nOracle的驱动：oracle.jdbc.driver.OracleDriver\nmySql的驱动： com.mysql.jdbc.Driver\n\n\n将上述jar包拷贝到Java工程的一个目录中，习惯上新建一个lib文件夹。\n\n\n在驱动jar上右键—&gt;Build Path—&gt;Add to Build Path\n注意：如果是Dynamic Web Project（动态的web项目）话，则是把驱动jar放到WebContent（有的开发工具叫WebRoot）目录中的WEB-INF目录中的lib目录下即可\n2.1.2 加载与注册JDBC驱动\n加载驱动：加载 JDBC 驱动需调用 Class 类的静态方法 forName()，向其传递要加载的 JDBC 驱动的类名\n\nClass.forName(“com.mysql.jdbc.Driver”);\n\n\n注册驱动：DriverManager 类是驱动程序管理器类，负责管理驱动程序\n\n使用DriverManager.registerDriver(com.mysql.jdbc.Driver)来注册驱动\n\n通常不用显式调用 DriverManager 类的 registerDriver() 方法来注册驱动程序类的实例，因为 Driver 接口的驱动程序类都包含了静态代码块，在这个静态代码块中，会调用 DriverManager.registerDriver() 方法来注册自身的一个实例。下图是MySQL的Driver实现类的源码：\n\n\n\n\n\n2.2 要素二：URL\nJDBC URL 用于标识一个被注册的驱动程序，驱动程序管理器通过这个 URL 选择正确的驱动程序，从而建立到数据库的连接。\n\nJDBC URL的标准由三部分组成，各部分间用冒号分隔。 \n\njdbc:子协议:子名称\n协议：JDBC URL中的协议总是jdbc \n子协议：子协议用于标识一个数据库驱动程序\n子名称：一种标识数据库的方法。子名称可以依不同的子协议而变化，用子名称的目的是为了定位数据库提供足够的信息。包含主机名(对应服务端的ip地址)，端口号，数据库名\n\n\n举例：\n\n\n几种常用数据库的 JDBC URL\n\nMySQL的连接URL编写方式：\n\njdbc:mysql://主机名称:mysql服务端口号/数据库名称?参数=值&amp;参数=值\njdbc:mysql://localhost:3306/atguigu\njdbc:mysql://localhost:3306/atguigu?useUnicode=true&amp;characterEncoding=utf8（如果JDBC程序与服务器端的字符集不一致，会导致乱码，那么可以通过参数指定服务器端的字符集）\njdbc:mysql://localhost:3306/atguigu?user=root&amp;password=123456\n\n\nOracle 9i的连接URL编写方式：\n\njdbc:oracle:thin:@主机名称:oracle服务端口号:数据库名称\njdbc:oracle:thin:@localhost:1521:atguigu\n\n\nSQLServer的连接URL编写方式：\n\njdbc:sqlserver://主机名称:sqlserver服务端口号:DatabaseName=数据库名称\n\njdbc:sqlserver://localhost:1433:DatabaseName=atguigu\n\n\n\n\n\n\n2.3 要素三：用户名和密码\nuser,password可以用“属性名=属性值”方式告诉数据库\n可以调用 DriverManager 类的 getConnection() 方法建立到数据库的连接\n\n2.4 数据库连接方式举例2.4.1 连接方式一@Test\n   public void testConnection1() {\n       try {\n           //1.提供java.sql.Driver接口实现类的对象\n           Driver driver = null;\n           driver = new com.mysql.jdbc.Driver();\n\n           //2.提供url，指明具体操作的数据\n           String url = \"jdbc:mysql://localhost:3306/test\";\n\n           //3.提供Properties的对象，指明用户名和密码\n           Properties info = new Properties();\n           info.setProperty(\"user\", \"root\");\n           info.setProperty(\"password\", \"abc123\");\n\n           //4.调用driver的connect()，获取连接\n           Connection conn = driver.connect(url, info);\n           System.out.println(conn);\n       } catch (SQLException e) {\n           e.printStackTrace();\n       }\n   }\n\n\n\n\n\n\n\n\n\n说明：上述代码中显式出现了第三方数据库的API\n2.4.2 连接方式二@Test\n   public void testConnection2() {\n       try {\n           //1.实例化Driver\n           String className = \"com.mysql.jdbc.Driver\";\n           Class clazz = Class.forName(className);\n           Driver driver = (Driver) clazz.newInstance();\n\n           //2.提供url，指明具体操作的数据\n           String url = \"jdbc:mysql://localhost:3306/test\";\n\n           //3.提供Properties的对象，指明用户名和密码\n           Properties info = new Properties();\n           info.setProperty(\"user\", \"root\");\n           info.setProperty(\"password\", \"abc123\");\n\n           //4.调用driver的connect()，获取连接\n           Connection conn = driver.connect(url, info);\n           System.out.println(conn);\n\n       } catch (Exception e) {\n           e.printStackTrace();\n       }\n   }\n\n\n\n\n\n\n\n\n\n说明：相较于方式一，这里使用反射实例化Driver，不在代码中体现第三方数据库的API。体现了面向接口编程思想。\n2.4.3 连接方式三@Test\n   public void testConnection3() {\n       try {\n           //1.数据库连接的4个基本要素：\n           String url = \"jdbc:mysql://localhost:3306/test\";\n           String user = \"root\";\n           String password = \"abc123\";\n           String driverName = \"com.mysql.jdbc.Driver\";\n\n           //2.实例化Driver\n           Class clazz = Class.forName(driverName);\n           Driver driver = (Driver) clazz.newInstance();\n           //3.注册驱动\n           DriverManager.registerDriver(driver);\n           //4.获取连接\n           Connection conn = DriverManager.getConnection(url, user, password);\n           System.out.println(conn);\n       } catch (Exception e) {\n           e.printStackTrace();\n       }\n\n   }\n\n\n\n\n\n\n\n\n\n说明：使用DriverManager实现数据库的连接。体会获取连接必要的4个基本要素。\n2.4.4 连接方式四@Test\n   public void testConnection4() {\n       try {\n           //1.数据库连接的4个基本要素：\n           String url = \"jdbc:mysql://localhost:3306/test\";\n           String user = \"root\";\n           String password = \"abc123\";\n           String driverName = \"com.mysql.jdbc.Driver\";\n\n           //2.加载驱动 （①实例化Driver ②注册驱动）\n           Class.forName(driverName);\n\n\n           //Driver driver = (Driver) clazz.newInstance();\n           //3.注册驱动\n           //DriverManager.registerDriver(driver);\n           /*\n           可以注释掉上述代码的原因，是因为在mysql的Driver类中声明有：\n           static {\n               try {\n                   DriverManager.registerDriver(new Driver());\n               } catch (SQLException var1) {\n                   throw new RuntimeException(\"Can't register driver!\");\n               }\n           }\n\n            */\n\n\n           //3.获取连接\n           Connection conn = DriverManager.getConnection(url, user, password);\n           System.out.println(conn);\n       } catch (Exception e) {\n           e.printStackTrace();\n       }\n\n   }\n\n\n\n\n\n\n\n\n\n说明：不必显式的注册驱动了。因为在DriverManager的源码中已经存在静态代码块，实现了驱动的注册。\n2.4.5 连接方式五(最终版)@Test\n   public  void testConnection5() throws Exception {\n   \t//1.加载配置文件\n       InputStream is = ConnectionTest.class.getClassLoader().getResourceAsStream(\"jdbc.properties\");\n       Properties pros = new Properties();\n       pros.load(is);\n       \n       //2.读取配置信息\n       String user = pros.getProperty(\"user\");\n       String password = pros.getProperty(\"password\");\n       String url = pros.getProperty(\"url\");\n       String driverClass = pros.getProperty(\"driverClass\");\n\n       //3.加载驱动\n       Class.forName(driverClass);\n\n       //4.获取连接\n       Connection conn = DriverManager.getConnection(url,user,password);\n       System.out.println(conn);\n\n   }\n其中，配置文件声明在工程的src目录下：【jdbc.properties】\nuser=root\npassword=abc123\nurl=jdbc:mysql://localhost:3306/test\ndriverClass=com.mysql.jdbc.Driver\n\n\n\n\n\n\n\n\n\n说明：使用配置文件的方式保存配置信息，在代码中加载配置文件\n使用配置文件的好处：\n①实现了代码和数据的分离，如果需要修改配置信息，直接在配置文件中修改，不需要深入代码②如果修改了配置信息，省去重新编译的过程。\n第3章：使用PreparedStatement实现CRUD操作3.1 操作和访问数据库\n数据库连接被用于向数据库服务器发送命令和 SQL 语句，并接受数据库服务器返回的结果。其实一个数据库连接就是一个Socket连接。\n\n在 java.sql 包中有 3 个接口分别定义了对数据库的调用的不同方式：\n\nStatement：用于执行静态 SQL 语句并返回它所生成结果的对象。 \nPrepatedStatement：SQL 语句被预编译并存储在此对象中，可以使用此对象多次高效地执行该语句。\nCallableStatement：用于执行 SQL 存储过程\n\n\n\n\n3.2 使用Statement操作数据表的弊端\n通过调用 Connection 对象的 createStatement() 方法创建该对象。该对象用于执行静态的 SQL 语句，并且返回执行结果。\n\nStatement 接口中定义了下列方法用于执行 SQL 语句：\nint excuteUpdate(String sql)：执行更新操作INSERT、UPDATE、DELETE\nResultSet executeQuery(String sql)：执行查询操作SELECT\n\n但是使用Statement操作数据表存在弊端：\n\n问题一：存在拼串操作，繁琐\n问题二：存在SQL注入问题\n\n\nSQL 注入是利用某些系统没有对用户输入的数据进行充分的检查，而在用户输入数据中注入非法的 SQL 语句段或命令(如：SELECT user, password FROM user_table WHERE user=’a’ OR 1 = ‘ AND password = ‘ OR ‘1’ = ‘1’) ，从而利用系统的 SQL 引擎完成恶意行为的做法。\n\n对于 Java 而言，要防范 SQL 注入，只要用 PreparedStatement(从Statement扩展而来) 取代 Statement 就可以了。\n\n代码演示：\n\n\npublic class StatementTest {\n\n\t// 使用Statement的弊端：需要拼写sql语句，并且存在SQL注入的问题\n\t@Test\n\tpublic void testLogin() {\n\t\tScanner scan = new Scanner(System.in);\n\n\t\tSystem.out.print(\"用户名：\");\n\t\tString userName = scan.nextLine();\n\t\tSystem.out.print(\"密   码：\");\n\t\tString password = scan.nextLine();\n\n\t\t// SELECT user,password FROM user_table WHERE USER = '1' or ' AND PASSWORD = '='1' or '1' = '1';\n\t\tString sql = \"SELECT user,password FROM user_table WHERE USER = '\" + userName + \"' AND PASSWORD = '\" + password\n\t\t\t\t+ \"'\";\n\t\tUser user = get(sql, User.class);\n\t\tif (user != null) {\n\t\t\tSystem.out.println(\"登陆成功!\");\n\t\t} else {\n\t\t\tSystem.out.println(\"用户名或密码错误！\");\n\t\t}\n\t}\n\n\t// 使用Statement实现对数据表的查询操作\n\tpublic &lt;T&gt; T get(String sql, Class&lt;T&gt; clazz) {\n\t\tT t = null;\n\n\t\tConnection conn = null;\n\t\tStatement st = null;\n\t\tResultSet rs = null;\n\t\ttry {\n\t\t\t// 1.加载配置文件\n\t\t\tInputStream is = StatementTest.class.getClassLoader().getResourceAsStream(\"jdbc.properties\");\n\t\t\tProperties pros = new Properties();\n\t\t\tpros.load(is);\n\n\t\t\t// 2.读取配置信息\n\t\t\tString user = pros.getProperty(\"user\");\n\t\t\tString password = pros.getProperty(\"password\");\n\t\t\tString url = pros.getProperty(\"url\");\n\t\t\tString driverClass = pros.getProperty(\"driverClass\");\n\n\t\t\t// 3.加载驱动\n\t\t\tClass.forName(driverClass);\n\n\t\t\t// 4.获取连接\n\t\t\tconn = DriverManager.getConnection(url, user, password);\n\n\t\t\tst = conn.createStatement();\n\n\t\t\trs = st.executeQuery(sql);\n\n\t\t\t// 获取结果集的元数据\n\t\t\tResultSetMetaData rsmd = rs.getMetaData();\n\n\t\t\t// 获取结果集的列数\n\t\t\tint columnCount = rsmd.getColumnCount();\n\n\t\t\tif (rs.next()) {\n\n\t\t\t\tt = clazz.newInstance();\n\n\t\t\t\tfor (int i = 0; i &lt; columnCount; i++) {\n\t\t\t\t\t// //1. 获取列的名称\n\t\t\t\t\t// String columnName = rsmd.getColumnName(i+1);\n\n\t\t\t\t\t// 1. 获取列的别名\n\t\t\t\t\tString columnName = rsmd.getColumnLabel(i + 1);\n\n\t\t\t\t\t// 2. 根据列名获取对应数据表中的数据\n\t\t\t\t\tObject columnVal = rs.getObject(columnName);\n\n\t\t\t\t\t// 3. 将数据表中得到的数据，封装进对象\n\t\t\t\t\tField field = clazz.getDeclaredField(columnName);\n\t\t\t\t\tfield.setAccessible(true);\n\t\t\t\t\tfield.set(t, columnVal);\n\t\t\t\t}\n\t\t\t\treturn t;\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t} finally {\n\t\t\t// 关闭资源\n\t\t\tif (rs != null) {\n\t\t\t\ttry {\n\t\t\t\t\trs.close();\n\t\t\t\t} catch (SQLException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (st != null) {\n\t\t\t\ttry {\n\t\t\t\t\tst.close();\n\t\t\t\t} catch (SQLException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (conn != null) {\n\t\t\t\ttry {\n\t\t\t\t\tconn.close();\n\t\t\t\t} catch (SQLException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn null;\n\t}\n}\n综上：\n\n3.3 PreparedStatement的使用3.3.1 PreparedStatement介绍\n可以通过调用 Connection 对象的 preparedStatement(String sql) 方法获取 PreparedStatement 对象\n\nPreparedStatement 接口是 Statement 的子接口，它表示一条预编译过的 SQL 语句\n\nPreparedStatement 对象所代表的 SQL 语句中的参数用问号(?)来表示，调用 PreparedStatement 对象的 setXxx() 方法来设置这些参数. setXxx() 方法有两个参数，第一个参数是要设置的 SQL 语句中的参数的索引(从 1 开始)，第二个是设置的 SQL 语句中的参数的值\n\n\n3.3.2 PreparedStatement vs Statement\n代码的可读性和可维护性。\n\nPreparedStatement 能最大可能提高性能：\n\nDBServer会对预编译语句提供性能优化。因为预编译语句有可能被重复调用，所以语句在被DBServer的编译器编译后的执行代码被缓存下来，那么下次调用时只要是相同的预编译语句就不需要编译，只要将参数直接传入编译过的语句执行代码中就会得到执行。\n在statement语句中,即使是相同操作但因为数据内容不一样,所以整个语句本身不能匹配,没有缓存语句的意义.事实是没有数据库会对普通语句编译后的执行代码缓存。这样每执行一次都要对传入的语句编译一次。\n(语法检查，语义检查，翻译成二进制命令，缓存)\n\n\nPreparedStatement 可以防止 SQL 注入 \n\n\n3.3.3 Java与SQL对应数据类型转换表\n\n\n\nJava类型\nSQL类型\n\n\n\n\nboolean\nBIT\n\n\nbyte\nTINYINT\n\n\nshort\nSMALLINT\n\n\nint\nINTEGER\n\n\nlong\nBIGINT\n\n\nString\nCHAR,VARCHAR,LONGVARCHAR\n\n\nbyte   array\nBINARY  ,    VAR BINARY\n\n\njava.sql.Date\nDATE\n\n\njava.sql.Time\nTIME\n\n\njava.sql.Timestamp\nTIMESTAMP\n\n\n\n\n3.3.4 使用PreparedStatement实现增、删、改操作//通用的增、删、改操作（体现一：增、删、改 ； 体现二：针对于不同的表）\npublic void update(String sql,Object ... args){\n\tConnection conn = null;\n\tPreparedStatement ps = null;\n\ttry {\n\t\t//1.获取数据库的连接\n\t\tconn = JDBCUtils.getConnection();\n\t\t\n\t\t//2.获取PreparedStatement的实例 (或：预编译sql语句)\n\t\tps = conn.prepareStatement(sql);\n\t\t//3.填充占位符\n\t\tfor(int i = 0;i &lt; args.length;i++){\n\t\t\tps.setObject(i + 1, args[i]);\n\t\t}\n\t\t\n\t\t//4.执行sql语句\n\t\tps.execute();\n\t} catch (Exception e) {\n\t\t\n\t\te.printStackTrace();\n\t}finally{\n\t\t//5.关闭资源\n\t\tJDBCUtils.closeResource(conn, ps);\n\t\t\n\t}\n}\n3.3.5 使用PreparedStatement实现查询操作// 通用的针对于不同表的查询:返回一个对象 (version 1.0)\npublic &lt;T&gt; T getInstance(Class&lt;T&gt; clazz, String sql, Object... args) {\n\n\tConnection conn = null;\n\tPreparedStatement ps = null;\n\tResultSet rs = null;\n\ttry {\n\t\t// 1.获取数据库连接\n\t\tconn = JDBCUtils.getConnection();\n\n\t\t// 2.预编译sql语句，得到PreparedStatement对象\n\t\tps = conn.prepareStatement(sql);\n\n\t\t// 3.填充占位符\n\t\tfor (int i = 0; i &lt; args.length; i++) {\n\t\t\tps.setObject(i + 1, args[i]);\n\t\t}\n\n\t\t// 4.执行executeQuery(),得到结果集：ResultSet\n\t\trs = ps.executeQuery();\n\n\t\t// 5.得到结果集的元数据：ResultSetMetaData\n\t\tResultSetMetaData rsmd = rs.getMetaData();\n\n\t\t// 6.1通过ResultSetMetaData得到columnCount,columnLabel；通过ResultSet得到列值\n\t\tint columnCount = rsmd.getColumnCount();\n\t\tif (rs.next()) {\n\t\t\tT t = clazz.newInstance();\n\t\t\tfor (int i = 0; i &lt; columnCount; i++) {// 遍历每一个列\n\n\t\t\t\t// 获取列值\n\t\t\t\tObject columnVal = rs.getObject(i + 1);\n\t\t\t\t// 获取列的别名:列的别名，使用类的属性名充当\n\t\t\t\tString columnLabel = rsmd.getColumnLabel(i + 1);\n\t\t\t\t// 6.2使用反射，给对象的相应属性赋值\n\t\t\t\tField field = clazz.getDeclaredField(columnLabel);\n\t\t\t\tfield.setAccessible(true);\n\t\t\t\tfield.set(t, columnVal);\n\n\t\t\t}\n\n\t\t\treturn t;\n\n\t\t}\n\t} catch (Exception e) {\n\n\t\te.printStackTrace();\n\t} finally {\n\t\t// 7.关闭资源\n\t\tJDBCUtils.closeResource(conn, ps, rs);\n\t}\n\n\treturn null;\n\n}\n\n\n\n\n\n\n\n\n\n说明：使用PreparedStatement实现的查询操作可以替换Statement实现的查询操作，解决Statement拼串和SQL注入问题。\n3.4 ResultSet与ResultSetMetaData3.4.1 ResultSet\n查询需要调用PreparedStatement 的 executeQuery() 方法，查询结果是一个ResultSet 对象\n\nResultSet 对象以逻辑表格的形式封装了执行数据库操作的结果集，ResultSet 接口由数据库厂商提供实现\n\nResultSet 返回的实际上就是一张数据表。有一个指针指向数据表的第一条记录的前面。\n\nResultSet 对象维护了一个指向当前数据行的游标，初始的时候，游标在第一行之前，可以通过 ResultSet 对象的 next() 方法移动到下一行。调用 next()方法检测下一行是否有效。若有效，该方法返回 true，且指针下移。相当于Iterator对象的 hasNext() 和 next() 方法的结合体。\n\n当指针指向一行时, 可以通过调用 getXxx(int index) 或 getXxx(int columnName) 获取每一列的值。\n\n例如: getInt(1), getString(“name”)\n注意：Java与数据库交互涉及到的相关Java API中的索引都从1开始。\n\n\nResultSet 接口的常用方法：\n\nboolean next()\n\ngetString()\n\n…\n\n\n\n\n3.4.2 ResultSetMetaData\n可用于获取关于 ResultSet 对象中列的类型和属性信息的对象\n\nResultSetMetaData meta = rs.getMetaData();\n\ngetColumnName(int column)：获取指定列的名称\ngetColumnLabel(int column)：获取指定列的别名\ngetColumnCount()：返回当前 ResultSet 对象中的列数。 \n\ngetColumnTypeName(int column)：检索指定列的数据库特定的类型名称。 \n\ngetColumnDisplaySize(int column)：指示指定列的最大标准宽度，以字符为单位。 \nisNullable(int column)：指示指定列中的值是否可以为 null。 \n\nisAutoIncrement(int column)：指示是否自动为指定列进行编号，这样这些列仍然是只读的。 \n\n\n\n\n\n问题1：得到结果集后, 如何知道该结果集中有哪些列 ？ 列名是什么？\n​     需要使用一个描述 ResultSet 的对象， 即 ResultSetMetaData\n问题2：关于ResultSetMetaData\n\n如何获取 ResultSetMetaData： 调用 ResultSet 的 getMetaData() 方法即可\n获取 ResultSet 中有多少列：调用 ResultSetMetaData 的 getColumnCount() 方法\n获取 ResultSet 每一列的列的别名是什么：调用 ResultSetMetaData 的getColumnLabel() 方法\n\n\n3.5 资源的释放\n释放ResultSet, Statement,Connection。\n数据库连接（Connection）是非常稀有的资源，用完后必须马上释放，如果Connection不能及时正确的关闭将导致系统宕机。Connection的使用原则是尽量晚创建，尽量早的释放。\n可以在finally中关闭，保证及时其他代码出现异常，资源也一定能被关闭。\n\n3.6 JDBC API小结\n两种思想\n\n面向接口编程的思想\n\nORM思想(object relational mapping)\n\n一个数据表对应一个java类\n表中的一条记录对应java类的一个对象\n表中的一个字段对应java类的一个属性\n\n\n\n\n\n\n\n\n\n\n\n\nsql是需要结合列名和表的属性名来写。注意起别名。\n\n两种技术\n\nJDBC结果集的元数据：ResultSetMetaData\n获取列数：getColumnCount()\n获取列的别名：getColumnLabel()\n\n\n通过反射，创建指定类的对象，获取指定的属性并赋值\n\n\n\n第4章 操作BLOB类型字段4.1 MySQL BLOB类型\nMySQL中，BLOB是一个二进制大型对象，是一个可以存储大量数据的容器，它能容纳不同大小的数据。\n插入BLOB类型的数据必须使用PreparedStatement，因为BLOB类型的数据无法使用字符串拼接写的。\n\nMySQL的四种BLOB类型(除了在存储的最大信息量上不同外，他们是等同的)\n\n\n\n实际使用中根据需要存入的数据大小定义不同的BLOB类型。\n需要注意的是：如果存储的文件过大，数据库的性能会下降。\n如果在指定了相关的Blob类型以后，还报错：xxx too large，那么在mysql的安装目录下，找my.ini文件加上如下的配置参数： max_allowed_packet=16M。同时注意：修改了my.ini文件之后，需要重新启动mysql服务。\n\n4.2 向数据表中插入大数据类型//获取连接\nConnection conn = JDBCUtils.getConnection();\n\t\t\nString sql = \"insert into customers(name,email,birth,photo)values(?,?,?,?)\";\nPreparedStatement ps = conn.prepareStatement(sql);\n\n// 填充占位符\nps.setString(1, \"徐海强\");\nps.setString(2, \"xhq@126.com\");\nps.setDate(3, new Date(new java.util.Date().getTime()));\n// 操作Blob类型的变量\nFileInputStream fis = new FileInputStream(\"xhq.png\");\nps.setBlob(4, fis);\n//执行\nps.execute();\n\t\t\nfis.close();\nJDBCUtils.closeResource(conn, ps);\n\n4.3 修改数据表中的Blob类型字段Connection conn = JDBCUtils.getConnection();\nString sql = \"update customers set photo = ? where id = ?\";\nPreparedStatement ps = conn.prepareStatement(sql);\n\n// 填充占位符\n// 操作Blob类型的变量\nFileInputStream fis = new FileInputStream(\"coffee.png\");\nps.setBlob(1, fis);\nps.setInt(2, 25);\n\nps.execute();\n\nfis.close();\nJDBCUtils.closeResource(conn, ps);\n4.4 从数据表中读取大数据类型String sql = \"SELECT id, name, email, birth, photo FROM customer WHERE id = ?\";\nconn = getConnection();\nps = conn.prepareStatement(sql);\nps.setInt(1, 8);\nrs = ps.executeQuery();\nif(rs.next()){\n\tInteger id = rs.getInt(1);\n    String name = rs.getString(2);\n\tString email = rs.getString(3);\n    Date birth = rs.getDate(4);\n\tCustomer cust = new Customer(id, name, email, birth);\n    System.out.println(cust); \n    //读取Blob类型的字段\n\tBlob photo = rs.getBlob(5);\n\tInputStream is = photo.getBinaryStream();\n\tOutputStream os = new FileOutputStream(\"c.jpg\");\n\tbyte [] buffer = new byte[1024];\n\tint len = 0;\n\twhile((len = is.read(buffer)) != -1){\n\t\tos.write(buffer, 0, len);\n\t}\n    JDBCUtils.closeResource(conn, ps, rs);\n\t\t\n\tif(is != null){\n\t\tis.close();\n\t}\n\t\t\n\tif(os !=  null){\n\t\tos.close();\n\t}\n    \n}\n\n第5章 批量插入5.1 批量执行SQL语句当需要成批插入或者更新记录时，可以采用Java的批量更新机制，这一机制允许多条语句一次性提交给数据库批量处理。通常情况下比单独提交处理更有效率\nJDBC的批量处理语句包括下面三个方法：\n\naddBatch(String)：添加需要批量处理的SQL语句或是参数；\nexecuteBatch()：执行批量处理语句；\nclearBatch():清空缓存的数据\n\n通常我们会遇到两种批量执行SQL语句的情况：\n\n多条SQL语句的批量处理；\n一个SQL语句的批量传参；\n\n5.2 高效的批量插入举例：向数据表中插入20000条数据\n\n数据库中提供一个goods表。创建如下：\n\nCREATE TABLE goods(\nid INT PRIMARY KEY AUTO_INCREMENT,\nNAME VARCHAR(20)\n);\n5.2.1 实现层次一：使用StatementConnection conn = JDBCUtils.getConnection();\nStatement st = conn.createStatement();\nfor(int i = 1;i &lt;= 20000;i++){\n\tString sql = \"insert into goods(name) values('name_' + \"+ i +\")\";\n\tst.executeUpdate(sql);\n}\n5.2.2 实现层次二：使用PreparedStatementlong start = System.currentTimeMillis();\n\t\t\nConnection conn = JDBCUtils.getConnection();\n\t\t\nString sql = \"insert into goods(name)values(?)\";\nPreparedStatement ps = conn.prepareStatement(sql);\nfor(int i = 1;i &lt;= 20000;i++){\n\tps.setString(1, \"name_\" + i);\n\tps.executeUpdate();\n}\n\t\t\nlong end = System.currentTimeMillis();\nSystem.out.println(\"花费的时间为：\" + (end - start));//82340\n\t\t\n\t\t\nJDBCUtils.closeResource(conn, ps);\n5.2.3 实现层次三/*\n * 修改1： 使用 addBatch() / executeBatch() / clearBatch()\n * 修改2：mysql服务器默认是关闭批处理的，我们需要通过一个参数，让mysql开启批处理的支持。\n * \t\t ?rewriteBatchedStatements=true 写在配置文件的url后面\n * 修改3：使用更新的mysql 驱动：mysql-connector-java-5.1.37-bin.jar\n * \n */\n@Test\npublic void testInsert1() throws Exception{\n\tlong start = System.currentTimeMillis();\n\t\t\n\tConnection conn = JDBCUtils.getConnection();\n\t\t\n\tString sql = \"insert into goods(name)values(?)\";\n\tPreparedStatement ps = conn.prepareStatement(sql);\n\t\t\n\tfor(int i = 1;i &lt;= 1000000;i++){\n\t\tps.setString(1, \"name_\" + i);\n\t\t\t\n\t\t//1.“攒”sql\n\t\tps.addBatch();\n\t\tif(i % 500 == 0){\n\t\t\t//2.执行\n\t\t\tps.executeBatch();\n\t\t\t//3.清空\n\t\t\tps.clearBatch();\n\t\t}\n\t}\n\t\t\n\tlong end = System.currentTimeMillis();\n\tSystem.out.println(\"花费的时间为：\" + (end - start));//20000条：625                                                                         //1000000条:14733  \n\t\t\n\tJDBCUtils.closeResource(conn, ps);\n}\n5.2.4 实现层次四/*\n* 层次四：在层次三的基础上操作\n* 使用Connection 的 setAutoCommit(false)  /  commit()\n*/\n@Test\npublic void testInsert2() throws Exception{\n\tlong start = System.currentTimeMillis();\n\t\t\n\tConnection conn = JDBCUtils.getConnection();\n\t\t\n\t//1.设置为不自动提交数据\n\tconn.setAutoCommit(false);\n\t\t\n\tString sql = \"insert into goods(name)values(?)\";\n\tPreparedStatement ps = conn.prepareStatement(sql);\n\t\t\n\tfor(int i = 1;i &lt;= 1000000;i++){\n\t\tps.setString(1, \"name_\" + i);\n\t\t\t\n\t\t//1.“攒”sql\n\t\tps.addBatch();\n\t\t\t\n\t\tif(i % 500 == 0){\n\t\t\t//2.执行\n\t\t\tps.executeBatch();\n\t\t\t//3.清空\n\t\t\tps.clearBatch();\n\t\t}\n\t}\n\t\t\n\t//2.提交数据\n\tconn.commit();\n\t\t\n\tlong end = System.currentTimeMillis();\n\tSystem.out.println(\"花费的时间为：\" + (end - start));//1000000条:4978 \n\t\t\n\tJDBCUtils.closeResource(conn, ps);\n}\n第6章： 数据库事务6.1 数据库事务介绍\n事务：一组逻辑操作单元,使数据从一种状态变换到另一种状态。\n\n事务处理（事务操作）：保证所有事务都作为一个工作单元来执行，即使出现了故障，都不能改变这种执行方式。当在一个事务中执行多个操作时，要么所有的事务都被提交(commit)，那么这些修改就永久地保存下来；要么数据库管理系统将放弃所作的所有修改，整个事务回滚(rollback)到最初状态。\n\n为确保数据库中数据的一致性，数据的操纵应当是离散的成组的逻辑单元：当它全部完成时，数据的一致性可以保持，而当这个单元中的一部分操作失败，整个事务应全部视为错误，所有从起始点以后的操作应全部回退到开始状态。 \n\n\n6.2 JDBC事务处理\n数据一旦提交，就不可回滚。\n\n数据什么时候意味着提交？\n\n当一个连接对象被创建时，默认情况下是自动提交事务：每次执行一个 SQL 语句时，如果执行成功，就会向数据库自动提交，而不能回滚。\n关闭数据库连接，数据就会自动的提交。如果多个操作，每个操作使用的是自己单独的连接，则无法保证事务。即同一个事务的多个操作必须在同一个连接下。\n\n\nJDBC程序中为了让多个 SQL 语句作为一个事务执行：\n\n调用 Connection 对象的 setAutoCommit(false); 以取消自动提交事务\n在所有的 SQL 语句都成功执行后，调用 commit(); 方法提交事务\n在出现异常时，调用 rollback(); 方法回滚事务\n\n\n\n\n\n\n\n\n\n\n若此时 Connection 没有被关闭，还可能被重复使用，则需要恢复其自动提交状态 setAutoCommit(true)。尤其是在使用数据库连接池技术时，执行close()方法前，建议恢复自动提交状态。\n\n\n【案例：用户AA向用户BB转账100】\npublic void testJDBCTransaction() {\n\tConnection conn = null;\n\ttry {\n\t\t// 1.获取数据库连接\n\t\tconn = JDBCUtils.getConnection();\n\t\t// 2.开启事务\n\t\tconn.setAutoCommit(false);\n\t\t// 3.进行数据库操作\n\t\tString sql1 = \"update user_table set balance = balance - 100 where user = ?\";\n\t\tupdate(conn, sql1, \"AA\");\n\n\t\t// 模拟网络异常\n\t\t//System.out.println(10 / 0);\n\n\t\tString sql2 = \"update user_table set balance = balance + 100 where user = ?\";\n\t\tupdate(conn, sql2, \"BB\");\n\t\t// 4.若没有异常，则提交事务\n\t\tconn.commit();\n\t} catch (Exception e) {\n\t\te.printStackTrace();\n\t\t// 5.若有异常，则回滚事务\n\t\ttry {\n\t\t\tconn.rollback();\n\t\t} catch (SQLException e1) {\n\t\t\te1.printStackTrace();\n\t\t}\n    } finally {\n        try {\n\t\t\t//6.恢复每次DML操作的自动提交功能\n\t\t\tconn.setAutoCommit(true);\n\t\t} catch (SQLException e) {\n\t\t\te.printStackTrace();\n\t\t}\n        //7.关闭连接\n\t\tJDBCUtils.closeResource(conn, null, null); \n    }  \n}\n\n其中，对数据库操作的方法为：\n//使用事务以后的通用的增删改操作（version 2.0）\npublic void update(Connection conn ,String sql, Object... args) {\n\tPreparedStatement ps = null;\n\ttry {\n\t\t// 1.获取PreparedStatement的实例 (或：预编译sql语句)\n\t\tps = conn.prepareStatement(sql);\n\t\t// 2.填充占位符\n\t\tfor (int i = 0; i &lt; args.length; i++) {\n\t\t\tps.setObject(i + 1, args[i]);\n\t\t}\n\t\t// 3.执行sql语句\n\t\tps.execute();\n\t} catch (Exception e) {\n\t\te.printStackTrace();\n\t} finally {\n\t\t// 4.关闭资源\n\t\tJDBCUtils.closeResource(null, ps);\n\n\t}\n}\n6.3 事务的ACID属性\n原子性（Atomicity）原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 \n\n一致性（Consistency）事务必须使数据库从一个一致性状态变换到另外一个一致性状态。\n\n隔离性（Isolation）事务的隔离性是指一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。\n\n持久性（Durability）持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来的其他操作和数据库故障不应该对其有任何影响。\n\n\n6.3.1 数据库的并发问题\n对于同时运行的多个事务, 当这些事务访问数据库中相同的数据时, 如果没有采取必要的隔离机制, 就会导致各种并发问题:\n\n脏读: 对于两个事务 T1, T2, T1 读取了已经被 T2 更新但还没有被提交的字段。之后, 若 T2 回滚, T1读取的内容就是临时且无效的。\n不可重复读: 对于两个事务T1, T2, T1 读取了一个字段, 然后 T2 更新了该字段。之后, T1再次读取同一个字段, 值就不同了。\n幻读: 对于两个事务T1, T2, T1 从一个表中读取了一个字段, 然后 T2 在该表中插入了一些新的行。之后, 如果 T1 再次读取同一个表, 就会多出几行。\n\n\n数据库事务的隔离性: 数据库系统必须具有隔离并发运行各个事务的能力, 使它们不会相互影响, 避免各种并发问题。\n\n一个事务与其他事务隔离的程度称为隔离级别。数据库规定了多种事务隔离级别, 不同隔离级别对应不同的干扰程度, 隔离级别越高, 数据一致性就越好, 但并发性越弱。\n\n\n6.3.2 四种隔离级别\n数据库提供的4种事务隔离级别：\n\n\nOracle 支持的 2 种事务隔离级别：READ COMMITED, SERIALIZABLE。 Oracle 默认的事务隔离级别为: READ COMMITED 。\n\n\n\nMysql 支持 4 种事务隔离级别。Mysql 默认的事务隔离级别为: REPEATABLE READ。\n\n6.3.3 在MySql中设置隔离级别\n每启动一个 mysql 程序, 就会获得一个单独的数据库连接. 每个数据库连接都有一个全局变量 @@tx_isolation, 表示当前的事务隔离级别。\n\n查看当前的隔离级别: \nSELECT @@tx_isolation;\n\n设置当前 mySQL 连接的隔离级别:  \nset  transaction isolation level read committed;\n\n设置数据库系统的全局的隔离级别:\nset global transaction isolation level read committed;\n\n补充操作：\n\n创建mysql数据库用户：\ncreate user tom identified by 'abc123';\n\n授予权限\n#授予通过网络方式登录的tom用户，对所有库所有表的全部权限，密码设为abc123.\ngrant all privileges on *.* to tom@'%'  identified by 'abc123'; \n\n #给tom用户使用本地命令行方式，授予atguigudb这个库下的所有表的插删改查的权限。\ngrant select,insert,delete,update on atguigudb.* to tom@localhost identified by 'abc123'; \n\n\n\n\n\n第7章：DAO及相关实现类\nDAO：Data Access Object访问数据信息的类和接口，包括了对数据的CRUD（Create、Retrival、Update、Delete），而不包含任何业务相关的信息。有时也称作：BaseDAO\n作用：为了实现功能的模块化，更有利于代码的维护和升级。\n下面是尚硅谷JavaWeb阶段书城项目中DAO使用的体现：\n\n\n层次结构：\n\n【BaseDAO.java】package com.atguigu.bookstore.dao;\n\nimport java.lang.reflect.ParameterizedType;\nimport java.lang.reflect.Type;\nimport java.sql.Connection;\nimport java.sql.SQLException;\nimport java.util.List;\n\nimport org.apache.commons.dbutils.QueryRunner;\nimport org.apache.commons.dbutils.handlers.BeanHandler;\nimport org.apache.commons.dbutils.handlers.BeanListHandler;\nimport org.apache.commons.dbutils.handlers.ScalarHandler;\n\n\n/**\n * 定义一个用来被继承的对数据库进行基本操作的Dao\n * \n * @author HanYanBing\n *\n * @param &lt;T&gt;\n */\npublic abstract class BaseDao&lt;T&gt; {\n\tprivate QueryRunner queryRunner = new QueryRunner();\n\t// 定义一个变量来接收泛型的类型\n\tprivate Class&lt;T&gt; type;\n\n\t// 获取T的Class对象，获取泛型的类型，泛型是在被子类继承时才确定\n\tpublic BaseDao() {\n\t\t// 获取子类的类型\n\t\tClass clazz = this.getClass();\n\t\t// 获取父类的类型\n\t\t// getGenericSuperclass()用来获取当前类的父类的类型\n\t\t// ParameterizedType表示的是带泛型的类型\n\t\tParameterizedType parameterizedType = (ParameterizedType) clazz.getGenericSuperclass();\n\t\t// 获取具体的泛型类型 getActualTypeArguments获取具体的泛型的类型\n\t\t// 这个方法会返回一个Type的数组\n\t\tType[] types = parameterizedType.getActualTypeArguments();\n\t\t// 获取具体的泛型的类型·\n\t\tthis.type = (Class&lt;T&gt;) types[0];\n\t}\n\n\t/**\n\t * 通用的增删改操作\n\t * \n\t * @param sql\n\t * @param params\n\t * @return\n\t */\n\tpublic int update(Connection conn,String sql, Object... params) {\n\t\tint count = 0;\n\t\ttry {\n\t\t\tcount = queryRunner.update(conn, sql, params);\n\t\t} catch (SQLException e) {\n\t\t\te.printStackTrace();\n\t\t} \n\t\treturn count;\n\t}\n\n\t/**\n\t * 获取一个对象\n\t * \n\t * @param sql\n\t * @param params\n\t * @return\n\t */\n\tpublic T getBean(Connection conn,String sql, Object... params) {\n\t\tT t = null;\n\t\ttry {\n\t\t\tt = queryRunner.query(conn, sql, new BeanHandler&lt;T&gt;(type), params);\n\t\t} catch (SQLException e) {\n\t\t\te.printStackTrace();\n\t\t} \n\t\treturn t;\n\t}\n\n\t/**\n\t * 获取所有对象\n\t * \n\t * @param sql\n\t * @param params\n\t * @return\n\t */\n\tpublic List&lt;T&gt; getBeanList(Connection conn,String sql, Object... params) {\n\t\tList&lt;T&gt; list = null;\n\t\ttry {\n\t\t\tlist = queryRunner.query(conn, sql, new BeanListHandler&lt;T&gt;(type), params);\n\t\t} catch (SQLException e) {\n\t\t\te.printStackTrace();\n\t\t} \n\t\treturn list;\n\t}\n\n\t/**\n\t * 获取一个但一值得方法，专门用来执行像 select count(*)...这样的sql语句\n\t * \n\t * @param sql\n\t * @param params\n\t * @return\n\t */\n\tpublic Object getValue(Connection conn,String sql, Object... params) {\n\t\tObject count = null;\n\t\ttry {\n\t\t\t// 调用queryRunner的query方法获取一个单一的值\n\t\t\tcount = queryRunner.query(conn, sql, new ScalarHandler&lt;&gt;(), params);\n\t\t} catch (SQLException e) {\n\t\t\te.printStackTrace();\n\t\t} \n\t\treturn count;\n\t}\n}\n【BookDAO.java】package com.atguigu.bookstore.dao;\n\nimport java.sql.Connection;\nimport java.util.List;\n\nimport com.atguigu.bookstore.beans.Book;\nimport com.atguigu.bookstore.beans.Page;\n\npublic interface BookDao {\n\n\t/**\n\t * 从数据库中查询出所有的记录\n\t * \n\t * @return\n\t */\n\tList&lt;Book&gt; getBooks(Connection conn);\n\n\t/**\n\t * 向数据库中插入一条记录\n\t * \n\t * @param book\n\t */\n\tvoid saveBook(Connection conn,Book book);\n\n\t/**\n\t * 从数据库中根据图书的id删除一条记录\n\t * \n\t * @param bookId\n\t */\n\tvoid deleteBookById(Connection conn,String bookId);\n\n\t/**\n\t * 根据图书的id从数据库中查询出一条记录\n\t * \n\t * @param bookId\n\t * @return\n\t */\n\tBook getBookById(Connection conn,String bookId);\n\n\t/**\n\t * 根据图书的id从数据库中更新一条记录\n\t * \n\t * @param book\n\t */\n\tvoid updateBook(Connection conn,Book book);\n\n\t/**\n\t * 获取带分页的图书信息\n\t * \n\t * @param page：是只包含了用户输入的pageNo属性的page对象\n\t * @return 返回的Page对象是包含了所有属性的Page对象\n\t */\n\tPage&lt;Book&gt; getPageBooks(Connection conn,Page&lt;Book&gt; page);\n\n\t/**\n\t * 获取带分页和价格范围的图书信息\n\t * \n\t * @param page：是只包含了用户输入的pageNo属性的page对象\n\t * @return 返回的Page对象是包含了所有属性的Page对象\n\t */\n\tPage&lt;Book&gt; getPageBooksByPrice(Connection conn,Page&lt;Book&gt; page, double minPrice, double maxPrice);\n\n}\n【UserDAO.java】package com.atguigu.bookstore.dao;\n\nimport java.sql.Connection;\n\nimport com.atguigu.bookstore.beans.User;\n\npublic interface UserDao {\n\n\t/**\n\t * 根据User对象中的用户名和密码从数据库中获取一条记录\n\t * \n\t * @param user\n\t * @return User 数据库中有记录 null 数据库中无此记录\n\t */\n\tUser getUser(Connection conn,User user);\n\n\t/**\n\t * 根据User对象中的用户名从数据库中获取一条记录\n\t * \n\t * @param user\n\t * @return true 数据库中有记录 false 数据库中无此记录\n\t */\n\tboolean checkUsername(Connection conn,User user);\n\n\t/**\n\t * 向数据库中插入User对象\n\t * \n\t * @param user\n\t */\n\tvoid saveUser(Connection conn,User user);\n}\n【BookDaoImpl.java】package com.atguigu.bookstore.dao.impl;\n\nimport java.sql.Connection;\nimport java.util.List;\n\nimport com.atguigu.bookstore.beans.Book;\nimport com.atguigu.bookstore.beans.Page;\nimport com.atguigu.bookstore.dao.BaseDao;\nimport com.atguigu.bookstore.dao.BookDao;\n\npublic class BookDaoImpl extends BaseDao&lt;Book&gt; implements BookDao {\n\n\t@Override\n\tpublic List&lt;Book&gt; getBooks(Connection conn) {\n\t\t// 调用BaseDao中得到一个List的方法\n\t\tList&lt;Book&gt; beanList = null;\n\t\t// 写sql语句\n\t\tString sql = \"select id,title,author,price,sales,stock,img_path imgPath from books\";\n\t\tbeanList = getBeanList(conn,sql);\n\t\treturn beanList;\n\t}\n\n\t@Override\n\tpublic void saveBook(Connection conn,Book book) {\n\t\t// 写sql语句\n\t\tString sql = \"insert into books(title,author,price,sales,stock,img_path) values(?,?,?,?,?,?)\";\n\t\t// 调用BaseDao中通用的增删改的方法\n\t\tupdate(conn,sql, book.getTitle(), book.getAuthor(), book.getPrice(), book.getSales(), book.getStock(),book.getImgPath());\n\t}\n\n\t@Override\n\tpublic void deleteBookById(Connection conn,String bookId) {\n\t\t// 写sql语句\n\t\tString sql = \"DELETE FROM books WHERE id = ?\";\n\t\t// 调用BaseDao中通用增删改的方法\n\t\tupdate(conn,sql, bookId);\n\t\t\t\n\t}\n\n\t@Override\n\tpublic Book getBookById(Connection conn,String bookId) {\n\t\t// 调用BaseDao中获取一个对象的方法\n\t\tBook book = null;\n\t\t// 写sql语句\n\t\tString sql = \"select id,title,author,price,sales,stock,img_path imgPath from books where id = ?\";\n\t\tbook = getBean(conn,sql, bookId);\n\t\treturn book;\n\t}\n\n\t@Override\n\tpublic void updateBook(Connection conn,Book book) {\n\t\t// 写sql语句\n\t\tString sql = \"update books set title = ? , author = ? , price = ? , sales = ? , stock = ? where id = ?\";\n\t\t// 调用BaseDao中通用的增删改的方法\n\t\tupdate(conn,sql, book.getTitle(), book.getAuthor(), book.getPrice(), book.getSales(), book.getStock(), book.getId());\n\t}\n\n\t@Override\n\tpublic Page&lt;Book&gt; getPageBooks(Connection conn,Page&lt;Book&gt; page) {\n\t\t// 获取数据库中图书的总记录数\n\t\tString sql = \"select count(*) from books\";\n\t\t// 调用BaseDao中获取一个单一值的方法\n\t\tlong totalRecord = (long) getValue(conn,sql);\n\t\t// 将总记录数设置都page对象中\n\t\tpage.setTotalRecord((int) totalRecord);\n\n\t\t// 获取当前页中的记录存放的List\n\t\tString sql2 = \"select id,title,author,price,sales,stock,img_path imgPath from books limit ?,?\";\n\t\t// 调用BaseDao中获取一个集合的方法\n\t\tList&lt;Book&gt; beanList = getBeanList(conn,sql2, (page.getPageNo() - 1) * Page.PAGE_SIZE, Page.PAGE_SIZE);\n\t\t// 将这个List设置到page对象中\n\t\tpage.setList(beanList);\n\t\treturn page;\n\t}\n\n\t@Override\n\tpublic Page&lt;Book&gt; getPageBooksByPrice(Connection conn,Page&lt;Book&gt; page, double minPrice, double maxPrice) {\n\t\t// 获取数据库中图书的总记录数\n\t\tString sql = \"select count(*) from books where price between ? and ?\";\n\t\t// 调用BaseDao中获取一个单一值的方法\n\t\tlong totalRecord = (long) getValue(conn,sql,minPrice,maxPrice);\n\t\t// 将总记录数设置都page对象中\n\t\tpage.setTotalRecord((int) totalRecord);\n\n\t\t// 获取当前页中的记录存放的List\n\t\tString sql2 = \"select id,title,author,price,sales,stock,img_path imgPath from books where price between ? and ? limit ?,?\";\n\t\t// 调用BaseDao中获取一个集合的方法\n\t\tList&lt;Book&gt; beanList = getBeanList(conn,sql2, minPrice , maxPrice , (page.getPageNo() - 1) * Page.PAGE_SIZE, Page.PAGE_SIZE);\n\t\t// 将这个List设置到page对象中\n\t\tpage.setList(beanList);\n\t\t\n\t\treturn page;\n\t}\n\n}\n【UserDaoImpl.java】package com.atguigu.bookstore.dao.impl;\n\nimport java.sql.Connection;\n\nimport com.atguigu.bookstore.beans.User;\nimport com.atguigu.bookstore.dao.BaseDao;\nimport com.atguigu.bookstore.dao.UserDao;\n\npublic class UserDaoImpl extends BaseDao&lt;User&gt; implements UserDao {\n\n\t@Override\n\tpublic User getUser(Connection conn,User user) {\n\t\t// 调用BaseDao中获取一个对象的方法\n\t\tUser bean = null;\n\t\t// 写sql语句\n\t\tString sql = \"select id,username,password,email from users where username = ? and password = ?\";\n\t\tbean = getBean(conn,sql, user.getUsername(), user.getPassword());\n\t\treturn bean;\n\t}\n\n\t@Override\n\tpublic boolean checkUsername(Connection conn,User user) {\n\t\t// 调用BaseDao中获取一个对象的方法\n\t\tUser bean = null;\n\t\t// 写sql语句\n\t\tString sql = \"select id,username,password,email from users where username = ?\";\n\t\tbean = getBean(conn,sql, user.getUsername());\n\t\treturn bean != null;\n\t}\n\n\t@Override\n\tpublic void saveUser(Connection conn,User user) {\n\t\t//写sql语句\n\t\tString sql = \"insert into users(username,password,email) values(?,?,?)\";\n\t\t//调用BaseDao中通用的增删改的方法\n\t\tupdate(conn,sql, user.getUsername(),user.getPassword(),user.getEmail());\n\t}\n\n}\n【Book.java】package com.atguigu.bookstore.beans;\n/**\n * 图书类\n * @author songhongkang\n *\n */\npublic class Book {\n\n\tprivate Integer id;\n\tprivate String title; // 书名\n\tprivate String author; // 作者\n\tprivate double price; // 价格\n\tprivate Integer sales; // 销量\n\tprivate Integer stock; // 库存\n\tprivate String imgPath = \"static/img/default.jpg\"; // 封面图片的路径\n\t//构造器，get()，set()，toString()方法略\n}\n【Page.java】package com.atguigu.bookstore.beans;\n\nimport java.util.List;\n/**\n * 页码类\n * @author songhongkang\n *\n */\npublic class Page&lt;T&gt; {\n\n\tprivate List&lt;T&gt; list; // 每页查到的记录存放的集合\n\tpublic static final int PAGE_SIZE = 4; // 每页显示的记录数\n\tprivate int pageNo; // 当前页\n//\tprivate int totalPageNo; // 总页数，通过计算得到\n\tprivate int totalRecord; // 总记录数，通过查询数据库得到\n\n【User.java】package com.atguigu.bookstore.beans;\n/**\n * 用户类\n * @author songhongkang\n *\n */\npublic class User {\n\n\tprivate Integer id;\n\tprivate String username;\n\tprivate String password;\n\tprivate String email;\n\n第8章：数据库连接池8.1 JDBC数据库连接池的必要性\n在使用开发基于数据库的web程序时，传统的模式基本是按以下步骤：　　\n\n在主程序（如servlet、beans）中建立数据库连接\n进行sql操作\n断开数据库连接\n\n\n这种模式开发，存在的问题:\n\n普通的JDBC数据库连接使用 DriverManager 来获取，每次向数据库建立连接的时候都要将 Connection 加载到内存中，再验证用户名和密码(得花费0.05s～1s的时间)。需要数据库连接的时候，就向数据库要求一个，执行完成后再断开连接。这样的方式将会消耗大量的资源和时间。数据库的连接资源并没有得到很好的重复利用。若同时有几百人甚至几千人在线，频繁的进行数据库连接操作将占用很多的系统资源，严重的甚至会造成服务器的崩溃。\n对于每一次数据库连接，使用完后都得断开。否则，如果程序出现异常而未能关闭，将会导致数据库系统中的内存泄漏，最终将导致重启数据库。（回忆：何为Java的内存泄漏？）\n这种开发不能控制被创建的连接对象数，系统资源会被毫无顾及的分配出去，如连接过多，也可能导致内存泄漏，服务器崩溃。 \n\n\n\n8.2 数据库连接池技术\n为解决传统开发中的数据库连接问题，可以采用数据库连接池技术。\n数据库连接池的基本思想：就是为数据库连接建立一个“缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”中取出一个，使用完毕之后再放回去。\n\n数据库连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而不是重新建立一个。\n\n数据库连接池在初始化时将创建一定数量的数据库连接放到连接池中，这些数据库连接的数量是由最小数据库连接数来设定的。无论这些数据库连接是否被使用，连接池都将一直保证至少拥有这么多的连接数量。连接池的最大数据库连接数量限定了这个连接池能占有的最大连接数，当应用程序向连接池请求的连接数超过最大连接数量时，这些请求将被加入到等待队列中。\n\n\n\n工作原理：\n\n\n\n数据库连接池技术的优点\n1. 资源重用\n由于数据库连接得以重用，避免了频繁创建，释放连接引起的大量性能开销。在减少系统消耗的基础上，另一方面也增加了系统运行环境的平稳性。\n2. 更快的系统反应速度\n数据库连接池在初始化过程中，往往已经创建了若干数据库连接置于连接池中备用。此时连接的初始化工作均已完成。对于业务请求处理而言，直接利用现有可用连接，避免了数据库连接初始化和释放过程的时间开销，从而减少了系统的响应时间\n3. 新的资源分配手段\n对于多应用共享同一数据库的系统而言，可在应用层通过数据库连接池的配置，实现某一应用最大可用数据库连接数的限制，避免某一应用独占所有的数据库资源\n4. 统一的连接管理，避免数据库连接泄漏\n在较为完善的数据库连接池实现中，可根据预先的占用超时设定，强制回收被占用连接，从而避免了常规数据库连接操作中可能出现的资源泄露\n\n\n8.3 多种开源的数据库连接池\nJDBC 的数据库连接池使用 javax.sql.DataSource 来表示，DataSource 只是一个接口，该接口通常由服务器(Weblogic, WebSphere, Tomcat)提供实现，也有一些开源组织提供实现：\nDBCP 是Apache提供的数据库连接池。tomcat 服务器自带dbcp数据库连接池。速度相对c3p0较快，但因自身存在BUG，Hibernate3已不再提供支持。\nC3P0 是一个开源组织提供的一个数据库连接池，速度相对较慢，稳定性还可以。hibernate官方推荐使用\nProxool 是sourceforge下的一个开源项目数据库连接池，有监控连接池状态的功能，稳定性较c3p0差一点\nBoneCP 是一个开源组织提供的数据库连接池，速度快\nDruid 是阿里提供的数据库连接池，据说是集DBCP 、C3P0 、Proxool 优点于一身的数据库连接池，但是速度不确定是否有BoneCP快\n\n\nDataSource 通常被称为数据源，它包含连接池和连接池管理两个部分，习惯上也经常把 DataSource 称为连接池\nDataSource用来取代DriverManager来获取Connection，获取速度快，同时可以大幅度提高数据库访问速度。\n特别注意：\n数据源和数据库连接不同，数据源无需创建多个，它是产生数据库连接的工厂，因此整个应用只需要一个数据源即可。\n当数据库访问结束后，程序还是像以前一样关闭数据库连接：conn.close(); 但conn.close()并没有关闭数据库的物理连接，它仅仅把数据库连接释放，归还给了数据库连接池。\n\n\n\n8.3.1 C3P0数据库连接池\n获取连接方式一\n\n//使用C3P0数据库连接池的方式，获取数据库的连接：不推荐\npublic static Connection getConnection1() throws Exception{\n\tComboPooledDataSource cpds = new ComboPooledDataSource();\n\tcpds.setDriverClass(\"com.mysql.jdbc.Driver\"); \n\tcpds.setJdbcUrl(\"jdbc:mysql://localhost:3306/test\");\n\tcpds.setUser(\"root\");\n\tcpds.setPassword(\"abc123\");\n\t\t\n//\tcpds.setMaxPoolSize(100);\n\t\n\tConnection conn = cpds.getConnection();\n\treturn conn;\n}\n\n获取连接方式二\n\n//使用C3P0数据库连接池的配置文件方式，获取数据库的连接：推荐\nprivate static DataSource cpds = new ComboPooledDataSource(\"helloc3p0\");\npublic static Connection getConnection2() throws SQLException{\n\tConnection conn = cpds.getConnection();\n\treturn conn;\n}\n其中，src下的配置文件为：【c3p0-config.xml】\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;c3p0-config&gt;\n\t&lt;named-config name=\"helloc3p0\"&gt;\n\t\t&lt;!-- 获取连接的4个基本信息 --&gt;\n\t\t&lt;property name=\"user\"&gt;root&lt;/property&gt;\n\t\t&lt;property name=\"password\"&gt;abc123&lt;/property&gt;\n\t\t&lt;property name=\"jdbcUrl\"&gt;jdbc:mysql:///test&lt;/property&gt;\n\t\t&lt;property name=\"driverClass\"&gt;com.mysql.jdbc.Driver&lt;/property&gt;\n\t\t\n\t\t&lt;!-- 涉及到数据库连接池的管理的相关属性的设置 --&gt;\n\t\t&lt;!-- 若数据库中连接数不足时, 一次向数据库服务器申请多少个连接 --&gt;\n\t\t&lt;property name=\"acquireIncrement\"&gt;5&lt;/property&gt;\n\t\t&lt;!-- 初始化数据库连接池时连接的数量 --&gt;\n\t\t&lt;property name=\"initialPoolSize\"&gt;5&lt;/property&gt;\n\t\t&lt;!-- 数据库连接池中的最小的数据库连接数 --&gt;\n\t\t&lt;property name=\"minPoolSize\"&gt;5&lt;/property&gt;\n\t\t&lt;!-- 数据库连接池中的最大的数据库连接数 --&gt;\n\t\t&lt;property name=\"maxPoolSize\"&gt;10&lt;/property&gt;\n\t\t&lt;!-- C3P0 数据库连接池可以维护的 Statement 的个数 --&gt;\n\t\t&lt;property name=\"maxStatements\"&gt;20&lt;/property&gt;\n\t\t&lt;!-- 每个连接同时可以使用的 Statement 对象的个数 --&gt;\n\t\t&lt;property name=\"maxStatementsPerConnection\"&gt;5&lt;/property&gt;\n\n\t&lt;/named-config&gt;\n&lt;/c3p0-config&gt;\n8.3.2 DBCP数据库连接池\nDBCP 是 Apache 软件基金组织下的开源连接池实现，该连接池依赖该组织下的另一个开源系统：Common-pool。如需使用该连接池实现，应在系统中增加如下两个 jar 文件：\nCommons-dbcp.jar：连接池的实现\nCommons-pool.jar：连接池实现的依赖库\n\n\nTomcat 的连接池正是采用该连接池来实现的。该数据库连接池既可以与应用服务器整合使用，也可由应用程序独立使用。\n数据源和数据库连接不同，数据源无需创建多个，它是产生数据库连接的工厂，因此整个应用只需要一个数据源即可。\n当数据库访问结束后，程序还是像以前一样关闭数据库连接：conn.close(); 但上面的代码并没有关闭数据库的物理连接，它仅仅把数据库连接释放，归还给了数据库连接池。\n配置属性说明\n\n\n\n\n\n属性\n默认值\n说明\n\n\n\n\ninitialSize\n0\n连接池启动时创建的初始化连接数量\n\n\nmaxActive\n8\n连接池中可同时连接的最大的连接数\n\n\nmaxIdle\n8\n连接池中最大的空闲的连接数，超过的空闲连接将被释放，如果设置为负数表示不限制\n\n\nminIdle\n0\n连接池中最小的空闲的连接数，低于这个数量会被创建新的连接。该参数越接近maxIdle，性能越好，因为连接的创建和销毁，都是需要消耗资源的；但是不能太大。\n\n\nmaxWait\n无限制\n最大等待时间，当没有可用连接时，连接池等待连接释放的最大时间，超过该时间限制会抛出异常，如果设置-1表示无限等待\n\n\npoolPreparedStatements\nfalse\n开启池的Statement是否prepared\n\n\nmaxOpenPreparedStatements\n无限制\n开启池的prepared 后的同时最大连接数\n\n\nminEvictableIdleTimeMillis\n\n连接池中连接，在时间段内一直空闲， 被逐出连接池的时间\n\n\nremoveAbandonedTimeout\n300\n超过时间限制，回收没有用(废弃)的连接\n\n\nremoveAbandoned\nfalse\n超过removeAbandonedTimeout时间后，是否进 行没用连接（废弃）的回收\n\n\n\n\n\n获取连接方式一：\n\npublic static Connection getConnection3() throws Exception {\n\tBasicDataSource source = new BasicDataSource();\n\t\t\n\tsource.setDriverClassName(\"com.mysql.jdbc.Driver\");\n\tsource.setUrl(\"jdbc:mysql:///test\");\n\tsource.setUsername(\"root\");\n\tsource.setPassword(\"abc123\");\n\t\t\n\t//\n\tsource.setInitialSize(10);\n\t\t\n\tConnection conn = source.getConnection();\n\treturn conn;\n}\n\n获取连接方式二：\n\n//使用dbcp数据库连接池的配置文件方式，获取数据库的连接：推荐\nprivate static DataSource source = null;\nstatic{\n\ttry {\n\t\tProperties pros = new Properties();\n\t\t\n\t\tInputStream is = DBCPTest.class.getClassLoader().getResourceAsStream(\"dbcp.properties\");\n\t\t\t\n\t\tpros.load(is);\n\t\t//根据提供的BasicDataSourceFactory创建对应的DataSource对象\n\t\tsource = BasicDataSourceFactory.createDataSource(pros);\n\t} catch (Exception e) {\n\t\te.printStackTrace();\n\t}\n\t\t\n}\npublic static Connection getConnection4() throws Exception {\n\t\t\n\tConnection conn = source.getConnection();\n\t\n\treturn conn;\n}\n其中，src下的配置文件为：【dbcp.properties】\ndriverClassName=com.mysql.jdbc.Driver\nurl=jdbc:mysql://localhost:3306/test?rewriteBatchedStatements=true&amp;useServerPrepStmts=false\nusername=root\npassword=abc123\n\ninitialSize=10\n#...\n8.3.3 Druid（德鲁伊）数据库连接池Druid是阿里巴巴开源平台上一个数据库连接池实现，它结合了C3P0、DBCP、Proxool等DB池的优点，同时加入了日志监控，可以很好的监控DB池连接和SQL的执行情况，可以说是针对监控而生的DB连接池，可以说是目前最好的连接池之一。\npackage com.atguigu.druid;\n\nimport java.sql.Connection;\nimport java.util.Properties;\n\nimport javax.sql.DataSource;\n\nimport com.alibaba.druid.pool.DruidDataSourceFactory;\n\npublic class TestDruid {\n\tpublic static void main(String[] args) throws Exception {\n\t\tProperties pro = new Properties();\t\t pro.load(TestDruid.class.getClassLoader().getResourceAsStream(\"druid.properties\"));\n\t\tDataSource ds = DruidDataSourceFactory.createDataSource(pro);\n\t\tConnection conn = ds.getConnection();\n\t\tSystem.out.println(conn);\n\t}\n}\n\n其中，src下的配置文件为：【druid.properties】\nurl=jdbc:mysql://localhost:3306/test?rewriteBatchedStatements=true\nusername=root\npassword=123456\ndriverClassName=com.mysql.jdbc.Driver\n\ninitialSize=10\nmaxActive=20\nmaxWait=1000\nfilters=wall\n\n详细配置参数：\n\n\n\n\n\n配置\n缺省\n说明\n\n\n\n\nname\n\n配置这个属性的意义在于，如果存在多个数据源，监控的时候可以通过名字来区分开来。   如果没有配置，将会生成一个名字，格式是：”DataSource-” +   System.identityHashCode(this)\n\n\nurl\n\n连接数据库的url，不同数据库不一样。例如：mysql :   jdbc:mysql://10.20.153.104:3306/druid2      oracle :   jdbc:oracle:thin:@10.20.149.85:1521:ocnauto\n\n\nusername\n\n连接数据库的用户名\n\n\npassword\n\n连接数据库的密码。如果你不希望密码直接写在配置文件中，可以使用ConfigFilter。详细看这里：https://github.com/alibaba/druid/wiki/%E4%BD%BF%E7%94%A8ConfigFilter\n\n\ndriverClassName\n\n根据url自动识别   这一项可配可不配，如果不配置druid会根据url自动识别dbType，然后选择相应的driverClassName(建议配置下)\n\n\ninitialSize\n0\n初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时\n\n\nmaxActive\n8\n最大连接池数量\n\n\nmaxIdle\n8\n已经不再使用，配置了也没效果\n\n\nminIdle\n\n最小连接池数量\n\n\nmaxWait\n\n获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁，并发效率会有所下降，如果需要可以通过配置useUnfairLock属性为true使用非公平锁。\n\n\npoolPreparedStatements\nfalse\n是否缓存preparedStatement，也就是PSCache。PSCache对支持游标的数据库性能提升巨大，比如说oracle。在mysql下建议关闭。\n\n\nmaxOpenPreparedStatements\n-1\n要启用PSCache，必须配置大于0，当大于0时，poolPreparedStatements自动触发修改为true。在Druid中，不会存在Oracle下PSCache占用内存过多的问题，可以把这个数值配置大一些，比如说100\n\n\nvalidationQuery\n\n用来检测连接是否有效的sql，要求是一个查询语句。如果validationQuery为null，testOnBorrow、testOnReturn、testWhileIdle都不会其作用。\n\n\ntestOnBorrow\ntrue\n申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。\n\n\ntestOnReturn\nfalse\n归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能\n\n\ntestWhileIdle\nfalse\n建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRunsMillis，执行validationQuery检测连接是否有效。\n\n\ntimeBetweenEvictionRunsMillis\n\n有两个含义： 1)Destroy线程会检测连接的间隔时间2)testWhileIdle的判断依据，详细看testWhileIdle属性的说明\n\n\nnumTestsPerEvictionRun\n\n不再使用，一个DruidDataSource只支持一个EvictionRun\n\n\nminEvictableIdleTimeMillis\n\n\n\n\nconnectionInitSqls\n\n物理连接初始化的时候执行的sql\n\n\nexceptionSorter\n\n根据dbType自动识别   当数据库抛出一些不可恢复的异常时，抛弃连接\n\n\nfilters\n\n属性类型是字符串，通过别名的方式配置扩展插件，常用的插件有：   监控统计用的filter:stat日志用的filter:log4j防御sql注入的filter:wall\n\n\nproxyFilters\n\n类型是List，如果同时配置了filters和proxyFilters，是组合关系，并非替换关系\n\n\n\n\n第9章：Apache-DBUtils实现CRUD操作9.1 Apache-DBUtils简介\ncommons-dbutils 是 Apache 组织提供的一个开源 JDBC工具类库，它是对JDBC的简单封装，学习成本极低，并且使用dbutils能极大简化jdbc编码的工作量，同时也不会影响程序的性能。\n\nAPI介绍：\n\norg.apache.commons.dbutils.QueryRunner\norg.apache.commons.dbutils.ResultSetHandler\n工具类：org.apache.commons.dbutils.DbUtils   \n\n\nAPI包说明：\n\n9.2 主要API的使用9.2.1 DbUtils\nDbUtils ：提供如关闭连接、装载JDBC驱动程序等常规工作的工具类，里面的所有方法都是静态的。主要方法如下：\npublic static void close(…) throws java.sql.SQLException：　DbUtils类提供了三个重载的关闭方法。这些方法检查所提供的参数是不是NULL，如果不是的话，它们就关闭Connection、Statement和ResultSet。\npublic static void closeQuietly(…): 这一类方法不仅能在Connection、Statement和ResultSet为NULL情况下避免关闭，还能隐藏一些在程序中抛出的SQLEeception。\npublic static void commitAndClose(Connection conn)throws SQLException： 用来提交连接的事务，然后关闭连接\npublic static void commitAndCloseQuietly(Connection conn)： 用来提交连接，然后关闭连接，并且在关闭连接时不抛出SQL异常。 \npublic static void rollback(Connection conn)throws SQLException：允许conn为null，因为方法内部做了判断\npublic static void rollbackAndClose(Connection conn)throws SQLException\nrollbackAndCloseQuietly(Connection)\npublic static boolean loadDriver(java.lang.String driverClassName)：这一方装载并注册JDBC驱动程序，如果成功就返回true。使用该方法，你不需要捕捉这个异常ClassNotFoundException。\n\n\n\n9.2.2 QueryRunner类\n该类简单化了SQL查询，它与ResultSetHandler组合在一起使用可以完成大部分的数据库操作，能够大大减少编码量。\n\nQueryRunner类提供了两个构造器：\n\n默认的构造器\n需要一个 javax.sql.DataSource 来作参数的构造器\n\n\nQueryRunner类的主要方法：\n\n更新\npublic int update(Connection conn, String sql, Object… params) throws SQLException:用来执行一个更新（插入、更新或删除）操作。\n……\n\n\n插入\npublic  T insert(Connection conn,String sql,ResultSetHandler rsh, Object… params) throws SQLException：只支持INSERT语句，其中 rsh - The handler used to create the result object from the ResultSet of auto-generated keys.  返回值: An object generated by the handler.即自动生成的键值\n….\n\n\n批处理\npublic int[] batch(Connection conn,String sql,Object[][] params)throws SQLException： INSERT, UPDATE, or DELETE语句\npublic  T insertBatch(Connection conn,String sql,ResultSetHandler rsh,Object[][] params)throws SQLException：只支持INSERT语句\n…..\n\n\n查询\npublic Object query(Connection conn, String sql, ResultSetHandler rsh,Object… params) throws SQLException：执行一个查询操作，在这个查询中，对象数组中的每个元素值被用来作为查询语句的置换参数。该方法会自行处理 PreparedStatement 和 ResultSet 的创建和关闭。\n…… \n\n\n\n\n测试\n\n\n// 测试添加\n@Test\npublic void testInsert() throws Exception {\n\tQueryRunner runner = new QueryRunner();\n\tConnection conn = JDBCUtils.getConnection3();\n\tString sql = \"insert into customers(name,email,birth)values(?,?,?)\";\n\tint count = runner.update(conn, sql, \"何成飞\", \"he@qq.com\", \"1992-09-08\");\n\n\tSystem.out.println(\"添加了\" + count + \"条记录\");\n\t\t\n\tJDBCUtils.closeResource(conn, null);\n\n}\n// 测试删除\n@Test\npublic void testDelete() throws Exception {\n\tQueryRunner runner = new QueryRunner();\n\tConnection conn = JDBCUtils.getConnection3();\n\tString sql = \"delete from customers where id &lt; ?\";\n\tint count = runner.update(conn, sql,3);\n\n\tSystem.out.println(\"删除了\" + count + \"条记录\");\n\t\t\n\tJDBCUtils.closeResource(conn, null);\n\n}\n9.2.3 ResultSetHandler接口及实现类\n该接口用于处理 java.sql.ResultSet，将数据按要求转换为另一种形式。\n\nResultSetHandler 接口提供了一个单独的方法：Object handle (java.sql.ResultSet .rs)。\n\n接口的主要实现类：\n\nArrayHandler：把结果集中的第一行数据转成对象数组。\n\nArrayListHandler：把结果集中的每一行数据都转成一个数组，再存放到List中。\n\nBeanHandler：将结果集中的第一行数据封装到一个对应的JavaBean实例中。\n\nBeanListHandler：将结果集中的每一行数据都封装到一个对应的JavaBean实例中，存放到List里。\n\nColumnListHandler：将结果集中某一列的数据存放到List中。\n\nKeyedHandler(name)：将结果集中的每一行数据都封装到一个Map里，再把这些map再存到一个map里，其key为指定的key。\n\nMapHandler：将结果集中的第一行数据封装到一个Map里，key是列名，value就是对应的值。\n\nMapListHandler：将结果集中的每一行数据都封装到一个Map里，然后再存放到List\n\nScalarHandler：查询单个值对象\n\n\n\n\n\n测试\n\n/*\n * 测试查询:查询一条记录\n * \n * 使用ResultSetHandler的实现类：BeanHandler\n */\n@Test\npublic void testQueryInstance() throws Exception{\n\tQueryRunner runner = new QueryRunner();\n\n\tConnection conn = JDBCUtils.getConnection3();\n\t\t\n\tString sql = \"select id,name,email,birth from customers where id = ?\";\n\t\t\n\t//\n\tBeanHandler&lt;Customer&gt; handler = new BeanHandler&lt;&gt;(Customer.class);\n\tCustomer customer = runner.query(conn, sql, handler, 23);\n\tSystem.out.println(customer);\t\n\tJDBCUtils.closeResource(conn, null);\n}\n/*\n * 测试查询:查询多条记录构成的集合\n * \n * 使用ResultSetHandler的实现类：BeanListHandler\n */\n@Test\npublic void testQueryList() throws Exception{\n\tQueryRunner runner = new QueryRunner();\n\n\tConnection conn = JDBCUtils.getConnection3();\n\t\t\n\tString sql = \"select id,name,email,birth from customers where id &lt; ?\";\n\t\t\n\t//\n\tBeanListHandler&lt;Customer&gt; handler = new BeanListHandler&lt;&gt;(Customer.class);\n\tList&lt;Customer&gt; list = runner.query(conn, sql, handler, 23);\n\tlist.forEach(System.out::println);\n\t\t\n\tJDBCUtils.closeResource(conn, null);\n}\n/*\n * 自定义ResultSetHandler的实现类\n */\n@Test\npublic void testQueryInstance1() throws Exception{\n\tQueryRunner runner = new QueryRunner();\n\n\tConnection conn = JDBCUtils.getConnection3();\n\t\t\n\tString sql = \"select id,name,email,birth from customers where id = ?\";\n\t\t\n\tResultSetHandler&lt;Customer&gt; handler = new ResultSetHandler&lt;Customer&gt;() {\n\n\t\t@Override\n\t\tpublic Customer handle(ResultSet rs) throws SQLException {\n\t\t\tSystem.out.println(\"handle\");\n//\t\t\treturn new Customer(1,\"Tom\",\"tom@126.com\",new Date(123323432L));\n\t\t\t\t\n\t\t\tif(rs.next()){\n\t\t\t\tint id = rs.getInt(\"id\");\n\t\t\t\tString name = rs.getString(\"name\");\n\t\t\t\tString email = rs.getString(\"email\");\n\t\t\t\tDate birth = rs.getDate(\"birth\");\n\t\t\t\t\t\n\t\t\t\treturn new Customer(id, name, email, birth);\n\t\t\t}\n\t\t\treturn null;\n\t\t\t\t\n\t\t}\n\t};\n\t\t\n\tCustomer customer = runner.query(conn, sql, handler, 23);\n\t\t\n\tSystem.out.println(customer);\n\t\t\n\tJDBCUtils.closeResource(conn, null);\n}\n/*\n * 如何查询类似于最大的，最小的，平均的，总和，个数相关的数据，\n * 使用ScalarHandler\n * \n */\n@Test\npublic void testQueryValue() throws Exception{\n\tQueryRunner runner = new QueryRunner();\n\n\tConnection conn = JDBCUtils.getConnection3();\n\t\t\n\t//测试一：\n//\tString sql = \"select count(*) from customers where id &lt; ?\";\n//\tScalarHandler handler = new ScalarHandler();\n//\tlong count = (long) runner.query(conn, sql, handler, 20);\n//\tSystem.out.println(count);\n\t\t\n\t//测试二：\n\tString sql = \"select max(birth) from customers\";\n\tScalarHandler handler = new ScalarHandler();\n\tDate birth = (Date) runner.query(conn, sql, handler);\n\tSystem.out.println(birth);\n\t\t\n\tJDBCUtils.closeResource(conn, null);\n}\nJDBC总结总结\n@Test\npublic void testUpdateWithTx() {\n\t\t\n\tConnection conn = null;\n\ttry {\n\t\t//1.获取连接的操作（\n\t\t//① 手写的连接：JDBCUtils.getConnection();\n\t\t//② 使用数据库连接池：C3P0;DBCP;Druid\n\t\t//2.对数据表进行一系列CRUD操作\n\t\t//① 使用PreparedStatement实现通用的增删改、查询操作（version 1.0 \\ version 2.0)\n//version2.0的增删改public void update(Connection conn,String sql,Object ... args){}\n//version2.0的查询 public &lt;T&gt; T getInstance(Connection conn,Class&lt;T&gt; clazz,String sql,Object ... args){}\n\t\t//② 使用dbutils提供的jar包中提供的QueryRunner类\n\t\t\t\n\t\t//提交数据\n\t\tconn.commit();\n\t\t\t\n\t\n\t} catch (Exception e) {\n\t\te.printStackTrace();\n\t\t\t\n\t\t\t\n\t\ttry {\n\t\t\t//回滚数据\n\t\t\tconn.rollback();\n\t\t} catch (SQLException e1) {\n\t\t\te1.printStackTrace();\n\t\t}\n\t\t\t\n\t}finally{\n\t\t//3.关闭连接等操作\n\t\t//① JDBCUtils.closeResource();\n\t\t//② 使用dbutils提供的jar包中提供的DbUtils类提供了关闭的相关操作\n\t\t\t\n\t}\n}\n","slug":"J8-JDBC","date":"2021-11-25T12:03:14.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"31b2ee58f98c3235a398e6dd8f51555c","title":"Hbase","content":"","slug":"B7-Hbase","date":"2021-11-19T13:59:53.000Z","categories_index":"数据库","tags_index":"Hive","author_index":"YFR718"},{"id":"82053cc328c59fdc49cec929f76fa81d","title":"编程错误锦集","content":"1111111111111111111111\n\n\n\n\n\n\n\n注意\n\n\n\n\n\n\n\n\n\n特别注意\n\n\nHashMap方法的错误\nif(ha.containKey(target-nums[i])){\nreturn new int[]{ha.get(target-nums[i]),i};\n}\n\n\n\n\n\n\n\n containsKey()\n方法名都能写错，少一个s！\n\n\npublic int myAtoi(String s) {\n\tif(f==-1 &amp;&amp; ans&lt;(-Math.pow(2,31)-x)/10) return (int)-Math.pow(2,31);\n\t...\n\n\n\n\n\n\n\n\n incompatible types : possible 1 ossy conversi on from double to int\n返回值为int，必须强制类型转换\n\n\nList&lt;List&lt;Integer&gt;&gt; ans = new List&lt;List&lt;Integer&gt;&gt;();//错误\n\n\n\n\n\n\n\n 右边应为Arraylist\nList&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;List&lt;Integer&gt;&gt;();\n\n\n\n\n\n\n\n\n\ndfs中要深复制，否则后面回溯结果为空\n//if(sums == target) ans.add(ls);\nif(sums == target) ans.add(new ArrayList&lt;Integer&gt;(ls));\n\n\n\n\n\n\n\n\n\n二维List定义问题\n//List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;();\nList&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;List&lt;Integer&gt;&gt;();\n\n\n","slug":"J7-编程错误锦集","date":"2021-11-18T08:55:03.000Z","categories_index":"JAVA","tags_index":"","author_index":"YFR718"},{"id":"5703ea14901e0d80ea4ba70122c1bc16","title":"B6.Hive基础","content":"1. Hive 基本概念1.1 Hive概述​        Hive：由 Facebook 开源用于解决海量结构化日志的数据统计工具。​        Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。\nHive 本质：将 HQL 转化成 MapReduce 程序\n\n（1）Hive 处理的数据存储在 HDFS（2）Hive 分析数据底层的实现是 MapReduce（3）执行程序运行在 Yarn 上\nHive优点：\n\n操作接口采用类 SQL 语法，提供快速开发的能力（简单、容易上手）。\n避免了去写 MapReduce，减少开发人员的学习成本。 \nHive 的执行延迟比较高，因此 Hive 常用于数据分析，对实时性要求不高的场合。 \nHive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。\nHive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。\n\nHive缺点：\n\nHive 的 HQL 表达能力有限\n迭代式算法无法表达\n数据挖掘方面不擅长，由于 MapReduce 数据处理流程的限制，效率更高的算法却无法实现。\n\n\nHive 的效率比较低\nHive 自动生成的 MapReduce 作业，通常情况下不够智能化\nHive 调优比较困难，粒度较粗\n\n\n\n1.2 Hive架构原理\n\n用户接口：ClientCLI（command-line interface）、JDBC/ODBC(jdbc 访问 hive)、WEBUI（浏览器访问 hive）\n元数据：Metastore元数据包括：表名、表所属的数据库（默认是 default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore\nHadoop使用 HDFS 进行存储，使用 MapReduce 进行计算。\n驱动器：Driver（1）解析器（SQL Parser）：将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第三方工具库完成，比如 antlr；对 AST 进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。（2）编译器（Physical Plan）：将 AST 编译生成逻辑执行计划。（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说，就是 MR/Spark。\n\n\n​        Hive 通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的 Driver，结合元数据(MetaStore)，将这些指令翻译成 MapReduce，提交到 Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。\n1.3  Hive 和数据库比较\n查询语言由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。\n数据更新由于 Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive 中 不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET 修改数据。\n执行延迟Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce 框架。由于 MapReduce 本身具有较高的延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive 的并行计算显然能体现出优势。\n数据规模由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。\n\n2. Hive 安装2.1  Hive 安装地址1）Hive 官网地址：http://hive.apache.org/2）文档查看地址：https://cwiki.apache.org/confluence/display/Hive/GettingStarted3）下载地址：http://archive.apache.org/dist/hive/4）github 地址：https://github.com/apache/hive\n2.2 Hive 安装部署安装 Hive1）把 apache-hive-3.1.2-bin.tar.gz 上传到 linux 的/opt/software 目录下2）解压 apache-hive-3.1.2-bin.tar.gz 到/opt/module/目录下面\n[atguigu@hadoop102 software]$ tar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/\n3）修改 apache-hive-3.1.2-bin.tar.gz 的名称为 hive\n[atguigu@hadoop102 software]$ mv /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive\n4）修改/etc/profile.d/my_env.sh，添加环境变量\n[atguigu@hadoop102 software]$ sudo vim /etc/profile.d/my_env.sh\n5）添加内容\n#HIVE_HOME\nexport HIVE_HOME=/opt/module/hive\nexport PATH=$PATH:$HIVE_HOME/bin\n6）解决日志 Jar 包冲突\n[atguigu@hadoop102 software]$ mv $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.bak\n7）初始化元数据库\n[atguigu@hadoop102 hive]$ bin/schematool -dbType derby -initSchema\n启动并使用 Hive# 1）启动 Hive\n[atguigu@hadoop102 hive]$ bin/hive\n# 2）使用 Hive\nhive&gt; show databases;\nhive&gt; show tables;\nhive&gt; create table test(id int);\nhive&gt; insert into test values(1);\nhive&gt; select * from test; \n# 3）在 CRT 窗口中开启另一个窗口开启 Hive，在/tmp/atguigu 目录下监控 hive.log 文件\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted \nthe database /opt/module/hive/metastore_db.\n at \norg.apache.derby.iapi.error.StandardException.newException(Unknown \nSource)\n at \norg.apache.derby.iapi.error.StandardException.newException(Unknown\n\nSource)\n at \norg.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockO\nnDB(Unknown Source)\n at \norg.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown \nSource)\n...\n#原因在于 Hive 默认使用的元数据库为 derby，开启 Hive 之后就会占用元数据库，且不与其他客户端共享数据，所以我们需要将 Hive 的元数据地址改为 MySQL。\n2.3 MySQL 安装1）检查当前系统是否安装过 MySQL\n[atguigu@hadoop102 ~]$ rpm -qa|grep mariadb\nmariadb-libs-5.5.56-2.el7.x86_64 \n//如果存在通过如下命令卸载\n[atguigu @hadoop102 ~]$ sudo rpm -e --nodeps mariadb-libs\n2）将 MySQL 安装包拷贝到/opt/software 目录下\n[atguigu @hadoop102 software]# ll\n总用量 528384\n-rw-r--r--. 1 root root 609556480 3 月 21 15:41 mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar\n3）解压 MySQL 安装包\n[atguigu @hadoop102 software]# tar -xf mysql-5.7.28-1.el7.x86_64.rpm bundle.tar\n4）在安装目录下执行 rpm 安装\n[atguigu @hadoop102 software]$ \nsudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm\nsudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm\nsudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm\nsudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm\nsudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm\n注意:按照顺序依次执行如果 Linux 是最小化安装的，在安装 mysql-community-server-5.7.28-1.el7.x86_64.rpm 时可能会出现如下错误\n[atguigu@hadoop102 software]$ sudo rpm -ivh mysql-community-server-5.7.281.el7.x86_64.rpm\n警告：mysql-community-server-5.7.28-1.el7.x86_64.rpm: 头 V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY\n错误：依赖检测失败：\n libaio.so.1()(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要\n libaio.so.1(LIBAIO_0.1)(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要\n libaio.so.1(LIBAIO_0.4)(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要\n通过 yum 安装缺少的依赖,然后重新安装 mysql-community-server-5.7.28-1.el7.x86_64 即可\n[atguigu@hadoop102 software] yum install -y libaio\n5）删除/etc/my.cnf 文件中 datadir 指向的目录下的所有内容,如果有内容的情况下: 查看 datadir 的值：\n[mysqld]\ndatadir=/var/lib/mysql\n 删除/var/lib/mysql 目录下的所有内容: \n[atguigu @hadoop102 mysql]# cd /var/lib/mysql\n[atguigu @hadoop102 mysql]# sudo rm -rf ./* //注意执行命令的位置\n6）初始化数据库\n[atguigu @hadoop102 opt]$ sudo mysqld --initialize --user=mysql\n7）查看临时生成的 root 用户的密码\n[atguigu @hadoop102 opt]$ sudo cat /var/log/mysqld.log \n8）启动 MySQL 服务\n[atguigu @hadoop102 opt]$ sudo systemctl start mysqld\n9）登录 MySQL 数据库\n[atguigu @hadoop102 opt]$​ mysql -uroot -p\nEnter password: 输入临时生成的密码\n 登录成功.\n10）必须先修改 root 用户的密码,否则执行其他的操作会报错\nmysql&gt; set password = password(\"新密码\");\n11）修改 mysql 库下的 user 表中的 root 用户允许任意 ip 连接\nmysql&gt; update mysql.user set host='%' where user='root';\nmysql&gt; flush privileges;\n2.4 Hive 元数据配置到 MySQL拷贝驱动将 MySQL 的 JDBC 驱动拷贝到 Hive 的 lib 目录下\n[atguigu@hadoop102 software]$ cp /opt/software/mysql-connector-java-5.1.37.jar $HIVE_HOME/lib\n配置 Metastore 到 MySQL1）在HIVE_HOME/conf 目录下新建 hive-site.xml 文件\n[atguigu@hadoop102 software]$ vim $HIVE_HOME/conf/hive-site.xml\n添加如下内容\n&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n&lt;configuration&gt;\n &lt;!-- jdbc 连接的 URL --&gt;\n &lt;property&gt;\n &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?useSSL=false&lt;/value&gt;\n&lt;/property&gt;\n &lt;!-- jdbc 连接的 Driver--&gt;\n &lt;property&gt;\n &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;\n&lt;/property&gt;\n&lt;!-- jdbc 连接的 username--&gt;\n &lt;property&gt;\n &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n &lt;value&gt;root&lt;/value&gt;\n &lt;/property&gt;\n &lt;!-- jdbc 连接的 password --&gt;\n &lt;property&gt;\n &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n &lt;value&gt;000000&lt;/value&gt;\n&lt;/property&gt;\n &lt;!-- Hive 元数据存储版本的验证 --&gt;\n &lt;property&gt;\n &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;\n &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n &lt;!--元数据存储授权--&gt;\n &lt;property&gt;\n &lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;/name&gt;\n &lt;value&gt;false&lt;/value&gt;\n &lt;/property&gt;\n &lt;!-- Hive 默认在 HDFS 的工作目录 --&gt;\n &lt;property&gt;\n&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n &lt;value&gt;/user/hive/warehouse&lt;/value&gt;\n &lt;/property&gt;\n&lt;/configuration&gt; \n2）登陆 MySQL\n[atguigu@hadoop102 software]$ mysql -uroot -p000000\n3）新建 Hive 元数据库\nmysql&gt; create database metastore;\nmysql&gt; quit;\n4） 初始化 Hive 元数据库\n[atguigu@hadoop102 software]$ schematool -initSchema -dbType mysql -verbose\n再次启动 Hive1）启动 Hive\n[atguigu@hadoop102 hive]$ bin/hive\n2）使用 Hive\nhive&gt; show databases;\nhive&gt; show tables;\nhive&gt; create table test (id int);\nhive&gt; insert into test values(1);\nhive&gt; select * from test; 3）在 CRT 窗口中开启另一个窗口开启 Hive\nhive&gt; show databases;\nhive&gt; show tables;\nhive&gt; select * from aa;\n2.5 使用元数据服务的方式访问 Hive1）在 hive-site.xml 文件中添加如下配置信息\n&lt;!-- 指定存储元数据要连接的地址 --&gt;\n&lt;property&gt;\n&lt;name&gt;hive.metastore.uris&lt;/name&gt;\n&lt;value&gt;thrift://hadoop102:9083&lt;/value&gt;\n&lt;/property&gt;\n2）启动 metastore\n[atguigu@hadoop202 hive]$ hive --service metastore\n2020-04-24 16:58:08: Starting Hive Metastore Server\n注意: 启动后窗口不能再操作，需打开一个新的 shell 窗口做别的操作3）启动 hive\n[atguigu@hadoop202 hive]$ bin/hive\n2.6 使用 JDBC 方式访问 Hive1）在 hive-site.xml 文件中添加如下配置信息\n&lt;!-- 指定 hiveserver2 连接的 host --&gt;\n&lt;property&gt;\n    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;\n    &lt;value&gt;hadoop102&lt;/value&gt;\n&lt;/property&gt;\n&lt;!-- 指定 hiveserver2 连接的端口号 --&gt;\n&lt;property&gt;\n    &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;\n    &lt;value&gt;10000&lt;/value&gt;\n&lt;/property&gt;\n2）启动 hiveserver2\n[atguigu@hadoop102 hive]$ bin/hive --service hiveserver2\n3）启动 beeline 客户端（需要多等待一会）\n[atguigu@hadoop102 hive]$ bin/beeline -u jdbc:hive2://hadoop102:10000 -n atguigu\n4）看到如下界面\nConnecting to jdbc:hive2://hadoop102:10000\nConnected to: Apache Hive (version 3.1.2)\nDriver: Hive JDBC (version 3.1.2)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\nBeeline version 3.1.2 by Apache Hive\n0: jdbc:hive2://hadoop102:10000&gt;\n5）编写 hive 服务启动脚本（了解）\n\n前台启动的方式导致需要打开多个 shell 窗口，可以使用如下方式后台方式启动nohup: 放在命令开头，表示不挂起,也就是关闭终端进程也继续保持运行状态/dev/null：是 Linux 文件系统中的一个文件，被称为黑洞，所有写入改文件的内容都会被自动丢弃2&gt;&amp;1 : 表示将错误重定向到标准输出上&amp;: 放在命令结尾,表示后台运行一般会组合使用: nohup [xxx 命令操作]&gt; file 2&gt;&amp;1 &amp;，表示将 xxx 命令运行的结果输出到 file 中，并保持命令启动的进程在后台运行。如上命令不要求掌握。\n[atguigu@hadoop202 hive]$ nohup hive --service metastore 2&gt;&amp;1 &amp;\n[atguigu@hadoop202 hive]$ nohup hive --service hiveserver2 2&gt;&amp;1 &amp;\n\n为了方便使用，可以直接编写脚本来管理服务的启动和关闭\n[atguigu@hadoop102 hive]$ vim $HIVE_HOME/bin/hiveservices.sh\n内容如下：此脚本的编写不要求掌握。直接拿来使用即可。\n#!/bin/bash\nHIVE_LOG_DIR=$HIVE_HOME/logs\nif [ ! -d $HIVE_LOG_DIR ]\nthen\n\tmkdir -p $HIVE_LOG_DIR\nfi#检查进程是否运行正常，参数 1 为进程名，参数 2 为进程端口\nfunction check_process(){\n\tpid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i $1 | awk '{print $2}')\n\tppid=$(netstat -nltp 2&gt;/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)\n\techo $pid\n\t[[ \"$pid\" =~ \"$ppid\" ]] &amp;&amp; [ \"$ppid\" ] &amp;&amp; return 0 || return 1\n}\nfunction hive_start(){\n\tmetapid=$(check_process HiveMetastore 9083)\n\tcmd=\"nohup hive --service metastore &gt;$HIVE_LOG_DIR/metastore.log 2&gt;&amp;1 &amp;\"\n\t[ -z \"$metapid\" ] &amp;&amp; eval $cmd || echo \"Metastroe 服务已启动\"\n\tserver2pid=$(check_process HiveServer2 10000)\n\tcmd=\"nohup hiveserver2 &gt;$HIVE_LOG_DIR/hiveServer2.log 2&gt;&amp;1 &amp;\"\n\t[ -z \"$server2pid\" ] &amp;&amp; eval $cmd || echo \"HiveServer2 服务已启动\" \n}\n\nfunction hive_stop(){\n\tmetapid=$(check_process HiveMetastore 9083)\n\t[ \"$metapid\" ] &amp;&amp; kill $metapid || echo \"Metastore 服务未启动\"\n\tserver2pid=$(check_process HiveServer2 10000)\n\t[ \"$server2pid\" ] &amp;&amp; kill $server2pid || echo \"HiveServer2 服务未启动\" \n}\ncase $1 in\n\"start\")\n\thive_start\n\t;;\n\"stop\")\n\thive_stop\n\t;;\n\"restart\")\n\thive_stop\n\tsleep 2\n\thive_start\n\t;;\n\"status\")\n\tcheck_process HiveMetastore 9083 &gt;/dev/null &amp;&amp; echo \"Metastore 服务运行正常\" || echo \"Metastore 服务运行异常\"\n\tcheck_process HiveServer2 10000 &gt;/dev/null &amp;&amp; echo \"HiveServer2 服务运行正常\" || echo \"HiveServer2 服务运行异常\"\n\t;;\n*)\n\techo Invalid Args!\n\techo 'Usage: '$(basename $0)' start|stop|restart|status'\n\t;;\nesac\n3）添加执行权限\n[atguigu@hadoop102 hive]$ chmod +x $HIVE_HOME/bin/hiveservices.sh\n4）启动 Hive 后台服务\n[atguigu@hadoop102 hive]$ hiveservices.sh start\n\n\n2.7 Hive 常用交互命令[atguigu@hadoop102 hive]$ bin/hive -help\nusage: hive\n-d,--define &lt;key=value&gt; Variable subsitution to apply to hive\n commands. e.g. -d A=B or --define A=B\n --database &lt;databasename&gt; Specify the database to use\n-e &lt;quoted-query-string&gt; SQL from command line\n-f &lt;filename&gt; SQL from files\n-H,--help Print help information\n --hiveconf &lt;property=value&gt; Use value for given property\n --hivevar &lt;key=value&gt; Variable subsitution to apply to hive\n commands. e.g. --hivevar A=B\n-i &lt;filename&gt; Initialization SQL file\n-S,--silent Silent mode in interactive shell\n-v,--verbose Verbose mode (echo executed SQL to the \nconsole)\n\n“-e”不进入 hive 的交互窗口执行 sql 语句\n\n[atguigu@hadoop102 hive]$ bin/hive -e \"select id from student;\"\n\n“-f”执行脚本中 sql 语句（1）在/opt/module/hive/下创建 datas 目录并在 datas 目录下创建 hivef.sql 文件\n[atguigu@hadoop102 datas]$ touch hivef.sql\n（2）文件中写入正确的 sql 语句\nselect *from student;\n（3）执行文件中的 sql 语句\n[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql\n（4）执行文件中的 sql 语句并将结果写入文件中\n[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt\n\n\n2.8 Hive 其他命令操作1）退出 hive 窗口：\nhive(default)&gt;exit;\nhive(default)&gt;quit;\n2）在 hive cli 命令窗口中如何查看 hdfs 文件系统\nhive(default)&gt;dfs -ls /;\n3）查看在 hive 中输入的所有历史命令（1）进入到当前用户的根目录 /root 或/home/atguigu（2）查看. hivehistory 文件\n[atguig2u@hadoop102 ~]$ cat .hivehistory\n2.9 Hive 常见属性配置Hive 运行日志信息配置1）Hive 的 log 默认存放在/tmp/atguigu/hive.log 目录下（当前用户名下）2）修改 hive 的 log 存放日志到/opt/module/hive/logs（1）修改/opt/module/hive/conf/hive-log4j2.properties.template 文件名称为hive-log4j2.properties\n[atguigu@hadoop102 conf]$ pwd\n/opt/module/hive/conf\n[atguigu@hadoop102 conf]$ mv hive-log4j2.properties.template hive log4j2.properties\n（2）在 hive-log4j2.properties 文件中修改 log 存放位置\nhive.log.dir=/opt/module/hive/logs\n2.9.2 打印 当前库 和 表头在 hive-site.xml 中加入如下两个配置:\n&lt;property&gt;\n&lt;name&gt;hive.cli.print.header&lt;/name&gt;\n&lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;\n&lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n2.9.3 参数配置方式1）查看当前所有的配置信息hive&gt;set;2）参数的配置三种方式（1）配置文件方式默认配置文件：hive-default.xml用户自定义配置文件：hive-site.xml注意：用户自定义配置会覆盖默认配置。另外，Hive 也会读入 Hadoop 的配置，因为 Hive是作为 Hadoop 的客户端启动的，Hive 的配置会覆盖 Hadoop 的配置。配置文件的设定对本机启动的所有 Hive 进程都有效。（2）命令行参数方式启动 Hive 时，可以在命令行添加-hiveconf param=value 来设定参数。\n[atguigu@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;\n注意：仅对本次 hive 启动有效查看参数设置：\nhive (default)&gt; set mapred.reduce.tasks;\n（3）参数声明方式可以在 HQL 中使用 SET 关键字设定参数\nhive (default)&gt; set mapred.reduce.tasks=100;\n注意：仅对本次 hive 启动有效。查看参数设置\nhive (default)&gt; set mapred.reduce.tasks;\n​        上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如 log4j 相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。\n3. Hive 数据类型3.1 基本数据类型\n\n\n\nHive数据类型\nJava数据类型\n长度\n例子\n\n\n\n\nTINYINT\nbyte\n1byte有符号整数\n20\n\n\nSMALINT\nshort\n2byte有符号整数\n20\n\n\nINT\nint\n4byte有符号整数\n20\n\n\nBIG INT\nlong\n8byte有符号整数\n20\n\n\nBOOL EAN\nboolean\n布尔类型，true或者false\nTRUE FALSE\n\n\nFLOAT\nfloat\n单精度浮点数\n3.14159\n\n\nDOUBLE\ndouble\n双精度浮点数\n3.14159\n\n\nSTRING\nstring\n字符系列。可以指定字符集。可以使用单引号或者双引号。\n\n\n\nTIMESTAMP\n\n时间类型\n\n\n\nBINARY\n\n字节数组\n\n\n\n\n​        对于 Hive 的 String 类型相当于数据库的 varchar 类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储 2GB 的字符数。\n3.2 集合数据类型\n\n\n\n数据类型\n描述\n语法示例\n\n\n\n\nSTRUCT\n和 c 语言中的 struct 类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是 STRUCT{first STRING, last STRING},那么第 1 个元素可以通过字段.first 来引用。\nstruct\n\n\nMAP\nMAP 是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是 MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素\nmap\n\n\nARRAY\n数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第 2 个元素可以通过数组名[1]进行引用。\narray\n\n\n\n\n​        Hive 有三种复杂数据类型 ARRAY、MAP 和 STRUCT。ARRAY 和 MAP 与 Java 中的 Array和 Map 类似，而 STRUCT 与 C 语言中的 Struct 类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。\n案例实操\n\n假设某表有如下一行，我们用 JSON 格式来表示其数据结构。在 Hive 下访问的格式为\n{\n    \"name\": \"songsong\",\n    \"friends\": [\"bingbing\" , \"lili\"] , //列表 Array, \n    \"children\": { //键值 Map,\n        \"xiao song\": 18 ,\n        \"xiaoxiao song\": 19\n    }\n    \"address\": { //结构 Struct,\n        \"street\": \"hui long guan\",\n        \"city\": \"beijing\"\n    } \n}\n\n基于上述数据结构，我们在 Hive 里创建对应的表，并导入数据。创建本地测试文件 test.txt\nsongsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long \nguan_beijing\nyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing\n注意：MAP，STRUCT 和 ARRAY 里的元素间关系都可以用同一个字符表示，这里用“_”。 \n\nHive 上创建测试表 test\ncreate table test(\n    name string,\n    friends array&lt;string&gt;,\n    children map&lt;string, int&gt;,\n    address struct&lt;street:string, city:string&gt; \n)\nrow format delimited fields terminated by ',' #列分隔符\ncollection items terminated by '_' #MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)\nmap keys terminated by ':' #MAP 中的 key 与 value 的分隔符\nlines terminated by '\\n'; #行分隔符\n\n导入文本数据到测试表load data local inpath ‘/opt/module/hive/datas/test.txt’ into table test;\n\n访问三种集合列里的数据，以下分别是 ARRAY，MAP，STRUCT 的访问方式\nhive (default)&gt; select friends[1],children['xiao song'],address.city from \ntest\nwhere name=\"songsong\";\nOK\n_c0 _c1 city\nlili 18 beijing\nTime taken: 0.076 seconds, Fetched: 1 row(s)\n\n\n3.3 类型转化​        Hive 的原子数据类型是可以进行隐式转换的，类似于 Java 的类型转换，例如某表达式使用 INT 类型，TINYINT 会自动转换为 INT 类型，但是 Hive 不会进行反向转化，例如，某表达式使用 TINYINT 类型，INT 不会自动转换为 TINYINT 类型，它会返回错误，除非使用 CAST操作。隐式类型转换规则如下\n\n任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成INT，INT 可以转换成 BIGINT。\n所有整数类型、FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE。\nTINYINT、SMALLINT、INT 都可以转换为 FLOAT。\nBOOLEAN 类型不可以转换为任何其它的类型。\n\n可以使用 CAST 操作显示进行数据类型转换        例如 CAST(‘1’ AS INT)将把字符串’1’ 转换成整数 1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。\n0: jdbc:hive2://hadoop102:10000&gt; select '1'+2, cast('1'as int) + 2;\n+------+------+--+\n| _c0 | _c1 |\n+------+------+--+\n| 3.0 | 3 |\n+------+------+--+\n4. DDL 数据定义4.1 创建数据库CREATE DATABASE [IF NOT EXISTS] database_name\n[COMMENT database_comment]\n[LOCATION hdfs_path]\n[WITH DBPROPERTIES (property_name=property_value, ...)];\n1）创建一个数据库，数据库在 HDFS 上的默认存储路径是/user/hive/warehouse/*.db。\nhive (default)&gt; create database db_hive;\n2）避免要创建的数据库已经存在错误，增加 if not exists 判断。（标准写法）\nhive (default)&gt; create database db_hive;\nFAILED: Execution Error, return code 1 from \norg.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists\nhive (default)&gt; create database if not exists db_hive;\n3）创建一个数据库，指定数据库在 HDFS 上存放的位置\nhive (default)&gt; create database db_hive2 location '/db_hive2.db';\n4.2 查询数据库显示数据库1）显示数据库\nhive&gt; show databases;\n2）过滤显示查询的数据库\nhive&gt; show databases like 'db_hive*';\nOK\ndb_hive\ndb_hive_1\n 查看数据库详情1）显示数据库信息\nhive&gt; desc database db_hive;\nOK\ndb_hive hdfs://hadoop102:9820/user/hive/warehouse/db_hive.db\natguiguUSER\n2）显示数据库详细信息，extended\nhive&gt; desc database extended db_hive;\nOK\ndb_hive hdfs://hadoop102:9820/user/hive/warehouse/db_hive.db\natguiguUSER\n4.2.3 切换当前数据库\nhive (default)&gt; use db_hive;\n4.3 修改数据库​        用户可以使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。\nhive (default)&gt; alter database db_hive \nset dbproperties('createtime'='20170830');\n​        在 hive 中查看修改结果\nhive&gt; desc database extended db_hive;\ndb_name comment location owner_name owner_type parameters\ndb_hive hdfs://hadoop102:9820/user/hive/warehouse/db_hive.db \natguigu USER {createtime=20170830}\n4.4 删除数据库1）删除空数据库\nhive&gt;drop database db_hive2;\n2）如果删除的数据库不存在，最好采用 if exists 判断数据库是否存在\nhive&gt; drop database db_hive;\nFAILED: SemanticException [Error 10072]: Database does not exist: db_hive\nhive&gt; drop database if exists db_hive2;\n3）如果数据库不为空，可以采用 cascade 命令，强制删除\nhive&gt; drop database db_hive;\nFAILED: Execution Error, return code 1 from \norg.apache.hadoop.hive.ql.exec.DDLTask. \nInvalidOperationException(message:Database db_hive is not empty. One or \nmore tables exist.)\nhive&gt; drop database db_hive cascade;\n4.5 创建表1）建表语法\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name\n[(col_name data_type [COMMENT col_comment], ...)]\n[COMMENT table_comment]\n[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]\n[CLUSTERED BY (col_name, col_name, ...)\n[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]\n[ROW FORMAT row_format]\n[STORED AS file_format]\n[LOCATION hdfs_path]\n[TBLPROPERTIES (property_name=property_value, ...)]\n[AS select_statement]\n2）字段解释说明\n\nCREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。\n\nEXTERNAL 关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 \n\nCOMMENT：为表和列添加注释。\n\nPARTITIONED BY 创建分区表\n\nCLUSTERED BY 创建分桶表\n\nSORTED BY 不常用，对桶中的一个或多个列另外排序\n\nROW FORMAT \nDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]\n [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]\n | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, \nproperty_name=property_value, ...)]\n\n\n​        用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据。​        SerDe 是 Serialize/Deserilize 的简称， hive 使用 Serde 进行行对象的序列与反序列化。\n\nSTORED AS 指定存储文件类型常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）\n      如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED \nAS SEQUENCEFILE。\nLOCATION ：指定表在 HDFS 上的存储位置。\nAS：后跟查询语句，根据查询结果创建表。 \nLIKE 允许用户复制现有的表结构，但是不复制数据。\n\n管理表1）理论        默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive 会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir (如，/user/hive/warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive 也会删除这个表中数据。管理表不适合和其他工具共享数据。2）案例实操（0）原始数据\n1001 ss1\n1002 ss2\n1003 ss3\n1004 ss4\n1005 ss5\n1006 ss6\n1007 ss7\n1008 ss8\n1009 ss9\n1010 ss10\n1011 ss11\n1012 ss12\n1013 ss13\n1014 ss14\n1015 ss15\n1016 ss16\n（1）普通创建表\ncreate table if not exists student(\nid int, name string\n)\nrow format delimited fields terminated by '\\t'\nstored as textfile\nlocation '/user/hive/warehouse/student';\n（2）根据查询结果创建表（查询的结果会添加到新创建的表中）\ncreate table if not exists student2 as select id, name from student;\n（3）根据已经存在的表结构创建表\ncreate table if not exists student3 like student;\n（4）查询表的类型\nhive (default)&gt; desc formatted student2;\nTable Type: MANAGED_TABLE\n外部表1）理论        因为表是外部表，所以 Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。2）管理表和外部表的使用场景        每天将收集到的网站日志定期流入 HDFS 文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过 SELECT+INSERT 进入内部表。3）案例实操        分别创建部门和员工外部表，并向表中导入数据。（0）原始数据dept:\n10 ACCOUNTING 1700\n20 RESEARCH 1800\n30 SALES 1900\n40 OPERATIONS 1700\nemp：\n7369 SMITH CLERK 7902 1980-12-17 800.00 20\n7499 ALLEN SALESMAN 7698 1981-2-20 1600.00 300.00 30\n7521 WARD SALESMAN 7698 1981-2-22 1250.00 500.00 30\n7566 JONES MANAGER 7839 1981-4-2 2975.00 20\n7654 MARTIN SALESMAN 7698 1981-9-28 1250.00 1400.00 30\n7698 BLAKE MANAGER 7839 1981-5-1 2850.00 30\n7782 CLARK MANAGER 7839 1981-6-9 2450.00 10\n7788 SCOTT ANALYST 7566 1987-4-19 3000.00 20\n7839 KING PRESIDENT 1981-11-17 5000.00 10\n7844 TURNER SALESMAN 7698 1981-9-8 1500.00 0.00 30\n7876 ADAMS CLERK 7788 1987-5-23 1100.00 20\n7900 JAMES CLERK 7698 1981-12-3 950.00 30\n7902 FORD ANALYST 7566 1981-12-3 3000.00 20\n7934 MILLER CLERK 7782 1982-1-23 1300.00 10\n（1）上传数据到 HDFS\nhive (default)&gt; dfs -mkdir /student;\nhive (default)&gt; dfs -put /opt/module/datas/student.txt /student;\n（2）建表语句，创建外部表创建部门表\ncreate external table if not exists dept(\ndeptno int,\ndname string,\nloc int\n)\nrow format delimited fields terminated by '\\t';\n创建员工表\ncreate external table if not exists emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nrow format delimited fields terminated by '\\t';\n（3）查看创建的表\nhive (default)&gt;show tables;\n（4）查看表格式化数据\nhive (default)&gt; desc formatted dept;\nTable Type: EXTERNAL_TABLE\n（5）删除外部表\nhive (default)&gt; drop table dept;\n外部表删除后，hdfs 中的数据还在，但是 metadata 中 dept 的元数据已被删除\n管理表与外部表的互相转换（1）查询表的类型\nhive (default)&gt; desc formatted student2;\nTable Type: MANAGED_TABLE\n（2）修改内部表 student2 为外部表\nalter table student2 set tblproperties('EXTERNAL'='TRUE');\n（3）查询表的类型\nhive (default)&gt; desc formatted student2;\nTable Type: EXTERNAL_TABLE\n（4）修改外部表 student2 为内部表\nalter table student2 set tblproperties('EXTERNAL'='FALSE');\n（5）查询表的类型\nhive (default)&gt; desc formatted student2;\nTable Type: MANAGED_TABLE\n注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！ \n4.6 修改表重命名表1）语法\nALTER TABLE table_name RENAME TO new_table_name\n2）实操案例\nhive (default)&gt; alter table dept_partition2 rename to dept_partition3;\n增加、修改和删除表分区详见 7.1 章分区表基本操作。\n增加/修改/替换列信息1）语法（1）更新列\nALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name \ncolumn_type [COMMENT col_comment] [FIRST|AFTER column_name]\n（2）增加和替换列\nALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT \ncol_comment], ...) \n注：ADD 是代表新增一字段，字段位置在所有列后面(partition 列前)，REPLACE 则是表示替换表中所有字段。2）实操案例（1）查询表结构\nhive&gt; desc dept;\n（2）添加列\nhive (default)&gt; alter table dept add columns(deptdesc string);\n（3）查询表结构\nhive&gt; desc dept;\n（4）更新列\nhive (default)&gt; alter table dept change column deptdesc desc string; \n（5）查询表结构\nhive&gt; desc dept;\n（6）替换列\nhive (default)&gt; alter table dept replace columns(deptno string, dname\nstring, loc string);\n（7）查询表结构\nhive&gt; desc dept;\n4.7 删除表\nhive (default)&gt; drop table dept;\n5. DML 数据操作5.1 数据导入向表中装载数据（Load） 1）语法\nhive&gt; load data [local] inpath '数据的 path' [overwrite] into table \nstudent [partition (partcol1=val1,…)];\n（1）load data:表示加载数据（2）local:表示从本地加载数据到 hive 表；否则从 HDFS（3）inpath:表示加载数据的路径（4）overwrite:表示覆盖表中已有数据，否则表示追加（5）into table:表示加载到哪张表（6）student:表示具体的表（7）partition:表示上传到指定分区2）实操案例（0）创建一张表\nhive (default)&gt; create table student(id string, name string) row format \ndelimited fields terminated by '\\t';\n（1）加载本地文件到 hive\nhive (default)&gt; load data local inpath \n'/opt/module/hive/datas/student.txt' into table default.student;\n（2）加载 HDFS 文件到 hive 中上传文件到 HDFS\nhive (default)&gt; dfs -put /opt/module/hive/data/student.txt \n/user/atguigu/hive;\n加载 HDFS 上数据\nhive (default)&gt; load data inpath '/user/atguigu/hive/student.txt' into \ntable default.student;\n（3）加载数据覆盖表中已有的数据上传文件到 HDFS\nhive (default)&gt; dfs -put /opt/module/data/student.txt /user/atguigu/hive;\n加载数据覆盖表中已有的数据\nhive (default)&gt; load data inpath '/user/atguigu/hive/student.txt' \noverwrite into table default.student;\n通过查询语句向表中插入数据（Insert）1）创建一张表\nhive (default)&gt; create table student_par(id int, name string) row format \ndelimited fields terminated by '\\t';\n2）基本插入数据\nhive (default)&gt; insert into table student_par \nvalues(1,'wangwu'),(2,'zhaoliu'); 3）基本模式插入（根据单张表查询结果）\nhive (default)&gt; insert overwrite table student_par\n select id, name from student where month='201709';\ninsert into：以追加数据的方式插入到表或分区，原有数据不会删除\ninsert overwrite：会覆盖表中已存在的数据\n注意：insert 不支持插入部分字段4）多表（多分区）插入模式（根据多张表查询结果）\nhive (default)&gt; from student\ninsert overwrite table student partition(month='201707')\nselect id, name where month='201709'\ninsert overwrite table student partition(month='201706')\nselect id, name where month='201709';\n查询语句中创建表并加载数据（As Select）详见 4.5.1 章创建表。根据查询结果创建表（查询的结果会添加到新创建的表中）\ncreate table if not exists student3\nas select id, name from student;\n创建表时通过 Location 指定加载数据路径1）上传数据到 hdfs 上\nhive (default)&gt; dfs -mkdir /student;\nhive (default)&gt; dfs -put /opt/module/datas/student.txt /student; 2）创建表，并指定在 hdfs 上的位置\nhive (default)&gt; create external table if not exists student5(\n id int, name string\n )\n row format delimited fields terminated by '\\t'\n location '/student; 3）查询数据\nhive (default)&gt; select * from student5;\nImport 数据到指定 Hive 表中注意：先用 export 导出后，再将数据导入。\nhive (default)&gt; import table student2\nfrom '/user/hive/warehouse/export/student';\n5.2 数据导出Insert 导出1）将查询的结果导出到本地\nhive (default)&gt; insert overwrite local directory \n'/opt/module/hive/data/export/student'\nselect * from student;\n2）将查询的结果格式化导出到本地\nhive(default)&gt;insert overwrite local directory \n'/opt/module/hive/data/export/student1'\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nselect * from student;\n3）将查询的结果导出到 HDFS 上(没有 local)\nhive (default)&gt; insert overwrite directory '/user/atguigu/student2'\n ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' \n select * from student;\nHadoop 命令导出到本地hive (default)&gt; dfs -get /user/hive/warehouse/student/student.txt\n/opt/module/data/export/student3.txt;\nHive Shell 命令导出基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）\n[atguigu@hadoop102 hive]$ bin/hive -e 'select * from default.student;' &gt;\n/opt/module/hive/data/export/student4.txt;\nExport 导出到 HDFS 上(defahiveult)&gt; export table default.student \nto '/user/hive/warehouse/export/student';\nexport 和 import 主要用于两个 Hadoop 平台集群之间 Hive 表迁移。\n5.2.5 Sqoop 导出后续课程专门讲。\n5.2.6 清除表中数据（Truncate）注意：Truncate 只能删除管理表，不能删除外部表中数据hive (default)&gt; truncate table student;\n6.  查询https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select查询语句语法：\nSELECT [ALL | DISTINCT] select_expr, select_expr, ...\nFROM table_reference\n[WHERE where_condition]\n[GROUP BY col_list]\n[ORDER BY col_list]\n[CLUSTER BY col_list\n| [DISTRIBUTE BY col_list] [SORT BY col_list]\n]\n[LIMIT number]\n6.1 基本查询（Select…From）全表和特定列查询（0）原始数据\ndept:\n10 ACCOUNTING 1700\n20 RESEARCH 1800\n30 SALES 1900\n40 OPERATIONS 1700\nemp：\n7369 SMITH CLERK 7902 1980-12-17 800.00 20\n7499 ALLEN SALESMAN 7698 1981-2-20 1600.00 300.00 30\n7521 WARD SALESMAN 7698 1981-2-22 1250.00 500.00 30\n7566 JONES MANAGER 7839 1981-4-2 2975.00 20\n7654 MARTIN SALESMAN 7698 1981-9-28 1250.00 1400.00 30\n7698 BLAKE MANAGER 7839 1981-5-1 2850.00 30\n7782 CLARK MANAGER 7839 1981-6-9 2450.00 10\n7788 SCOTT ANALYST 7566 1987-4-19 3000.00 20\n7839 KING PRESIDENT 1981-11-17 5000.00 10\n7844 TURNER SALESMAN 7698 1981-9-8 1500.00 0.00 30\n7876 ADAMS CLERK 7788 1987-5-23 1100.00 20\n7900 JAMES CLERK 7698 1981-12-3 950.00 30\n7902 FORD ANALYST 7566 1981-12-3 3000.00 20\n7934 MILLER CLERK 7782 1982-1-23 1300.00 10\n（1）创建部门表\ncreate table if not exists dept(\ndeptno int,\ndname string,\nloc int\n)\nrow format delimited fields terminated by '\\t';\n（2）创建员工表\ncreate table if not exists emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string, \nsal double, \ncomm double,\ndeptno int)\nrow format delimited fields terminated by '\\t';\n（3）导入数据\nload data local inpath '/opt/module/datas/dept.txt' into table dept;\nload data local inpath '/opt/module/datas/emp.txt' into table emp;\n1）全表查询\nhive (default)&gt; select * from emp;\nhive (default)&gt; select empno,ename,job,mgr,hiredate,sal,comm,deptno from \nemp ;\n2）选择特定列查询\nhive (default)&gt; select empno, ename from emp;\n注意：\n\nSQL 语言大小写不敏感。\nSQL 可以写在一行或者多行\n关键字不能被缩写也不能分行\n各子句一般要分行写。\n使用缩进提高语句的可读性。\n\n列别名\n1）重命名一个列2）便于计算3）紧跟列名，也可以在列名和别名之间加入关键字‘AS’ \n4）案例实操\nhive (default)&gt; select ename AS name, deptno dn from emp;\n算术运算符：+、-、*、/、%、&amp;、|、^、~常用函数\n#求总行数（count）\nhive (default)&gt; select count(*) cnt from emp;\n#求工资的最大值（max）\nhive (default)&gt; select max(sal) max_sal from emp;\n#求工资的最小值（min）\nhive (default)&gt; select min(sal) min_sal from emp;\n#求工资的总和（sum）\nhive (default)&gt; select sum(sal) sum_sal from emp; \n#求工资的平均值（avg）\nhive (default)&gt; select avg(sal) avg_sal from emp;\nLimit 语句典型的查询会返回多行数据。LIMIT 子句用于限制返回的行数。\nhive (default)&gt; select * from emp limit 5;\nhive (default)&gt; select * from emp limit 2;\nWhere 语句1）使用 WHERE 子句，将不满足条件的行过滤掉2）WHERE 子句紧随 FROM 子句3）案例实操查询出薪水大于 1000 的所有员工\nhive (default)&gt; select * from emp where sal &gt;1000;\n注意：where 子句中不能使用字段别名。比较运算符（Between/In/ Is Null） \n1）下面表中描述了谓词操作符，这些操作符同样可以用于 JOIN…ON 和 HAVING 语句中。\n\n\n\n\n=、&lt;=&gt;、&lt;&gt;、!=、&lt;=、&gt;、&gt;=、between and 、is null 、is not null 、in 、not like 、rlike 、regexp\n\n\n\n\n\n\n\n\n\n2）案例实操\n#查询出薪水等于 5000 的所有员工\nhive (default)&gt; select * from emp where sal =5000;\n#查询工资在 500 到 1000 的员工信息\nhive (default)&gt; select * from emp where sal between 500 and 1000;\n#查询 comm 为空的所有员工信息\nhive (default)&gt; select * from emp where comm is null;\n#查询工资是 1500 或 5000 的员工信息\nhive (default)&gt; select * from emp where sal IN (1500, 5000);\nLike 和 RLike1）使用 LIKE 运算选择类似的值2）选择条件可以包含字符或数字:\n\n% 代表零个或多个字符(任意个字符)。\n_ 代表一个字符。\n\n3）RLIKE 子句RLIKE 子句是 Hive 中这个功能的一个扩展，其可以通过 Java 的正则表达式这个更强大的语言来指定匹配条件。4）案例实操\n#查找名字以 A 开头的员工信息\nhive (default)&gt; select * from emp where ename LIKE 'A%';\n#查找名字中第二个字母为 A 的员工信息\nhive (default)&gt; select * from emp where ename LIKE '_A%';\n#查找名字中带有 A 的员工信息\nhive (default)&gt; select * from emp where ename RLIKE '[A]';\n逻辑运算符（And/Or/Not）1）案例实操\n#查询薪水大于 1000，部门是 30\nhive (default)&gt; select * from emp where sal&gt;1000 and deptno=30;\n#查询薪水大于 1000，或者部门是 30\nhive (default)&gt; select * from emp where sal&gt;1000 or deptno=30;\n#查询除了 20 部门和 30 部门以外的员工信息\nhive (default)&gt; select * from emp where deptno not IN(30, 20);\n6.2 分组Group By 语句GROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。1）案例实操：\n#计算 emp 表每个部门的平均工资\nhive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;\n#计算 emp 每个部门中每个岗位的最高薪水\nhive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t \ngroup by t.deptno, t.job;\nHaving 语句1）having 与 where 不同点（1）where 后面不能写分组函数，而 having 后面可以使用分组函数。（2）having 只用于 group by 分组统计语句。2）案例实操\n#求每个部门的平均工资\nhive (default)&gt; select deptno, avg(sal) from emp group by deptno;\n#求每个部门的平均薪水大于 2000 的部门\nhive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno \nhaving avg_sal &gt; 2000;\n6.3 Join 语句Hive 支持通常的 SQL JOIN 语句。 \n等值 Join\n1）案例实操\n（1）根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称；\nhive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e\njoin dept d on e.deptno = d.deptno;\n表的别名1）好处（1）使用别名可以简化查询。（2）使用表名前缀可以提高执行效率。2）案例实操\n# 合并员工表和部门表\nhive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d \non e.deptno = d.deptno;\n内连接\n内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。\nhive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d \non e.deptno = d.deptno;\n左外连接\n# 左外连接：JOIN 操作符左边表中符合 WHERE 子句的所有记录将会被返回。\nhive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join \ndept d on e.deptno = d.deptno;\n右外连接\n#右外连接：JOIN 操作符右边表中符合 WHERE 子句的所有记录将会被返回。\nhive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join \ndept d on e.deptno = d.deptno;\n满外连接满外连接：将会返回所有表中符合 WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值替代。\nhive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join \ndept d on e.deptno = d.deptno;\n多表连接注意：连接 n 个表，至少需要 n-1 个连接条件。例如：连接三个表，至少需要两个连接条件。数据准备1700 Beijing1800 London1900 Tokyo1）创建位置表\ncreate table if not exists location(\nloc int,\nloc_name string\n)\nrow format delimited fields terminated by '\\t';\n2）导入数据\nhive (default)&gt; load data local inpath '/opt/module/datas/location.txt' \ninto table location;\n3）多表连接查询\nhive (default)&gt;SELECT e.ename, d.dname, l.loc_name\nFROM emp e \nJOIN dept d\nON d.deptno = e.deptno \nJOIN location l\nON d.loc = l.loc;\n​        大多数情况下，Hive 会对每对 JOIN 连接对象启动一个 MapReduce 任务。本例中会首先启动一个 MapReduce job 对表 e 和表 d 进行连接操作，然后会再启动一个 MapReduce job 将第一个 MapReduce job 的输出和表 l;进行连接操作。注意：为什么不是表 d 和表 l 先进行连接操作呢？这是因为 Hive 总是按照从左到右的顺序执行的。优化：当对 3 个或者更多表进行 join 连接时，如果每个 on 子句都使用相同的连接键的话，那么只会产生一个 MapReduce job。 \n笛卡尔积1）笛卡尔集会在下面条件下产生（1）省略连接条件（2）连接条件无效（3）所有表中的所有行互相连接2）案例实操\nhive (default)&gt; select empno, dname from emp, dept;\n6.4 排序全局排序（Order By）Order By：全局排序，只有一个 Reducer \n1）使用 ORDER BY 子句排序ASC（ascend）: 升序（默认）DESC（descend）: 降序\n2）ORDER BY 子句在 SELECT 语句的结尾3）案例实操\n#查询员工信息按工资升序排列\nhive (default)&gt; select * from emp order by sal;\n#查询员工信息按工资降序排列\nhive (default)&gt; select * from emp order by sal desc;\n按照别名排序\n#按照员工薪水的 2 倍排序\nhive (default)&gt; select ename, sal*2 twosal from emp order by twosal;\n多个列排序\n#按照部门和工资升序排序\nhive (default)&gt; select ename, deptno, sal from emp order by deptno, sal;\n每个 Reduce 内部排序（Sort By）Sort By：对于大规模的数据集 order by 的效率非常低。在很多情况下，并不需要全局排序，此时可以使用 sort by。Sort by 为每个 reducer 产生一个排序文件。每个 Reducer 内部进行排序，对全局结果集来说不是排序。\n#设置 reduce 个数\nhive (default)&gt; set mapreduce.job.reduces=3;\n#查看设置 reduce 个数\nhive (default)&gt; set mapreduce.job.reduces;\n#根据部门编号降序查看员工信息\nhive (default)&gt; select * from emp sort by deptno desc;\n#将查询结果导入到文件中（按照部门编号降序排序）\nhive (default)&gt; insert overwrite local directory \n'/opt/module/data/sortby-result'\nselect * from emp sort by deptno desc;\n分区（Distribute By）Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个 reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。distribute by 类似 MR 中 partition（自定义分区），进行分区，结合 sort by 使用。对于 distribute by 进行测试，一定要分配多 reduce 进行处理，否则无法看到 distribute by 的效果。\n1）案例实操：\n#先按照部门编号分区，再按照员工编号降序排序。\nhive (default)&gt; set mapreduce.job.reduces=3;\nhive (default)&gt; insert overwrite local directory \n'/opt/module/data/distribute-result' select * from emp distribute by \ndeptno sort by empno desc;\n注意：➢ distribute by 的分区规则是根据分区字段的 hash 码与 reduce 的个数进行模除后，余数相同的分到一个区。 \n➢ Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前。Cluster By        当 distribute by 和 sorts by 字段相同时，可以使用 cluster by 方式。cluster by 除了具有 distribute by 的功能外还兼具 sort by 的功能。但是排序只能是升序排序，不能指定排序规则为 ASC 或者 DESC。 \n（1）以下两种写法等价\nhive (default)&gt; select * from emp cluster by deptno;\nhive (default)&gt; select * from emp distribute by deptno sort by deptno;\n注意：按照部门编号分区，不一定就是固定死的数值，可以是 20 号和 30 号部门分到一个分区里面去。\n7. 分区表和分桶表7.1 分区表​        分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。分区表基本操作1）引入分区表（需要根据日期对日志进行管理, 通过部门信息模拟）\ndept_20200401.log\ndept_20200402.log\ndept_20200403.log\n2）创建分区表语法\nhive (default)&gt; create table dept_partition(\ndeptno int, dname string, loc string\n)\npartitioned by (day string)\nrow format delimited fields terminated by '\\t';\n注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。3）加载数据到分区表中（1） 数据准备\ndept_20200401.log\n10 ACCOUNTING 1700\n20 RESEARCH 1800\n\ndept_20200402.log\n30 SALES 1900\n40 OPERATIONS 1700\n\ndept_20200403.log\n50 TEST 2000\n60 DEV 1900\n（2） 加载数据\nhive (default)&gt; load data local inpath \n'/opt/module/hive/datas/dept_20200401.log' into table dept_partition\npartition(day='20200401');\n\nhive (default)&gt; load data local inpath \n'/opt/module/hive/datas/dept_20200402.log' into table dept_partition \npartition(day='20200402');\n\nhive (default)&gt; load data local inpath \n'/opt/module/hive/datas/dept_20200403.log' into table dept_partition \npartition(day='20200403');\n注意：分区表加载数据时，必须指定分区4）查询分区表中数据\n#单分区查询\nhive (default)&gt; select * from dept_partition where day='20200401';\n#多分区联合查询\nhive (default)&gt; select * from dept_partition where day='20200401'\n union\n select * from dept_partition where day='20200402'\n union\n select * from dept_partition where day='20200403';\nhive (default)&gt; select * from dept_partition where day='20200401' or\nday='20200402' or day='20200403';\n 5）增加分区\n#创建单个分区\nhive (default)&gt; alter table dept_partition add partition(day='20200404');\n#同时创建多个分区\nhive (default)&gt; alter table dept_partition add partition(day='20200405') \npartition(day='20200406');\n6）删除分区\n#删除单个分区\nhive (default)&gt; alter table dept_partition drop partition (day='20200406');\n#同时删除多个分区\nhive (default)&gt; alter table dept_partition drop partition (day='20200404'), partition(day='20200405');\n7）查看分区表有多少分区\nhive&gt; show partitions dept_partition;\n8）查看分区表结构\nhive&gt; desc formatted dept_partition;\n# Partition Information \n# col_name data_type comment \nmonth string\n二级分区思考: 如何一天的日志数据量也很大，如何再将数据拆分? \n1）创建二级分区表\nhive (default)&gt; create table dept_partition2(\n deptno int, dname string, loc string\n )\n partitioned by (day string, hour string)\n row format delimited fields terminated by '\\t';\n2）正常的加载数据\n# 加载数据到二级分区表中\nhive (default)&gt; load data local inpath \n'/opt/module/hive/datas/dept_20200401.log' into table\ndept_partition2 partition(day='20200401', hour='12');\n# 查询分区数据\nhive (default)&gt; select * from dept_partition2 where day='20200401' and hour='12';\n3）把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式（1）方式一：上传数据后修复\n#上传数据\nhive (default)&gt; dfs -mkdir -p\n/user/hive/warehouse/mydb.db/dept_partition2/day=20200401/hour=13;\nhive (default)&gt; dfs -put /opt/module/datas/dept_20200401.log \n/user/hive/warehouse/mydb.db/dept_partition2/day=20200401/hour=13;\n#查询数据（查询不到刚上传的数据）\nhive (default)&gt; select * from dept_partition2 where day='20200401' and hour='13';\n#执行修复命令\nhive&gt; msck repair table dept_partition2;\n#再次查询数据\nhive (default)&gt; select * from dept_partition2 where day='20200401' and hour='13';\n（2）方式二：上传数据后添加分区\n#上传数据\nhive (default)&gt; dfs -mkdir -p\n/user/hive/warehouse/mydb.db/dept_partition2/day=20200401/hour=14;\nhive (default)&gt; dfs -put /opt/module/hive/datas/dept_20200401.log \n/user/hive/warehouse/mydb.db/dept_partition2/day=20200401/hour=14;\n#执行添加分区\nhive (default)&gt; alter table dept_partition2 add partition(day='201709',hour='14');\n#查询数据\nhive (default)&gt; select * from dept_partition2 where day='20200401' and hour='14';\n（3）方式三：创建文件夹后 load 数据到分区\n创建目录\nhive (default)&gt; dfs -mkdir -p\n/user/hive/warehouse/mydb.db/dept_partition2/day=20200401/hour=15;\n上传数据\nhive (default)&gt; load data local inpath \n'/opt/module/hive/datas/dept_20200401.log' into table\ndept_partition2 partition(day='20200401',hour='15');\n查询数据\nhive (default)&gt; select * from dept_partition2 where day='20200401' and \nhour='15';\n动态分区调整        关系型数据库中，对分区表 Insert 数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用 Hive 的动态分区，需要进行相应的配置。1）开启动态分区参数设置（1）开启动态分区功能（默认 true，开启）\nhive.exec.dynamic.partition=true\n（2）设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。）\nhive.exec.dynamic.partition.mode=nonstrict\n（3）在所有执行 MR 的节点上，最大一共可以创建多少个动态分区。默认 1000\nhive.exec.max.dynamic.partitions=1000\n（4）在每个执行 MR 的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即 day 字段有 365 个值，那么该参数就需要设置成大于 365，如果使用默认值 100，则会报错。\nhive.exec.max.dynamic.partitions.pernode=100\n（5）整个 MR Job 中，最大可以创建多少个 HDFS 文件。默认 100000\nhive.exec.max.created.files=100000\n（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认 false\nhive.error.on.empty.partition=false\n2）案例实操需求：将 dept 表中的数据按照地区（loc 字段），插入到目标表 dept_partition 的相应分区中。\n#创建目标分区表\nhive (default)&gt; create table dept_partition_dy(id int, name string) \npartitioned by (loc int) row format delimited fields terminated by '\\t';\n#设置动态分区\nset hive.exec.dynamic.partition.mode = nonstrict;\nhive (default)&gt; insert into table dept_partition_dy partition(loc) select \ndeptno, dname, loc from dept;\n#查看目标分区表的分区情况\nhive (default)&gt; show partitions dept_partition;\n思考：目标分区表是如何匹配到分区字段的？\n7.2 分桶表​        分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。​        分桶是将数据集分解成更容易管理的若干部分的另一个技术。​        分区针对的是数据的存储路径；分桶针对的是数据文件。1）先创建分桶表（1）数据准备\n1001 ss1\n1002 ss2\n1003 ss3\n1004 ss4\n1005 ss5\n1006 ss6\n1007 ss7\n1008 ss8\n1009 ss9\n1010 ss10\n1011 ss11\n1012 ss12\n1013 ss13\n1014 ss14\n1015 ss15\n1016 ss16\n（2）创建分桶表\ncreate table stu_buck(id int, name string)\nclustered by(id) \ninto 4 buckets\nrow format delimited fields terminated by '\\t';\n（3）查看表结构\nhive (default)&gt; desc formatted stu_buck;\nNum Buckets: 4 \n（4）导入数据到分桶表中，load 的方式\nhive (default)&gt; load data inpath '/student.txt' into table stu_buck;\n（5）查看创建的分桶表中是否分成 4 个桶（6）查询分桶的数据\nhive(default)&gt; select * from stu_buck;\n（7）分桶规则：        根据结果可知：Hive 的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中2）分桶表操作需要注意的事项: \n（1）reduce 的个数设置为-1,让 Job 自行决定需要用多少个 reduce 或者将 reduce 的个数设置为大于等于分桶表的桶数（2）从 hdfs 中 load 数据到分桶表中，避免本地文件找不到问题\n（3）不要使用本地模式3）insert 方式将数据导入分桶表\nhive(default)&gt;insert into table stu_buck select * from student_insert;\n7.3 抽样查询​        对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive 可以通过对表进行抽样来满足这个需求。语法: TABLESAMPLE(BUCKET x OUT OF y)查询表 stu_buck 中的数据。\nhive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 on id);\n# 注意：x 的值必须小于等于 y 的值，否则\nFAILED: SemanticException [Error 10061]: Numerator should not be bigger \nthan denominator in sample clause for table stu_buck\n第 8 章 函数8.1 系统内置函数# 查看系统自带的函数\nhive&gt; show functions;\n# 显示自带的函数的用法\nhive&gt; desc function upper;\n# 详细显示自带的函数的用法\nhive&gt; desc function extended upper;\n8.2 常用内置函数空字段赋值1）函数说明        NVL：给值为 NULL 的数据赋值，它的格式是 NVL( value，default_value)。它的功能是如果 value 为 NULL，则 NVL 函数返回 default_value 的值，否则返回 value 的值，如果两个参数都为 NULL ，则返回 NULL。 2）数据准备：采用员工表3）查询：如果员工的 comm 为 NULL，则用-1 代替\nhive (default)&gt; select comm,nvl(comm, -1) from emp;\nOK\ncomm _c1\n\nNULL -1.0\n300.0 300.0\n500.0 500.0\nNULL -1.0\n1400.0 1400.0\nNULL -1.0\nNULL -1.0\nNULL -1.0\nNULL -1.0\n0.0 0.0\nNULL -1.0\nNULL -1.0\nNULL -1.0\nNULL -1.0\n4）查询：如果员工的 comm 为 NULL，则用领导 id 代替\nhive (default)&gt; select comm, nvl(comm,mgr) from emp;\nOK\ncomm _c1\nNULL 7902.0\n300.0 300.0\n500.0 500.0\nNULL 7839.0\n1400.0 1400.0\nNULL 7839.0\nNULL 7839.0\nNULL 7566.0\nNULL NULL\n0.0 0.0\nNULL 7788.0\nNULL 7698.0\nNULL 7566.0\nNULL 7782.0\nCASE WHEN THEN ELSE END1）数据准备\n2）需求求出不同部门男女各多少人。结果如下：\ndept_Id 男 女\nA 2 1\nB 1 2\n3）创建本地 emp_sex.txt，导入数据\n[atguigu@hadoop102 datas]$ vi emp_sex.txt\n悟空 A 男\n大海 A 男\n宋宋 B 男\n凤姐 A 女\n婷姐 B 女\n婷婷 B 女 \n4）创建 hive 表并导入数据\ncreate table emp_sex(\nname string, \ndept_id string, \nsex string) \nrow format delimited fields terminated by \"\\t\";\nload data local inpath '/opt/module/hive/data/emp_sex.txt' into table \nemp_sex;\n5）按需求查询数据\nselect\n dept_id,\n sum(case sex when '男' then 1 else 0 end) male_count,\n sum(case sex when '女' then 1 else 0 end) female_count\nfrom emp_sex\ngroup by dept_id;\n行转列1）相关函数说明        CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;        CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;        注意: CONCAT_WS must be “string or array        COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生 Array 类型字段。2）数据准备\n3）需求\n# 把星座和血型一样的人归类到一起。结果如下：\n射手座,A 大海|凤姐\n白羊座,A 孙悟空|猪八戒\n白羊座,B 宋宋|苍老师\n4）创建本地 constellation.txt，导入数据\n[atguigu@hadoop102 datas]$ vim person_info.txt\n孙悟空 白羊座 A\n大海 射手座 A\n宋宋 白羊座 B\n猪八戒 白羊座 A\n凤姐 射手座 A\n苍老师 白羊座 B\n 5）创建 hive 表并导入数据\ncreate table person_info(\nname string, \nconstellation string, \nblood_type string) \nrow format delimited fields terminated by \"\\t\";\nload data local inpath \"/opt/module/hive/data/person_info.txt\" into table \nperson_info;\n6）按需求查询数据\nSELECT\nt1.c_b,\nCONCAT_WS(\"|\",collect_set(t1.name))\nFROM (\nSELECT\nNAME,\nCONCAT_WS(',',constellation,blood_type) c_b\nFROM person_info\n)t1\nGROUP BY t1.c_b\n列转行1）函数说明        EXPLODE(col)：将 hive 一列中复杂的 Array 或者 Map 结构拆分成多行。        LATERAL VIEW用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias解释：用于和 split, explode 等 UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。\n2）数据准备\nmovie category\n《疑犯追踪》 悬疑,动作,科幻,剧情\n《Lie to me》 悬疑,警匪,动作,心理,剧情\n《战狼 2》 战争,动作,灾难\n3）需求\n将电影分类中的数组数据展开。结果如下：\n《疑犯追踪》 悬疑\n《疑犯追踪》 动作\n《疑犯追踪》 科幻\n《疑犯追踪》 剧情\n《Lie to me》 悬疑\n《Lie to me》 警匪\n《Lie to me》 动作\n《Lie to me》 心理\n《Lie to me》 剧情\n《战狼 2》 战争\n《战狼 2》 动作\n《战狼 2》 灾难\n4）创建本地 movie.txt，导入数据\n[atguigu@hadoop102 datas]$ vi movie_info.txt\n《疑犯追踪》 悬疑,动作,科幻,剧情\n《Lie to me》悬疑,警匪,动作,心理,剧情\n《战狼 2》 战争,动作,灾难\n5）创建 hive 表并导入数据\ncreate table movie_info(\n movie string,\n category string)\nrow format delimited fields terminated by \"\\t\";\nload data local inpath \"/opt/module/data/movie.txt\" into table \nmovie_info;\n6）按需求查询数据\nSELECT\nmovie,\ncategory_name\nFROM\nmovie_info\nlateral VIEW\nexplode(split(category,\",\")) movie_info_tmp AS category_name;\n窗口函数（开窗函数）1）相关函数说明OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。CURRENT ROW：当前行n PRECEDING：往前 n 行数据n FOLLOWING：往后 n 行数据UNBOUNDED：起点，        UNBOUNDED PRECEDING 表示从前面的起点，        UNBOUNDED FOLLOWING 表示到后面的终点LAG(col,n,default_val)：往前第 n 行数据LEAD(col,n, default_val)：往后第 n 行数据NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从 1 开始，对于每一行，NTILE 返回此行所属的组的编号。注意：n 必须为 int 类型。2）数据准备：name，orderdate，cost\njack,2017-01-01,10\ntony,2017-01-02,15\njack,2017-02-03,23\ntony,2017-01-04,29\njack,2017-01-05,46\njack,2017-04-06,42\ntony,2017-01-07,50\njack,2017-01-08,55\nmart,2017-04-08,62\nmart,2017-04-09,68\nneil,2017-05-10,12\nmart,2017-04-11,75\nneil,2017-06-12,80\nmart,2017-04-13,94\n3）需求（1）查询在 2017 年 4 月份购买过的顾客及总人数（2）查询顾客的购买明细及月购买总额（3）上述的场景, 将每个顾客的 cost 按照日期进行累加（4）查询每个顾客上次的购买时间（5）查询前 20%时间的订单信息4）创建本地 business.txt，导入数据\n[atguigu@hadoop102 datas]$ vi business.txt\n5）创建 hive 表并导入数据\ncreate table business(\nname string,\norderdate string,\ncost int\n) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\nload data local inpath \"/opt/module/data/business.txt\" into table \nbusiness;\n6）按需求查询数据（1） 查询在 2017 年 4 月份购买过的顾客及总人数\nselect name,count(*) over () \nfrom business\nwhere substring(orderdate,1,7) = '2017-04'\ngroup by name; \n（2） 查询顾客的购买明细及月购买总额\nselect name,orderdate,cost,sum(cost) over(partition by month(orderdate)) \nfrom business;\n（3） 将每个顾客的 cost 按照日期进行累加\nselect name,orderdate,cost,\nsum(cost) over() as sample1,#所有行相加\nsum(cost) over(partition by name) as sample2,#按 name 分组，组内数据相加\nsum(cost) over(partition by name order by orderdate) as sample3,#按 name分组，组内数据累加\nsum(cost) over(partition by name order by orderdate rows between \nUNBOUNDED PRECEDING and current row ) as sample4 ,#和 sample3 一样,由起点到当前行的聚合\nsum(cost) over(partition by name order by orderdate rows between 1 \nPRECEDING and current row) as sample5, #当前行和前面一行做聚合\nsum(cost) over(partition by name order by orderdate rows between 1 \nPRECEDING AND 1 FOLLOWING ) as sample6,#当前行和前边一行及后面一行\nsum(cost) over(partition by name order by orderdate rows between current \nrow and UNBOUNDED FOLLOWING ) as sample7 #当前行及后面所有行\nfrom business; \n#rows 必须跟在 order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量\n（4） 查看顾客上次的购买时间\nselect name,orderdate,cost,\nlag(orderdate,1,'1900-01-01') over(partition by name order by orderdate ) \nas time1, lag(orderdate,2) over (partition by name order by orderdate) as \ntime2 \nfrom business; \n（5） 查询前 20%时间的订单信息\nselect * from (\n select name,orderdate,cost, ntile(5) over(order by orderdate) sorted\n from business\n) t\nwhere sorted = 1;\nRank1）函数说明\nRANK() 排序相同时会重复，总数不会变DENSE_RANK() 排序相同时会重复，总数会减少ROW_NUMBER() 会根据顺序计算\n2）数据准备\n3）需求计算每门学科成绩排名。4）创建本地 score.txt，导入数据\n[atguigu@hadoop102 datas]$ vi score.txt\n5）创建 hive 表并导入数据\ncreate table score(\nname string,\nsubject string, \nscore int) \nrow format delimited fields terminated by \"\\t\";\nload data local inpath '/opt/module/data/score.txt' into table score;\n6）按需求查询数据\nselect name,\nsubject,\nscore,\nrank() over(partition by subject order by score desc) rp,\ndense_rank() over(partition by subject order by score desc) drp,\nrow_number() over(partition by subject order by score desc) rmp\nfrom score;\n\nname subject score rp drp rmp\n孙悟空 数学 95 1 1 1\n宋宋 数学 86 2 2 2\n婷婷 数学 85 3 3 3\n大海 数学 56 4 4 4\n宋宋 英语 84 1 1 1\n大海 英语 84 1 1 2\n婷婷 英语 78 3 2 3\n孙悟空 英语 68 4 3 4\n大海 语文 94 1 1 1\n孙悟空 语文 87 2 2 2\n婷婷 语文 65 3 3 3\n宋宋 语文 64 4 4 4\n​        扩展：求出每门学科前三名的学生？\n8.3 自定义函数1）Hive 自带了一些函数，比如：max/min 等，但是数量有限，自己可以通过自定义 UDF 来方便的扩展。2）当 Hive 提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。 \n3）根据用户自定义函数类别分为以下三种：（1）UDF（User-Defined-Function）            一进一出（2）UDAF（User-Defined Aggregation Function）            聚集函数，多进一出            类似于：count/max/min（3）UDTF（User-Defined Table-Generating Functions）            一进多出            如 lateral view explode()4）官方文档地址https://cwiki.apache.org/confluence/display/Hive/HivePlugins5）编程步骤： \n（1）继承 Hive 提供的类\norg.apache.hadoop.hive.ql.udf.generic.GenericUDF\norg.apache.hadoop.hive.ql.udf.generic.GenericUDTF;\n（2）实现类中的抽象方法\n（3）在 hive 的命令行窗口创建函数\n#添加 jar\nadd jar linux_jar_path\n#创建 function\ncreate [temporary] function [dbname.]function_name AS class_name;\n（4）在 hive 的命令行窗口删除函数\ndrop [temporary] function [if exists] [dbname.]function_name;\n8.4 自定义 UDF 函数0）需求:自定义一个 UDF 实现计算给定字符串的长度，例如：\nhive(default)&gt; select my_len(\"abcd\"); \n4 \n1）创建一个 Maven 工程 Hive\n2）导入依赖\n&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;\n        &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;\n        &lt;version&gt;3.1.2&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n3）创建一个类\npackage com.atguigu.hive;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDF;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport \n    org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectIn\n    spectorFactory;\n/**\n\n* 自定义 UDF 函数，需要继承 GenericUDF 类\n* 需求: 计算指定字符串的长度\n  */\npublic class MyStringLength extends GenericUDF {\n    /**\n   *\n * @param arguments 输入参数类型的鉴别器对象\n * @return 返回值类型的鉴别器对象\n * @throws UDFArgumentException\n   */\n    @Override\n    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n        // 判断输入参数的个数\n        if(arguments.length !=1){\n            throw new UDFArgumentLengthException(\"Input Args Length Error!!!\");\n        }\n        // 判断输入参数的类型\n        if(!arguments[0].getCategory().equals(ObjectInspector.Category.PRIMITIVE)){\n            throw new UDFArgumentTypeException(0,\"Input Args Type Error!!!\");\n        }\n        //函数本身返回值为 int，需要返回 int 类型的鉴别器对象\n        return PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n    }\n    /**\n\n * 函数的逻辑处理\n * @param arguments 输入的参数\n * @return 返回值\n * @throws HiveException\n   */\n    @Override\n    public Object evaluate(DeferredObject[] arguments) throws HiveException {\n        if(arguments[0].get() == null){\n            return 0;\n        }\n        return arguments[0].get().toString().length();\n    }\n    @Override\n    public String getDisplayString(String[] children) {\n        return \"\";\n    } \n} \n4）打成 jar 包上传到服务器/opt/module/data/myudf.jar5）将 jar 包添加到 hive 的 classpath\nhive (default)&gt; add jar /opt/module/data/myudf.jar;\n6）创建临时函数与开发好的 java class 关联\nhive (default)&gt; create temporary function my_len as \"com.atguigu.hive.\nMyStringLength\";\n7）即可在 hql 中使用自定义的函数\nhive (default)&gt; select ename,my_len(ename) ename_len from emp;\n8.5 自定义 UDTF 函数0）需求        自定义一个 UDTF 实现将一个任意分割符的字符串切割成独立的单词，例如：\nhive(default)&gt; select myudtf(\"hello,world,hadoop,hive\", \",\");\nhello\nworld\nhadoop\nhive\n1）代码实现\n* package com.atguigu.udtf;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport \n    org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\nimport \n    org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\nimport \n    org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectIn\n    spectorFactory;\nimport java.util.ArrayList;\nimport java.util.List;\npublic class MyUDTF extends GenericUDTF {\n    private ArrayList&lt;String&gt; outList = new ArrayList&lt;&gt;();\n    @Override\n    public StructObjectInspector initialize(StructObjectInspector argOIs) \n        throws UDFArgumentException {\n        //1.定义输出数据的列名和类型\n        List&lt;String&gt; fieldNames = new ArrayList&lt;&gt;();\n        List&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;&gt;();\n        //2.添加输出数据的列名和类型\n        fieldNames.add(\"lineToWord\");\n        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n        return \n            ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);\n    }\n    @Override\n    public void process(Object[] args) throws HiveException {\n        //1.获取原始数据\n        String arg = args[0].toString();\n        //2.获取数据传入的第二个参数，此处为分隔符\n        String splitKey = args[1].toString();\n        //3.将原始数据按照传入的分隔符进行切分\n        String[] fields = arg.split(splitKey);\n        //4.遍历切分后的结果，并写出\n        for (String field : fields) {\n            //集合为复用的，首先清空集合\n            outList.clear();\n            //将每一个单词添加至集合\n            outList.add(field);\n            //将集合内容写出\n            forward(outList);\n        }\n    }\n    @Override\n    public void close() throws HiveException {\n    } \n} \n2）打成 jar 包上传到服务器/opt/module/hive/data/myudtf.jar3）将 jar 包添加到 hive 的 classpath 下\nhive (default)&gt; add jar /opt/module/hive/data/myudtf.jar;\n4）创建临时函数与开发好的 java class 关联\nhive (default)&gt; create temporary function myudtf as \n\"com.atguigu.hive.MyUDTF\";\n5）使用自定义的函数\nhive (default)&gt; select myudtf(\"hello,world,hadoop,hive\",\",\"); \n第 9 章 压缩和存储9.1 Hadoop 压缩配置MR 支持的压缩编码\n\n\n​        http://google.github.io/snappy/​        On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250MB/sec or more and decompresses at about 500 MB/sec or more.压缩参数配置​        要在 Hadoop 中启用压缩，可以配置如下参数（mapred-site.xml 文件中）：\n压缩参数配置        要在 Hadoop 中启用压缩，可以配置如下参数（mapred-site.xml 文件中）：\n\n9.2 开启 Map 输出阶段压缩（MR 引擎）开启 map 输出阶段压缩可以减少 job 中 map 和 Reduce task 间数据传输量。具体配置如下：1）案例实操：\n# 开启 hive 中间传输数据压缩功能\nhive (default)&gt;set hive.exec.compress.intermediate=true;\n# 开启 mapreduce 中 map 输出压缩功能\nhive (default)&gt;set mapreduce.map.output.compress=true;\n# 设置 mapreduce 中 map 输出数据的压缩方式\nhive (default)&gt;set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;\n# 执行查询语句\nhive (default)&gt; select count(ename) name from emp;\n9.3 开启 Reduce 输出阶段压缩​        当 Hive 将 输 出 写 入 到 表 中 时 ， 输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为 true，来开启输出结果压缩功能。1）案例实操：\n# 开启 hive 最终输出数据压缩功能\nhive (default)&gt;set hive.exec.compress.output=true;\n# 开启 mapreduce 最终输出数据压缩\nhive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;\n# 设置 mapreduce 最终数据输出压缩方式\nhive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec =\norg.apache.hadoop.io.compress.SnappyCodec;\n# 设置 mapreduce 最终数据输出压缩为块压缩\nhive (default)&gt; set \nmapreduce.output.fileoutputformat.compress.type=BLOCK;\n# 测试一下输出结果是否是压缩文件\nhive (default)&gt; insert overwrite local directory\n'/opt/module/data/distribute-result' select * from emp distribute by \ndeptno sort by empno desc;\n9.4 文件存储格式​        Hive 支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。\n\n列式存储和行式存储如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。1）行存储的特点        查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。2）列存储的特点        因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。        TEXTFILE 和 SEQUENCEFILE 的存储格式都是基于行存储的；        ORC 和 PARQUET 是基于列式存储的。TextFile 格式        默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合 Gzip、Bzip2 使用，但使用 Gzip 这种方式，hive 不会对数据进行切分，从而无法对数据进行并行操作。Orc 格式        Orc (Optimized Row Columnar)是 Hive 0.11 版里引入的新的存储格式。        如下图所示可以看到每个 Orc 文件由 1 个或多个 stripe 组成，每个 stripe 一般为 HDFS的块大小，每一个 stripe 包含多条记录，这些记录按照列进行独立存储，对应到 Parquet中的 row group 的概念。每个 Stripe 里有三部分组成，分别是 Index Data，Row Data，Stripe Footer：\n\n1）Index Data：一个轻量级的 index，默认是每隔 1W 行做一个索引。这里做的索引应该只是记录某行的各字段在 Row Data 中的 offset。 \n2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个 Stream 来存储。 \n3）Stripe Footer：存的是各个 Stream 的类型，长度等信息。\n​        每个文件有一个 File Footer，这里面存的是每个 Stripe 的行数，每个 Column 的数据类型信息等；每个文件的尾部是一个 PostScript，这里面记录了整个文件的压缩类型以及FileFooter 的长度信息等。在读取文件时，会 seek 到文件尾部读 PostScript，从里面解析到File Footer 长度，再读 FileFooter，从里面解析到各个 Stripe 信息，再读各个 Stripe，即从后往前读。\nParquet 格式        Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此 Parquet 格式文件是自解析的。（1）行组(Row Group)：每一个行组包含一定的行数，在一个 HDFS 文件中至少存储一个行组，类似于 orc 的 stripe 的概念。（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。        通常情况下，在存储 Parquet 数据的时候会按照 Block 大小设置行组的大小，由于一般情况下每一个 Mapper 任务处理数据的最小单位是一个 Block，这样可以把每一个行组由一个 Mapper 任务处理，增大任务执行并行度。Parquet 文件的格式。        上图展示了一个 Parquet 文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的 Magic Code，用于校验它是否是一个 Parquet 文件，Footer length 记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的 Schema 信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在 Parquet 中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前 Parquet 中还不支持索引页。主流文件存储格式对比实验        从存储文件的压缩比和查询速度两个角度对比。        存储文件的压缩比测试：1）测试数据2）TextFile（1)创建表，存储数据格式为 TEXTFILE\ncreate table log_text (\n    track_time string,\n    url string,\n    session_id string,\n    referer string,\n    ip string,\n    end_user_id string,\n    city_id string\n)\nrow format delimited fields terminated by '\\t'\nstored as textfile;\n（2）向表中加载数据\nhive (default)&gt; load data local inpath '/opt/module/hive/datas/log.data' \ninto table log_text ;\n（3）查看表中数据大小\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_text;\n18.13 M /user/hive/warehouse/log_text/log.data\n3）ORC（1）创建表，存储数据格式为 ORC\ncreate table log_orc(\n    track_time string,\n    url string,\n    session_id string,\n    referer string,\n    ip string,\n    end_user_id string,\n    city_id string\n)\nrow format delimited fields terminated by '\\t'\nstored as orc\ntblproperties(\"orc.compress\"=\"NONE\"); -- 设置 orc 存储不使用压缩\n（2）向表中加载数据\nhive (default)&gt; insert into table log_orc select * from log_text;\n（3）查看表中数据大小\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/ ;\n7.7 M /user/hive/warehouse/log_orc/000000_0\n4）Parquet（1）创建表，存储数据格式为 parquet\ncreate table log_parquet(\n    track_time string,\n    url string,\n    session_id string,\n    referer string,\n    ip string,\n    end_user_id string,\n    city_id string\n)\nrow format delimited fields terminated by '\\t'\nstored as parquet;\n（2）向表中加载数据\nhive (default)&gt; insert into table log_parquet select * from log_text;\n（3）查看表中数据大小\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/;\n13.1 M /user/hive/warehouse/log_parquet/000000_0\n存储文件的对比总结：ORC &gt; Parquet &gt; textFile存储文件的查询速度测试： \n# TextFile\nhive (default)&gt; insert overwrite local directory \n'/opt/module/data/log_text' select substring(url,1,4) from log_text;\n# ORC\nhive (default)&gt; insert overwrite local directory \n'/opt/module/data/log_orc' select substring(url,1,4) from log_orc;\n# Parquet\nhive (default)&gt; insert overwrite local directory \n'/opt/module/data/log_parquet' select substring(url,1,4) from \nlog_parquet;\n存储文件的查询速度总结：查询速度相近。\n9.5 存储和压缩结合测试存储和压缩官网：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORCORC 存储方式的压缩：\n\n注意：所有关于 ORCFile 的参数都是在 HQL 语句的 TBLPROPERTIES 字段里面出现1）创建一个 ZLIB 压缩的 ORC 存储方式（1）建表语句\ncreate table log_orc_zlib(\n    track_time string,\n    url string,\n    session_id string,\n    referer string,\n    ip string,\n    end_user_id string,\n    city_id string\n)\nrow format delimited fields terminated by '\\t'\nstored as orc\ntblproperties(\"orc.compress\"=\"ZLIB\");\n（2）插入数据\ninsert into log_orc_zlib select * from log_text;\n（3）查看插入后数据\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_zlib/ ;\n2.78 M /user/hive/warehouse/log_orc_none/000000_0\n2）创建一个 SNAPPY 压缩的 ORC 存储方式（1）建表语句\ncreate table log_orc_snappy(\n    track_time string,\n    url string,\n    session_id string,\n    referer string,\n    ip string,\n    end_user_id string,\n    city_id string\n)\nrow format delimited fields terminated by '\\t'\nstored as orc\ntblproperties(\"orc.compress\"=\"SNAPPY\");\n（2）插入数据\ninsert into log_orc_snappy select * from log_text;\n（3）查看插入后数据\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/;\n3.75 M /user/hive/warehouse/log_orc_snappy/000000_0\nZLIB 比 Snappy 压缩的还小。原因是 ZLIB 采用的是 deflate 压缩算法。比 snappy 压缩的压缩率高。3）创建一个 SNAPPY 压缩的 parquet 存储方式（1）建表语句\ncreate table log_parquet_snappy(\n    track_time string,\n    url string,\n    session_id string,\n    referer string,\n    ip string,\n    end_user_id string,\n    city_id string\n)\nrow format delimited fields terminated by '\\t'\nstored as parquet\ntblproperties(\"parquet.compression\"=\"SNAPPY\");\n（2）插入数据\ninsert into log_parquet_snappy select * from log_text;\n（3）查看插入后数据\nhive (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet_snappy/;\n6.39 MB /user/hive/warehouse/ log_parquet_snappy /000000_0\n4）存储方式和压缩总结在实际的项目开发当中，hive 表的数据存储格式一般选择：orc 或 parquet。压缩方式一般选择 snappy，lzo。 \n第 10 章 企业级调优10.1 执行计划（Explain） 1）基本语法\nEXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query\n2）案例实操（1）查看下面这条语句的执行计划没有生成 MR 任务的\nhive (default)&gt; explain select * from emp;\nExplain\nSTAGE DEPENDENCIES:\n Stage-0 is a root stage\nSTAGE PLANS:\n Stage: Stage-0\n Fetch Operator\n limit: -1\n Processor Tree:\n TableScan\n alias: emp\n Statistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE \nColumn stats: NONE\n Select Operator\n expressions: empno (type: int), ename (type: string), job \n(type: string), mgr (type: int), hiredate (type: string), sal (type: \n                                                               double), comm (type: double), deptno (type: int)\n outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, \n_col6, _col7\n Statistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE \nColumn stats: NONE\n ListSink\n有生成 MR 任务的\nhive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group by \ndeptno;\nExplain\nSTAGE DEPENDENCIES:\n Stage-1 is a root stage\n Stage-0 depends on stages: Stage-1\nSTAGE PLANS:\n Stage: Stage-1\n Map Reduce\n Map Operator Tree:\n TableScan\n alias: emp\n Statistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE \nColumn stats: NONE\n Select Operator\n expressions: sal (type: double), deptno (type: int)\n outputColumnNames: sal, deptno\n Statistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE \nColumn stats: NONE\n Group By Operator\n aggregations: sum(sal), count(sal)\n keys: deptno (type: int)\n mode: hash\n outputColumnNames: _col0, _col1, _col2\n Statistics: Num rows: 1 Data size: 7020 Basic stats:\n\nCOMPLETE Column stats: NONE\n Reduce Output Operator\n key expressions: _col0 (type: int)\n sort order: +\n Map-reduce partition columns: _col0 (type: int)\n Statistics: Num rows: 1 Data size: 7020 Basic stats: \nCOMPLETE Column stats: NONE\n value expressions: _col1 (type: double), _col2 (type: \nbigint)\n Execution mode: vectorized\n Reduce Operator Tree:\n Group By Operator\n aggregations: sum(VALUE._col0), count(VALUE._col1)\n keys: KEY._col0 (type: int)\n mode: mergepartial\n outputColumnNames: _col0, _col1, _col2\n Statistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE \nColumn stats: NONE\n Select Operator\n expressions: _col0 (type: int), (_col1 / _col2) (type: double)\n outputColumnNames: _col0, _col1\n Statistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE \nColumn stats: NONE\n File Output Operator\n compressed: false\n Statistics: Num rows: 1 Data size: 7020 Basic stats: COMPLETE \nColumn stats: NONE\n table:\n input format: \norg.apache.hadoop.mapred.SequenceFileInputFormat\n output format: \norg.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n Stage: Stage-0\n Fetch Operator\n limit: -1\n Processor Tree:\n ListSink\n（2）查看详细执行计划\nhive (default)&gt; explain extended select * from emp;\nhive (default)&gt; explain extended select deptno, avg(sal) avg_sal from emp \ngroup by deptno;\n10.2 Fetch 抓取​        Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。例如：SELECT * FROM employees;在这种情况下，Hive 可以简单地读取 employee 对应的存储目录下的文件，然后输出查询结果到控制台。​        在 hive-default.xml.template 文件中 hive.fetch.task.conversion 默认是 more，老版本 hive默认是 minimal，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走mapreduce。\n&lt;property&gt;\n    &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;\n    &lt;value&gt;more&lt;/value&gt;\n    &lt;description&gt;\n        Expects one of [none, minimal, more].\n        Some select queries can be converted to single FETCH task minimizing latency.\n        Currently the query should be single sourced not having any subquery \n        and should not have any aggregations or distincts (which incurs RS), \n        lateral views and joins.\n        0. none : disable hive.fetch.task.conversion\n        1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only\n        2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)\n    &lt;/description&gt;\n&lt;/property&gt;\n1）案例实操：（1）把 hive.fetch.task.conversion 设置成 none，然后执行查询语句，都会执行 mapreduce程序。\nhive (default)&gt; set hive.fetch.task.conversion=none;\nhive (default)&gt; select * from emp;\nhive (default)&gt; select ename from emp;\nhive (default)&gt; select ename from emp limit 3;\n（2）把 hive.fetch.task.conversion 设置成 more，然后执行查询语句，如下查询方式都不会执行 mapreduce 程序。\nhive (default)&gt; set hive.fetch.task.conversion=more;\nhive (default)&gt; select * from emp;\nhive (default)&gt; select ename from emp;\nhive (default)&gt; select ename from emp limit 3;\n10.3 本地模式​        大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过，有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际 job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。​        用户可以通过设置 hive.exec.mode.local.auto 的值为 true，来让 Hive 在适当的时候自动启动这个优化。\nset hive.exec.mode.local.auto=true; #开启本地 mr\n#设置 local mr 的最大输入数据量，当输入数据量小于这个值时采用 local mr 的方式，默认为 134217728，即 128M\nset hive.exec.mode.local.auto.inputbytes.max=50000000;\n#设置 local mr 的最大输入文件个数，当输入文件个数小于这个值时采用 local mr 的方式，默认为 4\nset hive.exec.mode.local.auto.input.files.max=10;\n1）案例实操：（2）关闭本地模式（默认是关闭的），并执行查询语句\nhive (default)&gt; select count(*) from emp group by deptno;\n（1）开启本地模式，并执行查询语句\nhive (default)&gt; set hive.exec.mode.local.auto=true;\nhive (default)&gt; select count(*) from emp group by deptno;\n10.4 表的优化小表大表 Join（MapJOIN） \n​        将 key 相对分散，并且数据量小的表放在 join 的左边，可以使用 map join 让小的维度表先进内存。在 map 端完成 join。​        实际测试发现：新版的 hive 已经对小表 JOIN 大表和大表 JOIN 小表进行了优化。小表放在左边和右边已经没有区别。案例实操1）需求介绍测试大表 JOIN 小表和小表 JOIN 大表的效率2）开启 MapJoin 参数设置\n（1）设置自动选择 Mapjoin\nset hive.auto.convert.join = true; 默认为 true\n（2）大表小表的阈值设置（默认 25M 以下认为是小表）：\nset hive.mapjoin.smalltable.filesize = 25000000;\n3）MapJoin 工作机制\n\n4）建大表、小表和 JOIN 后表的语句\n# 创建大表\ncreate table bigtable(id bigint, t bigint, uid string, keyword string, \nurl_rank int, click_num int, click_url string) row format delimited \nfields terminated by '\\t';\n# 创建小表\ncreate table smalltable(id bigint, t bigint, uid string, keyword string, \nurl_rank int, click_num int, click_url string) row format delimited \nfields terminated by '\\t';\n# 创建 join 后表的语句\ncreate table jointable(id bigint, t bigint, uid string, keyword string, \nurl_rank int, click_num int, click_url string) row format delimited \nfields terminated by '\\t';\n5）分别向大表和小表中导入数据\nhive (default)&gt; load data local inpath '/opt/module/data/bigtable' into \ntable bigtable;\nhive (default)&gt;load data local inpath '/opt/module/data/smalltable' into \ntable smalltable;\n6）小表 JOIN 大表语句\ninsert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom smalltable s\njoin bigtable b\non b.id = s.id;\n7）大表 JOIN 小表语句\ninsert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom bigtable b\njoin smalltable s\non s.id = b.id;\n大表 Join 大表1）空 KEY 过滤        有时 join 超时是因为某些 key 对应的数据太多，而相同 key 对应的数据都会发送到相同的 reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的 key，很多情况下，这些 key 对应的数据是异常数据，我们需要在 SQL 语句中进行过滤。例如 key 对应的字段为空，操作如下：案例实操（1）配置历史服务器配置 mapred-site.xml\n&lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;\n    &lt;value&gt;hadoop102:10020&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;\n    &lt;value&gt;hadoop102:19888&lt;/value&gt;\n&lt;/property&gt;\n# 启动历史服务器\nsbin/mr-jobhistory-daemon.sh start historyserver\n# 查看 jobhistory\nhttp://hadoop102:19888/jobhistory\n（2）创建原始数据空 id 表\n# 创建空 id 表\ncreate table nullidtable(id bigint, t bigint, uid string, keyword string, \nurl_rank int, click_num int, click_url string) row format delimited \nfields terminated by '\\t';\n（3）分别加载原始数据和空 id 数据到对应表中\nhive (default)&gt; load data local inpath '/opt/module/data/nullid' into \ntable nullidtable;\n（4）测试不过滤空 id\nhive (default)&gt; insert overwrite table jointable select n.* from \nnullidtable n left join bigtable o on n.id = o.id;\n（5）测试过滤空 id\nhive (default)&gt; insert overwrite table jointable select n.* from (select \nfrom nullidtable where id is not null) n left join bigtable o on n.id = o.id;\n2）空 key 转换        有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join 的结果中，此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。例如：案例实操：不随机分布空 null 值：（1）设置 5 个 reduce 个数\nset mapreduce.job.reduces = 5;\n（2）JOIN 两张表\ninsert overwrite table jointable\nselect n.* from nullidtable n left join bigtable b on n.id = b.id;\n结果：如下图所示，可以看出来，出现了数据倾斜，某些 reducer 的资源消耗远大于其他 reducer。\n\n随机分布空 null 值 （1）设置 5 个 reduce 个数\nset mapreduce.job.reduces = 5;\n（2）JOIN 两张表\ninsert overwrite table jointable\nselect n.* from nullidtable n full join bigtable o on \nnvl(n.id,rand()) = o.id;\n结果：如下图所示，可以看出来，消除了数据倾斜，负载均衡 reducer 的资源消耗\n\n3）SMB(Sort Merge Bucket join)（1）创建第二张大表\ncreate table bigtable2(\n    id bigint,\n    t bigint,\n    uid string,\n    keyword string,\n    url_rank int,\n    click_num int,\n    click_url string)\nrow format delimited fields terminated by '\\t';\nload data local inpath '/opt/module/data/bigtable' into table bigtable2;\n测试大表直接 JOIN\ninsert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom bigtable s\njoin bigtable2 b\non b.id = s.id;\n（2）创建分通表 1,桶的个数不要超过可用 CPU 的核数\ncreate table bigtable_buck1(\n    id bigint,\n    t bigint,\n    uid string,\n    keyword string,\n    url_rank int,\n    click_num int,\n    click_url string)\nclustered by(id) \nsorted by(id)\ninto 6 buckets\nrow format delimited fields terminated by '\\t';\nload data local inpath '/opt/module/data/bigtable' into table \nbigtable_buck1;\n（3）创建分通表 2,桶的个数不要超过可用 CPU 的核数\ncreate table bigtable_buck2(\n    id bigint,\n    t bigint,\n    uid string,\n    keyword string,\n    url_rank int,\n    click_num int,\n    click_url string)\nclustered by(id)\nsorted by(id) \ninto 6 buckets\nrow format delimited fields terminated by '\\t';\nload data local inpath '/opt/module/data/bigtable' into table \nbigtable_buck2;\n（4）设置参数\nset hive.optimize.bucketmapjoin = true;\nset hive.optimize.bucketmapjoin.sortedmerge = true;\nset \nhive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n（5）测试\ninsert overwrite table jointable\nselect b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom bigtable_buck1 s\njoin bigtable_buck2 b\non b.id = s.id;\nGroup By        默认情况下，Map 阶段同一 Key 数据分发给一个 reduce，当一个 key 数据过大时就倾斜了。\n\n​        并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行部分聚合，最后在 Reduce 端得出最终结果。1）开启 Map 端聚合参数设置\n# 是否在 Map 端进行聚合，默认为 True\nset hive.map.aggr = true\n# 在 Map 端进行聚合操作的条目数目\nset hive.groupby.mapaggr.checkinterval = 100000\n# 有数据倾斜的时候进行负载均衡（默认是 false）\nset hive.groupby.skewindata = true\n​        当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。\nhive (default)&gt; select deptno from emp group by deptno;\nStage-Stage-1: Map: 1 Reduce: 5 Cumulative CPU: 23.68 sec HDFS Read: \n19987 HDFS Write: 9 SUCCESS\nTotal MapReduce CPU Time Spent: 23 seconds 680 msec\nOK\ndeptno\n10\n20\n30\n优化以后\nhive (default)&gt; set hive.groupby.skewindata = true;\nhive (default)&gt; select deptno from emp group by deptno;\nStage-Stage-1: Map: 1 Reduce: 5 Cumulative CPU: 28.53 sec HDFS Read: \n18209 HDFS Write: 534 SUCCESS\nStage-Stage-2: Map: 1 Reduce: 5 Cumulative CPU: 38.32 sec HDFS Read: \n15014 HDFS Write: 9 SUCCESS\nTotal MapReduce CPU Time Spent: 1 minutes 6 seconds 850 msec\nOK\ndeptno\n10\n20\n30\nCount(Distinct) 去重统计        数据量小的时候无所谓，数据量大的情况下，由于 COUNT DISTINCT 操作需要用一个Reduce Task 来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个 Job 很难完成，一般 COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换,但是需要注意 group by 造成的数据倾斜问题. \n1）案例实操（1）创建一张大表\nhive (default)&gt; create table bigtable(id bigint, time bigint, uid string, \nkeyword\nstring, url_rank int, click_num int, click_url string) row format \ndelimited\nfields terminated by '\\t';\n（2）加载数据\nhive (default)&gt; load data local inpath '/opt/module/data/bigtable' into \ntable bigtable;\n（3）设置 5 个 reduce 个数\nset mapreduce.job.reduces = 5;\n（4）执行去重 id 查询\nhive (default)&gt; select count(distinct id) from bigtable;\nStage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 7.12 sec HDFS Read: \n120741990 HDFS Write: 7 SUCCESS\nTotal MapReduce CPU Time Spent: 7 seconds 120 msec\nOK\nc0\n100001\nTime taken: 23.607 seconds, Fetched: 1 row(s)\n（5）采用 GROUP by 去重 id\nhive (default)&gt; select count(id) from (select id from bigtable group by \nid) a;\nStage-Stage-1: Map: 1 Reduce: 5 Cumulative CPU: 17.53 sec HDFS Read: \n120752703 HDFS Write: 580 SUCCESS\nStage-Stage-2: Map: 1 Reduce: 1 Cumulative CPU: 4.29 sec2 HDFS Read: \n9409 HDFS Write: 7 SUCCESS\nTotal MapReduce CPU Time Spent: 21 seconds 820 msec\nOK\n_c0\n100001\nTime taken: 50.795 seconds, Fetched: 1 row(s)\n虽然会多用一个 Job 来完成，但在数据量大的情况下，这个绝对是值得的。笛卡尔积        尽量避免笛卡尔积，join 的时候不加 on 条件，或者无效的 on 条件，Hive 只能使用 1 个reducer 来完成笛卡尔积。\n行列过滤列处理：在 SELECT 中，只拿需要的列，如果有分区，尽量使用分区过滤，少用 SELECT *。行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在 Where 后面，那么就会先全表关联，之后再过滤，比如：案例实操：1）测试先关联两张表，再用 where 条件过滤\nhive (default)&gt; select o.id from bigtable b\njoin bigtable o on o.id = b.id\nwhere o.id &lt;= 10;\nTime taken: 34.406 seconds, Fetched: 100 row(s)\n2）通过子查询后，再关联表\nhive (default)&gt; select b.id from bigtable b\njoin (select id from bigtable where id &lt;= 10) o on b.id = o.id;\nTime taken: 30.058 seconds, Fetched: 100 row(s)\n分区详见 7.1 章。分桶详见 7.2 章。 \n10.5 合理设置 Map 及 Reduce 数1）通常情况下，作业会通过 input 的目录产生一个或者多个 map 任务。        主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。2）是不是 map 数越多越好？        答案是否定的。如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。3）是不是保证每个 map 处理接近 128m 的文件块，就高枕无忧了？        答案也是不一定。比如有一个 127m 的文件，正常会用一个 map 去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果 map 处理的逻辑比较复杂，用一个 map任务去做，肯定也比较耗时。\n​        针对上面的问题 2 和 3，我们需要采取两种方式来解决：即减少 map 数和增加 map 数；复杂文件增加 Map 数 \n​        当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数，来使得每个 map 处理的数据量减少，从而提高任务的执行效率。​        增加 map 的方法为：根据​        computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M 公式，调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。案例实操：\n# 执行查询\nhive (default)&gt; select count(*) from emp;\nHadoop job information for Stage-1: number of mappers: 1; number of \nreducers: 1\n# 设置最大切片值为 100 个字节\nhive (default)&gt; set mapreduce.input.fileinputformat.split.maxsize=100;\nhive (default)&gt; select count(*) from emp;\nHadoop job information for Stage-1: number of mappers: 6; number of \nreducers: 1\n小文件进行合并1）在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。\nset hive.input.format= \norg.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n2）在 Map-Reduce 的任务结束时合并小文件的设置： \n​        在 map-only 任务结束时合并小文件，默认 true\nSET hive.merge.mapfiles = true;\n​        在 map-reduce 任务结束时合并小文件，默认 false\nSET hive.merge.mapredfiles = true;\n​        合并文件的大小，默认 256M\nSET hive.merge.size.per.task = 268435456;\n​        当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge\nSET hive.merge.smallfiles.avgsize = 16777216;\n合理设置 Reduce 数 \n1）调整 reduce 个数方法一（1）每个 Reduce 处理的数据量默认是 256MB\nhive.exec.reducers.bytes.per.reducer=256000000\n（2）每个任务最大的 reduce 数，默认为 1009\nhive.exec.reducers.max=1009\n（3）计算 reducer 数的公式\nN=min(参数 2，总输入数据量/参数 1)\n2）调整 reduce 个数方法二        在 hadoop 的 mapred-default.xml 文件中修改        设置每个 job 的 Reduce 个数\nset mapreduce.job.reduces = 15;\n3）reduce 个数并不是越多越好（1）过多的启动和初始化 reduce 也会消耗时间和资源；（2）另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；        在设置 reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的 reduce 数；使单个 reduce 任务处理数据量大小要合适；\n10.6 并行执行​        Hive 会将一个查询转化成一个或者多个阶段。这样的阶段可以是 MapReduce 阶段、抽样阶段、合并阶段、limit 阶段。或者 Hive 执行过程中可能需要的其他阶段。默认情况下，Hive 一次只会执行一个阶段。不过，某个特定的 job 可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个 job 的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么 job 可能就越快完成。通过设置参数 hive.exec.parallel 值为 true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果 job 中并行阶段增多，那么集群利用率就会增加。\nset hive.exec.parallel=true; //打开任务并行执行\nset hive.exec.parallel.thread.number=16; //同一个 sql 允许最大并行度，默认为8。\n​        当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。\n10.7 严格模式Hive 可以通过设置防止一些危险操作：1）分区表不使用分区过滤         将 hive.strict.checks.no.partition.filter 设置为 true 时，对于分区表，除非 where 语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。2）使用 order by 没有 limit 过滤        将 hive.strict.checks.orderby.no.limit 设置为 true 时，对于使用了 order by 语句的查询，要求必须使用 limit 语句。因为 order by 为了执行排序过程会将所有的结果数据分发到同一个Reducer 中进行处理，强制要求用户增加这个 LIMIT 语句可以防止 Reducer 额外执行很长一段时间。3）笛卡尔积        将 hive.strict.checks.cartesian.product 设置为 true 时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在 执行 JOIN 查询的时候不使用 ON 语句而是使用 where 语句，这样关系数据库的执行优化器就可以高效地将 WHERE 语句转化成那个 ON 语句。不幸的是，Hive 并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。\n10.8 JVM 重用详见 hadoop 优化文档中 jvm 重用\n10.9 压缩详见第 9 章。\n第 11 章 Hive 实战11.1 需求描述统计硅谷影音视频网站的常规指标，各种 TopN 指标：— 统计视频观看数 Top10— 统计视频类别热度 Top10— 统计出视频观看数最高的 20 个视频的所属类别以及类别包含 Top20 视频的个数— 统计视频观看数 Top50 所关联视频的所属类别排序— 统计每个类别中的视频热度 Top10,以 Music 为例— 统计每个类别视频观看数 Top10 — 统计上传视频最多的用户 Top10 以及他们上传的视频观看次数在前 20 的视频\n11.2 数据结构1）视频表用户表\n11.3 准备工作准备表1）需要准备的表        创建原始数据表：gulivideo_ori，gulivideo_user_ori，        创建最终表：gulivideo_orc，gulivideo_user_orc 2）创建原始数据表： （1)gulivideo_ori\ncreate table gulivideo_ori(\n    videoId string, \n    uploader string, \n    age int, \n    category array&lt;string&gt;, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedId array&lt;string&gt;)\nrow format delimited fields terminated by \"\\t\"\ncollection items terminated by \"&amp;\"\nstored as textfile;\n（2）创建原始数据表: gulivideo_user_ori\ncreate table gulivideo_user_ori(\n    uploader string,\n    videos int,\n    friends int)\nrow format delimited \nfields terminated by \"\\t\" \nstored as textfile;\n2）创建 orc 存储格式带 snappy 压缩的表：（1）gulivideo_orc\ncreate table gulivideo_orc(\n    videoId string, \n    uploader string, \n    age int, \n    category array&lt;string&gt;, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedId array&lt;string&gt;)\nstored as orc\ntblproperties(\"orc.compress\"=\"SNAPPY\");\n（2）gulivideo_user_orc\ncreate table gulivideo_user_orc(\n    uploader string,\n    videos int,\n    friends int)\nrow format delimited \nfields terminated by \"\\t\" \nstored as orc\ntblproperties(\"orc.compress\"=\"SNAPPY\");\n（3）向 ori 表插入数据\nload data local inpath \"/opt/module/data/video\" into table gulivideo_ori;\nload data local inpath \"/opt/module/user\" into table gulivideo_user_ori; \n（4）向 orc 表插入数据\ninsert into table gulivideo_orc select * from gulivideo_ori;\ninsert into table gulivideo_user_orc select * from gulivideo_user_ori;\n安装 Tez 引擎（了解）Tez 是一个 Hive 的运行引擎，性能优于 MR。为什么优于 MR 呢？看下。\n\n​        用 Hive 直接编写 MR 程序，假设有四个有依赖关系的 MR 作业，上图中，绿色是 ReduceTask，云状表示写屏蔽，需要将中间结果持久化写到 HDFS。​        Tez 可以将多个有依赖的作业转换为一个作业，这样只需写一次 HDFS，且中间节点较少，从而大大提升作业的计算性能。 \n1）将 tez 安装包拷贝到集群，并解压 tar 包\n[atguigu@hadoop102 software]$ mkdir /opt/module/tez\n[atguigu@hadoop102 software]$ tar -zxvf /opt/software/tez-0.10.1-\nSNAPSHOT-minimal.tar.gz -C /opt/module/tez\n2）上传 tez 依赖到 HDFS\n[atguigu@hadoop102 software]$ hadoop fs -mkdir /tez\n[atguigu@hadoop102 software]$ hadoop fs -put /opt/software/tez-0.10.1-\nSNAPSHOT.tar.gz /tez\n3）新建 tez-site.xml[atguigu@hadoop102 software]HADOOP_HOME/etc/hadoop/tez-site.xml添加如下内容：\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;tez.lib.uris&lt;/name&gt;\n        &lt;value&gt;${fs.defaultFS}/tez/tez-0.10.1-SNAPSHOT.tar.gz&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;tez.use.cluster.hadoop-libs&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;tez.am.resource.memory.mb&lt;/name&gt;\n        &lt;value&gt;1024&lt;/value&gt;\n\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;tez.am.resource.cpu.vcores&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;tez.container.max.java.heap.fraction&lt;/name&gt;\n        &lt;value&gt;0.4&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;tez.task.resource.memory.mb&lt;/name&gt;\n        &lt;value&gt;1024&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;tez.task.resource.cpu.vcores&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n4）修改 Hadoop 环境变量\n[atguigu@hadoop102 software]$ vim\n$HADOOP_HOME/etc/hadoop/shellprofile.d/tez.sh\n​        添加 Tez 的 Jar 包相关信息\nhadoop_add_profile tez\nfunction _tez_hadoop_classpath\n{\n hadoop_add_classpath \"$HADOOP_HOME/etc/hadoop\" after\n hadoop_add_classpath \"/opt/module/tez/*\" after\n hadoop_add_classpath \"/opt/module/tez/lib/*\" after\n} \n\n5）修改 Hive 的计算引擎\n[atguigu@hadoop102 software]$ vim $HIVE_HOME/conf/hive-site.xml\n添加\n&lt;property&gt;\n &lt;name&gt;hive.execution.engine&lt;/name&gt;\n &lt;value&gt;tez&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n &lt;name&gt;hive.tez.container.size&lt;/name&gt;\n &lt;value&gt;1024&lt;/value&gt;\n&lt;/property&gt;\n6）解决日志 Jar 包冲突\n[atguigu@hadoop102 software]$ rm /opt/module/tez/lib/slf4j-log4j12-\n1.7.10.jar\n11.4 业务分析统计视频观看数 Top10思路：使用 order by 按照 views 字段做一个全局排序即可，同时我们设置只显示前 10条。最终代码：\nSELECT \n videoId,\n views\nFROM \n gulivideo_orc\nORDER BY \n views DESC\nLIMIT 10;\n统计视频类别热度 Top10思路： \n（1）即统计每个类别有多少个视频，显示出包含视频最多的前 10 个类别。（2）我们需要按照类别 group by 聚合，然后 count 组内的 videoId 个数即可。（3）因为当前表结构为：一个视频对应一个或多个类别。所以如果要 group by 类别，需要先将类别进行列转行(展开)，然后再进行 count 即可。（4）最后按照热度排序，显示前 10 条。\n# 最终代码：\nSELECT \n t1.category_name , \n COUNT(t1.videoId) hot\nFROM \n(\n    SELECT \n    videoId, \n    category_name \n    FROM \n    gulivideo_orc \n    lateral VIEW explode(category) gulivideo_orc_tmp AS category_name\n) t1\nGROUP BY \n t1.category_name \nORDER BY\n hot \nDESC \nLIMIT 10\n统计出视频观看数最高的 20 个视频的所属类别以及类别包含Top20 视频的个数思路：（1）先找到观看数最高的 20 个视频所属条目的所有信息，降序排列（2）把这 20 条信息中的 category 分裂出来(列转行) （3）最后查询视频分类名称和该分类下有多少个 Top20 的视频最终代码：\nSELECT \n t2.category_name,\n COUNT(t2.videoId) video_sum\nFROM \n(\n    SELECT\n    t1.videoId,\n    category_name\n    FROM \n    (\n        SELECT \n        videoId, \n        views ,\n        category \n        FROM \n        gulivideo_orc\n        ORDER BY \n        views \n        DESC \n        LIMIT 20 \n    ) t1\n    lateral VIEW explode(t1.category) t1_tmp AS category_name\n) t2\nGROUP BY t2.category_name\n统计视频观看数 Top50 所关联视频的所属类别排序代码：\nSELECT\n t6.category_name,\n t6.video_sum,\n rank() over(ORDER BY t6.video_sum DESC ) rk\nFROM\n(\n    SELECT\n    t5.category_name,\n    COUNT(t5.relatedid_id) video_sum\n    FROM\n    (\n        SELECT\n        t4.relatedid_id,\n        category_name\n        FROM\n        (\n            SELECT \n            t2.relatedid_id ,\n            t3.category \n            FROM \n            (\n                SELECT \n                relatedid_id\n                FROM \n                (\n                    SELECT \n                    videoId, \n                    views,\n                    relatedid \n                    FROM \n                    gulivideo_orc\n                    ORDER BY\n                    views \n                    DESC \n                    LIMIT 50\n                )t1\n                lateral VIEW explode(t1.relatedid) t1_tmp AS relatedid_id\n            )t2 \n            JOIN \n            gulivideo_orc t3 \n            ON \n            t2.relatedid_id = t3.videoId \n        ) t4 \n        lateral VIEW explode(t4.category) t4_tmp AS category_name\n    ) t5\n    GROUP BY\n    t5.category_name\n    ORDER BY \n    video_sum\n    DESC \n) t6\n统计每个类别中的视频热度 Top10，以 Music 为例思路： \n（1）要想统计 Music 类别中的视频热度 Top10，需要先找到 Music 类别，那么就需要将category 展开，所以可以创建一张表用于存放 categoryId 展开的数据。（2）向 category 展开的表中插入数据。（3）统计对应类别（Music）中的视频热度。统计 Music 类别的 Top10（也可以统计其他）\nSELECT \n t1.videoId, \n t1.views,\n t1.category_name\nFROM \n(\n    SELECT\n    videoId,\n    views,\n    category_name\n    FROM gulivideo_orc\n    lateral VIEW explode(category) gulivideo_orc_tmp AS category_name\n)t1 \nWHERE \n t1.category_name = \"Music\" \nORDER BY \n t1.views \nDESC \nLIMIT 10\n统计每个类别视频观看数 Top10最终代码：\nSELECT \n t2.videoId,\n t2.views,\n t2.category_name,\n t2.rk\nFROM \n(\n    SELECT \n    t1.videoId,\n    t1.views,\n    t1.category_name,\n    rank() over(PARTITION BY t1.category_name ORDER BY t1.views DESC ) rk\n    FROM \n    (\n        SELECT\n        videoId,\n        views,\n        category_name\n        FROM gulivideo_orc\n        lateral VIEW explode(category) gulivideo_orc_tmp AS category_name\n    )t1\n)t2\nWHERE t2.rk &lt;=10\n统计上传视频最多的用户 Top10以及他们上传的视频观看次数在前 20 的视频思路： \n（1）求出上传视频最多的 10 个用户（2）关联 gulivideo_orc 表，求出这 10 个用户上传的所有的视频，按照观看数取前 20最终代码:\nSELECT \n t2.videoId,\n t2.views,\n t2.uploader\nFROM\n(\n    SELECT \n    uploader,\n    videos\n    FROM gulivideo_user_orc \n    ORDER BY \n    videos\n    DESC\n    LIMIT 10 \n) t1\nJOIN gulivideo_orc t2 \nON t1.uploader = t2.uploader\nORDER BY \n t2.views \nDESC\nLIMIT 20\n附录：常见错误及解决方案0）如果更换 Tez 引擎后，执行任务卡住，可以尝试调节容量调度器的资源调度策略将$HADOOP_HOME/etc/hadoop/capacity-scheduler.xml 文件中的\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;\n    &lt;value&gt;0.1&lt;/value&gt;\n    &lt;description&gt;\n        Maximum percent of resources in the cluster which can be used to run \n        application masters i.e. controls number of concurrent running\n        applications.\n    &lt;/description&gt;\n&lt;/property&gt;\n改成\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n    &lt;description&gt;\n        Maximum percent of resources in the cluster which can be used to run \n        application masters i.e. controls number of concurrent running\n        applications.\n    &lt;/description&gt;\n&lt;/property&gt;\n1）连接不上 mysql 数据库（1）导错驱动包，应该把 mysql-connector-java-5.1.27-bin.jar 导入/opt/module/hive/lib 的不是这个包。错把 mysql-connector-java-5.1.27.tar.gz 导入 hive/lib 包下。（2）修改 user 表中的主机名称没有都修改为%，而是修改为 localhost2）hive 默认的输入格式处理是 CombineHiveInputFormat，会对小文件进行合并。\nhive (default)&gt; set hive.input.format;\nhive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat\n# 可以采用 HiveInputFormat 就会根据分区数输出相应的文件。\nhive (default)&gt; set \nhive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n3）不能执行 mapreduce 程序可能是 hadoop 的 yarn 没开启。4）启动 mysql 服务时，报 MySQL server PID file could not be found! 异常。 在/var/lock/subsys/mysql 路径下创建 hadoop102.pid，并在文件中添加内容：43965）报 service mysql status MySQL is not running, but lock file (/var/lock/subsys/mysql[失败])异 常。解决方案：在/var/lib/mysql 目录下创建： -rw-rw——. 1 mysql mysql 5 12 月 22 16:41 hadoop102.pid 文件，并修改权限为 777。\n 6）JVM 堆内存溢出描述：java.lang.OutOfMemoryError: Java heap space解决：在 yarn-site.xml 中加入如下代码\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n    &lt;value&gt;2048&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;\n    &lt;value&gt;2.1&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;mapred.child.java.opts&lt;/name&gt;\n    &lt;value&gt;-Xmx1024m&lt;/value&gt;\n&lt;/property&gt;\n7）虚拟内存限制在 yarn-site.xml 中添加如下配置:\n&lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n","slug":"B6-Hive基础","date":"2021-11-17T14:38:57.000Z","categories_index":"数据库","tags_index":"Hive","author_index":"YFR718"},{"id":"60afbd6afe5dbba07afaea373a4f245b","title":"B5.Hadoop案例","content":"MapReduceWordCount案例实操\n创建maven工程，MapReduceDemo\n\n在pom.xml文件中添加如下依赖\n&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n        &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;\n        &lt;version&gt;3.1.3&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;junit&lt;/groupId&gt;\n        &lt;artifactId&gt;junit&lt;/artifactId&gt;\n        &lt;version&gt;4.12&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.slf4j&lt;/groupId&gt;\n        &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;\n        &lt;version&gt;1.7.30&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n\n\n在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入。\nlog4j.rootLogger=INFO, stdout  \nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender  \nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  \nlog4j.appender.logfile=org.apache.log4j.FileAppender  \nlog4j.appender.logfile.File=target/spring.log  \nlog4j.appender.logfile.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n\n\n创建包名：com.atguigu.mapreduce.wordcount\n\n编写Mapper类\npackage com.atguigu.mapreduce.wordcount;\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\npublic class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{\n\t\n\tText k = new Text();\n\tIntWritable v = new IntWritable(1);\n\t\n\t@Override\n\tprotected void map(LongWritable key, Text value, Context context)\tthrows IOException, InterruptedException {\n\t\t\n\t\t// 1 获取一行\n\t\tString line = value.toString();\n\t\t\n\t\t// 2 切割\n\t\tString[] words = line.split(\" \");\n\t\t\n\t\t// 3 输出\n\t\tfor (String word : words) {\n\t\t\t\n\t\t\tk.set(word);\n\t\t\tcontext.write(k, v);\n\t\t}\n\t}\n}\n\n编写Reducer类\npackage com.atguigu.mapreduce.wordcount;\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\npublic class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{\n\nint sum;\nIntWritable v = new IntWritable();\n\n\t@Override\n\tprotected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException {\n\t\t\n\t\t// 1 累加求和\n\t\tsum = 0;\n\t\tfor (IntWritable count : values) {\n\t\t\tsum += count.get();\n\t\t}\n\t\t\n\t\t// 2 输出\n         v.set(sum);\n\t\tcontext.write(key,v);\n\t}\n}\n\n编写Driver驱动类\npackage com.atguigu.mapreduce.wordcount;\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCountDriver {\n\n\tpublic static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n\t\t// 1 获取配置信息以及获取job对象\n\t\tConfiguration conf = new Configuration();\n\t\tJob job = Job.getInstance(conf);\n\n\t\t// 2 关联本Driver程序的jar\n\t\tjob.setJarByClass(WordCountDriver.class);\n\n\t\t// 3 关联Mapper和Reducer的jar\n\t\tjob.setMapperClass(WordCountMapper.class);\n\t\tjob.setReducerClass(WordCountReducer.class);\n\n\t\t// 4 设置Mapper输出的kv类型\n\t\tjob.setMapOutputKeyClass(Text.class);\n\t\tjob.setMapOutputValueClass(IntWritable.class);\n\n\t\t// 5 设置最终输出kv类型\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(IntWritable.class);\n\t\t\n\t\t// 6 设置输入和输出路径\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\t// 7 提交job\n\t\tboolean result = job.waitForCompletion(true);\n\t\tSystem.exit(result ? 0 : 1);\n\t}\n}\n\n本地测试\n（1）需要首先配置好HADOOP_HOME变量以及Windows运行依赖\n（2）在IDEA/Eclipse上运行程序\n\n集群上测试\n（1）用maven打jar包，需要添加的打包插件依赖\n&lt;build&gt;\n    &lt;plugins&gt;\n        &lt;plugin&gt;\n            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n            &lt;version&gt;3.6.1&lt;/version&gt;\n            &lt;configuration&gt;\n                &lt;source&gt;1.8&lt;/source&gt;\n                &lt;target&gt;1.8&lt;/target&gt;\n            &lt;/configuration&gt;\n        &lt;/plugin&gt;\n        &lt;plugin&gt;\n            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;\n            &lt;configuration&gt;\n                &lt;descriptorRefs&gt;\n                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;\n                &lt;/descriptorRefs&gt;\n            &lt;/configuration&gt;\n            &lt;executions&gt;\n                &lt;execution&gt;\n                    &lt;id&gt;make-assembly&lt;/id&gt;\n                    &lt;phase&gt;package&lt;/phase&gt;\n                    &lt;goals&gt;\n                        &lt;goal&gt;single&lt;/goal&gt;\n                    &lt;/goals&gt;\n                &lt;/execution&gt;\n            &lt;/executions&gt;\n        &lt;/plugin&gt;\n    &lt;/plugins&gt;\n&lt;/build&gt;\n（2）将程序打成jar包\n（3）修改不带依赖的jar包名称为wc.jar，并拷贝该jar包到Hadoop集群的/opt/module/hadoop-3.1.3路径。\n（4）启动Hadoop集群\n[atguigu@hadoop102 hadoop-3.1.3]sbin/start-dfs.sh\n\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh\n（5）执行WordCount程序\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar\n\n com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/input /user/atguigu/output\n\n\n序列化案例实操统计每一个手机号耗费的总上行流量、总下行流量、总流量\n输入数据格式：\n\n\n\n\nid\n手机号码\n网络ip\n上行流\n下行流量\n网络状态码\n\n\n\n\n7\n13560436666\n13560436666\n1116\n954\n200\n\n\n\n\n期望输出数据格式:\n\n\n\n\n手机号码\n上行流量\n下行流量\n总流量\n\n\n\n\n13560436666\n1116\n954\n2070\n\n\n\n\nMap阶段(1)读取一行数据,切分字段(2) 抽取手机号、上行流量、下行流量(3)以手机号为key, bean对象为value输出,即context. write(手机号,bean);(4) bean对象要想能够传输，必须实现序列化接口\nReduce阶段(1) 累加上行流量和下行流量得到总流量。\n编写流量统计的Bean对象\npackage com.atguigu.mapreduce.writable;\n\nimport org.apache.hadoop.io.Writable;\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n//1 继承Writable接口\npublic class FlowBean implements Writable {\n\n    private long upFlow; //上行流量\n    private long downFlow; //下行流量\n    private long sumFlow; //总流量\n\n    //2 提供无参构造\n    public FlowBean() {\n    }\n\n    //3 提供三个参数的getter和setter方法\n    public long getUpFlow() {\n        return upFlow;\n    }\n\n    public void setUpFlow(long upFlow) {\n        this.upFlow = upFlow;\n    }\n\n    public long getDownFlow() {\n        return downFlow;\n    }\n\n    public void setDownFlow(long downFlow) {\n        this.downFlow = downFlow;\n    }\n\n    public long getSumFlow() {\n        return sumFlow;\n    }\n\n    public void setSumFlow(long sumFlow) {\n        this.sumFlow = sumFlow;\n    }\n\n    public void setSumFlow() {\n        this.sumFlow = this.upFlow + this.downFlow;\n    }\n\n    //4 实现序列化和反序列化方法,注意顺序一定要保持一致\n    @Override\n    public void write(DataOutput dataOutput) throws IOException {\n        dataOutput.writeLong(upFlow);\n        dataOutput.writeLong(downFlow);\n        dataOutput.writeLong(sumFlow);\n    }\n\n    @Override\n    public void readFields(DataInput dataInput) throws IOException {\n        this.upFlow = dataInput.readLong();\n        this.downFlow = dataInput.readLong();\n        this.sumFlow = dataInput.readLong();\n    }\n\n    //5 重写ToString\n    @Override\n    public String toString() {\n        return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow;\n    }\n}\n编写Mapper类\npackage com.atguigu.mapreduce.writable;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport java.io.IOException;\n\npublic class FlowMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; {\n    private Text outK = new Text();\n    private FlowBean outV = new FlowBean();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        //1 获取一行数据,转成字符串\n        String line = value.toString();\n\n        //2 切割数据\n        String[] split = line.split(\"\\t\");\n\n        //3 抓取我们需要的数据:手机号,上行流量,下行流量\n        String phone = split[1];\n        String up = split[split.length - 3];\n        String down = split[split.length - 2];\n\n        //4 封装outK outV\n        outK.set(phone);\n        outV.setUpFlow(Long.parseLong(up));\n        outV.setDownFlow(Long.parseLong(down));\n        outV.setSumFlow();\n\n        //5 写出outK outV\n        context.write(outK, outV);\n    }\n}\n编写Reducer类\npackage com.atguigu.mapreduce.writable;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;\n\npublic class FlowReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; {\n    private FlowBean outV = new FlowBean();\n    @Override\n    protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException {\n\n        long totalUp = 0;\n        long totalDown = 0;\n\n        //1 遍历values,将其中的上行流量,下行流量分别累加\n        for (FlowBean flowBean : values) {\n            totalUp += flowBean.getUpFlow();\n            totalDown += flowBean.getDownFlow();\n        }\n\n        //2 封装outKV\n        outV.setUpFlow(totalUp);\n        outV.setDownFlow(totalDown);\n        outV.setSumFlow();\n\n        //3 写出outK outV\n        context.write(key,outV);\n    }\n}\n编写Driver驱动类\npackage com.atguigu.mapreduce.writable;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport java.io.IOException;\n\npublic class FlowDriver {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        //1 获取job对象\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        //2 关联本Driver类\n        job.setJarByClass(FlowDriver.class);\n\n        //3 关联Mapper和Reducer\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n        \n\t\t//4 设置Map端输出KV类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(FlowBean.class);\n        \n\t\t//5 设置程序最终输出的KV类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n        \n\t\t//6 设置程序的输入输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\inputflow\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\flowoutput\"));\n        \n\t\t//7 提交Job\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\nCombineTextInputFormat案例实操将输入的大量小文件合并成一个切片统一处理。\n（1）不做任何处理，运行1.8节的WordCount案例程序，观察切片个数为4。\nnumber of splits:4\n（2）在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数为3。（a）驱动类中添加代码如下：\n// 如果不设置InputFormat，它默认用的是TextInputFormat.class\njob.setInputFormatClass(CombineTextInputFormat.class);\n\n//虚拟存储切片最大值设置4m\nCombineTextInputFormat.setMaxInputSplitSize(job, 4194304);\n​        （b）运行如果为3个切片。\nnumber of splits:3\n（3）在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数为1。        （a）驱动中添加代码如下：\n// 如果不设置InputFormat，它默认用的是TextInputFormat.class\njob.setInputFormatClass(CombineTextInputFormat.class);\n\n//虚拟存储切片最大值设置20m\nCombineTextInputFormat.setMaxInputSplitSize(job, 20971520);\n​        （b）运行如果为1个切片\nnumber of splits:1\nPartition分区案例实操将统计结果按照手机归属地不同省份输出到不同文件中（分区）\n在上面的基础上，增加一个分区类\npackage com.atguigu.mapreduce.partitioner;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\npublic class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; {\n\n    @Override\n    public int getPartition(Text text, FlowBean flowBean, int numPartitions) {\n        //获取手机号前三位prePhone\n        String phone = text.toString();\n        String prePhone = phone.substring(0, 3);\n\n        //定义一个分区号变量partition,根据prePhone设置分区号\n        int partition;\n\n        if(\"136\".equals(prePhone)){\n            partition = 0;\n        }else if(\"137\".equals(prePhone)){\n            partition = 1;\n        }else if(\"138\".equals(prePhone)){\n            partition = 2;\n        }else if(\"139\".equals(prePhone)){\n            partition = 3;\n        }else {\n            partition = 4;\n        }\n\n        //最后返回分区号partition\n        return partition;\n    }\n}\n在驱动函数中增加自定义数据分区设置和ReduceTask设置\npackage com.atguigu.mapreduce.partitioner;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport java.io.IOException;\n\npublic class FlowDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        //1 获取job对象\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        //2 关联本Driver类\n        job.setJarByClass(FlowDriver.class);\n\n        //3 关联Mapper和Reducer\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n\n        //4 设置Map端输出数据的KV类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(FlowBean.class);\n\n        //5 设置程序最终输出的KV类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        //8 指定自定义分区器\n        job.setPartitionerClass(ProvincePartitioner.class);\n\n        //9 同时指定相应数量的ReduceTask\n        job.setNumReduceTasks(5);\n\n        //6 设置输入输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\inputflow\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D\\\\partitionout\"));\n\n        //7 提交Job\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\nWritableComparable排序案例实操（全排序）根据前面序列化案例产生的结果再次对总流量进行倒序排序。\nFlowBean对象在在需求1基础上增加了比较功能\npackage com.atguigu.mapreduce.writablecompable;\n\nimport org.apache.hadoop.io.WritableComparable;\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\npublic class FlowBean implements WritableComparable&lt;FlowBean&gt; {\n\n    private long upFlow; //上行流量\n    private long downFlow; //下行流量\n    private long sumFlow; //总流量\n\n    //提供无参构造\n    public FlowBean() {\n    }\n\n    //生成三个属性的getter和setter方法\n    public long getUpFlow() {\n        return upFlow;\n    }\n\n    public void setUpFlow(long upFlow) {\n        this.upFlow = upFlow;\n    }\n\n    public long getDownFlow() {\n        return downFlow;\n    }\n\n    public void setDownFlow(long downFlow) {\n        this.downFlow = downFlow;\n    }\n\n    public long getSumFlow() {\n        return sumFlow;\n    }\n\n    public void setSumFlow(long sumFlow) {\n        this.sumFlow = sumFlow;\n    }\n\n    public void setSumFlow() {\n        this.sumFlow = this.upFlow + this.downFlow;\n    }\n\n    //实现序列化和反序列化方法,注意顺序一定要一致\n    @Override\n    public void write(DataOutput out) throws IOException {\n        out.writeLong(this.upFlow);\n        out.writeLong(this.downFlow);\n        out.writeLong(this.sumFlow);\n\n    }\n\n    @Override\n    public void readFields(DataInput in) throws IOException {\n        this.upFlow = in.readLong();\n        this.downFlow = in.readLong();\n        this.sumFlow = in.readLong();\n    }\n\n    //重写ToString,最后要输出FlowBean\n    @Override\n    public String toString() {\n        return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow;\n    }\n\n    @Override\n    public int compareTo(FlowBean o) {\n\n        //按照总流量比较,倒序排列\n        if(this.sumFlow &gt; o.sumFlow){\n            return -1;\n        }else if(this.sumFlow &lt; o.sumFlow){\n            return 1;\n        }else {\n            return 0;\n        }\n    }\n}\n编写Mapper类\npackage com.atguigu.mapreduce.writablecompable;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport java.io.IOException;\n\npublic class FlowMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; {\n    private FlowBean outK = new FlowBean();\n    private Text outV = new Text();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        //1 获取一行数据\n        String line = value.toString();\n\n        //2 按照\"\\t\",切割数据\n        String[] split = line.split(\"\\t\");\n\n        //3 封装outK outV\n        outK.setUpFlow(Long.parseLong(split[1]));\n        outK.setDownFlow(Long.parseLong(split[2]));\n        outK.setSumFlow();\n        outV.set(split[0]);\n\n        //4 写出outK outV\n        context.write(outK,outV);\n    }\n}\n编写Reducer类\npackage com.atguigu.mapreduce.writablecompable;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;\n\npublic class FlowReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; {\n    @Override\n    protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {\n\n        //遍历values集合,循环写出,避免总流量相同的情况\n        for (Text value : values) {\n            //调换KV位置,反向写出\n            context.write(value,key);\n        }\n    }\n}\n编写Driver类\npackage com.atguigu.mapreduce.writablecompable;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport java.io.IOException;\n\npublic class FlowDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        //1 获取job对象\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        //2 关联本Driver类\n        job.setJarByClass(FlowDriver.class);\n\n        //3 关联Mapper和Reducer\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n\n        //4 设置Map端输出数据的KV类型\n        job.setMapOutputKeyClass(FlowBean.class);\n        job.setMapOutputValueClass(Text.class);\n\n        //5 设置程序最终输出的KV类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        //6 设置输入输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\inputflow2\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\comparout\"));\n\n        //7 提交Job\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\nWritableComparable排序案例实操（区内排序）要求每个省份手机号输出的文件中按照总流量内部排序。基于前一个需求，增加自定义分区类，分区按照省份手机号设置。\n增加自定义分区类\npackage com.atguigu.mapreduce.partitionercompable;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\npublic class ProvincePartitioner2 extends Partitioner&lt;FlowBean, Text&gt; {\n\n    @Override\n    public int getPartition(FlowBean flowBean, Text text, int numPartitions) {\n        //获取手机号前三位\n        String phone = text.toString();\n        String prePhone = phone.substring(0, 3);\n\n        //定义一个分区号变量partition,根据prePhone设置分区号\n        int partition;\n        if(\"136\".equals(prePhone)){\n            partition = 0;\n        }else if(\"137\".equals(prePhone)){\n            partition = 1;\n        }else if(\"138\".equals(prePhone)){\n            partition = 2;\n        }else if(\"139\".equals(prePhone)){\n            partition = 3;\n        }else {\n            partition = 4;\n        }\n\n        //最后返回分区号partition\n        return partition;\n    }\n}\n在驱动类中添加分区类\n// 设置自定义分区器\njob.setPartitionerClass(ProvincePartitioner2.class);\n\n// 设置对应的ReduceTask的个数\njob.setNumReduceTasks(5);\nCombiner合并案例实操统计过程中对每一个MapTask的输出进行局部汇总，以减小网络传输量即采用Combiner功能。 \n期望：Combine输入数据多，输出时经过合并，输出数据降低。\n方案一1)增加一个WordcountCombiner类继承Reducer2)在WordcountCombiner中(1) 统计单词汇总(2)将统计结果输出\n增加一个WordCountCombiner类继承Reducer\npackage com.atguigu.mapreduce.combiner;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport java.io.IOException;\n\npublic class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n\nprivate IntWritable outV = new IntWritable();\n\n    @Override\n    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {\n\n        int sum = 0;\n        for (IntWritable value : values) {\n            sum += value.get();\n        }\n\n        //封装outKV\n        outV.set(sum);\n\n        //写出outKV\n        context.write(key,outV);\n    }\n}\n在WordcountDriver驱动类中指定Combiner\n// 指定需要使用combiner，以及用哪个类作为combiner的逻辑\njob.setCombinerClass(WordCountCombiner.class);\n方案二1 )将Wordc ountReducer作为Combiner在    WWordc ountDriver驱动类中指定    job. setCombiner Clas s(WordcountR educer. class);\n将WordcountReducer作为Combiner在WordcountDriver驱动类中指定\n// 指定需要使用Combiner，以及用哪个类作为Combiner的逻辑\njob.setCombinerClass(WordCountReducer.class);\n自定义OutputFormat案例实操过滤输入的log日志，包含atguigu的网站输出到e:/atguigu.log，不包含atguigu的网站输出到e:/other.log。\n输入数据：\nhttp://www.baidu.comhttp://www.google.com。。。\n自定义一个OutputFormat类(1) 创建一个类L ogRecordWniter继承RecordWniter(a)创建两个文件的输出流: atguiguOut、otherOut(b)如果输入数据包含atguigu,输出到atguiguOut流如果不包含atguigu,输出到otherOut流\n5、驱动类Driver.//要将自定义的输出格式组件设置到job中job. setOutputF ormatCla s(Lo gOutputF ormat. class);\n编写LogMapper类\npackage com.atguigu.mapreduce.outputformat;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class LogMapper extends Mapper&lt;LongWritable, Text,Text, NullWritable&gt; {\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        //不做任何处理,直接写出一行log数据\n        context.write(value,NullWritable.get());\n    }\n}\n编写LogReducer类\npackage com.atguigu.mapreduce.outputformat;\n\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class LogReducer extends Reducer&lt;Text, NullWritable,Text, NullWritable&gt; {\n    @Override\n    protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {\n        // 防止有相同的数据,迭代写出\n        for (NullWritable value : values) {\n            context.write(key,NullWritable.get());\n        }\n    }\n}\n自定义一个LogOutputFormat类\npackage com.atguigu.mapreduce.outputformat;\n\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class LogOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; {\n    @Override\n    public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {\n        //创建一个自定义的RecordWriter返回\n        LogRecordWriter logRecordWriter = new LogRecordWriter(job);\n        return logRecordWriter;\n    }\n}\n编写LogRecordWriter类\npackage com.atguigu.mapreduce.outputformat;\n\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\n\nimport java.io.IOException;\n\npublic class LogRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; {\n\n    private FSDataOutputStream atguiguOut;\n    private FSDataOutputStream otherOut;\n\n    public LogRecordWriter(TaskAttemptContext job) {\n        try {\n            //获取文件系统对象\n            FileSystem fs = FileSystem.get(job.getConfiguration());\n            //用文件系统对象创建两个输出流对应不同的目录\n            atguiguOut = fs.create(new Path(\"d:/hadoop/atguigu.log\"));\n            otherOut = fs.create(new Path(\"d:/hadoop/other.log\"));\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n    @Override\n    public void write(Text key, NullWritable value) throws IOException, InterruptedException {\n        String log = key.toString();\n        //根据一行的log数据是否包含atguigu,判断两条输出流输出的内容\n        if (log.contains(\"atguigu\")) {\n            atguiguOut.writeBytes(log + \"\\n\");\n        } else {\n            otherOut.writeBytes(log + \"\\n\");\n        }\n    }\n\n    @Override\n    public void close(TaskAttemptContext context) throws IOException, InterruptedException {\n        //关流\n        IOUtils.closeStream(atguiguOut);\n        IOUtils.closeStream(otherOut);\n    }\n}\n编写LogDriver类\npackage com.atguigu.mapreduce.outputformat;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class LogDriver {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(LogDriver.class);\n        job.setMapperClass(LogMapper.class);\n        job.setReducerClass(LogReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        //设置自定义的outputformat\n        job.setOutputFormatClass(LogOutputFormat.class);\n\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\input\"));\n        //虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat\n        //而fileoutputformat要输出一个_SUCCESS文件，所以在这还得指定一个输出目录\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\logoutput\"));\n\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\nReduce Join案例实操订单数据表t_order\n\n\n\n\nid\npid\namount\n\n\n\n\n1001\n01\n1\n\n\n1002\n02\n2\n\n\n1003\n03\n3\n\n\n1004\n01\n4\n\n\n1005\n02\n5\n\n\n1006\n03\n6\n\n\n\n\n商品信息表t_product\n\n\n\n\npid\npname\n\n\n\n\n01\n小米\n\n\n02\n华为\n\n\n03\n格力\n\n\n\n\n将商品信息表中数据根据商品pid合并到订单数据表中。\n\n\n\n\nid\npname\namount\n\n\n\n\n1001\n小米\n1\n\n\n1004\n小米\n4\n\n\n1002\n华为\n2\n\n\n1005\n华为\n5\n\n\n1003\n格力\n3\n\n\n1006\n格力\n6\n\n\n\n\n通过将关联条件作为Map输出的key，将两表满足Join条件的数据并携带数据所来源的文件信息，发往同一个ReduceTask，在Reduce中进行数据的串联。\n创建商品和订单合并后的TableBean类\npackage com.atguigu.mapreduce.reducejoin;\n\nimport org.apache.hadoop.io.Writable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\npublic class TableBean implements Writable {\n\n    private String id; //订单id\n    private String pid; //产品id\n    private int amount; //产品数量\n    private String pname; //产品名称\n    private String flag; //判断是order表还是pd表的标志字段\n\n    public TableBean() {\n    }\n\n    public String getId() {\n        return id;\n    }\n\n    public void setId(String id) {\n        this.id = id;\n    }\n\n    public String getPid() {\n        return pid;\n    }\n\n    public void setPid(String pid) {\n        this.pid = pid;\n    }\n\n    public int getAmount() {\n        return amount;\n    }\n\n    public void setAmount(int amount) {\n        this.amount = amount;\n    }\n\n    public String getPname() {\n        return pname;\n    }\n\n    public void setPname(String pname) {\n        this.pname = pname;\n    }\n\n    public String getFlag() {\n        return flag;\n    }\n\n    public void setFlag(String flag) {\n        this.flag = flag;\n    }\n\n    @Override\n    public String toString() {\n        return id + \"\\t\" + pname + \"\\t\" + amount;\n    }\n\n    @Override\n    public void write(DataOutput out) throws IOException {\n        out.writeUTF(id);\n        out.writeUTF(pid);\n        out.writeInt(amount);\n        out.writeUTF(pname);\n        out.writeUTF(flag);\n    }\n\n    @Override\n    public void readFields(DataInput in) throws IOException {\n        this.id = in.readUTF();\n        this.pid = in.readUTF();\n        this.amount = in.readInt();\n        this.pname = in.readUTF();\n        this.flag = in.readUTF();\n    }\n}\n编写TableMapper类\npackage com.atguigu.mapreduce.reducejoin;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.InputSplit;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.FileSplit;\n\nimport java.io.IOException;\n\npublic class TableMapper extends Mapper&lt;LongWritable,Text,Text,TableBean&gt; {\n\n    private String filename;\n    private Text outK = new Text();\n    private TableBean outV = new TableBean();\n\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n        //获取对应文件名称\n        InputSplit split = context.getInputSplit();\n        FileSplit fileSplit = (FileSplit) split;\n        filename = fileSplit.getPath().getName();\n    }\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        //获取一行\n        String line = value.toString();\n\n        //判断是哪个文件,然后针对文件进行不同的操作\n        if(filename.contains(\"order\")){  //订单表的处理\n            String[] split = line.split(\"\\t\");\n            //封装outK\n            outK.set(split[1]);\n            //封装outV\n            outV.setId(split[0]);\n            outV.setPid(split[1]);\n            outV.setAmount(Integer.parseInt(split[2]));\n            outV.setPname(\"\");\n            outV.setFlag(\"order\");\n        }else {                             //商品表的处理\n            String[] split = line.split(\"\\t\");\n            //封装outK\n            outK.set(split[0]);\n            //封装outV\n            outV.setId(\"\");\n            outV.setPid(split[0]);\n            outV.setAmount(0);\n            outV.setPname(split[1]);\n            outV.setFlag(\"pd\");\n        }\n\n        //写出KV\n        context.write(outK,outV);\n    }\n}\n编写TableReducer类\npackage com.atguigu.mapreduce.reducejoin;\n\nimport org.apache.commons.beanutils.BeanUtils;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\nimport java.lang.reflect.InvocationTargetException;\nimport java.util.ArrayList;\n\npublic class TableReducer extends Reducer&lt;Text,TableBean,TableBean, NullWritable&gt; {\n\n    @Override\n    protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException {\n\n        ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;();\n        TableBean pdBean = new TableBean();\n\n        for (TableBean value : values) {\n\n            //判断数据来自哪个表\n            if(\"order\".equals(value.getFlag())){   //订单表\n\n\t\t\t  //创建一个临时TableBean对象接收value\n                TableBean tmpOrderBean = new TableBean();\n\n                try {\n                    BeanUtils.copyProperties(tmpOrderBean,value);\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                } catch (InvocationTargetException e) {\n                    e.printStackTrace();\n                }\n\n\t\t\t  //将临时TableBean对象添加到集合orderBeans\n                orderBeans.add(tmpOrderBean);\n            }else {                                    //商品表\n                try {\n                    BeanUtils.copyProperties(pdBean,value);\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                } catch (InvocationTargetException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n\n        //遍历集合orderBeans,替换掉每个orderBean的pid为pname,然后写出\n        for (TableBean orderBean : orderBeans) {\n\n            orderBean.setPname(pdBean.getPname());\n\n\t\t   //写出修改后的orderBean对象\n            context.write(orderBean,NullWritable.get());\n        }\n    }\n}\n编写TableDriver类\npackage com.atguigu.mapreduce.reducejoin;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class TableDriver {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        Job job = Job.getInstance(new Configuration());\n\n        job.setJarByClass(TableDriver.class);\n        job.setMapperClass(TableMapper.class);\n        job.setReducerClass(TableReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(TableBean.class);\n\n        job.setOutputKeyClass(TableBean.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\input\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\output\"));\n\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\n缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。\n解决方案：Map端实现数据合并。\nMap Join案例实操先在MapJoinDriver驱动类中添加缓存文件\npackage com.atguigu.mapreduce.mapjoin;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\n\npublic class MapJoinDriver {\n\n    public static void main(String[] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException {\n\n        // 1 获取job信息\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n        // 2 设置加载jar包路径\n        job.setJarByClass(MapJoinDriver.class);\n        // 3 关联mapper\n        job.setMapperClass(MapJoinMapper.class);\n        // 4 设置Map输出KV类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n        // 5 设置最终输出KV类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        // 加载缓存数据\n        job.addCacheFile(new URI(\"file:///D:/input/tablecache/pd.txt\"));\n        // Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0\n        job.setNumReduceTasks(0);\n\n        // 6 设置输入输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\input\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\output\"));\n        // 7 提交\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\n在MapJoinMapper类中的setup方法中读取缓存文件\npackage com.atguigu.mapreduce.mapjoin;\n\nimport org.apache.commons.lang.StringUtils;\nimport org.apache.hadoop.fs.FSDataInputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.URI;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class MapJoinMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {\n\n    private Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;();\n    private Text text = new Text();\n\n    //任务开始前将pd数据缓存进pdMap\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n\n        //通过缓存文件得到小表数据pd.txt\n        URI[] cacheFiles = context.getCacheFiles();\n        Path path = new Path(cacheFiles[0]);\n\n        //获取文件系统对象,并开流\n        FileSystem fs = FileSystem.get(context.getConfiguration());\n        FSDataInputStream fis = fs.open(path);\n\n        //通过包装流转换为reader,方便按行读取\n        BufferedReader reader = new BufferedReader(new InputStreamReader(fis, \"UTF-8\"));\n\n        //逐行读取，按行处理\n        String line;\n        while (StringUtils.isNotEmpty(line = reader.readLine())) {\n            //切割一行    \n//01\t小米\n            String[] split = line.split(\"\\t\");\n            pdMap.put(split[0], split[1]);\n        }\n\n        //关流\n        IOUtils.closeStream(reader);\n    }\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        //读取大表数据    \n//1001\t01\t1\n        String[] fields = value.toString().split(\"\\t\");\n\n        //通过大表每行数据的pid,去pdMap里面取出pname\n        String pname = pdMap.get(fields[1]);\n\n        //将大表每行数据的pid替换为pname\n        text.set(fields[0] + \"\\t\" + pname + \"\\t\" + fields[2]);\n\n        //写出\n        context.write(text,NullWritable.get());\n    }\n}\n数据清洗（ETL）去除日志中字段个数小于等于11的日志。\n需要在Map阶段对输入的数据根据规则进行过滤清洗。\n编写WebLogMapper类\npackage com.atguigu.mapreduce.weblog;\nimport java.io.IOException;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\npublic class WebLogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;{\n\t\n\t@Override\n\tprotected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\t\t\n\t\t// 1 获取1行数据\n\t\tString line = value.toString();\n\t\t\n\t\t// 2 解析日志\n\t\tboolean result = parseLog(line,context);\n\t\t\n\t\t// 3 日志不合法退出\n\t\tif (!result) {\n\t\t\treturn;\n\t\t}\n\t\t\n\t\t// 4 日志合法就直接写出\n\t\tcontext.write(value, NullWritable.get());\n\t}\n\n\t// 2 封装解析日志的方法\n\tprivate boolean parseLog(String line, Context context) {\n\n\t\t// 1 截取\n\t\tString[] fields = line.split(\" \");\n\t\t\n\t\t// 2 日志长度大于11的为合法\n\t\tif (fields.length &gt; 11) {\n\t\t\treturn true;\n\t\t}else {\n\t\t\treturn false;\n\t\t}\n\t}\n}\n编写WebLogDriver类\npackage com.atguigu.mapreduce.weblog;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WebLogDriver {\n\tpublic static void main(String[] args) throws Exception {\n\n// 输入输出路径需要根据自己电脑上实际的输入输出路径设置\n        args = new String[] { \"D:/input/inputlog\", \"D:/output1\" };\n\n\t\t// 1 获取job信息\n\t\tConfiguration conf = new Configuration();\n\t\tJob job = Job.getInstance(conf);\n\n\t\t// 2 加载jar包\n\t\tjob.setJarByClass(LogDriver.class);\n\n\t\t// 3 关联map\n\t\tjob.setMapperClass(WebLogMapper.class);\n\n\t\t// 4 设置最终输出类型\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(NullWritable.class);\n\n\t\t// 设置reducetask个数为0\n\t\tjob.setNumReduceTasks(0);\n\n\t\t// 5 设置输入和输出路径\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\t// 6 提交\n         boolean b = job.waitForCompletion(true);\n         System.exit(b ? 0 : 1);\n\t}\n}\n压缩实操案例Map输出端采用压缩即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。\n给大家提供的Hadoop源码支持的压缩格式有：BZip2Codec、DefaultCodec\npackage com.atguigu.mapreduce.compress;\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.BZip2Codec;\t\nimport org.apache.hadoop.io.compress.CompressionCodec;\nimport org.apache.hadoop.io.compress.GzipCodec;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCountDriver {\n\n\tpublic static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n\t\tConfiguration conf = new Configuration();\n\n\t\t// 开启map端输出压缩\n\t\tconf.setBoolean(\"mapreduce.map.output.compress\", true);\n\n\t\t// 设置map端输出压缩方式\n\t\tconf.setClass(\"mapreduce.map.output.compress.codec\", BZip2Codec.class,CompressionCodec.class);\n\n\t\tJob job = Job.getInstance(conf);\n\n\t\tjob.setJarByClass(WordCountDriver.class);\n\n\t\tjob.setMapperClass(WordCountMapper.class);\n\t\tjob.setReducerClass(WordCountReducer.class);\n\n\t\tjob.setMapOutputKeyClass(Text.class);\n\t\tjob.setMapOutputValueClass(IntWritable.class);\n\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(IntWritable.class);\n\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\tboolean result = job.waitForCompletion(true);\n\n\t\tSystem.exit(result ? 0 : 1);\n\t}\n}\nMapper保持不变\npackage com.atguigu.mapreduce.compress;\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\npublic class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{\n\n\tText k = new Text();\n\tIntWritable v = new IntWritable(1);\n\n\t@Override\n\tprotected void map(LongWritable key, Text value, Context context)throws IOException, InterruptedException {\n\n\t\t// 1 获取一行\n\t\tString line = value.toString();\n\n\t\t// 2 切割\n\t\tString[] words = line.split(\" \");\n\n\t\t// 3 循环写出\n\t\tfor(String word:words){\n\t\t\tk.set(word);\n\t\t\tcontext.write(k, v);\n\t\t}\n\t}\n}\nReducer保持不变\npackage com.atguigu.mapreduce.compress;\nimport java.io.IOException;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\npublic class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{\n\n\tIntWritable v = new IntWritable();\n\n\t@Override\n\tprotected void reduce(Text key, Iterable&lt;IntWritable&gt; values,\n\t\t\tContext context) throws IOException, InterruptedException {\n\t\t\n\t\tint sum = 0;\n\n\t\t// 1 汇总\n\t\tfor(IntWritable value:values){\n\t\t\tsum += value.get();\n\t\t}\n\t\t\n         v.set(sum);\n\n         // 2 输出\n\t\tcontext.write(key, v);\n\t}\n}\nReduce输出端采用压缩基于WordCount案例处理。\n修改驱动\npackage com.atguigu.mapreduce.compress;\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.BZip2Codec;\nimport org.apache.hadoop.io.compress.DefaultCodec;\nimport org.apache.hadoop.io.compress.GzipCodec;\nimport org.apache.hadoop.io.compress.Lz4Codec;\nimport org.apache.hadoop.io.compress.SnappyCodec;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCountDriver {\n\n\tpublic static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\t\t\n\t\tConfiguration conf = new Configuration();\n\t\t\n\t\tJob job = Job.getInstance(conf);\n\t\t\n\t\tjob.setJarByClass(WordCountDriver.class);\n\t\t\n\t\tjob.setMapperClass(WordCountMapper.class);\n\t\tjob.setReducerClass(WordCountReducer.class);\n\t\t\n\t\tjob.setMapOutputKeyClass(Text.class);\n\t\tjob.setMapOutputValueClass(IntWritable.class);\n\t\t\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(IntWritable.class);\n\t\t\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\t\t\n\t\t// 设置reduce端输出压缩开启\n\t\tFileOutputFormat.setCompressOutput(job, true);\n\n\t\t// 设置压缩的方式\n\t    FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); \n//\t    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); \n//\t    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); \n\t    \n\t\tboolean result = job.waitForCompletion(true);\n\t\t\n\t\tSystem.exit(result?0:1);\n\t}\n}\nMapper和Reducer保持不变\nYarn注：调整下列参数之前尽量拍摄Linux快照，否则后续的案例，还需要重写准备集群。\nYarn生产环境核心参数配置案例1）需求：从1G数据中，统计每个单词出现次数。服务器3台，每台配置4G内存，4核CPU，4线程。\n2）需求分析：\n1G / 128m = 8个MapTask；1个ReduceTask；1个mrAppMaster\n平均每个节点运行10个 / 3台 ≈ 3个任务（4   3   3）\n3）修改yarn-site.xml配置参数如下：\n&lt;!-- 选择调度器，默认容量 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt;\n\t&lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;\n\t&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Number of threads to handle scheduler interface.&lt;/description&gt;\n\t&lt;name&gt;yarn.resourcemanager.scheduler.client.thread-count&lt;/name&gt;\n\t&lt;value&gt;8&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Enable auto-detection of node capabilities such as\n\tmemory and CPU.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.resource.detect-hardware-capabilities&lt;/name&gt;\n\t&lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Flag to determine if logical processors(such as\n\thyperthreads) should be counted as cores. Only applicable on Linux\n\twhen yarn.nodemanager.resource.cpu-vcores is set to -1 and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is true.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.resource.count-logical-processors-as-cores&lt;/name&gt;\n\t&lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 虚拟核数和物理核数乘数，默认是1.0 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Multiplier to determine how to convert phyiscal cores to\n\tvcores. This value is used if yarn.nodemanager.resource.cpu-vcores\n\tis set to -1(which implies auto-calculate vcores) and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is set to true. The\tnumber of vcores will be calculated as\tnumber of CPUs * multiplier.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.resource.pcores-vcores-multiplier&lt;/name&gt;\n\t&lt;value&gt;1.0&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- NodeManager使用内存数，默认8G，修改为4G内存 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Amount of physical memory, in MB, that can be allocated \n\tfor containers. If set to -1 and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is true, it is\n\tautomatically calculated(in case of Windows and Linux).\n\tIn other cases, the default is 8192MB.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n\t&lt;value&gt;4096&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Number of vcores that can be allocated\n\tfor containers. This is used by the RM scheduler when allocating\n\tresources for containers. This is not used to limit the number of\n\tCPUs used by YARN containers. If it is set to -1 and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is true, it is\n\tautomatically determined from the hardware in case of Windows and Linux.\n\tIn other cases, number of vcores is 8 by default.&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;\n\t&lt;value&gt;4&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 容器最小内存，默认1G --&gt;\n&lt;property&gt;\n\t&lt;description&gt;The minimum allocation for every container request at the RM\tin MBs. Memory requests lower than this will be set to the value of this\tproperty. Additionally, a node manager that is configured to have less memory\tthan this value will be shut down by the resource manager.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n\t&lt;value&gt;1024&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 容器最大内存，默认8G，修改为2G --&gt;\n&lt;property&gt;\n\t&lt;description&gt;The maximum allocation for every container request at the RM\tin MBs. Memory requests higher than this will throw an\tInvalidResourceRequestException.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n\t&lt;value&gt;2048&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 容器最小CPU核数，默认1个 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;The minimum allocation for every container request at the RM\tin terms of virtual CPU cores. Requests lower than this will be set to the\tvalue of this property. Additionally, a node manager that is configured to\thave fewer virtual cores than this value will be shut down by the resource\tmanager.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;\n\t&lt;value&gt;1&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 容器最大CPU核数，默认4个，修改为2个 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;The maximum allocation for every container request at the RM\tin terms of virtual CPU cores. Requests higher than this will throw an\n\tInvalidResourceRequestException.&lt;/description&gt;\n\t&lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;\n\t&lt;value&gt;2&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 虚拟内存检查，默认打开，修改为关闭 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Whether virtual memory limits will be enforced for\n\tcontainers.&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n\t&lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 虚拟内存和物理内存设置比例,默认2.1 --&gt;\n&lt;property&gt;\n\t&lt;description&gt;Ratio between virtual memory to physical memory when\tsetting memory limits for containers. Container allocations are\texpressed in terms of physical memory, and virtual memory usage\tis allowed to exceed this allocation by this ratio.\n\t&lt;/description&gt;\n\t&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;\n\t&lt;value&gt;2.1&lt;/value&gt;\n&lt;/property&gt;\n\n关闭虛拟内存检查原因\n\n&lt;property&gt;\n&lt;description&gt;Ratio between virtual memory to physi cal memory when setting memory liits for C0 ntainers. Container allocations are expressed in terms of physical memory, and vitual memory usage is allowed to exceed this allocation by this ratio.\n&lt;/description&gt;\n&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;\n&lt;value&gt;2.1 &lt;/value&gt;\n&lt;/property&gt;\n4）分发配置。\n注意：如果集群的硬件资源不一致，要每个NodeManager单独配置\n5）重启集群\n[atguigu@hadoop102 hadoop-3.1.3]$ sbin/stop-yarn.sh\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh\n6）执行WordCount程序\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output\n7）观察Yarn任务执行页面\nhttp://hadoop103:8088/cluster/apps\n容量调度器多队列提交案例1）在生产环境怎么创建队列？\n（1）调度器默认就1个default队列，不能满足生产要求。\n   （2）按照框架：hive /spark/ flink 每个框架的任务放入指定的队列（企业用的不是特别多）\n（3）按照业务模块：登录注册、购物车、下单、业务部门1、业务部门2\n2）创建多队列的好处？\n（1）因为担心员工不小心，写递归死循环代码，把所有资源全部耗尽。\n（2）实现任务的降级使用，特殊时期保证重要的任务队列资源充足。11.11 6.18\n业务部门1（重要）=》业务部门2（比较重要）=》下单（一般）=》购物车（一般）=》登录注册（次要）\n​    需求1：default队列占总内存的40%，最大资源容量占总资源60%，hive队列占总内存的60%，最大资源容量占总资源80%。\n​    需求2：配置队列优先级\n配置多队列的容量调度器1）在capacity-scheduler.xml中配置如下：\n（1）修改如下配置\n&lt;!-- 指定多队列，增加hive队列 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;\n    &lt;value&gt;default,hive&lt;/value&gt;\n    &lt;description&gt;\n      The queues at the this level (root is the root queue).\n    &lt;/description&gt;\n&lt;/property&gt;\n\n&lt;!-- 降低default队列资源额定容量为40%，默认100% --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.default.capacity&lt;/name&gt;\n    &lt;value&gt;40&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 降低default队列资源最大容量为60%，默认100% --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.default.maximum-capacity&lt;/name&gt;\n    &lt;value&gt;60&lt;/value&gt;\n&lt;/property&gt;\n（2）为新加队列添加必要属性：\n&lt;!-- 指定hive队列的资源额定容量 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.capacity&lt;/name&gt;\n    &lt;value&gt;60&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 用户最多可以使用队列多少资源，1表示 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.user-limit-factor&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 指定hive队列的资源最大容量 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-capacity&lt;/name&gt;\n    &lt;value&gt;80&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 启动hive队列 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.state&lt;/name&gt;\n    &lt;value&gt;RUNNING&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 哪些用户有权向队列提交作业 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_submit_applications&lt;/name&gt;\n    &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 哪些用户有权操作队列，管理员权限（查看/杀死） --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_administer_queue&lt;/name&gt;\n    &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 哪些用户有权配置提交任务优先级 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.acl_application_max_priority&lt;/name&gt;\n    &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 任务的超时时间设置：yarn application -appId appId -updateLifetime Timeout\n参考资料：https://blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ --&gt;\n\n&lt;!-- 如果application指定了超时时间，则提交到该队列的application能够指定的最大超时时间不能超过该值。 \n--&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.maximum-application-lifetime&lt;/name&gt;\n    &lt;value&gt;-1&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 如果application没指定超时时间，则用default-application-lifetime作为默认值 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.capacity.root.hive.default-application-lifetime&lt;/name&gt;\n    &lt;value&gt;-1&lt;/value&gt;\n&lt;/property&gt;\n2）分发配置文件\n3）重启Yarn或者执行yarn rmadmin -refreshQueues刷新队列，就可以看到两条队列：\n[atguigu@hadoop102 hadoop-3.1.3]$ yarn rmadmin -refreshQueues\n公平调度器案例创建两个队列，分别是test和atguigu（以用户所属组命名）。期望实现以下效果：若用户提交任务时指定队列，则任务提交到指定队列运行；若未指定队列，test用户提交的任务到root.group.test队列运行，atguigu提交的任务到root.group.atguigu队列运行（注：group为用户所属组）。\n公平调度器的配置涉及到两个文件，一个是yarn-site.xml，另一个是公平调度器队列分配文件fair-scheduler.xml（文件名可自定义）。\n（1）配置文件参考资料：\nhttps://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html\n（2）任务队列放置规则参考资料：\nhttps://blog.cloudera.com/untangling-apache-hadoop-yarn-part-4-fair-scheduler-queue-basics/\n修改yarn-site.xml文件，加入以下参数\n&lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;\n    &lt;description&gt;配置使用公平调度器&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.fair.allocation.file&lt;/name&gt;\n    &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml&lt;/value&gt;\n    &lt;description&gt;指明公平调度器队列分配配置文件&lt;/description&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;yarn.scheduler.fair.preemption&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n    &lt;description&gt;禁止队列间资源抢占&lt;/description&gt;\n&lt;/property&gt;\n配置fair-scheduler.xml\n&lt;?xml version=\"1.0\"?&gt;\n&lt;allocations&gt;\n  &lt;!-- 单个队列中Application Master占用资源的最大比例,取值0-1 ，企业一般配置0.1 --&gt;\n  &lt;queueMaxAMShareDefault&gt;0.5&lt;/queueMaxAMShareDefault&gt;\n  &lt;!-- 单个队列最大资源的默认值 test atguigu default --&gt;\n  &lt;queueMaxResourcesDefault&gt;4096mb,4vcores&lt;/queueMaxResourcesDefault&gt;\n\n  &lt;!-- 增加一个队列test --&gt;\n  &lt;queue name=\"test\"&gt;\n    &lt;!-- 队列最小资源 --&gt;\n    &lt;minResources&gt;2048mb,2vcores&lt;/minResources&gt;\n    &lt;!-- 队列最大资源 --&gt;\n    &lt;maxResources&gt;4096mb,4vcores&lt;/maxResources&gt;\n    &lt;!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 --&gt;\n    &lt;maxRunningApps&gt;4&lt;/maxRunningApps&gt;\n    &lt;!-- 队列中Application Master占用资源的最大比例 --&gt;\n    &lt;maxAMShare&gt;0.5&lt;/maxAMShare&gt;\n    &lt;!-- 该队列资源权重,默认值为1.0 --&gt;\n    &lt;weight&gt;1.0&lt;/weight&gt;\n    &lt;!-- 队列内部的资源分配策略 --&gt;\n    &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;\n  &lt;/queue&gt;\n  &lt;!-- 增加一个队列atguigu --&gt;\n  &lt;queue name=\"atguigu\" type=\"parent\"&gt;\n    &lt;!-- 队列最小资源 --&gt;\n    &lt;minResources&gt;2048mb,2vcores&lt;/minResources&gt;\n    &lt;!-- 队列最大资源 --&gt;\n    &lt;maxResources&gt;4096mb,4vcores&lt;/maxResources&gt;\n    &lt;!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 --&gt;\n    &lt;maxRunningApps&gt;4&lt;/maxRunningApps&gt;\n    &lt;!-- 队列中Application Master占用资源的最大比例 --&gt;\n    &lt;maxAMShare&gt;0.5&lt;/maxAMShare&gt;\n    &lt;!-- 该队列资源权重,默认值为1.0 --&gt;\n    &lt;weight&gt;1.0&lt;/weight&gt;\n    &lt;!-- 队列内部的资源分配策略 --&gt;\n    &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt;\n  &lt;/queue&gt;\n\n  &lt;!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 --&gt;\n  &lt;queuePlacementPolicy&gt;\n    &lt;!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; false表示：如果指定队列不存在,不允许自动创建--&gt;\n    &lt;rule name=\"specified\" create=\"false\"/&gt;\n    &lt;!-- 提交到root.group.username队列,若root.group不存在,不允许自动创建；若root.group.user不存在,允许自动创建 --&gt;\n    &lt;rule name=\"nestedUserQueue\" create=\"true\"&gt;\n        &lt;rule name=\"primaryGroup\" create=\"false\"/&gt;\n    &lt;/rule&gt;\n    &lt;!-- 最后一个规则必须为reject或者default。Reject表示拒绝创建提交失败，default表示把任务提交到default队列 --&gt;\n    &lt;rule name=\"reject\" /&gt;\n  &lt;/queuePlacementPolicy&gt;\n&lt;/allocations&gt;\n分发配置并重启Yarn\n[atguigu@hadoop102 hadoop]$ xsync yarn-site.xml\n[atguigu@hadoop102 hadoop]$ xsync fair-scheduler.xml\n\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh\n测试提交任务1）提交任务时指定队列，按照配置规则，任务会到指定的root.test队列 \n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -Dmapreduce.job.queuename=root.test 1 1\n​                                   \n2）提交任务时不指定队列，按照配置规则，任务会到root.atguigu.atguigu队列\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 1 1\n​     \nYarn的Tool接口案例0）回顾：\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.atguigu.mapreduce.wordcount2.WordCountDriver /input /output1\n​    期望可以动态传参，结果报错，误认为是第一个输入参数。\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.atguigu.mapreduce.wordcount2.WordCountDriver -Dmapreduce.job.queuename=root.test /input /output1\n1）需求：自己写的程序也可以动态修改参数。编写Yarn的Tool接口。\n2）具体步骤：\n（1）新建Maven项目YarnDemo，pom如下：\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;com.atguigu.hadoop&lt;/groupId&gt;\n    &lt;artifactId&gt;yarn_tool_test&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;\n            &lt;version&gt;3.1.3&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n&lt;/project&gt;\n（2）新建com.atguigu.yarn报名\n（3）创建类WordCount并实现Tool接口：\npackage com.atguigu.yarn;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\n\nimport java.io.IOException;\n\npublic class WordCount implements Tool {\n\n    private Configuration conf;\n\n    @Override\n    public int run(String[] args) throws Exception {\n\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(WordCountDriver.class);\n\n        job.setMapperClass(WordCountMapper.class);\n        job.setReducerClass(WordCountReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        return job.waitForCompletion(true) ? 0 : 1;\n    }\n\n    @Override\n    public void setConf(Configuration conf) {\n        this.conf = conf;\n    }\n\n    @Override\n    public Configuration getConf() {\n        return conf;\n    }\n\n    public static class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {\n\n        private Text outK = new Text();\n        private IntWritable outV = new IntWritable(1);\n\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n            String line = value.toString();\n            String[] words = line.split(\" \");\n\n            for (String word : words) {\n                outK.set(word);\n\n                context.write(outK, outV);\n            }\n        }\n    }\n\n    public static class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n        private IntWritable outV = new IntWritable();\n\n        @Override\n        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {\n\n            int sum = 0;\n\n            for (IntWritable value : values) {\n                sum += value.get();\n            }\n            outV.set(sum);\n\n            context.write(key, outV);\n        }\n    }\n}\n（4）新建WordCountDriver\npackage com.atguigu.yarn;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport java.util.Arrays;\n\npublic class WordCountDriver {\n\n    private static Tool tool;\n\n    public static void main(String[] args) throws Exception {\n        // 1. 创建配置文件\n        Configuration conf = new Configuration();\n\n        // 2. 判断是否有tool接口\n        switch (args[0]){\n            case \"wordcount\":\n                tool = new WordCount();\n                break;\n            default:\n                throw new RuntimeException(\" No such tool: \"+ args[0] );\n        }\n        // 3. 用Tool执行程序\n        // Arrays.copyOfRange 将老数组的元素放到新数组里面\n        int run = ToolRunner.run(conf, tool, Arrays.copyOfRange(args, 1, args.length));\n\n        System.exit(run);\n    }\n}\n3）在HDFS上准备输入文件，假设为/input目录，向集群提交该Jar包\n3）在HDFS上准备输入文件，假设为/input目录，向集群提交该Jar包\n[atguigu@hadoop102 hadoop-3.1.3]$ yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver wordcount /input /output\n注意此时提交的3个参数，第一个用于生成特定的Tool，第二个和第三个为输入输出目录。此时如果我们希望加入设置参数，可以在wordcount后面添加参数，例如：\n[atguigu@hadoop102 hadoop-3.1.3]$ yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver wordcount -Dmapreduce.job.queuename=root.test /input /output1\n4）注：以上操作全部做完过后，快照回去或者手动将配置文件修改成之前的状态，因为本身资源就不够，分成了这么多，不方便以后测试。\n","slug":"B5-MapReduce案例","date":"2021-11-17T11:17:27.000Z","categories_index":"大数据","tags_index":"Hadoop","author_index":"YFR718"},{"id":"c8964ce203e761cbb3c6f466ff1217cb","title":"B4.大数据常用shell脚本","content":"常见shell脚本集群见文件同步#!/bin/bash\n\n#1. 判断参数个数\nif [ $# -lt 1 ]\nthen\n    echo Not Enough Arguement!\n    exit;\nfi\n\n#2. 遍历集群所有机器\nfor host in hadoop102 hadoop103 hadoop104\ndo\n    echo ====================  $host  ====================\n    #3. 遍历所有目录，挨个发送\n\n    for file in $@\n    do\n        #4. 判断文件是否存在\n        if [ -e $file ]\n            then\n                #5. 获取父目录\n                pdir=$(cd -P $(dirname $file); pwd)\n\n                #6. 获取当前文件的名称\n                fname=$(basename $file)\n                ssh $host \"mkdir -p $pdir\"\n                rsync -av $pdir/$fname $host:$pdir\n            else\n                echo $file does not exists!\n        fi\n    done\ndone\n\n启动停止hadoop集群\n#!/bin/bash\n\nif [ $# -lt 1 ]\nthen\n    echo \"No Args Input...\"\n    exit ;\nfi\n\ncase $1 in\n\"start\")\n        echo \" =================== 启动 hadoop集群 ===================\"\n\n        echo \" --------------- 启动 hdfs ---------------\"\n        ssh hadoop102 \"/opt/module/hadoop-3.1.3/sbin/start-dfs.sh\"\n        echo \" --------------- 启动 yarn ---------------\"\n        ssh hadoop103 \"/opt/module/hadoop-3.1.3/sbin/start-yarn.sh\"\n        echo \" --------------- 启动 historyserver ---------------\"\n        ssh hadoop102 \"/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver\"\n;;\n\"stop\")\n        echo \" =================== 关闭 hadoop集群 ===================\"\n\n        echo \" --------------- 关闭 historyserver ---------------\"\n        ssh hadoop102 \"/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver\"\n        echo \" --------------- 关闭 yarn ---------------\"\n        ssh hadoop103 \"/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh\"\n        echo \" --------------- 关闭 hdfs ---------------\"\n        ssh hadoop102 \"/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh\"\n;;\n*)\n    echo \"Input Args Error...\"\n;;\nesac\n\n\n查看集群jps#!/bin/bash\n\nfor host in hadoop102 hadoop103 hadoop104\ndo\n        echo =============== $host ===============\n        ssh $host jps\ndone\n\nhive#!/bin/bash \nHIVE_LOG_DIR=$HIVE_HOME/logs\nif [ ! -d $HIVE_LOG_DIR ]\nthen\n    mkdir -p $HIVE_LOG_DIR\nfi\n#检查进程是否运行正常，参数 1为进程名，参数 2为进程端口\nfunction check_process() \n{\n   pid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i $1 | awk '{print $2}')\n   ppid=$(netstat -nltp 2&gt;/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)\n   echo $pid  \n   [[ \"$pid\" =~ \"$ppid\" ]] &amp;&amp; [ \"$ppid\" ] &amp;&amp; return 0 || return 1\n}\nfunction hive_start() \n{\n   metapid=$(check_process HiveMetastore 9083)\n   cmd=\"nohup hive --service metastore &gt;$HIVE_LOG_DIR/metastore.log 2&gt;&amp;1 &amp;\"\n   [ -z \"$metapid\" ] &amp;&amp; eval $cmd || echo \"Metastroe服务已启动\"\n   server2pid=$(check_process HiveServer2 10000)\n   cmd=\"nohup hiveserver2 &gt;$HIVE_LOG_DIR/hiveServer2.log 2&gt;&amp;1 &amp;\"\n   [ -z \"$server2pid\" ] &amp;&amp; eval $cmd || echo \"HiveServer2服务已启动\"\n}\nfunction hive_stop() \n{\n    metapid=$(check_process HiveMetastore 9083)\n    [ \"$metapid\" ] &amp;&amp; kill $metapid || echo \"Metastore服务未启动\"\n    server2pid=$(check_process HiveServer2 10000)\n    [ \"$server2pid\" ] &amp;&amp; kill $server2pid || echo \"HiveServer2服务未启动\"\n}\ncase $1 in\n\"start\")\nhive_start\n;;\n\"stop\")\nhive_stop\n;;\n\"restart\")\nhive_stop\nsleep 2\nhive_start\n;;\n\n","slug":"B4-大数据常用shell脚本","date":"2021-11-16T08:46:58.000Z","categories_index":"大数据","tags_index":"shell","author_index":"YFR718"},{"id":"35292c50fe93e1c9375839d3cf5431cb","title":"shell编程ing","content":"shell编程\nshell基础Shell是一个命令行解释器，它接收应用程序/用户命令,然后调用操作系统内核。\nShel解析器（1）Linux提供的Shell解析器有：\n[atguigu@hadoop101 ~]$ cat /etc/shells \n/bin/sh\n/bin/bash\n/sbin/nologin\n/bin/dash\n/bin/tcsh\n/bin/csh\n（2）bash和sh的关系\n[atguigu@hadoop101 bin]$ ll | grep bash\n-rwxr-xr-x. 1 root root 941880 5月  11 2016 bash\nlrwxrwxrwx. 1 root root      4 5月  27 2017 sh -&gt; bash\n（3）Centos默认的解析器是bash\n[atguigu@hadoop102 bin]$ echo $SHELL\n/bin/bash\nShell脚本入门1．脚本格式脚本以==#!/bin/bash==开头（指定解析器）2．第一个Shell脚本：helloworld\n[atguigu@hadoop101 datas]$ touch helloworld.sh\n[atguigu@hadoop101 datas]$ vi helloworld.sh\n在helloworld.sh中输入如下内容\n#!/bin/bash\necho \"helloworld\"\n脚本的常用执行方式第一种：采用bash或sh+脚本的相对路径或绝对路径（不用赋予脚本+x权限）\n#\tsh+脚本的相对路径\n[atguigu@hadoop101 datas]$ sh helloworld.sh \nHelloworld\n#\tsh+脚本的绝对路径\n[atguigu@hadoop101 datas]$ sh /home/atguigu/datas/helloworld.sh \nhelloworld\n#\tbash+脚本的相对路径\n[atguigu@hadoop101 datas]$ bash helloworld.sh \nHelloworld\n#\tbash+脚本的绝对路径\n[atguigu@hadoop101 datas]$ bash /home/atguigu/datas/helloworld.sh \nHelloworld\n第二种：采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）\n#首先要赋予helloworld.sh 脚本的+x权限\n[atguigu@hadoop101 datas]$ chmod 777 helloworld.sh\n#执行脚本\n相对路径\n[atguigu@hadoop101 datas]$ ./helloworld.sh \nHelloworld\n#绝对路径\n[atguigu@hadoop101 datas]$ /home/atguigu/datas/helloworld.sh \nHelloworld\n注意：第一种执行方法，本质是bash解析器帮你执行脚本，所以脚本本身不需要执行权限。第二种执行方法，本质是脚本需要自己执行，所以需要执行权限。\nShell中变量\n\n\n\n常用系统变量\n\n\n\n\n\n$HOME\n\n\n\n$PWD\n\n\n\n$SHELL\n\n\n\n$USER\n\n\n\n\n显示当前Shell中所有变量：set\n自定义变量#定义变量：\n变量=值 \n#撤销变量：\nunset 变量\n#声明静态变量：\nreadonly变量，注意：不能unset\n#可把变量提升为全局环境变量，可供其他Shell程序使用\nexport 变量名\n[atguigu@hadoop101 datas]$ vim helloworld.sh \n\n# 在helloworld.sh文件中增加echo $B\n#!/bin/bash\necho \"helloworld\"\necho $B\n[atguigu@hadoop101 datas]$ ./helloworld.sh \nHelloworld\n发现并没有打印输出变量B的值。\n[atguigu@hadoop101 datas]$ export B\n[atguigu@hadoop101 datas]$ ./helloworld.sh \nhelloworld\n2\n变量定义规则    （1）变量名称可以由字母、数字和下划线组成，但是不能以数字开头，环境变量名建议大写。    （2）等号两侧不能有空格    （3）在bash中，变量默认类型都是字符串类型，无法直接进行数值运算。    （4）变量的值如果有空格，需要使用双引号或单引号括起来。\n特殊变量\n\n\n\n特殊变量\n\n\n\n\n\n$n\nn为数字，代表该脚本名称，1-代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如{10}\n\n\n$#\n获取所有输入参数==个数==，常用于循环\n\n\n$*\n这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体\n\n\n$@\n这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待\n\n\n$？\n最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。\n\n\n\n\n\n\n\n# $0 $# $* $@用法\n[atguigu@hadoop101 datas]$ vim parameter.sh\n#!/bin/bash\necho \"$0  $1   $2\"\necho $#\necho $*\necho $@\n[atguigu@hadoop101 datas]$ bash parameter.sh 1 2 3\nparameter.sh  1   2\n3\n1 2 3\n1 2 3\n# $？ 用法\n[atguigu@hadoop101 datas]$ ./helloworld.sh \nhello world\t\n[atguigu@hadoop101 datas]$ echo $?\n0\n运算符\n\n\n\n运算符\n\n\n\n\n\n$((运算式))\n\n\n\n$[运算式]\nS=$[(2+3)*4]\n\n\n\nexpr  + , - , *, /, %\nexpr `expr 2 + 3` * 4\n\n\n\n\n条件判断\n\n\n\n[ condition ]\n（注意condition前后要有空格）\n\n\n\n\n\n\n两个整数之间比较\n\n\n\n\n\n=\n字符串比较\n-lt\n小于（less than）\n\n\n-le\n小于等于（less equal）\n-eq\n等于（equal）\n\n\n-gt\n大于（greater than）\n-ge\n大于等于（greater equal）\n\n\n-ne\n不等于（Not equal）\n\n\n\n\n按照文件权限进行判断\n\n\n\n\n\n-r\n读的权限（read）\n-w\n有写的权限（write）\n\n\n-x\n有执行的权限（execute）\n\n\n\n\n按照文件类型进行判断\n\n\n\n\n\n-f\n文件存在并且是一个常规的文件（file）\n-e\n文件存在（existence）\n\n\n-d\n文件存在并是一个目录（directory）\n\n\n\n\n\n# 23是否大于等于22\n[atguigu@hadoop101 datas]$ [ 23 -ge 22 ]\n[atguigu@hadoop101 datas]$ echo $?\n0\n# helloworld.sh是否具有写权限\n[atguigu@hadoop101 datas]$ [ -w helloworld.sh ]\n[atguigu@hadoop101 datas]$ echo $?\n0\n# /home/atguigu/cls.txt目录中的文件是否存在\n[atguigu@hadoop101 datas]$ [ -e /home/atguigu/cls.txt ]\n[atguigu@hadoop101 datas]$ echo $?\n1\n# 多条件判断（&amp;&amp; 表示前一条命令执行成功时，才执行后一条命令，|| 表示上一条命令执行失败后，才执行下一条命令）\n[atguigu@hadoop101 ~]$ [ condition ] &amp;&amp; echo OK || echo notok\nOK\n[atguigu@hadoop101 datas]$ [ condition ] &amp;&amp; [ ] || echo notok\nnotok\n\n流程控制if. case. for. whileif [ 条件判断式 ];then \n  程序 \nfi \n# [ 条件判断式 ]，中括号和条件判断式之间必须有空格\n# if后要有空格\n\nif [ 条件判断式 ] \n  then \n    程序 \nfi\n# 列\nif [ $1 -eq \"1\" ]\nthen\n        echo \"banzhang zhen shuai\"\nelif [ $1 -eq \"2\" ]\nthen\n        echo \"cls zhen mei\"\nfi\n\ncase $变量名 in \n  \"值1\"） \n    如果变量的值等于值1，则执行程序1 \n    ;; \n  \"值2\"） \n    如果变量的值等于值2，则执行程序2 \n    ;; \n  …省略其他分支… \n  *） \n    如果变量的值都不是以上的值，则执行此程序 \n    ;; \nesac\n#注意事项：\n#1)\tcase行尾必须为单词“in”，每一个模式匹配必须以右括号“）”结束。\n#2)\t双分号“;;”表示命令序列结束，相当于java中的break。\n#3)\t最后的“*）”表示默认模式，相当于java中的default。\n#\ncase $1 in\n\"1\")\n        echo \"banzhang\"\n;;\n\n\"2\")\n        echo \"cls\"\n;;\n*)\n        echo \"renyao\"\n;;\nesac\n\nfor (( 初始值;循环控制条件;变量变化 )) \n  do \n    程序 \n  done\n# 列\ns=0\nfor((i=0;i&lt;=100;i++))\ndo\n        s=$[$s+$i]\ndone\n\n\nfor 变量 in 值1 值2 值3… \n  do \n    程序 \n  done\n \n# 列\nfor i in $*\n    do\n      echo \"ban zhang love $i \"\n    done\nwhile [ 条件判断式 ] \n  do \n    程序\n  done\n\n# 列\ns=0\ni=1\nwhile [ $i -le 100 ]\ndo\n        s=$[$s+$i]\n        i=$[$i+1]\ndone\n\nread读取控制台输入read(选项)(参数)\n选项：\n-p：指定读取值时的提示符；\n-t：指定读取值时等待的时间（秒）。\n参数：变量，指定读取值的变量名\nread -t 7 -p \"Enter your name in 7 seconds \" NAME\necho $NAME\n\nshell高级系统函数basename基本语法\nbasename [string / pathname] [suffix] \n功能描述：basename命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。\n选项：\nsuffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。\n# 截取该/home/atguigu/banzhang.txt路径的文件名称\n[atguigu@hadoop101 datas]$ basename /home/atguigu/banzhang.txt \nbanzhang.txt\n[atguigu@hadoop101 datas]$ basename /home/atguigu/banzhang.txt .txt\nbanzhang\ndirname基本语法\ndirname 文件绝对路径    \n功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）\n# 获取banzhang.txt文件的路径\n[atguigu@hadoop101 ~]$ dirname /home/atguigu/banzhang.txt \n/home/atguigu\n自定义函数基本语法\n[ function ] funname[()]\n{\n\tAction;\n\t[return int;]\n}\nfunname\n经验技巧\n​    （1）必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先编译。\n​    （2）函数返回值，只能通过$?系统变量获得，可以显示加：return返回，如果不加，将以最后一条命令运行结果，作为返回值。return后跟数值n(0-255)\n计算两个输入参数的和\n[atguigu@hadoop101 datas]$ touch fun.sh\n[atguigu@hadoop101 datas]$ vim fun.sh\n\n#!/bin/bash\nfunction sum()\n{\n    s=0\n    s=$[ $1 + $2 ]\n    echo \"$s\"\n}\n\nread -p \"Please input the number1: \" n1;\nread -p \"Please input the number2: \" n2;\nsum $n1 $n2;\n\n[atguigu@hadoop101 datas]$ chmod 777 fun.sh\n[atguigu@hadoop101 datas]$ ./fun.sh \nPlease input the number1: 2\nPlease input the number2: 5\n7\nShell工具cutcut的工作就是“剪”，具体的说就是在文件中负责剪切数据用的。cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出。\n基本用法\ncut [选项参数]  filename\n说明：默认分隔符是制表符\n\n\n\n\n选项参数\n功能\n\n\n\n\n-f\n列号，提取第几列\n\n\n-d\n分隔符，按照指定分隔符分割列\n\n\n\n\n案例实操\n#（0）数据准备\n[atguigu@hadoop101 datas]$ touch cut.txt\n[atguigu@hadoop101 datas]$ vim cut.txt\ndong shen\nguan zhen\nwo  wo\nlai  lai\nle  le\n#（1）切割cut.txt第一列\n[atguigu@hadoop101 datas]$ cut -d \" \" -f 1 cut.txt \ndong\nguan\nwo\nlai\nle\n#（2）切割cut.txt第二、三列\n[atguigu@hadoop101 datas]$ cut -d \" \" -f 2,3 cut.txt \nshen\nzhen\n wo\n lai\n le\n#（3）在cut.txt文件中切割出guan\n[atguigu@hadoop101 datas]$ cat cut.txt | grep \"guan\" | cut -d \" \" -f 1\nguan\n#（4）选取系统PATH变量值，第2个“：”开始后的所有路径：\n[atguigu@hadoop101 datas]$ echo $PATH\n/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/atguigu/bin\n\n[atguigu@hadoop102 datas]$ echo $PATH | cut -d: -f 2-\n/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/atguigu/bin\n#（5）切割ifconfig 后打印的IP地址\n[atguigu@hadoop101 datas]$ ifconfig eth0 | grep \"inet addr\" | cut -d: -f 2 | cut -d\" \" -f1\n192.168.1.102\nsed​        sed是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”，接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。\n基本用法sed [选项参数]  ‘command’  filename\n\n\n\n\n选项参数\n功能\n\n\n\n\n-e\n直接在指令列模式上进行sed的动作编辑。\n\n\n\n\n\n\n\n\n命令\n功能描述\n\n\n\n\na\n新增，a的后面可以接字串，在下一行出现\n\n\nd\n删除\n\n\ns\n查找并替换\n\n\n\n\n案例实操\n#（0）数据准备\n[atguigu@hadoop102 datas]$ touch sed.txt\n[atguigu@hadoop102 datas]$ vim sed.txt\ndong shen\nguan zhen\nwo  wo\nlai  lai\nle  le\n#（1）将“mei nv”这个单词插入到sed.txt第二行下，打印。\n[atguigu@hadoop102 datas]$ sed '2a mei nv' sed.txt \ndong shen\nguan zhen\nmei nv\nwo  wo\nlai  lai\nle  le\n[atguigu@hadoop102 datas]$ cat sed.txt \ndong shen\nguan zhen\nwo  wo\nlai  lai\nle  le\n#注意：文件并没有改变\n#（2）删除sed.txt文件所有包含wo的行\n[atguigu@hadoop102 datas]$ sed '/wo/d' sed.txt\ndong shen\nguan zhen\nlai  lai\nle  le\n#（3）将sed.txt文件中wo替换为ni\n[atguigu@hadoop102 datas]$ sed 's/wo/ni/g' sed.txt \ndong shen\nguan zhen\nni  ni\nlai  lai\nle  le\n#注意：‘g’表示global，全部替换\n#（4）将sed.txt文件中的第二行删除并将wo替换为ni\n[atguigu@hadoop102 datas]$ sed -e '2d' -e 's/wo/ni/g' sed.txt \ndong shen\nni  ni\nlai  lai\nle  le\nawrk一个强大的文本分析工具，把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行分析处理。\n基本用法\nawk [选项参数] ‘pattern1{action1} pattern2{action2}…’ filename\npattern：表示AWK在数据中查找的内容，就是匹配模式\naction：在找到匹配内容时所执行的一系列命令\n\n\n\n\n选项参数\n功能\n\n\n\n\n-F\n指定输入文件折分隔符\n\n\n-v\n赋值一个用户定义变量\n\n\n\n\n#（0）数据准备\n[atguigu@hadoop102 datas]$ sudo cp /etc/passwd ./\n#（1）搜索passwd文件以root关键字开头的所有行，并输出该行的第7列。\n[atguigu@hadoop102 datas]$ awk -F: '/^root/{print $7}' passwd \n/bin/bash\n#（2）搜索passwd文件以root关键字开头的所有行，并输出该行的第1列和第7列，中间以“，”号分割。\n[atguigu@hadoop102 datas]$ awk -F: '/^root/{print $1\",\"$7}' passwd \nroot,/bin/bash\n注意：只有匹配了pattern的行才会执行action\n#（3）只显示/etc/passwd的第一列和第七列，以逗号分割，且在所有行前面添加列名user，shell在最后一行添加\"dahaige，/bin/zuishuai\"。\n[atguigu@hadoop102 datas]$ awk -F : 'BEGIN{print \"user, shell\"} {print $1\",\"$7} END{print \"dahaige,/bin/zuishuai\"}' passwd\nuser, shell\nroot,/bin/bash\nbin,/sbin/nologin\n。。。\natguigu,/bin/bash\ndahaige,/bin/zuishuai\n#注意：BEGIN 在所有数据读取行之前执行；END 在所有数据执行之后执行。\n#（4）将passwd文件中的用户id增加数值1并输出\n[atguigu@hadoop102 datas]$ awk -v i=1 -F: '{print $3+i}' passwd\n1\n2\n3\n4\nawk的内置变量\n\n\n\n\n变量\n说明\n\n\n\n\nFILENAME\n文件名\n\n\nNR\n已读的记录数\n\n\nNF\n浏览记录的域的个数（切割后，列的个数）\n\n\n\n\n# （1）统计passwd文件名，每行的行号，每行的列数\n[atguigu@hadoop102 datas]$ awk -F: '{print \"filename:\"  FILENAME \", linenumber:\" NR  \",columns:\" NF}' passwd \nfilename:passwd, linenumber:1,columns:7\nfilename:passwd, linenumber:2,columns:7\nfilename:passwd, linenumber:3,columns:7\n#（2）切割IP\n[atguigu@hadoop102 datas]$ ifconfig eth0 | grep \"inet addr\" | awk -F: '{print $2}' | awk -F \" \" '{print $1}' \n192.168.1.102\n#（3）查询sed.txt中空行所在的行号\n[atguigu@hadoop102 datas]$ awk '/^$/{print NR}' sed.txt \n5\nsortsort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。\n基本语法\nsort(选项)(参数)\n\n\n\n\n选项\n说明\n\n\n\n\n-n\n依照数值的大小排序\n\n\n-r\n以相反的顺序来排序\n\n\n-t\n设置排序时所用的分隔字符\n\n\n-k\n指定需要排序的列\n\n\n\n\n参数：指定待排序的文件列表\n#（0）数据准备\n[atguigu@hadoop102 datas]$ touch sort.sh\n[atguigu@hadoop102 datas]$ vim sort.sh \nbb:40:5.4\nbd:20:4.2\nxz:50:2.3\ncls:10:3.5\nss:30:1.6\n#（1）按照“：”分割后的第三列倒序排序。\n[atguigu@hadoop102 datas]$ sort -t : -nrk 3  sort.sh \nbb:40:5.4\nbd:20:4.2\ncls:10:3.5\nxz:50:2.3\nss:30:1.6\n面试题问题1：使用Linux命令查询file1中空行所在的行号\n[atguigu@hadoop102 datas]$ awk '/^$/{print NR}' sed.txt \n5\n问题2：有文件chengji.txt内容如下:张三 40李四 50王五 60使用Linux命令计算第二列的和并输出\n[atguigu@hadoop102 datas]$ cat chengji.txt | awk -F \" \" '{sum+=$2} END{print sum}'\n150\n问题3：Shell脚本里如何检查一个文件是否存在？如果不存在该如何处理？\n#!/bin/bash\n\nif [ -f file.txt ]; then\n   echo \"文件存在!\"\nelse\n   echo \"文件不存在!\"\nfi\n问题4：用shell写一个脚本，对文本中无序的一列数字排序\n[root@CentOS6-2 ~]# cat test.txt\n9\n8\n7\n6\n5\n4\n3\n2\n10\n1\n[root@CentOS6-2 ~]# sort -n test.txt|awk '{a+=$0;print $0}END{print \"SUM=\"a}'\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nSUM=55\n问题5：请用shell脚本写出查找当前文件夹（/home）下所有的文本文件内容中包含有字符”shen”的文件名称\n[atguigu@hadoop102 datas]$ grep -r \"shen\" /home | cut -d \":\" -f 1\n/home/atguigu/datas/sed.txt\n/home/atguigu/datas/cut.txt\n","slug":"B3-shell编程","date":"2021-11-16T08:46:40.000Z","categories_index":"大数据","tags_index":"Hadoop","author_index":"YFR718"},{"id":"144f0b4d9b9f80ea377106e9c6a54a45","title":"Linux基础","content":"Linux概述Linux的创始人:Linus Torvalds 林纳斯\nLinux主要的发行版:Ubuntu(乌班图)、RedHat(红帽)、CentOS、Debain[蝶变]、Fedora、SuSE、OpenSUSE\nLinux由Unix发展而来。\nlinux与windows区别VMWare安装见学习资料\n⭐虚拟机的网络连接三种模式\n\n\n\n模式\n特点\n\n\n\n\n桥连接\nVMWare虚拟出来的操作系统就像是局域网中的一台独立的主机，它可以访问网内任何一台机器。需要手工为虚拟系统配置IP地址、子网掩码，而且还要和宿主机器处于同一网段，这样虚拟系统才能和宿主机器进行通信。同时，由于这个虚拟系统是局域网中的一个独立的主机系统，那么就可以手工配置它的TCP/IP配置信息，以实现通过局域网的网关或路由器访问互联网。可能照成ip冲突。\n\n\nNAT\n网络地址转换方式，通过宿主机器所在的网络来访问公网。使用NAT模式可以实现在虚拟系统里访问互联网。NAT模式下的虚拟系统的TCP/IP配置信息是由VMnet8(NAT)虚拟网络的DHCP服务器提供的，无法进行手工修改，因此虚拟系统也就无法和本局域网中的其他真实主机进行通讯。\n\n\n仅主机模式\n所有的虚拟系统是可以相互通信的，但虚拟系统和真实的网络是被隔离开的。\n\n\n\n\nCentOS安装见学习资料\nlinux目录结构linux的文件系统是采用级层式的树状目录结构，在此结构中的最上层是根目录“/”，然后在此目录下再创建其他的目录。\n\n\n\n\n目录结构\n含义\n\n\n\n\n/bin  (/usr/bin 、 /usr/local/bin)⭐\nBinary的缩写, 这个目录存放二进制可执行文件和最经常使用的命令\n\n\n/sbin  (/usr/sbin 、 /usr/local/sbin)\nSuper User，这里存放的是系统管理员使用的系统管理程序。\n\n\n/home⭐\n存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。\n\n\n/root ⭐\n系统管理员，也称作超级权限者的用户主目录。\n\n\n/lib\n系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。\n\n\n/lost+found\n这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。\n\n\n/etc ⭐\n所有的系统管理所需要的配置文件和子目录 my.conf\n\n\n/usr ⭐\n用户很多应用程序和文件都放在这个目录下，类似与windows下的program files目录。\n\n\n/boot ⭐\n启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件\n\n\n/proc\n虚拟的目录，它是系统内存的映射，访问这个目录来获取系统信息。\n\n\n/srv\nservice，该目录存放一些服务启动之后需要提取的数据。\n\n\n/sys\n这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。\n\n\n/tmp\n存放一些临时文件的。\n\n\n/dev\n类似于windows的设备管理器，把所有的硬件用文件的形式存储。\n\n\n/media ⭐\nlinux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。\n\n\n/mnt ⭐\n让用户临时挂载别的文件系统的，我们可以将外部的存储挂载在/mnt/上，然后进入该目录就可以查看里的内容了。 d:/myshare\n\n\n/opt\n给主机额外安装软件所摆放的目录。\n\n\n/usr/local ⭐\n另一个给主机额外安装软件所==安装的目录==。一般是通过编译源码方式安装的程序。\n\n\n/var ⭐\n不断扩充着的东西，习惯将经常被修改的目录放在这个目录下。包括各种日志文件。\n\n\n/selinux  [security-enhanced linux]  类似 360\nSELinux是一种安全子系统,它能控制程序只能访问特定文件。\n\n\n\n\nVI/VIMVI/VIM一般模式以 vim 打开一个文件就直接进入一般模式了(这是默认的模式)。在这个模式中， 你可以使用『上下左右』按键来移动光标，你可以使用『删除字符』或『删除整行』来处理档案内容， 也可以使用『复制、贴上』来处理你的文件数据。在正常模式下可以使用快捷键\nVI/VIM编辑模式按下i（insert）, I, o, O, a, A, r, R等任何一个字母之后才会进入编辑模式, 一般来说按i即可.\nVI/VIM指令模式在这个模式当中， 可以提供你相关指令，完成读取、存盘、替换、离开 vim 、显示行号等的动作则是在此模式中达成的！\nVI/VIM模式间转换一般模式+i=&gt;编辑模式        编辑模式+ESC=&gt;一般模式\n一般模式+：=&gt;命令模式      命令模式+ESC=&gt;一般模式\n\n\n\n\n命令\n功能\n\n\n\n\n:w\n保存\n\n\n:q\n退出\n\n\n:!\n强制执行\n\n\n/要查找的词\nn 查找下一个，N 往上查找\n\n\n? 要查找的词\nn是查找上一个，shift+n是往下查找\n\n\n:set nu\n显示行号\n\n\n:set nonu\n关闭行号\n\n\n\n\n\n\n\n\n语法\n功能描述\n\n\n\n\nyy\n复制光标当前一行\n\n\ny数字y\n复制一段（从第几行到第几行）\n\n\np\n箭头移动到目的行粘贴\n\n\nu\n撤销上一步\n\n\ndd\n删除光标当前行\n\n\nd数字d\n删除光标（含）后多少行\n\n\nx\n删除一个字母，相当于del\n\n\nX\n删除一个字母，相当于Backspace\n\n\nyw\n复制一个词\n\n\ndw\n删除一个词\n\n\nshift+^\n移动到行头\n\n\nshift+$\n移动到行尾\n\n\n1+shift+g\n移动到页头，数字\n\n\nshift+g\n移动到页尾\n\n\n数字N+shift+g\n移动到目标行\n\n\n\n\n系统管理操作和远程操作# 查看网络IP和网关\nifconfig\n# 配置网络ip地址\n# 配置主机名\n# 防火墙配置\n\n# 测试主机之间网络连通性\nping 目的主机\n\n\n\n\n关机重启命令\n\n\n\n\n\nshutdown  –h  now\n立该进行关机\n\n\nshutdown  –r   now\n现在重新启动计算机\n\n\nhalt\n立该进行关机\n\n\nreboot\n现在重新启动计算机\n\n\nsync\n把内存的数据同步到磁盘.\n\n\n\n\n切换成系统管理员身份.: su - 用户名\n注意细节:不管是重启系统还是关闭系统，首先要运行sync命令，把内存中的数据写到磁盘中\n用户管理Linux系统是一个多用户多任务的操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。\n用户组:类似于角色，系统可以对有共性的多个用户进行统一的管理。\n#添加用户\nuseradd [-d 家目录] [-g 用户组]  用户名\n#指定/修改密码\npasswd    用户名   \n#删除用户\nuserdel   用户名\n#查询用户信息\nid  用户名\n#切换用户\nsu  –  切换用户名\n#查看当前用户/登录用户\nwhoami/ who am I\n#新增组\ngroupadd 组名\n#修改用户的组\nusermod  –g 新的组名 用户名\n#查看文件/目录所在组/所有者\nls –ahl\n#修改文件所有者\nchown 用户名 文件名\n#修改文件所在的组\nchgrp 组名 文件名\n\n\n\n细节说明从权限高的用户切换到权限低的用户，不需要输入密码，反之需要。当需要返回到原来用户时，使用exit指令如果 su – 没有带用户名，则默认切换到root用户\n\n\n\n\n\n用户和组的相关文件\n含义\n每行含义\n\n\n\n\n/etc/passwd\n用户（user）的配置文件，记录用户的各种信息\n用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录Shell\n\n\n/etc/shadow\n口令的配置文件\n登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:保留\n\n\n/etc/group\n组(group)的配置文件，记录Linux包含的组的信息\n组名:口令:组标识号:组内用户列表\n\n\n\n\n\n\n\n\n运行级别\n\n\n\n级别\n\n\n\n\n\n0\n关机\n\n\n1\n单用户 [类似安全模式， 这个模式可以帮助找回root密码]\n\n\n2\n多用户状态没有网络服务\n\n\n3\n多用户状态有网络服务 [使用]\n\n\n4\n系统未使用保留给用户\n\n\n5\n图形界面 【】\n\n\n6\n系统重启\n\n\n\n\n\n\n\n常用运行级别是3和5 ，要修改默认的运行级别可改文件\n/etc/inittab的id:5:initdefault:这一行中的数字\n命令：init [012356] https协议\n找回root密码⭐1.启动时-&gt;快速输入enter-&gt;输入e-&gt; 进入到编辑界面-&gt; 选择中间有kernel 项-&gt; 输入e(edit)-&gt; 在该行的最后写入 1 [表示修改内核，临时生效]-&gt; 输入enter-&gt; 输入b [boot]-&gt; 进入到单用模式 【这里就可以做补救工作】\n常用命令\n\n\n\n帮助命令\n\n\n\n\n\nman [命令或配置文件]\n获得帮助信息\n\n\nhelp 命令\n获得shell内置命令的帮助信息\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n文件目录美命令\n\n\n\n\n\n\npwd\n显示当前工作目录的绝对路径\n\n\n\nls  [选项]\n-a ：显示当前目录所有的文件和目录，包括隐藏的 (文件名以.开头就是隐藏)。-l  ：以列表的方式显示信息-h  : 显示文件大小时，以 k , m, G单位显示\n\n\n\ncd [参数]\ncd ~  或者cd ：回到自己的家目录cd ..  回到当前目录的上一级目录\n\n\n\nmkdir 文件名\n创建目录-p ：创建多级目录\nmkdir -p /home/animal/tiger\n\n\nrmdir 文件名\n删除空目录\n\n\n\ntouch 文件名\n创建空文件\ntouch hello.txt\n\n\ncp [参数] 源文件 目的目录\n拷贝文件到指定目录-r ：递归复制整个文件夹\n\n\n\nrm [选项] 文件/目\n-r ：递归删除整个文件夹-f ： 强制删除不提示-v:    显示指令的详细执行过程\nrm –rf /home/bbb\n\n\nmv\n移动文件与目录：mv /temp/movefile /targetFolder 重命名 ：mv oldNameFile newNameFile\n\n\n\ncat [选项] 要查看的文件\n查看文件内容-n ：显示行号\ncat a.txt \\\nmore\n\n\nmore\n按页显示文本内容\n\n\n\nless\n分屏查看文件内容\n\n\n\necho [选项] [内容]\n输出内容到控制台 -e： 支持反斜线控制的字符转换\n\n\n\nhead\n显示文件头部内容-n : 查看前几行内容\n\n\n\ntai\n输出文件中尾部的内容-n : 查看前几行内容\n\n\n\n&gt;\n输出重定向，覆盖写\nls -l &gt;文件\n\n\n&gt;&gt;\n列表的内容追加到文件\nls -al &gt;&gt;文件\n\n\nln\n软链接，类似于windows里的快捷方式rm -rf houzi （删除软连接）\nln -s [原文件或目录] [软链接名]\n\n\nhistory\n查看已经执行过历史命令\n\n\n\n\n\n\n\n\nmore 操作\n功能说明\n\n\n\n\n空白键 (space)\n代表向下翻一页；\n\n\nEnter\n代表向下翻『一行』；\n\n\nq\n代表立刻离开 more ，不再显示该文件内容。\n\n\nCtrl+F\n向下滚动一屏\n\n\nCtrl+B\n返回上一屏\n\n\n=\n输出当前行的行号\n\n\n:f\n输出文件名和当前行的行号\n\n\n\n\n\n\n\n\nless  操作\n功能说明\n\n\n\n\n空白键\n向下翻动一页；\n\n\n[pagedown]\n向下翻动一页\n\n\n[pageup]\n向上翻动一页；\n\n\n/字串\n向下搜寻『字串』的功能；n：向下查找；N：向上查找；\n\n\n?字串\n向上搜寻『字串』的功能；n：向上查找；N：向下查找；\n\n\nq\n离开 less 这个程序；\n\n\n\n\n\n\n\n\necho  控制字符\n作用\n\n\n\n\n\\\n输出\\本身\n\n\n\\n\n换行符\n\n\n\\t\n制表符，也就是Tab键\n\n\n\n\n时间日期类命令1) date （功能描述：显示当前时间）2) date +%Y （功能描述：显示当前年份）3) date +%m （功能描述：显示当前月份）4) date +%d （功能描述：显示当前是哪一天）5) date “+%Y-%m-%d %H:%M:%S”（功能描述：显示年月日时分秒）6) date -s 字符串时间 （设定系统时间）7) cal 查看日历\n权限类命令-rwxrw-r— 1 root police 1213 Feb 2 09:39 abc.txt\n0-9位说明\n\n第0位确定文件类型(说明: -:普通文件, d:目录，l : 连接文件, c: 字符设备文件[键盘,鼠标] b: 块设备文件[硬盘] )\n第1-3位确定所有者（该文件的所有者）拥有该文件的权限。R: 读 ， w : 写权限 x: 执行权限 \n第4-6位确定所属组（同用户组的）拥有该文件的权限\n第7-9位确定其他用户拥有该文件的权限\n1: 如果是文件，表示硬链接的数目， 如果是目录，则表示有多少个子目录\n1213： 表示文件大小，如果是目录，则统一为 4096\n\nrwx作用到文件⭐\n1) [ r ]代表可读(read): 可以读取,查看2) [ w ]代表可写(write): 可以修改,但是不代表可以删除该文件,删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件.3) [ x ]代表可执行(execute):可以被执行  \nrwx作用到目录⭐\n1) [ r ]代表可读(read): 可以读取，ls查看目录内容2) [ w ]代表可写(write): 可以修改,目录内创建+删除+重命名目录3) [ x ]代表可执行(execute):可以进入该目录  \n变更权限第一种方式：+ 、-、= u:所有者  g:所有组  o:其他人  a:所有人(u、g、o的总和)\n  1) chmod   u=rwx,g=rx,o=x   文件、目录 【表示：给所有者rwx, 给所在组的用户 rx, 给其他人 x】  2) chmod   o+w    文件、目录 【表示：给其它用户增加w 的权限】  3) chmod   a-x    文件、目录    【表示：给所有用户 去掉 x权限】\n第二种方式：通过数字变更权限 r=4 w=2 x=1   rwx=4+2+1=7 chmod u=rwx,g=rx,o=x    文件、目录相当于 chmod   751  文件、目录\n修改文件所有者chown  newowner  file  改变文件的所有者chown  newowner:newgroup  file  改变用户的所有者和所有组-R   如果是目录 则使其下所有子文件或目录递归生效\n修改文件所在组chgrp newgroup file \n搜索查找类命令\nfind：find指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件或者目录显示在终端。\n\n\n\n\n选项\n功能\n\n\n\n\n-name&lt;查询方式&gt;\n按照指定的文件名查找模式查找文件 , 可以使用通配符 * ？\n\n\n-user&lt;用户名&gt;\n查找属于指定用户名所有文件\n\n\n-size&lt;文件大小&gt;\n按照指定的文件大小查找文件。\n\n\n\n\nfind /home  -name hello.txt \nlocate：快速定位文件路径。locate hello.txt\ngrep：过滤查找 \n\n\n\n\n选项\n功能\n\n\n\n\n-n\n显示匹配行及行号。\n\n\n-i\n忽略字母大小写\n\n\n\n\ngrep –n  if /etc/profile [在/etc/profile 中查找 if ,并显示行，区别大小写]\n\n\n\n\n压缩和解压类命令\n\n\n\n\n\ngzip 文件\n压缩文件，只能将文件压缩为*.gz文件\n\n\ngunzip 文件.gz\n解压缩文件命令\n\n\nzip   [选项] XXX.zip\n压缩文件和目录-r：递归压缩，即压缩目录\n\n\nunzip [选项] XXX.zip\n解压缩文件-d&lt;目录&gt; ：指定解压后文件的存放目录\n\n\ntar\n打包指令\n\n\n\n\n\n\n\n\ntar 选项\n功能\n\n\n\n\n-c\n产生.tar打包文件\n\n\n-v\n显示详细信息\n\n\n-f\n指定压缩后的文件名\n\n\n-z\n打包同时压缩\n\n\n-x\n解包.tar文件\n\n\n\n\n# 压缩多个文件, 将/homne/a1.txt 和/homne/a2.txt 压缩成 a.tar.gz\ntar -zcvf a.tar.gz al.txt a2.txt [注意， 路径要写清楚]\n# 将/home 的文件夹压缩成myhome.tar.gz\ntar -zcvf myhome.tar:gz /home/ [注意， 路径写清楚]\n# 将 a.tar.gz 解压到当前目录\ntar -zxvf a.tar.gz\n# 将myhome.tar:gz解压到/opt/tmp2 目录下[ -C ]\ntar -zxvf myhome.tar.gz _C /opt/tmp2 [注 意; /opt/tmp2事先需要创建好]\n磁盘分区类\n\n\n\n磁盘分区类\n\n\n\n\n\ndf 选项\n列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况  -h    以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；\n\n\nfdisk -l\n查看磁盘分区详情-l 显示所有硬盘的分区列表\n\n\nmount 参数\n挂载设备\n\n\numount 设备文件名或挂载点\n卸载设备\n\n\n\n\n\n\n\n\nmount   参数\n功能\n\n\n\n\n-t vfstype\n指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。常用类型有：  光盘或光盘镜像：iso9660  DOS fat16文件系统：msdos  Windows 9x fat32文件系统：vfat  Windows NT ntfs文件系统：ntfs  Mount Windows文件网络共享：smbfs  UNIX(LINUX) 文件网络共享：nfs\n\n\n-o options\n主要用来描述设备或档案的挂接方式。常用的参数有：  loop：用来把一个文件当成硬盘分区挂接上系统  ro：采用只读方式挂接设备  rw：采用读写方式挂接设备  　 iocharset：指定访问文件系统所用字符集\n\n\ndevice\n要挂接(mount)的设备\n\n\ndir\n设备在系统上的挂接点(mount point)\n\n\n\n\n进程线程类命令\n\n\n\n进程线程类\n\n\n\n\n\nps\n查看当前系统进程状态\n\n\nps aux  \\\ngrep xxx\n查看系统中所有进程\n\n\nps -ef \\\ngrep xxx\n可以查看子父进程之间的关系\n\n\n\n\n\n\n\n\n\n\n\nps  选项\n功能\n\n\n\n\n-a\n选择所有进程\n\n\n-u\n显示所有用户的所有进程\n\n\n-x\n显示没有终端的进程\n\n\n\n\n功能说明（1）ps aux显示信息说明USER：该进程是由哪个用户产生的PID：进程的ID号%CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；VSZ：该进程占用虚拟内存的大小，单位KB；RSS：该进程占用实际物理内存的大小，单位KB；TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台START：该进程的启动时间TIME：该进程占用CPU的运算时间，注意不是系统时间COMMAND：产生此进程的命令名（2）ps -ef显示信息说明UID：用户IDPID：进程IDPPID：父进程IDC：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高STIME：进程启动的时间TTY：完整的终端名称TIME：CPU时间CMD：启动进程所用的命令和参数\n\n\n\n\nkill 终止进程\n\n\n\n\n\nkill [选项] 进程号\n通过进程号杀死进程-9 表示强迫进程立即停止\n\n\nkillall 进程名称\n通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用\n\n\n\n\n\n\n\n\npstree 查看进程树\n\n\n\n\n\npstree [选项]\n\n\n\n-p\n显示进程的PID\n\n\n-u\n显示进程的所属用户\n\n\n\n\n\n\n\n\ntop 查看系统健康状态\n\n\n\n\n\n\ntop [选项]\n\n\n\n\n\n-d 秒数\n指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：\n\n\n\n-i\n使top不显示任何闲置或者僵死进程。\n\n\n\n-p\n通过指定监控进程ID来仅仅监控某个进程的状态。\n\n\n\n\n\n\n\n\ntop 操作\n功能\n\n\n\n\nP\n以CPU使用率排序，默认就是此项\n\n\nM\n以内存的使用率排序\n\n\nN\n以PID排序\n\n\nq\n退出top\n\n\n\n\n查询结果字段解释\n第一行信息为任务队列信息\n\n\n\n\n内容\n说明\n\n\n\n\n12:26:46\n系统当前时间\n\n\nup 1 day, 13:32\n系统的运行时间，本机已经运行1天  13小时32分钟\n\n\n2 users\n当前登录了两个用户\n\n\nload average:   0.00, 0.00, 0.00\n系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。\n\n\n\n\n第二行为进程信息\n\n\n\n\nTasks: 95 total\n系统中的进程总数\n\n\n\n\n1 running\n正在运行的进程数\n\n\n94 sleeping\n睡眠的进程\n\n\n0 stopped\n正在停止的进程\n\n\n0 zombie\n僵尸进程。如果不是0，需要手工检查僵尸进程\n\n\n\n\n第三行为CPU信息\n\n\n\n\nCpu(s): 0.1%us\n用户模式占用的CPU百分比\n\n\n\n\n0.1%sy\n系统模式占用的CPU百分比\n\n\n0.0%ni\n改变过优先级的用户进程占用的CPU百分比\n\n\n99.7%id\n空闲CPU的CPU百分比\n\n\n0.1%wa\n等待输入/输出的进程的占用CPU百分比\n\n\n0.0%hi\n硬中断请求服务占用的CPU百分比\n\n\n0.1%si\n软中断请求服务占用的CPU百分比\n\n\n0.0%st\nst（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。\n\n\n\n\n第四行为物理内存信息\n\n\n\n\nMem:  625344k total\n物理内存的总量，单位KB\n\n\n\n\n571504k used\n已经使用的物理内存数量\n\n\n53840k free\n空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了\n\n\n65800k buffers\n作为缓冲的内存数量\n\n\n\n\n第五行为交换分区（swap）信息\n\n\n\n\nSwap:  524280k total\n交换分区（虚拟内存）的总大小\n\n\n\n\n0k used\n已经使用的交互分区的大小\n\n\n524280k free\n空闲交换分区的大小\n\n\n409280k cached\n作为缓存的交互分区的大小\n\n\n\n\n\n\n\n\nnetstat\n示网络统计信息和端口占用情况\n\n\n\n\n\nnetstat -anp \\\ngrep 进程号\n查看该进程网络信息\n\n\n\nnetstat -nlp \\\ngrep 端口号\n查看网络端口号占用情况\n\n\n\n\n-n\n拒绝显示别名，能显示数字的全部转化成数字\n\n\n\n-l\n仅列出有在listen（监听）的服务状态\n\n\n\n\ncrond系统定时任务\n\n\n\ncrond 服务管理\n\n\n\n\n\n\nservice crond restart\n重新启动crond服务\n\n\n\ncrontab [选项]\n-e\n编辑crontab定时任务\n\n\n\n-l\n查询crontab任务\n\n\n\n-r\n删除当前用户所有的crontab任务\n\n\n\n\n（1）进入crontab编辑界面。会打开vim编辑你的工作。\n*     执行的任务\n\n\n\n\n项目\n含义\n范围\n\n\n\n\n第一个“*”\n一小时当中的第几分钟\n0-59\n\n\n第二个“*”\n一天当中的第几小时\n0-23\n\n\n第三个“*”\n一个月当中的第几天\n1-31\n\n\n第四个“*”\n一年当中的第几月\n1-12\n\n\n第五个“*”\n一周当中的星期几\n0-7（0和7都代表星期日）\n\n\n\n\n（2）特殊符号\n\n\n\n\n特殊符号\n含义\n\n\n\n\n*\n代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。\n\n\n，\n代表不连续的时间。比如“0 8,12,16   * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令\n\n\n-\n代表连续的时间范围。比如“0 5     1-6命令”，代表在周一到周六的凌晨5点0分执行命令\n\n\n*/n\n代表每隔多久执行一次。比如“/10      * 命令”，代表每隔10分钟就执行一遍命令\n\n\n\n\n（3）特定时间执行命令\n\n\n\n\n时间\n含义\n\n\n\n\n45 22   * 命令\n在22点45分执行命令\n\n\n0 17   1 命令\n每周1 的17点0分执行命令\n\n\n0 5 1,15   命令\n每月1号和15号的凌晨5点0分执行命令\n\n\n40 4   1-5 命令\n每周一到周五的凌晨4点40分执行命令\n\n\n/10 4    命令\n每天的凌晨4点，每隔10分钟执行一次命令\n\n\n0 0 1,15 * 1 命令\n每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。\n\n\n\n\n# 每隔1分钟，向/root/bailongma.txt文件中添加一个11的数字\n*/1 * * * * /bin/echo ”11” &gt;&gt; /root/bailongma.txt\n\n软件包管理RPMRPM（RedHat Package Manager），RedHat软件包管理工具，类似windows里面的setup.exe 是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。RPM包的名称格式Apache-1.3.23-11.i386.rpm\n\n“apache” 软件名称\n“1.3.23-11”软件的版本号，主版本和此版本\n“i386”是软件所运行的硬件平台，Intel 32位微处理器的统称\n“rpm”文件扩展名，代表RPM包\n\n\n\n\n\nRPM命令\n\n\n\n\n\n\nrpm -qa\n查询所安装的所有rpm软件包\n\n\n\nrpm -e RPM软件包\n卸载软件包\n\n\n\nrpm -e —nodeps 软件包\n卸载软件时，不检查依赖。\n\n\n\nrpm -ivh RPM包全名\n-i\n-i=install，安装\n\n\n\n-v\n-v=verbose，显示详细信息\n\n\n\n-h\n-h=hash，进度条\n\n\n\n—nodeps\n—nodeps，不检测依赖进度\n\n\n\n\n# 由于软件包比较多，一般都会采取过滤。rpm -qa | grep rpm软件包\n[root@hadoop101 Packages]# rpm -qa |grep firefox \nfirefox-45.0.1-1.el6.centos.x86_64\n# 卸载 firefox\n[root@hadoop101 Packages]# rpm -e firefox\n# 安装 firefox\n[root@hadoop101 Packages]# rpm -ivh firefox-45.0.1-1.el6.centos.x86_64.rpm \nwarning: firefox-45.0.1-1.el6.centos.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY\nPreparing...                ########################################### [100%]\n   1:firefox                ########################################### [100%]\nYUMYUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。\n\n\n\n\nYUM的常用命令\n\n\n\n\n\n\nyum [选项] [参数]\n选项\n功能\n\n\n\n-y\n对所有提问都回答“yes”\n\n\n\n参数\n功能\n\n\n\ninstall\n安装rpm软件包\n\n\n\nupdate\n更新rpm软件包\n\n\n\ncheck-update\n检查是否有可用的更新rpm软件包\n\n\n\nremove\n删除指定的rpm软件包\n\n\n\nlist\n显示软件包信息\n\n\n\nclean\n清理yum过期的缓存\n\n\n\ndeplist\n显示yum软件包的所有依赖关系\n\n\n\n\n# 采用yum方式安装firefox\n[root@hadoop101 ~]#yum -y install firefox.x86_64\n\n修改网络YUM源\n#前提条件linux系统必须可以联网\n#在Linux环境中访问该网络地址：http://mirrors.163.com/.help/centos.html，在使用说明中点击CentOS6-&gt;再点击保存\n#查看文件保存的位置，在打开的终端中输入如下命令，就可以找到文件的保存位置。\n[atguigu@hadoop101 下载]$ pwd\n/home/atguigu/下载\n#2.替换本地yum文件\n#把下载的文件移动到/etc/yum.repos.d/目录\n[root@hadoop101 下载]# mv CentOS6-Base-163.repo /etc/yum.repos.d/\t\n#进入到/etc/yum.repos.d/目录\n[root@hadoop101 yum.repos.d]# pwd\n/etc/yum.repos.d\n#用CentOS6-Base-163.repo替换CentOS-Base.repo\n[root@hadoop101 yum.repos.d]# mv CentOS6-Base-163.repo  CentOS-Base.repo\n#3．安装命令\n[root@hadoop101 yum.repos.d]#yum clean all\n[root@hadoop101 yum.repos.d]#yum makecache\nyum makecache就是把服务器的包信息下载到本地电脑缓存起来\n#4．测试\n[root@hadoop101 yum.repos.d]#yum list | grep firefox\n[root@hadoop101 ~]#yum -y install firefox.x86_64\n\n常见错误及解决方案百度&amp;考满分问题：Linux常用命令参考答案：find、df、tar、ps、top、netstat等。（尽量说一些高级命令）10.2 瓜子二手车问题：Linux查看内存、磁盘存储、io 读写、端口占用、进程等命令答案：1、查看内存：top2、查看磁盘存储情况：df -h3、查看磁盘IO读写情况：iotop（需要安装一下：yum install iotop）、iotop -o（直接查看输出比较高的磁盘读写程序）4、查看端口占用情况：netstat -tunlp | grep 端口号5、查看进程：ps aux\n","slug":"B0-Linux基础","date":"2021-11-16T08:35:41.000Z","categories_index":"大数据","tags_index":"Linux","author_index":"YFR718"},{"id":"d627fd9c1bed4de81fff7ae3f54e977a","title":"Hadoop基础","content":"0. 大数据概念大数据（Big Data）：指无法在==一定时间范围内==用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的==海量、高增长率和多样化==的==信息资产==。大数据主要解决，海量数据的采集、存储和分析计算问题。\n⭐大数据特点（4V） ：\n\nVolume（大量）\nVelocity（高速）\nVariety（多样）\nValue（低价值密度）\n\n1. Hadoop 概述\nHadoop是什么1）Hadoop是一个由Apache基金会所开发的分布式系统基础架构。2）主要解决，海量数据的存储和海量数据的分析计算问题。3）广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。\n\nHadoop发展历史\n1）Hadoop创始人Doug\n2）2001年年底Lucene成为Apache基金会的一个子项目。3）对于海量数据的场景，Lucene框架面对与Google同样的困难，存储海量数据困难，检索海量速度慢。4）学习和模仿Google解决这些问题的办法 ：微型版Nutch。5）可以说Google是Hadoop的思想之源（Google在大数据方面的三篇论文）\nGFS —-&gt;HDFS、Map-Reduce —-&gt;MR、BigTable —-&gt;HBase6）2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。\n7）2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。8）2006 年 3 月份，Map-Reduce和Nutch Distributed File System （NDFS）分别被纳入到 Hadoop 项目中，Hadoop就此正式诞生，标志着大数据时代来临。\n9）名字来源于Doug Cutting儿子的玩具大象\n\n\nHadoop 优势（4 高）\n高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。\n高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。\n高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。\n高容错性：能够自动将失败的任务重新分配。\n\nHadoop 组成（面试重点） ⭐⭐\n\n\n\nHadoop1.x\nHadoop2.x\n\n\n\n\nMapReduce（计算+资源调度）\nMapReduce（计算）\n\n\n\nYarn（资源调度）\n\n\nHDFS（数据存储）\nHDFS（数据存储）\n\n\nCommon（辅助工具）\nCommon（辅助工具）\n\n\n\n\n在 Hadoop1.x 时代 ， Hadoop中的MapReduce同时处理业务逻辑运算和资源的调度，耦合性较大。在Hadoop2.x时代，增加了Yarn。Yarn只负责资 源 的 调 度 ， MapReduce 只负责运算。在Hadoop3.x在组成上没有变化。\nHDFS 架构概述Hadoop Distributed File System，简称 HDFS，是一个分布式文件系统。 \nHDFS架构组成1）NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。2）DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。3）Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份。\nYARN 架构概述Yet Another Resource Negotiator 简称 YARN ，另一种资源协调者，是 Hadoop 的资源管理器。 \nYarn架构的组成\n1）    ResourceManager（RM）：整个集群资源（内存、CPU等）的老大2）    NodeManager（NM）：单个节点服务器资源老大3）    ApplicationMaster（AM）：单个任务运行的老大4）    Container：容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络\n\nMapReduce 架构概述MapReduce 将计算过程分为两个阶段：Map 和Reduce \n1）Map 阶段并行处理输入数据2）Reduce 阶段对 Map 结果进行汇总\n大数据技术生态体系HDFS、YARN、MapReduce三者关系\n\n大数据技术生态体系\n\n图中涉及的技术名词解释如下：1）Sqoop：Sqoop 是一款开源的工具，主要用于在Hadoop、Hive 与传统的数据库（MySQL）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop 的HDFS 中，也可以将HDFS 的数据导进到关系型数据库中。2）Flume：Flume 是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统， Flume 支持在日志系统中定制各类数据发送方，用于收集数据；3）Kafka：Kafka 是一种高吞吐量的分布式发布订阅消息系统；4）Spark：Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数据进行计算。5）Flink：Flink 是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。6）Oozie：Oozie 是一个管理Hadoop 作业（job）的工作流程调度管理系统。7）Hbase：HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。8）Hive：Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开发专门的MapReduce 应用，十分适合数据仓库的统计分析。9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。 \n推荐系统框架图 \n\n2. Hadoop 运行环境搭建（开发重点）基础环境准备\n安装VMware、xshell\n配置静态IP、hostname、hosts\n创建用户，设置权限，克隆环境\n安装JDK、安装Hadoop\n\nHadoop 运行模式本地运行模式啥都不用配置，直接运行即可\n完全分布式运行模式（开发重点）⭐⭐⭐\n配置台虚拟机\n设置免密通信、shell脚本分发安装的JDK、Hadoop环境\n配置集群、历史服务器、日志服务器\nshell控制集群启停和jps情况\n\n集群部署规划：\n\nNameNode和 SecondaryNameNode不要安装在同一台服务器\nResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在 配置在 同一台机器上。\n\n\n\n\n\n\nhadoop102\nhadoop103\nhadoop104\n\n\n\n\nHDFS\nNameNodeDataNode\nDataNode\nSecondaryNameNodeDataNode\n\n\nYARN\nNodeManager\nResourceManagerNodeManager\nNodeManager\n\n\n\n\n常用端口号说明⭐⭐\n\n\n\n端口名称\nHadoop2.x\nHadoop3.x\n\n\n\n\nNameNode内部通信端口\n8020 / 9000\n8020 / 9000/9820\n\n\nNameNode HTTP UI\n50070\n9870\n\n\nMapReduce查看执行任务端口\n8088\n8088\n\n\n历史服务器通信端口\n19888\n19888\n\n\n\n\n\n\n\n\n\nweb\n服务\n\n\n\n\nHDFS\n9870\n8020\n\n\nYARN\n8088\n8032\n\n\nhistory\n19888\n10020\n\n\n\n\n常用配置文件⭐⭐\n\n\n\n配置文件\n2.x/3.x\n配置内容\n\n\n\n\n核心配置文件\ncore-site.xml\nNameNode的地址指定hadoop数据的存储目录\n\n\nHDFS配置文件\nhdfs-site.xml\nnn web端访问地址2nn web端访问地址\n\n\nYARN配置文件\nyarn-site.xml\n指定MR走shuffleResourceManager的地址环境变量的继承\n\n\nMapReduce配置文件\nmapred-site.xml\n指定MapReduce程序运行在Yarn上\n\n\n\nslaves/workers\n\n\n\n\n配置 core-site.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;configuration&gt;\n    &lt;!-- 指定NameNode的地址 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://hadoop102:8020&lt;/value&gt;\n    &lt;/property&gt;\n    \n    &lt;!-- 指定hadoop数据的存储目录 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n        &lt;value&gt;/opt/module/hadoop-3.1.3/data&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;\n        &lt;value&gt;atguigu&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n\n配置 hdfs-site.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;configuration&gt;\n\t&lt;!-- nn web端访问地址--&gt;\n\t&lt;property&gt;\n        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n        &lt;value&gt;hadoop102:9870&lt;/value&gt;\n    &lt;/property&gt;\n\t&lt;!-- 2nn web端访问地址--&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;\n        &lt;value&gt;hadoop104:9868&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n\n配置 yarn-site.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n&lt;configuration&gt;\n    &lt;!-- 指定MR走shuffle --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 指定ResourceManager的地址--&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n        &lt;value&gt;hadoop103&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 环境变量的继承 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;\n        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n\n\n配置 mapred-site.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;\n\n&lt;configuration&gt;\n\t&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n\n配置历史服务器：配置 mapred-site.xml\n&lt;!-- 历史服务器端地址 --&gt;\n&lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;\n    &lt;value&gt;hadoop102:10020&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;!-- 历史服务器web端地址 --&gt;\n&lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;\n    &lt;value&gt;hadoop102:19888&lt;/value&gt;\n&lt;/property&gt;\n\n配置日志的聚集：配置 yarn-site.xml\n&lt;!-- 开启日志聚集功能 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;!-- 设置日志聚集服务器地址 --&gt;\n&lt;property&gt;  \n    &lt;name&gt;yarn.log.server.url&lt;/name&gt;  \n    &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt;\n&lt;/property&gt;\n&lt;!-- 设置日志保留时间为7天 --&gt;\n&lt;property&gt;\n    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;\n    &lt;value&gt;604800&lt;/value&gt;\n&lt;/property&gt;\n\n3. HDFSHDFS概述HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。\nHDFS的使用场景：适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变。\nHDFS优缺点HDFS优点\n1）    高容错性➢    数据自动保存多个副本。它通过增加副本的形式，提高容错性。\n➢    某一个副本丢失以后，它可以自动恢复。\n2）    适合处理大数据➢    数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据；➢    文件规模：能够处理百万规模以上的文件数量，数量相当之大。3）    可构建在廉价机器上，通过多副本机制，提高可靠性。\nHDFS缺点\n1）    不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。2）    无法高效的对大量小文件进行存储。➢    存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；➢    小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。\n3）    不支持并发写入、文件随机修改。➢    一个文件只能有一个写，不允许多个线程同时写；\n➢    仅支持数据append（追加），不支持文件的随机修改。\nHDFS组成架构\n\n\n\nHDFS组成架构\n\n\n\n\n\nNameNode（nn）\n就是Master，它是一个主管、管理者。\n\n\n\n（1）    管理HDFS的名称空间；（2）    配置副本策略；（3）    管理数据块（Block）映射信息；（4）    处理客户端读写请求。\n\n\nDataNode\n就是Slave。NameNode下达命令，DataNode执行实际的操作。\n\n\n\n（1）    存储实际的数据块；（2）    执行数据块的读/写操作。\n\n\nClient\n就是客户端。\n\n\n\n（1）    文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传；（2）    与NameNode交互，获取文件的位置信息；（3）    与DataNode交互，读取或者写入数据；（4）    Client提供一些命令来管理HDFS，比如NameNode格式化；\n\n\nSecondary NameNode\n并非NameNode的热备份，当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。\n\n\n\n（1）    辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode ；（2）    在紧急情况下，可辅助恢复NameNode。\n\n\n\n\nHDFS 文件块大小（面试重点） ⭐⭐HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数( dfs.blocksize）来规定，默认大小在Hadoop2.x/3.x版本中是128M，1.x版本中是64M。\n1）集群中的block\n2）如果寻址时间约为10ms，即查找到目标block的时间为10ms。3）寻址时间为传输时间的1%时，则为最佳状态。（专家）因此，传输时间=10ms/0.01=1000ms=1s4）而目前磁盘的传输速率普遍为100MB/s。\n5） block大小=1s*100MB/s=100MB\n思考：为什么块的大小不能设置太小，也不能设置太大？\n（1）    HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；（2）    如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。\n总结：HDFS块的大小设置主要取决于==磁盘传输速率==。\nHDFS 的Shell 操作hadoop fs 具体命令  OR  hdfs dfs 具体命令 两个是完全相同的。 \n// 命令大全\n[-cat [-ignoreCrc] &lt;src&gt; ...]\n[-chgrp [-R] GROUP PATH...]\n[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]\n[-chown [-R] [OWNER][:[GROUP]] PATH...]\n[-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]\n[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]\n[-count [-q] &lt;path&gt; ...]\n[-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;]\n[-df [-h] [&lt;path&gt; ...]]\n[-du [-s] [-h] &lt;path&gt; ...]\n[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]\n[-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]\n[-help [cmd ...]]\n[-ls [-d] [-h] [-R] [&lt;path&gt; ...]]\n[-mkdir [-p] &lt;path&gt; ...]\n[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]\n[-moveToLocal &lt;src&gt; &lt;localdst&gt;]\n[-mv &lt;src&gt; ... &lt;dst&gt;]\n[-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]\n[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]\n[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]&lt;acl_spec&gt; &lt;path&gt;]]\n[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]\n[-stat [format] &lt;path&gt; ...]\n[-tail [-f] &lt;file&gt;]\n[-test -[defsz] &lt;path&gt;]\n[-text [-ignoreCrc] &lt;src&gt; ...]\n\n\n\n\n常用命令实操\n\n\n\n\n\nsbin/start-dfs.shsbin/start-yarn.sh\n启动Hadoop集群\n\n\nhadoop fs -help rm\n输出这个命令参数\n\n\nhadoop fs -mkdir /a\n创建/sanguo文件夹\n\n\nhadoop fs -moveFromLocal ./a.txt /a\n从本地剪切粘贴到HDFS\n\n\nhadoop fs -copyFromLocal a.txt /a\n从本地文件系统中拷贝文件到HDFS路径去\n\n\nhadoop fs -put ./a.txt /sangauo\n等同于copyFromLocal，生产环境更习惯用put\n\n\nhadoop fs -appendToFile a.txt /a/a.txt\n追加一个文件到已经存在的文件末尾\n\n\nhadoop fs -copyToLocal /a/a.txt ./\n从HDFS拷贝到本地\n\n\nhadoop fs -get /a/a.txt ./a.txt\n等同于copyToLocal，生产环境更习惯用get\n\n\nhadoop fs -ls /a\n显示目录信息\n\n\nhadoop fs -cat /a/a.txt\n显示文件内容\n\n\nhadoop fs -chmod 666\n-chgrp、-chmod、-chown：\n\n\nhadoop fs -chown yfr:yfr  /a/a.txt\nLinux文件系统中的用法一样，修改文件所属权限\n\n\nhadoop fs -mkdir /a\n创建路径\n\n\nhadoop fs -cp /a/a.txt /b\n从HDFS的一个路径拷贝到HDFS的另一个路径\n\n\nhadoop fs -mv /a/a.txt /b\n在HDFS目录中移动文件\n\n\nhadoop fs -tail /a/a.txt\n显示一个文件的末尾1kb的数据\n\n\nhadoop fs -rm /a/a.txt\n删除文件或文件夹\n\n\nhadoop fs -rm -r /a\n删除文件或文件夹\n\n\nhadoop fs -du -s -h /a\n递归删除目录及目录里面内容\n\n\nhadoop fs -du -h /a\n统计文件夹的大小信息\n\n\nhadoop fs -setrep 10 /a/a.txt\n设置HDFS中文件的副本数量\n\n\n\n\n\n\n\n\n\n\n\nHDFS的API操作//HDFS文件上传\n@Test\npublic void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException {\n\n    // 1 获取文件系统\n    Configuration configuration = new Configuration();\n    configuration.set(\"dfs.replication\", \"2\");\n    FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\");\n\n    // 2 上传文件\n    fs.copyFromLocalFile(new Path(\"d:/sunwukong.txt\"), new Path(\"/xiyou/huaguoshan\"));\n\n    // 3 关闭资源\n    fs.close();\n｝\n//HDFS文件下载\n@Test\npublic void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{\n\n    // 1 获取文件系统\n    Configuration configuration = new Configuration();\n    FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\");\n    \n    // 2 执行下载操作\n    // boolean delSrc 指是否将原文件删除\n    // Path src 指要下载的文件路径\n    // Path dst 指将文件下载到的路径\n    // boolean useRawLocalFileSystem 是否开启文件校验\n    fs.copyToLocalFile(false, new Path(\"/xiyou/huaguoshan/sunwukong.txt\"), new Path(\"d:/sunwukong2.txt\"), true);\n    \n    // 3 关闭资源\n    fs.close();\n}\n//HDFS更名和移动\n@Test\npublic void testRename() throws IOException, InterruptedException, URISyntaxException{\n\n\t// 1 获取文件系统\n\tConfiguration configuration = new Configuration();\n\tFileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\"); \n\t\t\n\t// 2 修改文件名称\n\tfs.rename(new Path(\"/xiyou/huaguoshan/sunwukong.txt\"), new Path(\"/xiyou/huaguoshan/meihouwang.txt\"));\n\t\t\n\t// 3 关闭资源\n\tfs.close();\n}\n\n//HDFS删除文件和目录\n@Test\npublic void testDelete() throws IOException, InterruptedException, URISyntaxException{\n\n\t// 1 获取文件系统\n\tConfiguration configuration = new Configuration();\n\tFileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\");\n\t\t\n\t// 2 执行删除\n\tfs.delete(new Path(\"/xiyou\"), true);\n\t\t\n\t// 3 关闭资源\n\tfs.close();\n}\n//查看文件名称、权限、长度、块信息\n@Test\npublic void testListFiles() throws IOException, InterruptedException, URISyntaxException {\n\n\t// 1获取文件系统\n\tConfiguration configuration = new Configuration();\n\tFileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\");\n\n\t// 2 获取文件详情\n\tRemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(\"/\"), true);\n\n\twhile (listFiles.hasNext()) {\n\t\tLocatedFileStatus fileStatus = listFiles.next();\n\n\t\tSystem.out.println(\"========\" + fileStatus.getPath() + \"=========\");\n\t\tSystem.out.println(fileStatus.getPermission());\n\t\tSystem.out.println(fileStatus.getOwner());\n\t\tSystem.out.println(fileStatus.getGroup());\n\t\tSystem.out.println(fileStatus.getLen());\n\t\tSystem.out.println(fileStatus.getModificationTime());\n\t\tSystem.out.println(fileStatus.getReplication());\n\t\tSystem.out.println(fileStatus.getBlockSize());\n\t\tSystem.out.println(fileStatus.getPath().getName());\n\n\t\t// 获取块信息\n\t\tBlockLocation[] blockLocations = fileStatus.getBlockLocations();\n\t\tSystem.out.println(Arrays.toString(blockLocations));\n\t}\n\t// 3 关闭资源\n\tfs.close();\n}\n//HDFS文件和文件夹判断\n@Test\npublic void testListStatus() throws IOException, InterruptedException, URISyntaxException{\n\n    // 1 获取文件配置信息\n    Configuration configuration = new Configuration();\n    FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:8020\"), configuration, \"atguigu\");\n\n    // 2 判断是文件还是文件夹\n    FileStatus[] listStatus = fs.listStatus(new Path(\"/\"));\n\n    for (FileStatus fileStatus : listStatus) {\n\n        // 如果是文件\n        if (fileStatus.isFile()) {\n            System.out.println(\"f:\"+fileStatus.getPath().getName());\n        }else {\n            System.out.println(\"d:\"+fileStatus.getPath().getName());\n        }\n    }\n\n    // 3 关闭资源\n    fs.close();\n}\nHDFS的读写流程（面试重点）⭐⭐HDFS写数据流程\n（1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。（2）NameNode返回是否可以上传。（3）客户端请求第一个 Block上传到哪几个DataNode服务器上。（4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。（5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。（6）dn1、dn2、dn3逐级应答客户端。（7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。（8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。\n网络拓扑-节点距离计算节点距离：两个节点到达最近的共同祖先的距离总和。 \n例如，假设有数据中心 d1 机架 r1 中的节点 n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。 \nDistance(/d1/r1/n0, /d1/r1/n0)=0（同一节点上的进程）Distance(/d1/r1/n1, /d1/r1/n2)=2（同一机架上的不同节点）Distance(/d1/r2/n0, /d1/r3/n2)=4（同一数据中心不同机架上的节点）Distance(/d1/r2/n1, /d2/r4/n1)=6（不同数据中心的节点）\nHadoop3.1.3副本节点选择第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。第二个副本在另一个机架的随机一个节点第三个副本在第二个副本所在机架的随机节点\nHDFS的读数据流程\n（1）客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。（2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。（3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。（4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。\nNameNode和SecondaryNameNode思考：NameNode中的元数据是存储在哪里的？        首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，==元数据需要存放在内存中==。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在==磁盘中备份元数据的FsImage==。        这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件（只进行追加操作，效率很高）。==每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中==。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。        但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要==定期进行FsImage和Edits的合并==，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点==SecondaryNamenode，专门用于FsImage和Edits的合并==。\nNN和2NN工作机制\n1）第一阶段：NameNode启动（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。（2）客户端对元数据进行增删改的请求。（3）NameNode记录操作日志，更新滚动日志。（4）NameNode在内存中对元数据进行增删改。\n2）第二阶段：Secondary NameNode工作（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。（2）Secondary NameNode请求执行CheckPoint。（3）NameNode滚动正在写的Edits日志。（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。（6）生成新的镜像文件fsimage.chkpoint。（7）拷贝fsimage.chkpoint到NameNode。（8）NameNode将fsimage.chkpoint重新命名成fsimage。\nFsimage和Edits概念\n（1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。（2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。（3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字（4）每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。\n查看oiv 和oev 命令\n查看fsimage：hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径 \n查看edits：hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径\nDataNodeDataNode 工作机制\n（1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。（2）DataNode启动后向NameNode注册，通过后，周期性（6小时）的向NameNode上报所有的块信息。\n\nDN向NN汇报当前解读信息的时间间隔，默认6小时；\nDN扫描自己节点块信息列表的时间，默认6小时\n\n（3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。（4）集群运行中可以安全加入和退出一些机器。\nDataNode节点保证数据完整性的方法：（1）当DataNode读取Block的时候，它会计算CheckSum。（2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。（3）Client读取其他DataNode上的Block。（4）常见的校验算法crc（32），md5（128），sha1（160）（5）DataNode在其文件创建后周期验证CheckSum。\nDataNode掉线时限参数设置1、DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信2、NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。3、HDFS默认的超时时长为10分钟+30秒。4、如果定义超时时间为TimeOut，则超时时长的计算公式为：TimeOut = 2  dfs.namenode.heartbeat.recheck-interval + 10  dfs.heartbeat.interval。而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。\n4. MapReduceMapReduce概述MapReduce定义​        MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。\n​        MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。\n优缺点 优点1）MapReduce易于编程        它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。2）良好的扩展性        当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。3）高容错性        MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。4）适合PB级以上海量数据的离线处理        可以实现上千台服务器集群并发工作，提供数据处理能力。缺点1）不擅长实时计算        MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。2）不擅长流式计算        流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。3）不擅长DAG（有向无环图）计算        多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。\nMapReduce核心思想\n（1）分布式的运算程序往往需要分成至少2个阶段。（2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。（3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。（4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。总结：分析WordCount数据流走向深入理解MapReduce核心思想。\nMapReduce进程一个完整的MapReduce程序在分布式运行时有三类实例进程：（1）MrAppMaster：负责整个程序的过程调度及状态协调。（2）MapTask：负责Map阶段的整个数据处理流程。（3）ReduceTask：负责Reduce阶段的整个数据处理流程。\n常用数据序列化类型\n\n\n\nJava类型\nHadoop Writable类型\n\n\n\n\nBoolean\nBooleanWritable\n\n\nByte\nByteWritable\n\n\nInt\nIntWritable\n\n\nFloat\nFloatWritable\n\n\nLong\nLongWritable\n\n\nDouble\nDoubleWritable\n\n\nString\nText\n\n\nMap\nMapWritable\n\n\nArray\nArrayWritable\n\n\nNull\nNullWritable\n\n\n\n\nMapReduce编程规范\nMapper阶段(1)用户自定义的Mapper要继承自己的父类(2) Mapper的输入数据是KV对的形式(KV的类型可自定义)(3) Mapper中的业务逻辑写在map()方法中(4) Mapper的输 出数据是KV对的形式(KV的类型可自定义)(5) map()方 法(MapTask进程) 对每一个 调用- -次\n\nReducer阶 段(1) 用户自定义的Reducer要继承自己的父类(2) Reducer的输入数据类型对应Mapper的输出数据类型，也是KV(3) Reducer的业 务逻辑写在reduce()方法中(4) ReduceTask进程对每- -组相同k的组调用- -次reduce()方法\n\nDriver阶段相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象\n\nHadoop序列化序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 \n反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。\n为什么要序列化?        一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。为什么不用Java的序列化?        Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）。Hadoop序列化特点：（1）紧凑 ：高效使用存储空间。（2）快速：读写数据的额外开销小。（3）互操作：支持多语言的交互\n自定义bean对象实现序列化接口（Writable）（1）必须实现Writable接口（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造\npublic FlowBean() {\n\tsuper();\n}\n（3）重写序列化方法\n@Override\npublic void write(DataOutput out) throws IOException {\n\tout.writeLong(upFlow);\n\tout.writeLong(downFlow);\n\tout.writeLong(sumFlow);\n}\n（4）重写反序列化方法\n@Override\npublic void readFields(DataInput in) throws IOException {\n\tupFlow = in.readLong();\n\tdownFlow = in.readLong();\n\tsumFlow = in.readLong();\n}\n（5）注意反序列化的顺序和序列化的顺序完全一致（6）要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用。（7）如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。详见后面排序案例。\n@Override\npublic int compareTo(FlowBean o) {\n\t// 倒序排列，从大到小\n\treturn this.sumFlow &gt; o.getSumFlow() ? -1 : 1;\n}\nMapReduce框架原理\nInputFormat数据输入MapTask并行度决定机制\n数据块：Block是HDFS物理上把数据分成一块一块。数据块是HDFS存储数据单位。\n数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是MapReduce程序计算输入数据的单位，一个切片会对应启动一个MapTask。\n1)一个Job的Map阶段并行度由 客户端在提交Job时的切片数决定2)每一个Spli切片分配一个MapTask并行实例处理3)默认情况下，切大小=BlockSize4)切片时不考虑数据集整体,而是逐个针对每一个文件单独切片\nJob提交流程源码详解\nwaitForCompletion()\n\nsubmit();\n\n// 1建立连接\nconnect();\t\n// 1）创建提交Job的代理\nnew Cluster(getConfiguration());\n// 2）判断是本地运行环境还是yarn集群运行环境\ninitialize(jobTrackAddr, conf); \n\n// 2 提交job\nsubmitter.submitJobInternal(Job.this, cluster)\n\n// 1）创建给集群提交数据的Stag路径\nPath jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);\n\n// 2）获取jobid ，并创建Job路径\nJobID jobId = submitClient.getNewJobID();\n\n// 3）拷贝jar包到集群\ncopyAndConfigureFiles(job, submitJobDir);\t\nrUploader.uploadFiles(job, jobSubmitDir);\n\n// 4）计算切片，生成切片规划文件\nwriteSplits(job, submitJobDir);\nmaps = writeNewSplits(job, jobSubmitDir);\ninput.getSplits(job);\n\n// 5）向Stag路径写XML配置文件\nwriteConf(conf, submitJobFile);\nconf.writeXml(out);\n\n// 6）提交Job,返回提交状态\nstatus = submitClient.submitJob(jobId, submitJobDir.toString(),job.getCredentials());\n\nFileInputFormat切片源码解析\n(1)程序先找到你数据存储的目录。(2)开始遍历处理(规划切片)目录下的每一个文件(3)遍历第-个文件ss.txt    a)获取文件大小fs.sizeOf(ss .txt)    b)计算切片大小.        computeSplitSize(Math. max(minSize,Math .min(maxSize,blocksize)))=blocksize= l 28M    c) 默认情况下，切片大小=blocksize    d)开始切，形成第1个切片: ss.txt- - -0:128M第2个切片ss.txt- 128:256M 第3个切片ss.txt一256M:300M        (每次切片时，都要判断切完剩下的部分是否大于块的1 .1倍，不大于1 .1倍就划分-块切片)    e)将切片信息写到一个切片规划文件中    f)整个切片的核心过程在getS$plit()方 法中完成    g) InputSplit只记录 了切片的元数据信息，比如起始位置、长度以及所在的节列表等。(4)提交切片规划文件到YARN上，YARN 上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数。\nFileInputFormat切片机制\n(1)简单地按照文件的内容长度进行切片(2) 切片大小，默认等于Block大小(3)切片时不考虑数据集整体，而是逐个针对每一个文件单独切片\nFileInputFormat切片大小的参数配置\n(1)源码中计算切片大小的公式.Math. max(ninSize, Math. min( maxSize, blockS ize));mapreduce. input. fileinputformat. split. minsize=l默认值为1mapreduce. input fileinputformat. split maxsize= Long MAXValue默认值Long .MAXValue因此，默认情况下，切片大小=blocksize。(2)切片大小设置maxsize (切片最大值) :参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。minsize (切片 最小值) :参数调的比blockSize大， 则可以让切片变得比blockSize还大。(3)获取切片信息API//获取切片的文件名称String name = inputSplit. getPath() . getName() ;//根据文件类型获取切片信息Fi leSplit inputSplit = (FileSplit) context. getInputSplit () ;\nTextInputFormat​        FileInputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。\n​        TextInputFormat是默认的FileInputFormat实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text类型。\n以下是一个示例，比如，一个分片包含了如下4条文本记录。\n\n\n\n\n\n\n\n\n\nRich learning form\nIntelligent learning engine\nLearning more convenient\nFrom the real demand for more close to the enterprise\n每条记录表示为以下键/值对：\n\n\n\n\n\n\n\n\n\n(0,Rich learning form)\n(20,Intelligent learning engine)\n(49,Learning more convenient)\n(74,From the real demand for more close to the enterprise)\nCombineTextInputFormat切片机制​        框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。1）应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。2）虚拟存储切片最大值设置CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。3）切片机制生成切片过程包括：虚拟存储过程和切片过程二部分。\n存储与切片过程\n（1）虚拟存储过程：        将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。        例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。（2）切片过程：    （a）判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。    （b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。    （c）测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个        文件块，大小分别为：1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M）最终会形成3个切片，大小分        别为：（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M\nMapReduce工作流程\n\n上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下：（1）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中（2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件（3）多个溢出文件会被合并成大的溢出文件（4）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序（5）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据（6）ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）（7）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）注意：（1）Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。（2）缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb默认100M。\nShuffle机制Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。\n\nPartition分区\n问题引出要求将统计结果按照条件输出到不同文件中(分区) .比如:将统计结果按照手机归属地不同省份输出到不同文件中(分区)\n默认Partitioner分区\n\npublic class HashRartitioner&lt;K, V&gt; extends Partitioner&lt;K, v&gt; {\n    public int getPartition(K key, v value, int numReduceTasks) {\n    \treturn (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;\n    }\n}\n默认分区是根据key的hashCode对Reduce Iasks个数取模得到的。用户没法控制哪个key存储到哪个分区。\n\n自定义Partitioner步骤(1) 自定义类继承Partitioner, 重写getPartition()方法\n\npublic class C ustomPar titioner extends Partitioner &lt;Text，FlowBe an&gt; {\n    @Override\n    public int getPartition (Text key, FlowBean value, int numPartitions) {\n    //控制分区代码逻辑\n    .. ..\n    return partition;\n    }\n}\n​    (2)在Job驱动中，设置自定义Partitioner\njob. setP artitionerClas s(CustomPartitioner.class);\n​    (3)自定义Partition后， 要根据自定义P artitioner的逻辑设置相应数量的ReduceTask\njob. setNumReduceTasks(5);\n\n分区总结(1)如果ReduceTask的数 量&gt; getPartition的结果数,则会多产生几个空的输出文件pat-r 000xx;(2)如果1&lt;ReduceTask的数 量&lt;getP artition的结果数，则有-部分分区数据无处安放，会Exception;(3)如果ReduceTask的数 量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件pat-00000;(4)分区号必须从零开始,逐一累加。\n素例分析例如:假设自定义分区数为5，则(1) job.setNumReduceTasks(1)， 会正常运行， 只不过会产生一个输出文件(2) job.setENumReduceTasks(2), 会报错(3) job.setNumReduceTasks(6), 大于5，程序会正常运行，会产生空文件\n\nWritableComparable排序​        排序是MapReduc e框架中最重要的操作之一。​        MapTask和ReduceTask均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。​        默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。\n​        对于MapTask,它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘.上，而当数据处理完毕后，它会对磁盘.上所有文件进行归并排序。​        对于ReduceTask，它从每个M ap Task上远程拷贝相应的数据文件，如果文件大小超过一 定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到-定阈值，则进行一次归并排序以生成一个更大文件;如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统-对内存和磁盘上的所有数据进行一次归并排序。\n排序分类\n(1)部分排序MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。(2)全排序.最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。 但该方法在处理大型文件时效率极低，因为-台机器处理所有文件，完全丧失了 MapRe duce所提供的并行架构。(3)辅助排序: (GroupingCompar ator分组)在Reduce端对key进行分组。应用于:在接收的key为bean对象时，想让一个或几个字段相同(全部.字段比较不相同)的k&lt;ey进 入到同一个reduce方法时，可以采用分组排序。(4)二次排序在自定义排序过程中，如果c ompareTo中的判断条件为两个即为二次排序。\n自定义排序WritableComparable原理分析\nbean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序。\n@Override\npublic int compareTo(FlowBean bean) {\n\n\tint result;\n\t\t\n\t// 按照总流量大小，倒序排列\n\tif (this.sumFlow &gt; bean.getSumFlow()) {\n\t\tresult = -1;\n\t}else if (this.sumFlow &lt; bean.getSumFlow()) {\n\t\tresult = 1;\n\t}else {\n\t\tresult = 0;\n\t}\n\n\treturn result;\n}\nCombiner合并(1) Combiner是MR程 序中Mapper和Reducer之外的一种组件。(2) Combiner组件的父类就是Reducer。(3) Combiner和Reducer的区别在 于运行的位置      Combiner是在每一个MapTask所在的节点运行;      Reducer是接收全局所有Mapper的输出结果;(4) Combiner的意义就是对每一个 MapTask的输出进行局部汇总，以减小网络传输量。(5) Combiner能够应 用的前提是不能影响最终的业务逻辑，而且，Combiner的输 出kv应该跟Reducer的输，入Iv类型要对应起来。\n自定义Combiner实现步骤\n（a）自定义一个Combiner继承Reducer，重写Reduce方法\npublic class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n\n    private IntWritable outV = new IntWritable();\n\n    @Override\n    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {\n\n        int sum = 0;\n        for (IntWritable value : values) {\n            sum += value.get();\n        }\n     \n        outV.set(sum);\n     \n        context.write(key,outV);\n    }\n}\n（b）在Job驱动类中设置： \njob.setCombinerClass(WordCountCombiner.class);\nOutputFormat数据输出OupltFoma是Mpduce输出的基类,所有实现MapReduce输出都实现了OupuFormat接口。下面我们介绍几种常见的OutputFormat实现类。\n\nOutputFormat实现类\n默认输出格式TextOutputFormat\n自定义OutputFormat\n\n应用场景:输出数据到MySQL/HBase/Elasticsearch等存储框架中。自定义OutputFormat步骤➢自定义一个类继承FileOutputF ormat。➢改写RecordWriter， 具体改写输出数据的方法write0。\nMapReduce内核源码解析\n1）Read阶段：MapTask通过InputFormat获得的RecordReader，从输入InputSplit中解析出一个个key/value。\n2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。\n3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。\n4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。\n溢写阶段详情：\n​    步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。\n​    步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。\n​    步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。\n 5）Merge阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。\n​    当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。\n​    在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并mapreduce.task.io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。\n​    让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。\nReduceTask工作机制\n 1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。\n 2）Sort阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。\n 3）Reduce阶段：reduce()函数将计算结果写到HDFS上。\nReduceTask并行度决定机制回顾：MapTask并行度由切片个数决定，切片个数由输入文件和切片规则决定。\n思考：ReduceTask并行度由谁决定？\n1）设置ReduceTask并行度（个数）\nReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置：\n// 默认值是1，手动设置为4\njob.setNumReduceTasks(4);\n注意事项\n(1) ReduceTask=0, 表示没有Reduce阶段，输出文件个数和Map个数-致。(2) ReduceTask默认值就是1 ，所以输出文件个数为一一个。(3)如果数据分布不均匀，就有可能在R educe阶段产生数据倾斜(4) ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask.(5)具体多少个ReduceTask,需要根据集群性能而定。(6)如果分区数不是1,但是ReduceTask为1，是否执行分区过程。答案是:不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1 .肯定不执行。.\nJoin应用Reduce Join​        Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。\n     Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志）分开，最后进行合并就ok了。\nMap Join1）使用场景\nMap Join适用于一张表十分小、一张表很大的场景。\n2）优点\n思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？\n在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。\n3）具体办法：采用DistributedCache\n​    （1）在Mapper的setup阶段，将文件读取到缓存集合中。\n​    （2）在Driver驱动类中加载缓存。\n//缓存普通文件到Task运行节点。\njob.addCacheFile(new URI(“file:///e:/cache/pd.txt”));\n//如果是集群运行,需要设置HDFS路径\njob.addCacheFile(new URI(“hdfs://hadoop102:8020/cache/pd.txt”));\n数据清洗（ETL）“ETL，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL一词较常用在数据仓库，但其对象并不限于数据仓库\n在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。\nMapReduce开发总结1）输入数据接口：InputFormat（1）默认使用的实现类是：TextInputFormat（2）TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。（3）CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。\n2）逻辑处理接口：Mapper用户根据业务需求实现其中三个方法：map()   setup()   cleanup () \n3）Partitioner分区（1）有默认实现 HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces（2）如果业务上有特别的需求，可以自定义分区。\n4）Comparable排序（1）当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法。（2）部分排序：对最终输出的每一个文件进行内部排序。（3）全排序：对所有数据进行排序，通常只有一个Reduce。（4）二次排序：排序的条件有两个。\n5）Combiner合并Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果。\n6）逻辑处理接口：Reducer用户根据业务需求实现其中三个方法：reduce()   setup()   cleanup () \n7）输出数据接口：OutputFormat（1）默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对，向目标文本文件输出一行。（2）用户还可以自定义OutputFormat。\nHadoop数据压缩1）压缩的好处和坏处压缩的优点：以减少磁盘IO、减少磁盘存储空间。压缩的缺点：增加CPU开销。2）压缩原则（1）运算密集型的Job，少用压缩（2）IO密集型的Job，多用压缩\nMR支持的压缩编码\n\n\n\n\n压缩格式\nHadoop自带？\n算法\n文件扩展名\n是否可切片\n换成压缩格式后，原来的程序是否需要修改\n\n\n\n\nDEFLATE\n是，直接使用\nDEFLATE\n.deflate\n否\n和文本处理一样，不需要修改\n\n\nGzip\n是，直接使用\nDEFLATE\n.gz\n否\n和文本处理一样，不需要修改\n\n\nbzip2\n是，直接使用\nbzip2\n.bz2\n是\n和文本处理一样，不需要修改\n\n\nLZO\n否，需要安装\nLZO\n.lzo\n是\n需要建索引，还需要指定输入格式\n\n\nSnappy\n是，直接使用\nSnappy\n.snappy\n否\n和文本处理一样，不需要修改\n\n\n\n\n压缩性能的比较\n\n\n\n\n压缩算法\n原始文件大小\n压缩文件大小\n压缩速度\n解压速度\n\n\n\n\ngzip\n8.3GB\n1.8GB\n17.5MB/s\n58MB/s\n\n\nbzip2\n8.3GB\n1.1GB\n2.4MB/s\n9.5MB/s\n\n\nLZO\n8.3GB\n2.9GB\n49.3MB/s\n74.6MB/s\n\n\n\n\n压缩方式选择压缩方式选择时重点考虑：压缩/解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片。\nGzip压缩\n优点：压缩率比较高； \n缺点：不支持Split；压缩/解压速度一般；\nBzip2压缩\n优点：压缩率高；支持Split； \n缺点：压缩/解压速度慢。\nLzo压缩\n优点：压缩/解压速度比较快；支持Split；\n缺点：压缩率一般；想支持切片需要额外创建索引。\nSnappy压缩\n优点：压缩和解压缩速度快； \n缺点：不支持Split；压缩率一般； \n压缩位置选择\n压缩可以在MapReduce作用的任意阶段启用。\n\n压缩参数配置1）为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器\n\n\n\n\n压缩格式\n对应的编码/解码器\n\n\n\n\nDEFLATE\norg.apache.hadoop.io.compress.DefaultCodec\n\n\ngzip\norg.apache.hadoop.io.compress.GzipCodec\n\n\nbzip2\norg.apache.hadoop.io.compress.BZip2Codec\n\n\nLZO\ncom.hadoop.compression.lzo.LzopCodec\n\n\nSnappy\norg.apache.hadoop.io.compress.SnappyCodec\n\n\n\n\n2）要在Hadoop中启用压缩，可以配置如下参数\n\n\n\n\n参数\n默认值\n阶段\n建议\n\n\n\n\nio.compression.codecs    （在core-site.xml中配置）\n无，这个需要在命令行输入hadoop checknative查看\n输入压缩\nHadoop使用文件扩展名判断是否支持某种编解码器\n\n\nmapreduce.map.output.compress（在mapred-site.xml中配置）\nfalse\nmapper输出\n这个参数设为true启用压缩\n\n\nmapreduce.map.output.compress.codec（在mapred-site.xml中配置）\norg.apache.hadoop.io.compress.DefaultCodec\nmapper输出\n企业多使用LZO或Snappy编解码器在此阶段压缩数据\n\n\nmapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）\nfalse\nreducer输出\n这个参数设为true启用压缩\n\n\nmapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）\norg.apache.hadoop.io.compress.DefaultCodec\nreducer输出\n使用标准工具或者编解码器，如gzip和bzip2\n\n\n\n\n常见错误及解决方案1）导包容易出错。尤其Text和CombineTextInputFormat。2）Mapper中第一个输入的参数必须是LongWritable或者NullWritable，不可以是IntWritable.  报的错误是类型转换异常。3）java.lang.Exception: java.io.IOException: Illegal partition for 13926435656 (4)，说明Partition和ReduceTask个数没对上，调整ReduceTask个数。4）如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。5）在Windows环境编译的jar包导入到Linux环境中运行，\nhadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/ /user/atguigu/output\n报如下错误：\nException in thread \"main\" java.lang.UnsupportedClassVersionError: com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version 52.0\n原因是Windows环境用的jdk1.7，Linux环境用的jdk1.8。解决方案：统一jdk版本。6）缓存pd.txt小文件案例中，报找不到pd.txt文件原因：大部分为路径书写错误。还有就是要检查pd.txt.txt的问题。还有个别电脑写相对路径找不到pd.txt，可以修改为绝对路径。7）报类型转换异常。通常都是在驱动函数中设置Map输出和最终输出时编写错误。Map输出的key如果没有排序，也会报类型转换异常。8）集群中运行wc.jar时出现了无法获得输入文件。原因：WordCount案例的输入文件不能放用HDFS集群的根目录。9）出现了如下相关异常\nException in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:609)\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)\njava.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n\tat org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:356)\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:371)\n\tat org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:364)\n解决方案：拷贝hadoop.dll文件到Windows目录C:\\Windows\\System32。个别同学电脑还需要修改Hadoop源码。方案二：创建如下包名，并将NativeIO.java拷贝到该包名下\n10）自定义Outputformat时，注意在RecordWirter中的close方法必须关闭流资源。否则输出的文件内容中数据为空。\n@Override\npublic void close(TaskAttemptContext context) throws IOException, InterruptedException {\n\t\tif (atguigufos != null) {\n\t\t\tatguigufos.close();\n\t\t}\n\t\tif (otherfos != null) {\n\t\t\totherfos.close();\n\t\t}\n}\n5. Yarn​        Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。\nYarn资源调度器Yarn基础架构YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。\n\nYarn工作机制\n1）MR程序提交到客户端所在的节点。\n2）YarnRunner向ResourceManager申请一个Application。\n3）RM将该应用程序的资源路径返回给YarnRunner。\n4）该程序将运行所需资源提交到HDFS上。\n5）程序资源提交完毕后，申请运行mrAppMaster。\n6）RM将用户的请求初始化成一个Task。\n7）其中一个NodeManager领取到Task任务。\n8）该NodeManager创建容器Container，并产生MRAppmaster。\n9）Container从HDFS上拷贝资源到本地。\n10）MRAppmaster向RM 申请运行MapTask资源。\n11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。\n12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。\n13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。\n14）ReduceTask向MapTask获取相应分区的数据。\n15）程序运行完毕后，MR会向RM申请注销自己。\n作业提交全过程\n\n\n作业提交全过程详解\n（1）作业提交\n第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。\n第2步：Client向RM申请一个作业id。\n第3步：RM给Client返回该job资源的提交路径和作业id。\n第4步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。\n第5步：Client提交完资源后，向RM申请运行MrAppMaster。\n（2）作业初始化\n第6步：当RM收到Client的请求后，将该job添加到容量调度器中。\n第7步：某一个空闲的NM领取到该Job。\n第8步：该NM创建Container，并产生MRAppmaster。\n第9步：下载Client提交的资源到本地。\n（3）任务分配\n第10步：MrAppMaster向RM申请运行多个MapTask任务资源。\n第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。\n（4）任务运行\n第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。\n第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。\n第14步：ReduceTask向MapTask获取相应分区的数据。\n第15步：程序运行完毕后，MR会向RM申请注销自己。\n（5）进度和状态更新\n​        YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。\n（6）作业完成\n​        除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。\nYarn调度器和调度算法目前，Hadoop作业调度器主要有三种：FIFO、容量（Capacity Scheduler）和公平（Fair Scheduler）。Apache Hadoop3.1.3默认的资源调度器是Capacity Scheduler。\nCDH框架默认调度器是Fair Scheduler。具体设置详见：yarn-default.xml文件\n&lt;property&gt;\n    &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt;\n    &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;\n&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;\n&lt;/property&gt;\n先进先出调度器（FIFO）FIFO调度器（First In First Out）：单队列，根据提交作业的先后顺序，先来先服务。\n\n优点：简单易懂；\n缺点：不支持多队列，生产环境很少使用；\n容量调度器（Capacity Scheduler）Capacity Scheduler是Yahoo开发的多用户调度器。\n\n1、多队列:每个队列可配置-定的资源量, 每个队列采用FIF0调度策略。2、容量保证:管理员可为每个队列设置资源最低保证和资源使用上限3、灵活性:如果一个队列中的资源有剩余,可以暂时共享给那些需要资源的队列，而- -旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。4、多租户:支持多用户共享集群和多应用程序同时运行。为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。\n\n公平调度器（Fair Scheduler）Fair Schedulere是Facebook开发的多用户调度器。\n\n1)与容量调度器相同点(1)多队列:支持多队列多作业(2)容量保证:管理员可为每个队列设置资源最低保证和资源使用上线(3)灵活性:如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。(4)多租户:支持多用户共享集群和多应用程序同时运行;为了防止同一个用户的作业独占队列中的资源，该调度器:会对同一用户提交的作业所占资源量进行限定。\n2)与容量调度器不同点(1)核心调度策略不同\n容量调度器:优先选择资源利用率低的队列\n公平调度器:优先选择对资源的缺额比例大的\n(2)每个队列可以单独设置资源分配方式\n容量调度器: FIFO、DRF公平调度器: FIFO、 FAIR、k DRF\n\n公平调度器设计目标是: 在时我尔度占。所有作业获得公平的资源。某一时刻一个作业应获资源际获取资的差叫’缺额”调度器会优先为缺额大的作业分配资源\n公平调度器队列资源分配方式\n1) FIFO策略公平调度器每个队列资源分配策略如果选择FIFO的话，此时公平调度器相当于上面讲过的容量调度器。2) Fair策略    Fair策略(默认)是一-种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资    源。这意味着，如果一-个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源;如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。    具体资源分配流程和容量调度器一致;    (l)选择队列    (2)选择作业    (3)选择容器    以上三步，每一一步都是按照公平策略分配资源    ➢实际最小资源份额: mindshare = Min (资源需求量，配置的最小资源)    ➢是否饥饿: isNeedy =资源使用量&lt; mindshare (实际最小资源份额)    ➢资源分配比: minShareRatio =资源使用量/ Max (mindshare, 1 )    ➢资源使用权重比: use ToWeightRatio=资源使用量/权重\n\n\n3)DRF策略\n​        DRF ( Dominant Resource Faimness)，我们之前说的资源，都是单一标准，例如只考虑内存(也是Yarn默认的情况)。但是很多时候我们资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。​        那么在YARN中，我们用DRF来决定如何调度:​        假设集群- :共有100 CPU和10T内存，而应用A需要(2 CPU, 300GB)，应用B需要(6 CPU，100GB) 。则两个应用分别需要A (2%CPU, 3%内存)和B (6%CPU, 1%内存)的资源，这就意味着A是内存主导的，B是CPU主导的，针对这种情况，我们可以选择DRF策略对不同应用进行不同资源(CPU和内存)的一个不同比例的限制。\nYarn常用命令yarn application查看任务Yarn状态的查询，除了可以在hadoop103:8088页面查看外，还可以通过命令操作。常见的命令操作如下所示：\n需求：执行WordCount案例，并用Yarn命令查看任务运行情况。\n[atguigu@hadoop102 hadoop-3.1.3]$ myhadoop.sh start\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output\nyarn application查看任务\nyarn application -list\n根据Application状态过滤：\nyarn application -list -appStates （所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）\nKill掉Application：\nyarn application -kill application_1612577921195_0001\nyarn logs查看日志查询Application日志：yarn logs -applicationId \nyarn logs -applicationId application_1612577921195_0001\n查询Container日志：yarn logs -applicationId  -containerId \nyarn logs -applicationId application_1612577921195_0001 -containerId container_1612577921195_0001_01_000001\nyarn applicationattempt查看尝试运行的任务列出所有Application尝试的列表：yarn applicationattempt -list \nyarn applicationattempt -list application_1612577921195_0001\n打印ApplicationAttemp状态：yarn applicationattempt -status \nyarn applicationattempt -status appattempt_1612577921195_0001_000001\nyarn container查看容器列出所有Container：yarn container -list \nyarn container -list appattempt_1612577921195_0001_000001\n打印Container状态： yarn container -status \nyarn container -status container_1612577921195_0001_01_000001\nyarn node查看节点状态列出所有节点：yarn node -list -all\nyarn rmadmin更新配置加载队列配置：yarn rmadmin -refreshQueues\nyarn queue查看队列打印队列信息：yarn queue -status \nYarn生产环境核心参数\n向Hive队列提交任务1）hadoop jar的方式\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output\n注: -D表示运行时改变参数值\n2）打jar包的方式\n默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：\npublic class WcDrvier {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        Configuration conf = new Configuration();\n\n        conf.set(\"mapreduce.job.queuename\",\"hive\");\n\n        //1. 获取一个Job实例\n        Job job = Job.getInstance(conf);\n\n        。。。 。。。\n\n        //6. 提交Job\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\n这样，这个任务在集群提交时，就会提交到hive队列：\n任务优先级容量调度器，支持任务优先级的配置，在资源紧张时，优先级高的任务将优先获取资源。默认情况，Yarn将所有任务的优先级限制为0，若想使用任务的优先级功能，须开放该限制。\n1）修改yarn-site.xml文件，增加以下参数\n&lt;property&gt;\n  &lt;name&gt;yarn.cluster.max-application-priority&lt;/name&gt;\n  &lt;value&gt;5&lt;/value&gt;\n&lt;/property&gt;\n2）分发配置，并重启Yarn\n[atguigu@hadoop102 hadoop]$ xsync yarn-site.xml\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh\n[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh\n3）模拟资源紧张环境，可连续提交以下任务，直到新提交的任务申请不到资源为止。\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 2000000\n4）再次重新提交优先级高的任务\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi  -D mapreduce.job.priority=5 5 2000000\n5）也可以通过以下命令修改正在执行的任务的优先级。\nyarn application -appID  -updatePriority 优先级\n[atguigu@hadoop102 hadoop-3.1.3]$ yarn application -appID application_1611133087930_0009 -updatePriority 5\n","slug":"B2-Hadoop","date":"2021-11-12T11:13:22.000Z","categories_index":"大数据","tags_index":"Hadoop","author_index":"YFR718"},{"id":"047a0157ce631e99397979ea9f68fcda","title":"meta","content":"","slug":"000meta","date":"2021-11-12T06:22:18.000Z","categories_index":"学习规划","tags_index":"","author_index":"YFR718"},{"id":"9a9e7d03fd63b8b9dd7648918fd31ad5","title":"反射、新特性","content":"1. Java反射机制1.1 Java反射机制概述\nReflection（反射）是被视为动态语言的关键，反射机制允许程序在执行期借助于Reflection API取得任何类的内部信息，并能直接操作任意对象的内部属性及方法。 \n加载完类之后，在堆内存的方法区中就产生了一个Class类型的对象（一个类只有一个Class对象），这个对象就包含了完整的类的结构信息。我们可以通过这个对象看到类的结构。这个对象就像一面镜子，透过这个镜子看到类的结构，所以，我们形象的称之为：反射。\n\n正常方式：引入需要的”包类”名称—&gt;通过new实例化–&gt;取得实例化对象反射方式：实例化对象–&gt;getClass()方法–&gt;得到完整的“包类”名称\n补充：动态语言 vs 静态语言\n动态语言\n    是一类在运行时可以改变其结构的语言：例如新的函数、对象、甚至代码可以被引进，已有的函数可以被删除或是其他结构上的变化。通俗点说就是在运行时代码可以根据某些条件改变自身结构。\n主要动态语言：Object-C、C#、JavaScript、PHP、Python、Erlang。\n\n静态语言\n与动态语言相对应的，运行时结构不可变的语言就是静态语言。如Java、C、C++。\n\n\n​        Java不是动态语言，但Java可以称之为“准动态语言”。即Java有一定的动态性，我们可以利用反射机制、字节码操作获得类似动态语言的特性。Java的动态性让编程的时候更加灵活！\nJava反射机制研究及应用 Java反射机制提供的功能\n\n在运行时判断任意一个对象所属的类\n在运行时构造任意一个类的对象\n在运行时判断任意一个类所具有的成员变量和方法\n在运行时获取泛型信息\n在运行时调用任意一个对象的成员变量和方法\n在运行时处理注解\n生成动态代理\n\n反射相关的主要API\n\njava.lang.Class:代表一个类 \njava. lang.reflect.Method:代表类的方法\njava.lang.reflect.Field:代表类的成员变量\njava.lang.reflect.Constructor:代表类的构造器 \n\n1.2 理解Class类并获取Class实例Class 类 在Object类中定义了以下的方法，此方法将被所有子类继承：● public final Class getClass()\n​        以上的方法返回值的类型是一个Class类，此类是Java反射的源头，实际上所谓反射从程序的运行结果来看也很好理解，即：可以通过对象反射求出类的名称。\n​        对象照镜子后可以得到的信息：某个类的属性、方法和构造器、某个类到底实现了哪些接口。对于每个类而言，JRE 都为其保留一个不变的 Class 类型的对象。一个 Class 对象包含了特定某个结构(class/interface/enum/annotation/primitive type/void/[])的有关信息。 \n\nClass本身也是一个类\nClass 对象只能由系统建立对象\n一个加载的类在 JVM 中只会有一个Class实例\n一个Class对象对应的是一个加载到JVM中的一个.class文件\n每个类的实例都会记得自己是由哪个 Class 实例所生成\n通过Class可以完整地得到一个类中的所有被加载的结构\nClass类是Reflection的根源，针对任何你想动态加载、运行的类，唯有先获得相应的\n\n\n\n\n\nClass类的常用方法\n功能说明\n\n\n\n\nstatic Class forName(String name)\n返回指定类名 name 的 Class 对象\n\n\nObject newInstance()\n调用缺省构造函数，返回该Class对象的一个实例\n\n\ngetName()\n返回此Class对象所表示的实体（类、接口、数组类、基本类型或void）名称\n\n\nClass getSuperClass()\n返回当前Class对象的父类的Class对象\n\n\nClass [] getInterfaces()\n获取当前Class对象的接口\n\n\nClassLoader getClassLoader()\n返回该类的类加载器\n\n\nClass getSuperclass()\n返回该类的类加载器\n\n\nConstructor[] getConstructors()\n返回一个包含某些Constructor对象的数组\n\n\nField[] getDeclaredFields()\n返回Field对象的一个数组\n\n\nMethod getMethod(String name,Class … paramTypes)\n返回一个Method对象，此对象的形参类型为paramType\n\n\n\n\n反射的应用举例\nString str = \"test4.Person\";\nClass clazz = Class.forName(str);\nObject obj = clazz.newInstance();\nField field = clazz.getField(\"name\");\nfield.set(obj, \"Peter\");\nObject name = field.get(obj);\nSystem.out.println(name);\n注：test4.Person是test4包下的Person类\n获取Class类的实例(四种方法) \n1）前提：若已知具体的类，通过类的class属性获取，该方法最为安全可靠，程序性能最高        实例：Class clazz = String.class; \n2）前提：已知某个类的实例，调用该实例的getClass()方法获取Class对象        实例：Class clazz = “www.atguigu.com”.getClass();3）前提：已知一个类的全类名，且该类在类路径下，可通过Class类的静态方法forName()获取，可能抛出ClassNotFoundException        实例：Class clazz = Class.forName(“java.lang.String”);4）其他方式(不做要求)        ClassLoader cl = this.getClass().getClassLoader();        Class clazz4 = cl.loadClass(“类的全类名”);哪些类型可以有Class对象？（1）class： 外部类，成员(成员内部类，静态内部类)，局部内部类，匿名内部类（2）interface：接口（3）[]：数组（4）enum：枚举（5）annotation：注解@interface（6）primitive type：基本数据类型（7）void\nClass c1 = Object.class;\nClass c2 = Comparable.class;\nClass c3 = String[].class;\nClass c4 = int[][].class;\nClass c5 = ElementType.class;\nClass c6 = Override.class;\nClass c7 = int.class;\nClass c8 = void.class;\nClass c9 = Class.class;\nint[] a = new int[10];\nint[] b = new int[100];\nClass c10 = a.getClass();\nClass c11 = b.getClass();\n// 只要元素类型与维度一样，就是同一个Class\nSystem.out.println(c10 == c11);\n1.3 类的加载与ClassLoader的理解了解：类的加载过程\n当程序主动使用某个类时，如果该类还未被加载到内存中，则系统会通过如下三个步骤来对该类进行初始化。\n\n类的加载(Load)将类的class文件读入内存，并为之创建一个java.lang.Class对象。此过程由类加载器完成\n类的链接(Link)将类的二进制数据合并到JRE中\n类的初始化(Initialize)JVM负责对类进行初始化\n\n类的加载与ClassLoader的理解\n\n加载：将class文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时数据结构，然后生成一个代表这个类的java.lang.Class对象，作为方法区中类数据的访问入口（即引用地址）。所有需要访问和使用类数据只能通过这个Class对象。这个加载的过程需要类加载器参与。 \n链接：将Java类的二进制代码合并到JVM的运行状态之中的过程。\n验证：确保加载的类信息符合JVM规范，例如：以cafe开头，没有安全方面的问题\n准备：正式为类变量（static）分配内存并设置类变量默认初始值的阶段，这些内存都将在方法区中进行分配。 \n解析：虚拟机常量池内的符号引用（常量名）替换为直接引用（地址）的过程。 \n\n\n初始化：\n执行类构造器()方法的过程。类构造器()方法是由编译期自动收集类中所有类变量的赋值动作和静态代码块中的语句合并产生的。（类构造器是构造类信息的，不是构造该类对象的构造器）。 \n当初始化一个类的时候，如果发现其父类还没有进行初始化，则需要先触发其父类的初始化。 \n虚拟机会保证一个类的()方法在多线程环境中被正确加锁和同步。\n\n\n\npublic class ClassLoadingTest {\n    public static void main(String[] args) {\n    \tSystem.out.println(A.m);\n    } \n}\nclass A {\n    static { \n        m = 300;\n    }\n    static int m = 100;\n}\n//第二步：链接结束后m=0\n//第三步：初始化后，m的值由&lt;clinit&gt;()方法执行决定\n// 这个A的类构造器&lt;clinit&gt;()方法由类变量的赋值和静态代码块中的语句按照顺序合并\n产生，类似于\n// &lt;clinit&gt;(){\n// m = 300;\n// m = 100;\n// }\n什么时候会发生类初始化？ \n\n类的主动引用（一定会发生类的初始化） \n当虚拟机启动，先初始化main方法所在的类  new一个类的对象\n调用类的静态成员（除了final常量）和静态方法\n使用java.lang.reflect包的方法对类进行反射调用\n当初始化一个类，如果其父类没有被初始化，则先会初始化它的父类\n\n\n类的被动引用（不会发生类的初始化）  当访问一个静态域时，只有真正声明这个域的类才会被初始化\n当通过子类引用父类的静态变量，不会导致子类初始化\n通过数组定义类引用，不会触发此类的初始化\n引用常量不会触发此类的初始化（常量在链接阶段就存入调用类的常量池中了）\n\n\n\npublic class ClassLoadingTest {\npublic static void main(String[] args) {\n    // 主动引用：一定会导致A和Father的初始化\n    // A a = new A();\n    // System.out.println(A.m);\n    // Class.forName(\"com.atguigu.java2.A\");\n    // 被动引用\n    A[] array = new A[5];//不会导致A和Father的初始化\n    // System.out.println(A.b);//只会初始化Father\n    // System.out.println(A.M);//不会导致A和Father的初始化\n    }\n    static {\n    \tSystem.out.println(\"main所在的类\");\n    } \n}\nclass Father {\n    static int b = 2;\n    static {\n    \tSystem.out.println(\"父类被加载\");\n    } \n}\nclass A extends Father {\n    static {\n        System.out.println(\"子类被加载\");\n        m = 300;\n    }\n    static int m = 100;\n    static final int M = 1;\n}\n类加载的作用：将class文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时数据结构，然后在堆中生成一个代表这个类的java.lang.Class对象，作为方法区中类数据的访问入口。 \n类缓存：标准的JavaSE类加载器可以按要求查找类，但一旦某个类被加载到类加载器中，它将维持加载（缓存）一段时间。不过JVM垃圾回收机制可以回收这些Class对象。\n了解：ClassLoader\n类加载器作用是用来把类(class)装载进内存的。JVM 规范定义了如下类型的类的加载器。引导类加载器：用C++编写的，是JVM自带的类加载器，负责Java平台核心库，用来装载核心类库。该加载器无法直接获取扩展类加载器：负责jre/lib/ext目录下的jar包或 –D java.ext.dirs 指定目录下的jar包装入工作库系统类加载器：负责java –classpath 或 –D java.class.path所指的目录下的类与jar包装入工作 ，是最常用的加载器\n//1.获取一个系统类加载器\nClassLoader classloader = ClassLoader.getSystemClassLoader();\nSystem.out.println(classloader);\n//2.获取系统类加载器的父类加载器，即扩展类加载器\nclassloader = classloader.getParent();\nSystem.out.println(classloader);\n//3.获取扩展类加载器的父类加载器，即引导类加载器\nclassloader = classloader.getParent();\nSystem.out.println(classloader);\n//4.测试当前类由哪个类加载器进行加载\nclassloader = Class.forName(\"exer2.ClassloaderDemo\").getClassLoader();\nSystem.out.println(classloader);\n//5.测试JDK提供的Object类由哪个类加载器加载\nclassloader = Class.forName(\"java.lang.Object\").getClassLoader();\nSystem.out.println(classloader);\n//*6.关于类加载器的一个主要方法：getResourceAsStream(String str):获取类路径下的指定文件的输入流\nInputStream in = null;\nin = this.getClass().getClassLoader().getResourceAsStream(\"exer2\\\\test.properties\");\nSystem.out.println(in);\n1.4 创建运行时类的对象创建类的对象：调用Class对象的newInstance()方法要 求： \n1）类必须有一个无参数的构造器。2）类的构造器的访问权限需要足够。\n难道没有无参的构造器就不能创建对象了吗？不是！只要在操作的时候明确的调用类中的构造器，并将参数传递进去之后，才可以实例化操作。步骤如下：1）通过Class类的getDeclaredConstructor(Class … parameterTypes)取得本类的指定形参类型的构造器2）向构造器的形参中传递一个对象数组进去，里面包含了构造器中所需的各个参数。3）通过Constructor实例化对象。\n有了Class对象，能做什么？以上是反射机制应用最多的地方。\n//1.根据全类名获取对应的Class对象\nString name = “atguigu.java.Person\";\nClass clazz = null;\nclazz = Class.forName(name);\n//2.调用指定参数结构的构造器，生成Constructor的实例\nConstructor con = clazz.getConstructor(String.class,Integer.class);\n//3.通过Constructor的实例创建对应类的对象，并初始化类属性\nPerson p2 = (Person) con.newInstance(\"Peter\",20);\nSystem.out.println(p2);\n1.5 获取运行时类的完整结构通过反射获取运行时类的完整结构Field、Method、Constructor、Superclass、Interface、Annotation\n\n实现的全部接口\n所继承的父类\n全部的构造器\n全部的方法\n全部的Field\n\n使用反射可以取得：\n\n实现的全部接口public Class&lt;?&gt;[] getInterfaces()确定此对象所表示的类或接口实现的接口。\n所继承的父类public Class&lt;? Super T&gt; getSuperclass()返回表示此 Class 所表示的实体（类、接口、基本类型）的父类的Class。\n全部的构造器public Constructor[] getConstructors()返回此 Class 对象所表示的类的所有public构造方法。public Constructor[] getDeclaredConstructors()返回此 Class 对象表示的类声明的所有构造方法。\n\nConstructor类中：\n\n取得修饰符: public int getModifiers();\n取得方法名称: public String getName();\n\n取得参数的类型：public Class&lt;?&gt;[] getParameterTypes();\n\n\n\n全部的方法public Method[] getDeclaredMethods()返回此Class对象所表示的类或接口的全部方法public Method[] getMethods()返回此Class对象所表示的类或接口的public的方法\n\n\nMethod类中：public Class&lt;?&gt; getReturnType()取得全部的返回值public Class&lt;?&gt;[] getParameterTypes()取得全部的参数public int getModifiers()取得修饰符public Class&lt;?&gt;[] getExceptionTypes()取得异常信息\n\n\n全部的Fieldpublic Field[] getFields()返回此Class对象所表示的类或接口的public的Field。 \npublic Field[] getDeclaredFields()返回此Class对象所表示的类或接口的全部Field。\n\n\n\nField方法中：public int getModifiers() 以整数形式返回此Field的修饰符public Class&lt;?&gt; getType() 得到Field的属性类型public String getName() 返回Field的名称。\n\n\nAnnotation相关\nget Annotation(Class annotationClass)getDeclaredAnnotations() \n\n泛型相关获取父类泛型类型：Type getGenericSuperclass()泛型类型：ParameterizedType获取实际的泛型类型参数数组：getActualTypeArguments()\n\n类所在的包 Package getPackage() \n\n小 结：\n\n在实际的操作中，取得类的信息的操作代码，并不会经常开发。\n一定要熟悉java.lang.reflect包的作用，反射机制。\n如何取得属性、方法、构造器的名称，修饰符等。\n\n1.6 调用运行时类的指定结构调用指定方法通过反射，调用类中的方法，通过Method类完成。步骤：\n\n通过Class类的getMethod(String name,Class…parameterTypes)方法取得一个Method对象，并设置此方法操作时所需要的参数类型。\n之后使用Object invoke(Object obj, Object[] args)进行调用，并向方法中传递要设置的obj对象的参数信息。\n\nObject invoke(Object obj, Object … args)说明：\n\nObject 对应原方法的返回值，若原方法无返回值，此时返回null\n若原方法若为静态方法，此时形参Object obj可为null\n若原方法形参列表为空，则Object[] args为null\n若原方法声明为private,则需要在调用此invoke()方法前，显式调用方法对象的setAccessible(true)方法，将可访问private的方法。\n\n调用指定属性在反射机制中，可以直接通过Field类操作类中的属性，通过Field类提供的set()和get()方法就可以完成设置和取得属性内容的操作。\n\npublic Field getField(String name) 返回此Class对象表示的类或接口的指定的public的Field。 \npublic Field getDeclaredField(String name)返回此Class对象表示的类或接口的指定的Field。 \n\n在Field中：\n\npublic Object get(Object obj) 取得指定对象obj上此Field的属性内容\npublic void set(Object obj,Object value) 设置指定对象obj上此Field的属性内容\n\n关于setAccessible方法的使用\n\nMethod和Field、Constructor对象都有setAccessible()方法。\nsetAccessible启动和禁用访问安全检查的开关。 \n参数值为true则指示反射的对象在使用时应该取消Java语言访问检查。\n提高反射的效率。如果代码中必须用反射，而该句代码需要频繁的被调用，那么请设置为true。 \n使得原本无法访问的私有成员也可以访问\n参数值为false则指示反射的对象应该实施Java语言访问检查。\n\n1.7 反射的应用：动态代理代理设计模式的原理:使用一个代理将对象包装起来, 然后用该代理对象取代原始对象。任何对原始对象的调用都要通过代理。代理对象决定是否以及何时将方法调用转到原始对象上。  之前为大家讲解过代理机制的操作，属于静态代理，特征是代理类和目标对象的类都是在编译期间确定下来，不利于程序的扩展。同时，每一个代理类只能为一个接口服务，这样一来程序开发中必然产生过多的代理。最好可以通过一个代理类完成全部的代理功能。15.7 反射的应用：动态代理动态代理是指客户通过代理类来调用其它对象的方法，并且是在程序运行时根据需要动态创建目标类的代理对象。动态代理使用场合: 调试远程方法调用动态代理相比于静态代理的优点：抽象角色中（接口）声明的所有方法都被转移到调用处理器一个集中的方法中处理，这样，我们可以更加灵活和统一的处理众多的方法。 Proxy ：专门完成代理的操作类，是所有动态代理类的父类。通过此类为一个或多个接口动态地生成实现类。 提供用于创建动态代理类和动态代理对象的静态方法static Class&lt;?&gt; getProxyClass(ClassLoader loader, Class&lt;?&gt;… interfaces) 创建一个动态代理类所对应的Class对象static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces,InvocationHandler h) 直接创建一个动态代理对象Java动态代理相关API类加载器 得到被代理类实现的全部接口得到InvocationHandler接 口的实现类实例15.7 反射的应用：动态代理动态代理步骤1.创建一个实现接口InvocationHandler的类，它必须实现invoke方法，以完成代理的具体操作。public Object invoke(Object theProxy, Method method, Object[] params)throws Throwable{try{Object retval = method.invoke(targetObj, params);// Print out the resultSystem.out.println(retval);return retval;}catch (Exception exc){}}代理类的对象要调用的方法方法调用时所需要的参数15.7 反射的应用：动态代理动态代理步骤2.创建被代理的类以及接口Subject RealSubject implementssay(String name,int age)15.7 反射的应用：动态代理15.7 反射的应用：动态代理动态代理步骤3.通过Proxy的静态方法newProxyInstance(ClassLoader loader, Class[] interfaces, InvocationHandler h) 创建一个Subject接口代理RealSubject target = new RealSubject();// Create a proxy to wrap the original implementationDebugProxy proxy = new DebugProxy(target);// Get a reference to the proxy through the Subject interfaceSubject sub = (Subject) Proxy.newProxyInstance(Subject.class.getClassLoader(),new Class[] { Subject.class }, proxy);15.7 反射的应用：动态代理动态代理步骤4.通过 Subject代理调用RealSubject实现类的方法String info = sub.say(“Peter”, 24);System.out.println(info);动态代理与AOP（Aspect Orient Programming)前面介绍的Proxy和InvocationHandler，很难看出这种动态代理的优势，下面介绍一种更实用的动态代理机制相同的代码段相同的代码段相同的代码段代码段2代码段3代码段1通过复制、粘贴的部分15.7 反射的应用：动态代理动态代理与AOP（Aspect Orient Programming)调用方法调用方法调用方法代码段2代码段3代码段1相同的代码段方法A改进后的说明：代码段1、代码段2、代码段3和深色代码段分离开了，但代码段1、2、3又和一个特定的方法A耦合了！最理想的效果是：代码块1、2、3既可以执行方法A，又无须在程序中以硬编码的方式直接调用深色代码的方法15.7 反射的应用：动态代理15.7 反射的应用：动态代理动态代理与AOP（Aspect Orient Programming)public interface Dog{void info();void run();}public class HuntingDog implements Dog{public void info(){System.out.println(“我是一只猎狗”);}public void run(){System.out.println(“我奔跑迅速”);} }15.7 反射的应用：动态代理动态代理与AOP（Aspect Orient Programming)public class DogUtil{public void method1(){System.out.println(“=====模拟通用方法一=====”);}public void method2(){System.out.println(“=====模拟通用方法二=====”);} }15.7 反射的应用：动态代理动态代理与AOP（Aspect Orient Programming)public class MyInvocationHandler implements InvocationHandler{// 需要被代理的对象private Object target;public void setTarget(Object target){this.target = target;}// 执行动态代理对象的所有方法时，都会被替换成执行如下的invoke方法public Object invoke(Object proxy, Method method, Object[] args)throws Exception{DogUtil du = new DogUtil();// 执行DogUtil对象中的method1。du.method1();// 以target作为主调来执行method方法Object result = method.invoke(target , args);// 执行DogUtil对象中的method2。du.method2();return result;}}15.7 反射的应用：动态代理动态代理与AOP（Aspect Orient Programming)public class MyProxyFactory{// 为指定target生成动态代理对象public static Object getProxy(Object target)throws Exception{// 创建一个MyInvokationHandler对象MyInvokationHandler handler =new MyInvokationHandler();// 为MyInvokationHandler设置target对象handler.setTarget(target);// 创建、并返回一个动态代理对象returnProxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces() , handler);} }15.7 反射的应用：动态代理动态代理与AOP（Aspect Orient Programming)public class Test{public static void main(String[] args)throws Exception{// 创建一个原始的HuntingDog对象，作为targetDog target = new HuntingDog();// 以指定的target来创建动态代理Dog dog = (Dog)MyProxyFactory.getProxy(target);dog.info();dog.run();} }15.7 反射的应用：动态代理动态代理与AOP（Aspect Orient Programming) 使用Proxy生成一个动态代理时，往往并不会凭空产生一个动态代理，这样没有太大的意义。通常都是为指定的目标对象生成动态代理 这种动态代理在AOP中被称为AOP代理，AOP代理可代替目标对象，AOP代理包含了目标对象的全部方法。但AOP代理中的方法与目标对象的方法存在差异：AOP代理里的方法可以在执行目标方法之前、之后插入一些通用处理动态代理与AOP (Aspect Orient Programming)动态代理增加的通用方法1回调目标对象的方法动态代理增加的通用方法2\n2. Java8的其它新特性2.1 Lambda表达式为什么使用 Lambda 表达式Lambda 是一个匿名函数，我们可以把 Lambda 表达式理解为是一段可以传递的代码（将代码像数据一样进行传递）。使用它可以写出更简洁、更灵活的代码。作为一种更紧凑的代码风格，使Java的语言表达能力得到了提升。16.1 Lambda 表达式 从匿名类到 Lambda 的转换举例116.1 Lambda 表达式 从匿名类到 Lambda 的转换举例216.1 Lambda 表达式：语法Lambda 表达式：在Java 8 语言中引入的一种新的语法元素和操作符。这个操作符为 “-&gt;” ， 该操作符被称为 Lambda 操作符或箭头操作符。它将 Lambda 分为两个部分：左侧：指定了 Lambda 表达式需要的参数列表右侧：指定了 Lambda 体，是抽象方法的实现逻辑，也即Lambda 表达式要执行的功能。16.1 Lambda 表达式：语法语法格式一：无参，无返回值语法格式二：Lambda 需要一个参数，但是没有返回值。语法格式三：数据类型可以省略，因为可由编译器推断得出，称为“类型推断”16.1 Lambda 表达式：语法语法格式四：Lambda 若只需要一个参数时，参数的小括号可以省略语法格式五：Lambda 需要两个或以上的参数，多条执行语句，并且可以有返回值语法格式六：当 Lambda 体只有一条语句时，return 与大括号若有，都可以省略16.1 Lambda 表达式类型推断上述 Lambda 表达式中的参数类型都是由编译器推断得出的。Lambda表达式中无需指定类型，程序依然可以编译，这是因为 javac 根据程序的上下文，在后台推断出了参数的类型。Lambda 表达式的类型依赖于上下文环境，是由编译器推断出来的。这就是所谓的“类型推断”。\n2.2 函数式(Functional)接口什么是函数式(Functional)接口 只包含一个抽象方法的接口，称为函数式接口。  你可以通过 Lambda 表达式来创建该接口的对象。（若 Lambda 表达式抛出一个受检异常(即：非运行时异常)，那么该异常需要在目标接口的抽象方法上进行声明）。 我们可以在一个接口上使用 @FunctionalInterface 注解，这样做可以检查它是否是一个函数式接口。同时 javadoc 也会包含一条声明，说明这个接口是一个函数式接口。 在java.util.function包下定义了Java 8 的丰富的函数式接口16.2 函数式接口如何理解函数式接口 Java从诞生日起就是一直倡导“一切皆对象”，在Java里面面向对象(OOP)编程是一切。但是随着python、scala等语言的兴起和新技术的挑战，Java不得不做出调整以便支持更加广泛的技术要求，也即java不但可以支持OOP还可以支持OOF（面向函数编程）  在函数式编程语言当中，函数被当做一等公民对待。在将函数作为一等公民的编程语言中，Lambda表达式的类型是函数。但是在Java8中，有所不同。在Java8中，Lambda表达式是对象，而不是函数，它们必须依附于一类特别的对象类型——函数式接口。  简单的说，在Java8中，Lambda表达式就是一个函数式接口的实例。这就是Lambda表达式和函数式接口的关系。也就是说，只要一个对象是函数式接口的实例，那么该对象就可以用Lambda表达式来表示。 所以以前用匿名实现类表示的现在都可以用Lambda表达式来写。16.2 函数式接口函数式接口举例16.2 函数式接口自定义函数式接口函数式接口中使用泛型：16.2 函数式接口作为参数传递 Lambda 表达式实例化作为参数传递 Lambda 表达式：作为参数传递 Lambda 表达式：为了将 Lambda 表达式作为参数传递，接收Lambda表达式的参数类型必须是与该 Lambda 表达式兼容的函数式接口的类型。Java 内置四大核心函数式接口函数式接口 参数类型 返回类型 用途Consumer消费型接口 T void 对类型为T的对象应用操作，包含方法：void accept(T t)Supplier供给型接口 无 T 返回类型为T的对象，包含方法：T get()Function函数型接口 T R对类型为T的对象应用操作，并返回结果。结果是R类型的对象。包含方法：R apply(T t)Predicate断定型接口 T boolean 确定类型为T的对象是否满足某约束，并返回boolean 值。包含方法：boolean test(T t)16.2 函数式接口其他接口函数式接口 参数类型 返回类型 用途BiFunction T, U R对类型为 T, U 参数应用操作，返回 R 类型的结果。包含方法为： R apply(T t, U u);UnaryOperator(Function子接口) T T对类型为T的对象进行一元运算，并返回T类型的结果。包含方法为：T apply(T t);BinaryOperator(BiFunction 子接口)T, T T对类型为T的对象进行二元运算，并返回T类型的结果。包含方法为： T apply(T t1, T t2);BiConsumer T, U void 对类型为T, U 参数应用操作。包含方法为： void accept(T t, U u)BiPredicate T,U boolean 包含方法为： boolean test(T t,U u)ToIntFunctionToLongFunctionToDoubleFunctionTintlongdouble分别计算int、long、double值的函数IntFunctionLongFunctionDoubleFunctionintlongdoubleR 参数分别为int、long、double 类型的函数\n2.3 方法引用与构造器引用方法引用(Method References) 当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用！ 方法引用可以看做是Lambda表达式深层次的表达。换句话说，方法引用就是Lambda表达式，也就是函数式接口的一个实例，通过方法的名字来指向一个方法，可以认为是Lambda表达式的一个语法糖。 要求：实现接口的抽象方法的参数列表和返回值类型，必须与方法引用的方法的参数列表和返回值类型保持一致！ 格式：使用操作符 “::” 将类(或对象) 与 方法名分隔开来。 如下三种主要使用情况：  对象::实例方法名 类::静态方法名 类::实例方法名16.3 方法引用与构造器引用方法引用例如：等同于：例如：等同于：16.3 方法引用与构造器引用方法引用例如：等同于：注意：当函数式接口方法的第一个参数是需要引用方法的调用者，并且第二个参数是需要引用方法的参数(或无参数)时：ClassName::methodName16.3 方法引用与构造器引用构造器引用格式： ClassName::new与函数式接口相结合，自动与函数式接口中方法兼容。可以把构造器引用赋值给定义的方法，要求构造器参数列表要与接口中抽象方法的参数列表一致！且方法的返回值即为构造器对应类的对象。例如：等同于：16.3 方法引用与构造器引用数组引用格式： type[] :: new例如：等同于：16-4 强大的Stream AP\n2.4 强大的Stream APIStream API说明 Java8中有两大最为重要的改变。第一个是 Lambda 表达式；另外一个则是 Stream API。  Stream API ( java.util.stream) 把真正的函数式编程风格引入到Java中。这是目前为止对Java类库最好的补充，因为Stream API可以极大提供Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。 Stream 是 Java8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。 使用Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。也可以使用 Stream API 来并行执行操作。简言之，Stream API 提供了一种高效且易于使用的处理数据的方式。16.4 强大的Stream API为什么要使用Stream API 实际开发中，项目中多数数据源都来自于Mysql，Oracle等。但现在数据源可以更多了，有MongDB，Radis等，而这些NoSQL的数据就需要Java层面去处理。  Stream 和 Collection 集合的区别：Collection 是一种静态的内存数据结构，而 Stream 是有关计算的。前者是主要面向内存，存储在内存中，后者主要是面向 CPU，通过 CPU 实现计算。16.4 强大的Stream API什么是 StreamStream到底是什么呢？是数据渠道，用于操作数据源（集合、数组等）所生成的元素序列。“集合讲的是数据，Stream讲的是计算！”注意：①Stream 自己不会存储元素。②Stream 不会改变源对象。相反，他们会返回一个持有结果的新Stream。 ③Stream 操作是延迟执行的。这意味着他们会等到需要结果的时候才执行。16.4 强大的Stream APIStream 的操作三个步骤 1- 创建 Stream一个数据源（如：集合、数组），获取一个流 2- 中间操作一个中间操作链，对数据源的数据进行处理 3- 终止操作(终端操作) 一旦执行终止操作，就执行中间操作链，并产生结果。之后，不会再被使用16.4 强大的Stream API创建 Stream方式一：通过集合Java8 中的 Collection 接口被扩展，提供了两个获取流的方法：  default Stream stream() : 返回一个顺序流 default Stream parallelStream() : 返回一个并行流16.4 强大的Stream API创建 Stream方式二：通过数组Java8 中的 Arrays 的静态方法 stream() 可以获取数组流： static  Stream stream(T[] array): 返回一个流重载形式，能够处理对应基本类型的数组： public static IntStream stream(int[] array) public static LongStream stream(long[] array) public static DoubleStream stream(double[] array)16.4 强大的Stream API创建 Stream方式三：通过Stream的of()可以调用Stream类静态方法 of(), 通过显示值创建一个流。它可以接收任意数量的参数。 public static Stream of(T… values) : 返回一个流16.4 强大的Stream API创建 Stream方式四：创建无限流可以使用静态方法 Stream.iterate() 和 Stream.generate(),创建无限流。 迭代public static Stream iterate(final T seed, final UnaryOperator f) 生成public static Stream generate(Supplier s)16.4 强大的Stream API// 方式四：创建无限流@Testpublic void test4() {// 迭代// public static Stream iterate(final T seed, final// UnaryOperator f)Stream stream = Stream.iterate(0, x -&gt; x + 2);stream.limit(10).forEach(System.out::println);// 生成// public static Stream generate(Supplier s)Stream stream1 = Stream.generate(Math::random);stream1.limit(10).forEach(System.out::println);}Stream 的中间操作1-筛选与切片多个中间操作可以连接起来形成一个流水线，除非流水线上触发终止操作，否则中间操作不会执行任何的处理！而在终止操作时一次性全部处理，称为“惰性求值”。 方 法 描 述filter(Predicate p) 接收 Lambda ， 从流中排除某些元素distinct() 筛选，通过流所生成元素的 hashCode() 和 equals() 去除重复元素limit(long maxSize) 截断流，使其元素不超过给定数量skip(long n)跳过元素，返回一个扔掉了前 n 个元素的流。若流中元素不足 n 个，则返回一个空流。与 limit(n) 互补16.4 强大的Stream APIStream 的中间操作2-映 射方法 描述map(Function f)接收一个函数作为参数，该函数会被应用到每个元素上，并将其映射成一个新的元素。mapToDouble(ToDoubleFunction f)接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的 DoubleStream。mapToInt(ToIntFunction f)接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的 IntStream。mapToLong(ToLongFunction f)接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的 LongStream。flatMap(Function f)接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流16.4 强大的Stream APIStream 的中间操作3-排序方法 描述sorted() 产生一个新流，其中按自然顺序排序sorted(Comparator com) 产生一个新流，其中按比较器顺序排序16.4 强大的Stream APIStream 的终止操作1-匹配与查找 终端操作会从流的流水线生成结果。其结果可以是任何不是流的值，例如：List、Integer，甚至是 void 。  流进行了终止操作后，不能再次使用。方法 描述allMatch(Predicate p) 检查是否匹配所有元素anyMatch(Predicate p) 检查是否至少匹配一个元素noneMatch(Predicate p) 检查是否没有匹配所有元素findFirst() 返回第一个元素findAny() 返回当前流中的任意元素16.4 强大的Stream APIStream 的终止操作方法 描述count() 返回流中元素总数max(Comparator c) 返回流中最大值min(Comparator c) 返回流中最小值forEach(Consumer c)内部迭代(使用 Collection 接口需要用户去做迭代，称为外部迭代。相反，Stream API 使用内部迭代——它帮你把迭代做了)16.4 强大的Stream APIStream 的终止操作2-归约备注：map 和 reduce 的连接通常称为 map-reduce 模式，因 Google用它来进行网络搜索而出名。方法 描述reduce(T iden, BinaryOperator b) 可以将流中元素反复结合起来，得到一个值。返回 Treduce(BinaryOperator b) 可以将流中元素反复结合起来，得到一个值。返回 Optional16.4 强大的Stream APIStream 的终止操作3-收集Collector 接口中方法的实现决定了如何对流执行收集的操作(如收集到 List、Set、Map)。另外， Collectors 实用类提供了很多静态方法，可以方便地创建常见收集器实例，具体方法与实例如下表：方 法 描 述collect(Collector c)将流转换为其他形式。接收一个 Collector接口的实现，用于给Stream中元素做汇总的方法16.4 强大的Stream API方法 返回类型 作用toList List 把流中元素收集到ListList emps= list.stream().collect(Collectors.toList());toSet Set 把流中元素收集到SetSet emps= list.stream().collect(Collectors.toSet());toCollection Collection 把流中元素收集到创建的集合Collection emps =list.stream().collect(Collectors.toCollection(ArrayList::new));counting Long 计算流中元素的个数long count = list.stream().collect(Collectors.counting());summingInt Integer 对流中元素的整数属性求和int total=list.stream().collect(Collectors.summingInt(Employee::getSalary));averagingInt Double 计算流中元素Integer属性的平均值double avg = list.stream().collect(Collectors.averagingInt(Employee::getSalary));summarizingInt IntSummaryStatistics 收集流中Integer属性的统计值。如：平均值int SummaryStatisticsiss= list.stream().collect(Collectors.summarizingInt(Employee::getSalary));16.4 强大的Stream API： Collectorsjoining String 连接流中每个字符串String str= list.stream().map(Employee::getName).collect(Collectors.joining());maxBy Optional 根据比较器选择最大值Optionalmax= list.stream().collect(Collectors.maxBy(comparingInt(Employee::getSalary)));minBy Optional 根据比较器选择最小值Optional min = list.stream().collect(Collectors.minBy(comparingInt(Employee::getSalary)));reducing 归约产生的类型从一个作为累加器的初始值开始，利用BinaryOperator与流中元素逐个结合，从而归约成单个值int total=list.stream().collect(Collectors.reducing(0, Employee::getSalar, Integer::sum));collectingAndThen 转换函数返回的类型 包裹另一个收集器，对其结果转换函数int how= list.stream().collect(Collectors.collectingAndThen(Collectors.toList(), List::size));groupingBy Map&gt; 根据某属性值对流分组，属性为K，结果为VMap&gt; map= list.stream().collect(Collectors.groupingBy(Employee::getStatus));partitioningBy Map&gt; 根据true或false进行分区Map&gt; vd = list.stream().collect(Collectors.partitioningBy(Employee::getManage));\n2.5 Optional类 到目前为止，臭名昭著的空指针异常是导致Java应用程序失败的最常见原因。以前，为了解决空指针异常，Google公司著名的Guava项目引入了Optional类，Guava通过使用检查空值的方式来防止代码污染，它鼓励程序员写更干净的代码。受到Google Guava的启发，Optional类已经成为Java 8类库的一部分。 Optional 类(java.util.Optional) 是一个容器类，它可以保存类型T的值，代表这个值存在。或者仅仅保存null，表示这个值不存在。原来用 null 表示一个值不存在，现在 Optional 可以更好的表达这个概念。并且可以避免空指针异常。  Optional类的Javadoc描述如下：这是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。16.5 Optional 类  Optional提供很多有用的方法，这样我们就不用显式进行空值检测。  创建Optional类对象的方法： Optional.of(T t) : 创建一个 Optional 实例，t必须非空；  Optional.empty() : 创建一个空的 Optional 实例 Optional.ofNullable(T t)：t可以为null 判断Optional容器中是否包含对象： boolean isPresent() : 判断是否包含对象 void ifPresent(Consumer&lt;? super T&gt; consumer) ：如果有值，就执行Consumer接口的实现代码，并且该值会作为参数传给它。 获取Optional容器的对象： T get(): 如果调用对象包含值，返回该值，否则抛异常 T orElse(T other) ：如果有值则将其返回，否则返回指定的other对象。 T orElseGet(Supplier&lt;? extends T&gt; other) ：如果有值则将其返回，否则返回由Supplier接口实现提供的对象。 T orElseThrow(Supplier&lt;? extends X&gt; exceptionSupplier) ：如果有值则将其返回，否则抛出由Supplier接口实现提供的异常。16.5 Optional 类@Testpublic void test1() {Boy b = new Boy(“张三”);Optional opt = Optional.ofNullable(b.getGrilFriend());// 如果女朋友存在就打印女朋友的信息opt.ifPresent(System.out::println);}@Testpublic void test2() {Boy b = new Boy(“张三”);Optional opt = Optional.ofNullable(b.getGrilFriend());// 如果有女朋友就返回他的女朋友，否则只能欣赏“嫦娥”了Girl girl = opt.orElse(new Girl(“嫦娥”));System.out.println(“他的女朋友是：” + girl.getName());}16.5 Optional 类@Testpublic void test3(){Optional opt = Optional.of(new Employee(“张三”, 8888));//判断opt中员工对象是否满足条件，如果满足就保留，否则返回空Optional emp = opt.filter(e -&gt; e.getSalary()&gt;10000);System.out.println(emp);}@Testpublic void test4(){Optional opt = Optional.of(new Employee(“张三”, 8888));//如果opt中员工对象不为空，就涨薪10%Optional emp = opt.map(e -&gt;{e.setSalary(e.getSalary()%1.1);return e;});System.out.println(emp);}\nJava9&amp;Java10&amp; Java11新特性3.1 Java 9 的新特性JDK 9 的发布 经过4次跳票，历经曲折的Java 9 终于终于在2017年9月21日发布。  从Java 9 这个版本开始，Java 的计划发布周期是 6 个月，下一个 Java 的主版本将于 2018 年 3 月发布，命名为 Java 18.3，紧接着再过六个月将发布 Java18.9。  这意味着Java的更新从传统的以特性驱动的发布周期，转变为以时间驱动的 （6 个月为周期）发布模式，并逐步的将 Oracle JDK 原商业特性进行开源。 针对企业客户的需求，Oracle 将以三年为周期发布长期支持版本（long termsupport）。  Java 9 提供了超过150项新功能特性，包括备受期待的模块化系统、可交互的 REPL 工具：jshell，JDK 编译工具，Java 公共 API 和私有代码，以及安全增强、扩展提升、性能管理改善等。可以说Java 9是一个庞大的系统工程，完全做了一个整体改变。Java 9 中有哪些不得不说的新特性？ 模块化系统 jShell命令 多版本兼容jar包  接口的私有方法 钻石操作符的使用升级 语法改进：try语句 String存储结构变更 便利的集合特性：of() 增强的Stream API 全新的HTTP客户端API Deprecated的相关API javadoc的HTML 5支持 Javascript引擎升级：Nashorn java的动态编译器Java 9 中有哪些不得不说的新特性？ 官方提供的新特性列表：https://docs.oracle.com/javase/9/whatsnew/toc.htm#JSNEW-GUID- C23AFD78-C777-460B-8ACE-58BE5EA681F6  或参考 Open JDKhttp://openjdk.java.net/projects/jdk9/  在线Oracle JDK 9 Documentationhttps://docs.oracle.com/javase/9/一、JDK 和 JRE 目录结构的改变JDK 8 的目录结构bin 目录 包含命令行开发和调试工具，如javac，jar和javadoc。include目录 包含在编译本地代码时使用的C/C++头文件lib 目录 包含JDK工具的几个JAR和其他类型的文件。 它有一个tools.jar文件，其中包含javac编译器的Java类jre/bin 目录 包含基本命令，如java命令。 在Windows平台上，它包含系统的运行时动态链接库（DLL）。jre/lib 目录 包含用户可编辑的配置文件，如.properties和.policy文件。包含几个JAR。rt.jar文件包含运行时的Java类和资源文件。JDK 9 的目录结构没有名为jre的子目录bin 目录 包含所有命令。 在Windows平台上，它继续包含系统的运行时动态链接库。conf 目录 包含用户可编辑的配置文件，例如以前位于jre\\lib目录中的.properties和.policy文件include 目录 包含要在以前编译本地代码时使用的C/C++头文件。 它只存在于JDK中jmods 目录 包含JMOD格式的平台模块。 创建自定义运行时映像时需要它。 它只存在于JDK中legal 目录 包含法律声明lib 目录 包含非Windows平台上的动态链接本地库。 其子目录和文件不应由开发人员直接编辑或使用一、JDK 和 JRE 目录结构的改变二、模块化系统: Jigsaw  Modularity 谈到 Java 9 大家往往第一个想到的就是 Jigsaw 项目。众所周知，Java 已经发展超过 20 年（95 年最初发布），Java 和相关生态在不断丰富的同时也越来越暴露出一些问题：  Java 运行环境的膨胀和臃肿。每次JVM启动的时候，至少会有30～60MB的内存加载，主要原因是JVM需要加载rt.jar，不管其中的类是否被classloader加载，第一步整个jar都会被JVM加载到内存当中去（而模块化可以根据模块的需要加载程序运行需要的class）  当代码库越来越大，创建复杂，盘根错节的“意大利面条式代码”的几率呈指数级的增长。不同版本的类库交叉依赖导致让人头疼的问题，这些都阻碍了 Java 开发和运行效率的提升。  很难真正地对代码进行封装, 而系统并没有对不同部分（也就是 JAR 文件）之间的依赖关系有个明确的概念。每一个公共类都可以被类路径之下任何其它的公共类所访问到，这样就会导致无意中使用了并不想被公开访问的 API。二、模块化系统: Jigsaw  Modularity 本质上讲也就是说，用模块来管理各个package，通过声明某个package暴露，，模块(module)的概念，其实就是package外再裹一层，不声明默认就是隐藏。因此，模块化使得代码组织上更安全，因为它可以指定哪些部分可以暴露，哪些部分隐藏。  实现目标 模块化的主要目的在于减少内存的开销 只须必要模块，而非全部jdk模块，可简化各种类库和大型应用的开发和维护 改进 Java SE 平台，使其可以适应不同大小的计算设备 改进其安全性，可维护性，提高性能二、模块化系统: Jigsaw  Modularity模块将由通常的类和新的模块声明文件（module-info.java）组成。该文件是位于java代码结构的顶层，该模块描述符明确地定义了我们的模块需要什么依赖关系，以及哪些模块被外部使用。在exports子句中未提及的所有包默认情况下将封装在模块中，不能在外部使用。二、模块化系统: Jigsaw  Modularity要想在java9demo模块中调用java9test模块下包中的结构，需要在java9test的module-info.java中声明：/**\n\n@author songhongkang\n@create 2019 下午 11:57/module java9test {//package we exportexports com.atguigui.bean;}exports：控制着哪些包可以被其它模块访问到。所有不被导出的包默认都被封装在模块里面。二、模块化系统: Jigsaw  Modularity对应在java 9demo 模块的src 下创建module-info.java文件：/*\n@author songhongkang\n@create 2019 下午 11:51*/module java9demo {requires java9test;}requires：指明对其它模块的依赖。三、Java的REPL工具： jShell命令 产生背景像Python 和 Scala 之类的语言早就有交互式编程环境 REPL (read - evaluate - print -loop)了，以交互式的方式对语句和表达式进行求值。开发者只需要输入一些代码，就可以在编译前获得对程序的反馈。而之前的Java版本要想执行代码，必须创建文件、声明类、提供测试方法方可实现。  设计理念即写即得、快速运行 实现目标 Java 9 中终于拥有了 REPL工具：jShell。让Java可以像脚本语言一样运行，从控制台启动jShell，利用jShell在没有创建类的情况下直接声明变量，计算表达式，执行语句。即开发时可以在命令行里直接运行Java的代码，而无需创建Java文件，无需跟人解释”public static void main(String[] args)”这句废话。 jShell也可以从文件中加载语句或者将语句保存到文件中。 jShell也可以是tab键进行自动补全和自动添加分号。三、Java的REPL工具： jShell命令调出jShell获取帮助基本使用Tips：在 JShell 环境下，语句末尾的“;” 是可选的。但推荐还是最好加上。提高代码可读性。导入指定的包默认已经导入如下的所有包：(包含java.lang包)三、Java的REPL工具： jShell命令三、Java的REPL工具： jShell命令只需按下 Tab 键，就能自动补全代码列出当前 session 里所有有效的代码片段三、Java的REPL工具： jShell命令查看当前 session 下所有创建过的变量Tips：我们还可以重新定义相同方法名和参数列表的方法，即为对现有方法的修改（或覆盖）。查看当前 session 下所有创建过的方法使用外部代码编辑器来编写 Java 代码三、Java的REPL工具： jShell命令使用/open命令调用：没有受检异常（编译时异常）说明：本来应该强迫我们捕获一个IOException，但却没有出现。因为jShell在后台为我们隐藏了。退出jShell四、语法改进：接口的私有方法Java 8中规定接口中的方法除了抽象方法之外，还可以定义静态方法和默认的方法。一定程度上，扩展了接口的功能，此时的接口更像是一个抽象类。 在Java 9中，接口更加的灵活和强大，连方法的访问权限修饰符都可以声明为private的了，此时方法将不会成为你对外暴露的API的一部分。四、语法改进：接口的私有方法interface MyInterface {void normalInterfaceMethod();default void methodDefault1() {init();}public default void methodDefault2() {init();}// This method is not part of the public API exposed by MyInterfaceprivate void init() {System.out.println(“默认方法中的通用操作”);} }四、语法改进：接口的私有方法class MyInterfaceImpl implements MyInterface {@Overridepublic void normalInterfaceMethod() {System.out.println(“实现接口的方法”);} }public class MyInterfaceTest {public static void main(String[] args) {MyInterfaceImpl impl = new MyInterfaceImpl();impl.methodDefault1();// impl.init();//不能调用} }五、语法改进:钻石操作符使用升级我们将能够与匿名实现类共同使用钻石操作符（diamond operator）在Java 8中如下的操作是会报错的：Comparator com = new Comparator&lt;&gt;(){@Overridepublic int compare(Object o1, Object o2) {return 0; }};编译报错信息：Cannot use “&lt;&gt;” with anonymous inner classes.五、语法改进:钻石操作符使用升级Java 9中如下操作可以正常执行通过：// anonymous classes can now use type inferenceComparator com = new Comparator&lt;&gt;(){@Overridepublic int compare(Object o1, Object o2) {return 0; }};六、语法改进：try语句Java 8 中，可以实现资源的自动关闭，但是要求执行后必须关闭的所有资源必须在try子句中初始化，否则编译不通过。如下例所示：try(InputStreamReader reader = new InputStreamReader(System.in)){//读取数据细节省略}catch (IOException e){e.printStackTrace();}六、语法改进：try语句Java 9 中，用资源语句编写try将更容易，我们可以在try子句中使用已经初始化过的资源，此时的资源是final的：InputStreamReader reader = new InputStreamReader(System.in);OutputStreamWriter writer = new OutputStreamWriter(System.out);try (reader; writer) {//reader是final的，不可再被赋值//reader = null;//具体读写操作省略} catch (IOException e) {e.printStackTrace();}七、String存储结构变更MotivationThe current implementation of the String class stores characters in a chararray, using two bytes (sixteen bits) for each character. Data gathered frommany different applications indicates that strings are a major component ofheap usage and, moreover, that most String objects contain only Latin-1characters. Such characters require only one byte of storage, hence half of thespace in the internal char arrays of such String objects is going unused.DescriptionWe propose to change the internal representation of the String class from aUTF-16 char array to a byte array plus an encoding-flag field. The new Stringclass will store characters encoded either as ISO-8859-1/Latin-1 (one byte percharacter), or as UTF-16 (two bytes per character), based upon the contentsof the string. The encoding flag will indicate which encoding is used.七、String存储结构变更结论：String 再也不用 char[] 来存储啦，改成了 byte[] 加上编码标记，节约了一些空间。public final class Stringimplements java.io.Serializable, Comparable, CharSequence {@Stableprivate final byte[] value; }那StringBuffer 和 StringBuilder 是否仍无动于衷呢？String-related classes such as AbstractStringBuilder, StringBuilder,and StringBuffer will be updated to use the same representation, as will theHotSpot VM‘s intrinsic(固有的、内置的) string operations.八、集合工厂方法：快速创建只读集合要创建一个只读、不可改变的集合，必须构造和分配它，然后添加元素，最后包装成一个不可修改的集合。List namesList = new ArrayList &lt;&gt;();namesList.add(“Joe”);namesList.add(“Bob”);namesList.add(“Bill”);namesList = Collections.unmodifiableList(namesList);System.out.println(namesList);缺点：我们一下写了五行。即：它不能表达为单个表达式。八、集合工厂方法：快速创建只读集合List list = Collections.unmodifiableList(Arrays.asList(“a”, “b”, “c”));Set set = Collections.unmodifiableSet(new HashSet&lt;&gt;(Arrays.asList(“a”,“b”, “c”)));// 如下操作不适用于jdk 8 及之前版本,适用于jdk 9Map map = Collections.unmodifiableMap(new HashMap&lt;&gt;() {{put(“a”, 1);put(“b”, 2);put(“c”, 3);}});map.forEach((k, v) -&gt; System.out.println(k + “:” + v));八、集合工厂方法：快速创建只读集合Java 9因此引入了方便的方法，这使得类似的事情更容易表达。八、集合工厂方法：快速创建只读集合List firsnamesList = List.of(“Joe”,”Bob”,”Bill”);调用集合中静态方法of()，可以将不同数量的参数传输到此工厂方法中。此功能可用于Set和List，也可用于Map的类似形式。此时得到的集合，是不可变的：在创建后，继续添加元素到这些集合会导致 “UnsupportedOperationException” 。由于Java 8中接口方法的实现，可以直接在List，Set和Map的接口内定义这些方法，便于调用。List list = List.of(“a”, “b”, “c”);Set set = Set.of(“a”, “b”, “c”);Map map1 = Map.of(“Tom”, 12, “Jerry”, 21, “Lilei”, 33,“HanMeimei”, 18);Map map2 = Map.ofEntries(Map.entry(“Tom”, 89),Map.entry(“Jim”, 78), Map.entry(“Tim”, 98));九、InputStream 加强InputStream 终于有了一个非常有用的方法：transferTo，可以用来将数据直接传输到 OutputStream，这是在处理原始数据流时非常常见的一种用法，如下示例。ClassLoader cl = this.getClass().getClassLoader();try (InputStream is = cl.getResourceAsStream(“hello.txt”);OutputStream os = new FileOutputStream(“src\\hello1.txt”)) {is.transferTo(os); // 把输入流中的所有数据直接自动地复制到输出流中} catch (IOException e) {e.printStackTrace();}十、增强的 Stream API Java 的 Steam API 是java标准库最好的改进之一，让开发者能够快速运算，从而能够有效的利用数据并行计算。Java 8 提供的 Steam 能够利用多核架构实现声明式的数据处理。 在 Java 9 中，Stream API 变得更好，Stream 接口中添加了 4 个新的方法：takeWhile, dropWhile, ofNullable，还有个 iterate 方法的新重载方法，可以让你提供一个 Predicate (判断条件)来指定什么时候结束迭代。  除了对 Stream 本身的扩展，Optional 和 Stream 之间的结合也得到了改进。现在可以通过 Optional 的新方法 stream() 将一个 Optional 对象转换为一个(可能是空的) Stream 对象。十、增强的 Stream APItakeWhile()的使用用于从 Stream 中获取一部分数据，接收一个 Predicate 来进行选择。在有序的Stream 中，takeWhile 返回从开头开始的尽量多的元素。List list = Arrays.asList(45, 43, 76, 87, 42, 77, 90, 73, 67, 88);list.stream().takeWhile(x -&gt; x &lt; 50).forEach(System.out::println);System.out.println();list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8);list.stream().takeWhile(x -&gt; x &lt; 5).forEach(System.out::println);十、增强的 Stream APIdropWhile()的使用dropWhile 的行为与 takeWhile 相反，返回剩余的元素。List list = Arrays.asList(45, 43, 76, 87, 42, 77, 90, 73, 67, 88);list.stream().dropWhile(x -&gt; x &lt; 50).forEach(System.out::println);System.out.println();list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8);list.stream().dropWhile(x -&gt; x &lt; 5).forEach(System.out::println);十、增强的 Stream APIofNullable()的使用Java 8 中 Stream 不能完全为null，否则会报空指针异常。而 Java 9 中的 ofNullable 方法允许我们创建一个单元素 Stream，可以包含一个非空元素，也可以创建一个空Stream。// 报NullPointerException// Stream stream1 = Stream.of(null);// System.out.println(stream1.count());// 不报异常，允许通过Stream stringStream = Stream.of(“AA”, “BB”, null);System.out.println(stringStream.count());// 3// 不报异常，允许通过List list = new ArrayList&lt;&gt;();list.add(“AA”);list.add(null);System.out.println(list.stream().count());// 2// ofNullable()：允许值为nullStream stream1 = Stream.ofNullable(null);System.out.println(stream1.count());// 0Stream stream = Stream.ofNullable(“hello world”);System.out.println(stream.count());// 1十、增强的 Stream APIiterate()重载的使用这个 iterate 方法的新重载方法，可以让你提供一个 Predicate (判断条件)来指定什么时候结束迭代。// 原来的控制终止方式：Stream.iterate(1, i -&gt; i + 1).limit(10).forEach(System.out::println);// 现在的终止方式：Stream.iterate(1, i -&gt; i &lt; 100, i -&gt; i + 1).forEach(System.out::println);十一、Optional获取Stream的方法Optional类中stream()的使用List list = new ArrayList&lt;&gt;();list.add(“Tom”);list.add(“Jerry”);list.add(“Tim”);Optional&gt; optional = Optional.ofNullable(list);Stream&gt; stream = optional.stream();stream.flatMap(x -&gt; x.stream()).forEach(System.out::println);十二、Javascript引擎升级：Nashorn Nashorn 项目在 JDK 9 中得到改进，它为 Java 提供轻量级的 Javascript 运行时。Nashorn 项目跟随 Netscape 的 Rhino 项目，目的是为了在 Java 中实现一个高性能但轻量级的 Javascript 运行时。Nashorn 项目使得 Java 应用能够嵌入Javascript。它在 JDK 8 中为 Java 提供一个 Javascript 引擎。  JDK 9 包含一个用来解析 Nashorn 的 ECMAScript 语法树的 API。这个 API 使得IDE 和服务端框架不需要依赖 Nashorn 项目的内部实现类，就能够分析ECMAScript 代码\n\n3.2 Java 10 的新特性 2018年3月21日，Oracle官方宣布Java10正式发布。  需要注意的是 Java 9 和 Java 10 都不是 LTS (Long-Term-Support) 版本。和过去的 Java 大版本升级不同，这两个只有半年左右的开发和维护期。而未来的 Java 11，也就是 18.9 LTS，才是 Java 8 之后第一个 LTS 版本。  JDK10一共定义了109个新特性，其中包含12个JEP（对于程序员来讲，真正的新特性其实就一个），还有一些新API和JVM规范以及JAVA语言规范上的改动。  JDK10的12个JEP（JDK Enhancement Proposal特性加强提议）参阅官方文档：http://openjdk.java.net/projects/jdk/10/JDK10的12个JEP286: Local-Variable Type Inference 局部变量类型推断296: Consolidate the JDK Forest into a Single Repository JDK库的合并304: Garbage-Collector Interface 统一的垃圾回收接口307: Parallel Full GC for G1 为G1提供并行的Full GC310: Application Class-Data Sharing 应用程序类数据（AppCDS）共享312: Thread-Local Handshakes ThreadLocal握手交互313: Remove the Native-Header Generation Tool (javah) 移除JDK中附带的javah工具314: Additional Unicode Language-Tag Extensions 使用附加的Unicode语言标记扩展316: Heap Allocation on Alternative Memory Devices 能将堆内存占用分配给用户指定的备用内存设备317: Experimental Java-Based JIT Compiler 使用基于Java的JIT编译器319: Root Certificates 根证书322: Time-Based Release Versioning 基于时间的发布版本一、局部变量类型推断 产生背景开发者经常抱怨Java中引用代码的程度。局部变量的显示类型声明，常常被认为是不必须的，给一个好听的名字经常可以很清楚的表达出下面应该怎样继续。  好处：减少了啰嗦和形式的代码，避免了信息冗余，而且对齐了变量名，更容易阅读！ 举例如下： 场景一：类实例化时作为 Java开发者，在声明一个变量时，我们总是习惯了敲打两次变量类型，第一次用于声明变量类型，第二次用于构造器。LinkedHashSet set = new LinkedHashSet&lt;&gt;(); 场景二：返回值类型含复杂泛型结构变量的声明类型书写复杂且较长，尤其是加上泛型的使用Iterator&gt; iterator = set.iterator();一、局部变量类型推断 场景三：我们也经常声明一种变量，它只会被使用一次，而且是用在下一行代码中，比如：URL url = new URL(“http://www.atguigu.com“);URLConnection connection = url.openConnection();Reader reader = new BufferedReader(newInputStreamReader(connection.getInputStream()));尽管 IDE可以帮我们自动完成这些代码，但当变量总是跳来跳去的时候，可读性还是会受到影响，因为变量类型的名称由各种不同长度的字符组成。而且，有时候开发人员会尽力避免声明中间变量，因为太多的类型声明只会分散注意力，不会带来额外的好处。一、局部变量类型推断适用于以下情况：//1.局部变量的初始化var list = new ArrayList&lt;&gt;();//2.增强for循环中的索引for(var v : list) {System.out.println(v);}//3.传统for循环中for(var i = 0;i &lt; 100;i++) {System.out.println(i);}初始值为nullLambda表达式为数组静态初始化一、局部变量类型推断在局部变量中使用时，如下情况不适用：方法引用一、局部变量类型推断不适用以下的结构中：  情况1：没有初始化的局部变量声明 情况2：方法的返回类型 情况3：方法的参数类型 情况4：构造器的参数类型 情况5：属性 情况6：catch块一、局部变量类型推断工作原理在处理 var时，编译器先是查看表达式右边部分，并根据右边变量值的类型进行推断，作为左边变量的类型，然后将该类型写入字节码当中。 注 意  var不是一个关键字你不需要担心变量名或方法名会与 var发生冲突，因为 var实际上并不是一个关键字，而是一个类型名，只有在编译器需要知道类型的地方才需要用到它。除此之外，它就是一个普通合法的标识符。也就是说，除了不能用它作为类名，其他的都可以，但极少人会用它作为类名。  这不是JavaScript首先我要说明的是，var并不会改变Java是一门静态类型语言的事实。编译器负责推断出类型，并把结果写入字节码文件，就好像是开发人员自己敲入类型一样。下面是使用 IntelliJ（实际上是 Fernflower的反编译器）反编译器反编译出的代码：一、局部变量类型推断var url = new URL(“http://www.atguigu.com“);var connection = url.openConnection();var reader = new BufferedReader(new InputStreamReader(connection.getInputStream()));反编译后URL url = new URL(“http://www.atguigu.com“);URLConnection connection = url.openConnection();BufferedReader reader = new BufferedReader(new InputStreamReader(connection.getInputStream()));从代码来看，就好像之前已经声明了这些类型一样。事实上，这一特性只发生在编译阶段，与运行时无关，所以对运行时的性能不会产生任何影响。所以请放心，这不是 JavaScript。二、集合新增创建不可变集合的方法自 Java 9 开始，Jdk 里面为集合（List / Set / Map）都添加了 of (jdk9新增)和copyOf (jdk10新增)方法，它们两个都用来创建不可变的集合，来看下它们的使用和区别。//示例1：var list1 = List.of(“Java”, “Python”, “C”);var copy1 = List.copyOf(list1);System.out.println(list1 == copy1); // true//示例2：var list2 = new ArrayList();var copy2 = List.copyOf(list2);System.out.println(list2 == copy2); // false//示例1和2代码基本一致，为什么一个为true,一个为false?从 源 码 分 析 ， 可 以 看 出 copyOf 方 法 会 先 判 断 来 源 集 合 是 不 是AbstractImmutableList 类型的，如果是，就直接返回，如果不是，则调用 of 创建一个新的集合。示例2因为用的 new 创建的集合，不属于不可变 AbstractImmutableList 类的子类，所以 copyOf 方法又创建了一个新的实例，所以为false。注意：使用of和copyOf创建的集合为不可变集合，不能进行添加、删除、替换、排序等操作，不然会报 java.lang.UnsupportedOperationException 异常。上面演示了 List 的 of 和 copyOf 方法，Set 和 Map 接口都有。\n3.3 Java 11 的新特性北京时间 2018年9 月 26 日，Oracle 官方宣布 Java 11 正式发布。这是 Java 大版本周期变化后的第一个长期支持版本，非常值得关注。从官网即可下载,最新发布的 Java11 将带来 ZGC、Http Client 等重要特性，一共包含 17 个 JEP（JDK EnhancementProposals，JDK 增强提案）。其 实，总共更新不止17个，只是我们更关注如下的17个JEP更新。JDK 11 将是一个 企业不可忽视的版本。从时间节点来看，JDK 11 的发布正好处在 JDK 8 免费更新到期的前夕，同时 JDK 9、10 也陆续成为“历史版本”，下面是 Oracle JDK 支持路线图：JDK 11 是一个长期支持版本（LTS, Long-Term-Support）  对于企业来说，选择 11 将意味着长期的、可靠的、可预测的技术路线图。其中免费的OpenJDK11 确定将得到 OpenJDK 社区的长期支持， LTS 版本将是可以放心选择的版本。  从 JVM GC 的角度，JDK11 引入了两种新的 GC，其中包括也许是划时代意义的 ZGC，虽然其目前还是实验特性，但是从能力上来看，这是 JDK 的一个巨大突破，为特定生产环境的苛刻需求提供了一个可能的选择。例如，对部分企业核心存储等产品，如果能够保证不超过 10ms 的 GC 暂停，可靠性会上一个大的台阶，这是过去我们进行 GC 调优几乎做不到的，是能与不能的问题。按照官方的说法，新的发布周期会严格遵循时间点，将于每年的3月份和9月份发布。所以 Java 11 的版本号是18.9(LTS)。不过与 Java 9 和 Java 10 这两个被称为“功能性的版本”不同（两者均只提供半年的技术支持），Java 11 不仅提供了长期支持服务，还将作为Java 平台的参考实现。Oracle 直到2023年9月都会为Java 11 提供技术支持，而补丁和安全警告等扩展支持将持续到2026年。新的长期支持版本每三年发布一次，根据后续的发布计划，下一个长期支持版 Java 17 将于2021年发布。官网公开的 17 个 JEP（JDK Enhancement Proposal 特性增强提议）181: Nest-Based Access Control（基于嵌套的访问控制）309: Dynamic Class-File Constants（动态的类文件常量）315: Improve Aarch64 Intrinsics（改进 Aarch64 Intrinsics）318: Epsilon: A No-Op Garbage Collector（Epsilon 垃圾回收器，又被称为”No-Op（无操作）“回收器）320: Remove the Java EE and CORBA Modules（移除 Java EE 和 CORBA 模块，JavaFX也已被移除）321: HTTP Client (Standard)323: Local-Variable Syntax for Lambda Parameters（用于 Lambda 参数的局部变量语法）324: Key Agreement with Curve25519 and Curve448（采用 Curve25519 和 Curve448 算法实现的密钥协议）官网公开的 17 个 JEP（JDK Enhancement Proposal 特性增强提议）327: Unicode 10328: Flight Recorder（飞行记录仪）329: ChaCha20 and Poly1305 Cryptographic Algorithms（实现 ChaCha20 和 Poly1305 加密算法）330: Launch Single-File Source-Code Programs（启动单个 Java 源代码文件的程序）331: Low-Overhead Heap Profiling（低开销的堆分配采样方法）332: Transport Layer Security (TLS) 1.3（对 TLS 1.3 的支持）333: ZGC: A Scalable Low-Latency Garbage Collector (Experimental)（ZGC：可伸缩的低延迟垃圾回收器，处于实验性阶段）335: Deprecate the Nashorn JavaScript Engine（弃用 Nashorn JavaScript 引擎）336: Deprecate the Pack200 Tools and API（弃用 Pack200 工具及其 API）一、新增了一系列字符串处理方法描述 举例判断字符串是否为空白 “ “.isBlank(); // true去除首尾空白 “ Javastack “.strip(); // “Javastack”去除尾部空格 “ Javastack “.stripTrailing(); // “ Javastack”去除首部空格 “ Javastack “.stripLeading(); // “Javastack “复制字符串 “Java”.repeat(3);// “JavaJavaJava”行数统计 “A\\nB\\nC”.lines().count(); // 3二、Optional 加强Optional 也增加了几个非常酷的方法，现在可以很方便的将一个 Optional 转换成一个 Stream, 或者当一个空 Optional 时给它一个替代的。新增方法 描述 新增的版本boolean isEmpty() 判断value是否为空 JDK 11ifPresentOrElse(Consumer&lt;?super T&gt; action, Runnable emptyAction)value非空，执行参数1功能；如果value为空，执行参数2功能 JDK 9Optional or(Supplier&lt;?extends Optional&lt;? extends T&gt;&gt; supplier)value非空，返回对应的Optional；value为空，返回形参封装的OptionalJDK 9Stream stream() value非空，返回仅包含此value的Stream；否则，返回一个空的Stream JDK 9T orElseThrow() value非空，返回value；否则抛异常NoSuchElementException JDK 10三、局部变量类型推断升级在var上添加注解的语法格式，在jdk10中是不能实现的。在JDK11中加入了这样的语法。//错误的形式: 必须要有类型, 可以加上var//Consumer con1 = (@Deprecated t) -&gt;System.out.println(t.toUpperCase());//正确的形式://使用var的好处是在使用lambda表达式时给参数加上注解。Consumer con2 = (@Deprecated var t) -&gt;System.out.println(t.toUpperCase());四、全新的HTTP 客户端API HTTP，用于传输网页的协议，早在1997年就被采用在目前的1.1版本中。直到2015年，HTTP2才成为标准。  HTTP/1.1和HTTP/2的主要区别是如何在客户端和服务器之间构建和传输数据。HTTP/1.1依赖于请求/响应周期。 HTTP/2允许服务器“push”数据：它可以发送比客户端请求更多的数据。这使得它可以优先处理并发送对于首先加载网页至关重要的数据。  这是 Java 9 开始引入的一个处理 HTTP 请求的的 HTTP Client API，该API 支持同步和异步，而在 Java 11 中已经为正式可用状态，你可以在java.net 包中找到这个 API。  它 将 替 代 仅 适 用 于 blocking 模式的 HttpURLConnection（HttpURLConnection是在HTTP 1.0的时代创建的，并使用了协议无关的方法），并提供对WebSocket 和 HTTP/2的支持。四、全新的HTTP 客户端APIHttpClient client = HttpClient.newHttpClient();HttpRequest request =HttpRequest.newBuilder(URI.create(“http://127.0.0.1:8080/test/\")).build();BodyHandler responseBodyHandler = BodyHandlers.ofString();HttpResponse response = client.send(request, responseBodyHandler);String body = response.body();System.out.println(body);HttpClient client = HttpClient.newHttpClient();HttpRequest request =HttpRequest.newBuilder(URI.create(“http://127.0.0.1:8080/test/\")).build();BodyHandler responseBodyHandler = BodyHandlers.ofString();CompletableFuture&gt; sendAsync =client.sendAsync(request, responseBodyHandler);sendAsync.thenApply(t -&gt; t.body()).thenAccept(System.out::println);//HttpResponse response = sendAsync.get();//String body = response.body();//System.out.println(body);五、更简化的编译运行程序看下面的代码。// 编译javac Javastack.java// 运行java Javastack在我们的认知里面，要运行一个 Java 源代码必须先编译，再运行，两步执行动作。而在未来的 Java 11 版本中，通过一个 java 命令就直接搞定了，如以下所示：java Javastack.java一个命令编译运行源代码的注意点： 执行源文件中的第一个类, 第一个类必须包含主方法。  并且不可以使用其它源文件中的自定义类, 本文件中的自定义类是可以使用的。六、废弃Nashorn引擎废除Nashorn javascript引擎，在后续版本准备移除掉，有需要的可以考虑使用GraalVM。七、ZGC GC是java主要优势之一。 然而, 当GC停顿太长, 就会开始影响应用的响应时间。消除或者减少GC停顿时长, java将对更广泛的应用场景是一个更有吸引力的平台。此外, 现代系统中可用内存不断增长,用户和程序员希望JVM能够以高效的方式充分利用这些内存, 并且无需长时间的GC暂停时间。  ZGC, A Scalable Low-Latency Garbage Collector(Experimental)ZGC, 这应该是JDK11最为瞩目的特性, 没有之一。 但是后面带了Experimental,说明这还不建议用到生产环境。  ZGC是一个并发, 基于region, 压缩型的垃圾收集器, 只有root扫描阶段会STW(stop the world), 因此GC停顿时间不会随着堆的增长和存活对象的增长而变长。七、ZGC 优势： GC暂停时间不会超过10ms 既能处理几百兆的小堆, 也能处理几个T的大堆(OMG) 和G1相比, 应用吞吐能力不会下降超过15%  为未来的GC功能和利用colord指针以及Load barriers优化奠定基础 初始只支持64位系统 ZGC的设计目标是：支持TB级内存容量，暂停时间低（&lt;10ms），对整个程序吞吐量的影响小于15%。 将来还可以扩展实现机制，以支持不少令人兴奋的功能，例如多层堆（即热对象置于DRAM和冷对象置于NVMe闪存），或压缩堆。八、其它新特性 Unicode 10 Deprecate the Pack200 Tools and API 新的Epsilon垃圾收集器 完全支持Linux容器（包括Docker）  支持G1上的并行完全垃圾收集 最新的HTTPS安全协议TLS 1.3 Java Flight Recorder在当前JDK中看不到什么？一个标准化和轻量级的JSON API一个标准化和轻量级的JSON API被许多Java开发人员所青睐。但是由于资金问题无法在Java当前版本中见到，但并不会削减掉。Java平台首席架构师MarkReinhold在JDK 9邮件列中说：“这个JEP将是平台上的一个有用的补充，但是在计划中，它并不像Oracle资助的其他功能那么重要，可能会重新考虑JDK 10或更高版本中实现。 ” 对许多应用而言货币价值都是一个关键的特性，但JDK对此却几乎没有任何支持。严格来讲，现有的java.util.Currency类只是代表了当前ISO 4217货币的一个数据结构，但并没有关联的值或者自定义货币。JDK对货币的运算及转换也没有内建的支持，更别说有一个能够代表货币值的标准类型了。  此前，Oracle 公布的JSR 354定义了一套新的Java货币API：JavaMoney，计划会在Java9中正式引入。但是目前没有出现在JDK 新特性 中。  不过，如果你用的是Maven的话，可以做如下的添加，即可使用相关的API处理货币：新的货币 API\n\norg.javamoney\nmoneta\n0.9&lt;/dependency&gt;在当前JDK中看不到什么？展 望  随着云计算和 AI 等技术浪潮，当前的计算模式和场景正在发生翻天覆地的变化，不仅对 Java 的发展速度提出了更高要求，也深刻影响着 Java 技术的发展方向。传统的大型企业或互联网应用，正在被云端、容器化应用、模块化的微服务甚至是函数(FaaS， Function-as-a-Service)所替代。  Java虽然标榜面向对象编程，却毫不顾忌的加入面向接口编程思想，又扯出匿名对象之概念，每增加一个新的东西，对Java的根本所在的面向对象思想的一次冲击。反观Python，抓住面向对象的本质，又能在函数编程思想方面游刃有余。Java对标C/C++，以抛掉内存管理为卖点，却又陷入了JVM优化的噩梦。选择比努力更重要，选择Java的人更需要对它有更清晰的认识。 Java 需要在新的计算场景下，改进开发效率。这话说的有点笼统，我谈一些自己的体会，Java 代码虽然进行了一些类型推断等改进，更易用的集合 API 等，但仍然给开发者留下了过于刻板、形式主义的印象，这是一个长期的改进方向。\n","slug":"J6-反射、新特性","date":"2021-11-12T02:31:21.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"8bbd211a4ba9676d95abee1aaae0937f","title":"IO、网络编程","content":"1. IO流1.1 File类的使用\njava.io.File类：文件和文件目录路径的抽象表示形式，与平台无关\nFile 能新建、删除、重命名文件和目录，但 File 不能访问文件内容本身。如果需要访问文件内容本身，则需要使用输入/输出流。  想要在Java程序中表示一个真实存在的文件或录，那么必须有一个File对 象，但是Java程序中的一个File对象，可能没有一个真实存在的文件或目录。 \nFile对象可以作为参数传递给流的构造器\n\n常用构造器\npublic File(String pathname) \n以pathname为路径创建File对象，可以是绝对路径或者相对路径，如果pathname是相对路径，则默认的当前路径在系统属性user.dir中存储。 绝对路径：是一个固定的路径,从盘符开始 相对路径：是相对于某个位置开始\n\npublic File(String parent,String child)以parent为父路径，child为子路径创建File对象。 \n\npublic File(File parent,String child)根据一个父File对象和子文件路径创建File对象\n\n\n路径分隔符\n路径中的每级目录之间用一个路径分隔符隔开。\n\n路径分隔符和系统有关： windows和DOS系统默认使用“\\”来表示 UNIX和URL使用“/”来表示\n\nJava程序支持跨平台运行，因此路径分隔符要慎用。\n\n为了解决这个隐患，File类提供了一个常量：public static final String separator。根据操作系统，动态的提供分隔符。\n\n举例：\nFile file1 = new File(\"d:\\\\atguigu\\\\info.txt\");\nFile file2 = new File(\"d:\" + File.separator + \"atguigu\" + File.separator + \"info.txt\");\nFile file3 = new File(\"d:/atguigu\");\n\n\n常用方法\n\n\n\n获取方法\n功能\n\n\n\n\npublic String getAbsolutePath()\n获取绝对路径\n\n\npublic String getPath()\n获取路径\n\n\npublic String getName()\n获取名称\n\n\npublic String getParent()\n获取上层文件目录路径。若无，返回null\n\n\npublic long length()\n获取文件长度（即：字节数）。不能获取目录的长度。\n\n\npublic long lastModified()\n获取最后一次的修改时间，毫秒值\n\n\npublic String[] list()\n获取指定目录下的所有文件或者文件目录的名称数组\n\n\npublic File[] listFiles()\n获取指定目录下的所有文件或者文件目录的File数组\n\n\npublic boolean renameTo(File dest)\n把文件重命名为指定的文件路径\n\n\n\n\n\n\n\n\n判断方法\n功能\n\n\n\n\npublic boolean isDirectory()\n判断是否是文件目录\n\n\npublic boolean isFile()\n判断是否是文件\n\n\npublic boolean exists()\n判断是否存在\n\n\npublic boolean canRead()\n判断是否可读\n\n\npublic boolean canWrite()\n判断是否可写\n\n\npublic boolean isHidden()\n判断是否隐藏\n\n\n\n\n\n\n\n\n创建和删除方法\n功能\n\n\n\n\npublic boolean createNewFile()\n创建文件。若文件存在，则不创建，返回false\n\n\npublic boolean mkdir()\n创建文件目录。如果此文件目录存在，就不创建了。如果此文件目录的上层目录不存在，也不创建。\n\n\npublic boolean mkdirs()\n创建文件目录。如果上层文件目录不存在，一并创建\n\n\npublic boolean delete()\n删除文件或者文件夹\n\n\n\n\n注意事项：\n\n如果你创建文件或者文件目录没有写盘符路径，那么，默认在项目路径下。\n\nJava中的删除不走回收站。 要删除一个文件目录，请注意该文件目录内不能包含文件或者文件目录\n\n\nFile dir1 = new File(\"D:/IOTest/dir1\");\nif (!dir1.exists()) { // 如果D:/IOTest/dir1不存在，就创建为目录\n\tdir1.mkdir();\n}\n// 创建以dir1为父目录,名为\"dir2\"的File对象\nFile dir2 = new File(dir1, \"dir2\");\nif (!dir2.exists()) { // 如果还不存在，就创建为目录\n\tdir2.mkdirs();\n}\nFile dir4 = new File(dir1, \"dir3/dir4\");\nif (!dir4.exists()) {\n\tdir4.mkdirs();\n}\n// 创建以dir2为父目录,名为\"test.txt\"的File对象\nFile file = new File(dir2, \"test.txt\");\nif (!file.exists()) { // 如果还不存在，就创建为文件\n\tfile.createNewFile();\n}\n1.2 IO流原理及流的分类\nI/O是Input/Output的缩写， I/O技术是非常实用的技术，用于处理设备之间的数据传输。如读/写文件，网络通讯等。\nJava程序中，对于数据的输入/输出操作以“流(stream)” 的方式进行。\njava.io包下提供了各种“流”类和接口，用以获取不同种类的数据，并通过标准的方法输入或输出数据。\n\n流的分类\n按操作数据单位不同分为：字节流(8 bit)，字符流(16 bit)\n按数据流的流向不同分为：输入流，输出流\n按流的角色的不同分为：节点流，处理流\n\n\n\n\n\n抽象基类\n字节流\n字符流\n\n\n\n\n输入流\nInputStream\nReader\n\n\n输出流\nOutputStream\nWriter\n\n\n\n\nIO 流体系\n节点流和处理流节点流：直接从数据源或目的地读写数据。\n处理流：不直接连接到数据源或目的地，而是“连接”在已存在的流（节点流或处理流）之上，通过对数据的处理为程序提供更为强大的读写功能。\nInputStream &amp; Reader\nInputStream 和 Reader 是所有输入流的基类。\n\nInputStream（典型实现：FileInputStream） int read()int read(byte[] b)int read(byte[] b, int off, int len) \n\nReader（典型实现：FileReader） \nint read()int read(char [] c)\nint read(char [] c, int off, int len) \n\n程序中打开的文件 IO 资源不属于内存里的资源，垃圾回收机制无法回收该资源，所以应该显式关闭文件 IO 资源。 \n\nFileInputStream 从文件系统中的某个文件中获得输入字节。FileInputStream 用于读取非文本数据之类的原始字节流。要读取字符流，需要使用 FileReader\n\nInputStream\nint read()从输入流中读取数据的下一个字节。返回 0 到 255 范围内的 int 字节值。如果因为已经到达流末尾而没有可用的字节，则返回值 -1。 \nint read(byte[] b)从此输入流中将最多 b.length 个字节的数据读入一个 byte 数组中。如果因为已经到达流末尾而没有可用的字节，则返回值 -1。否则以整数形式返回实际读取的字节数。 \nint read(byte[] b, int off,int len)将输入流中最多 len 个数据字节读入 byte 数组。尝试读取 len 个字节，但读取的字节也可能小于该值。以整数形式返回实际读取的字节数。如果因为流位于文件末尾而没有可用的字节，则返回值 -1。 \npublic void close() throws IOException关闭此输入流并释放与该流关联的所有系统资源。\n\nReader\nint read()读取单个字符。作为整数读取的字符，范围在 0 到 65535 之间 (0x00-0xffff)（2个字节的Unicode码），如果已到达流的末尾，则返回 -1  int read(char[] cbuf)将字符读入数组。如果已到达流的末尾，则返回 -1。否则返回本次读取的字符数。 \nint read(char[] cbuf,int off,int len)将字符读入数组的某一部分。存到数组cbuf中，从off处开始存储，最多读len个字符。如果已到达流的末尾，则返回 -1。否则返回本次读取的字符数。 \npublic void close() throws IOException关闭此输入流并释放与该流关联的所有系统资源。\n\nOutputStream &amp; Writer\nOutputStream 和 Writer 也非常相似：void write(int b/int c);void write(byte[] b/char[] cbuf);void write(byte[] b/char[] buff, int off, int len);void flush();void close(); 需要先刷新，再关闭此流\n因为字符流直接以字符作为操作单位，所以 Writer 可以用字符串来替换字符数组，即以 String 对象作为参数 void write(String str); void write(String str, int off, int len);\nFileOutputStream 从文件系统中的某个文件中获得输出字节。FileOutputStream 用于写出非文本数据之类的原始字节流。要写出字符流，需要使用 FileWriter\n\nOutputStream\nvoid write(int b)将指定的字节写入此输出流。write 的常规协定是：向输出流写入一个字节。要写入的字节是参数 b 的八个低位。b 的 24 个高位将被忽略。 即写入0~255范围的。 \nvoid write(byte[] b)将 b.length 个字节从指定的 byte 数组写入此输出流。write(b) 的常规协定是：应该与调用 write(b, 0, b.length) 的效果完全相同。 \nvoid write(byte[] b,int off,int len)将指定 byte 数组中从偏移量 off 开始的 len 个字节写入此输出流。 \npublic void flush()throws IOException刷新此输出流并强制写出所有缓冲的输出字节，调用此方法指示应将这些字节立即写入它们预期的目标。 \npublic void close() throws IOException关闭此输出流并释放与该流关联的所有系统资源。\n\nWriter\nvoid write(int c)写入单个字符。要写入的字符包含在给定整数值的 16 个低位中，16 高位被忽略。 即写入0 到 65535 之间的Unicode码。  void write(char[] cbuf)写入字符数组。 \nvoid write(char[] cbuf,int off,int len)写入字符数组的某一部分。从off开始，写入len个字符\nvoid write(String str)写入字符串。 \nvoid write(String str,int off,int len)写入字符串的某一部分。 \nvoid flush()刷新该流的缓冲，则立即将它们写入预期目标。 \npublic void close() throws IOException关闭此输出流并释放与该流关联的所有系统资源。\n\n1.3 节点流(或文件流)读取文件\n建立一个流对象，将已存在的一个文件加载进流。FileReader fr = new FileReader(new File(“Test.txt”));\n创建一个临时存放数据的数组。char[] ch = new char[1024];\n调用流对象的读取方法将流中的数据读入到数组中。fr.read(ch);\n\n关闭资源。fr.close();\n\n\nFileReader fr = null;\ntry {\n    fr = new FileReader(new File(\"c:\\\\test.txt\"));\n    char[] buf = new char[1024];\n    int len;\n    while ((len = fr.read(buf)) != -1) {\n    \tSystem.out.print(new String(buf, 0, len));\n    }\n} catch (IOException e) {\n\tSystem.out.println(\"read-Exception :\" + e.getMessage());\n} finally {\n    if (fr != null) {\n        try {\n            fr.close();\n        } catch (IOException e) {\n            System.out.println(\"close-Exception :\" + e.getMessage());\n        } \n    } \n}\n写入文件\n创建流对象，建立数据存放文件FileWriter fw = new FileWriter(new File(“Test.txt”));\n调用流对象的写入方法，将数据写入流fw.write(“atguigu-songhongkang”);\n关闭流资源，并将流中的数据清空到文件中。fw.close();\n\nFileWriter fw = null;\ntry {\n\tfw = new FileWriter(new File(\"Test.txt\"));\n\tfw.write(\"atguigu-songhongkang\");\n} catch (IOException e) {\n\te.printStackTrace();\n} finally {\n    if (fw != null)\n    \ttry {\n    \tfw.close();\n    \t} catch (IOException e) {\n    \te.printStackTrace();\n    \t} \n}\n注意点\n\n定义文件路径时，注意：可以用“/”或者“\\”。 \n在写入一个文件时，如果使用构造器FileOutputStream(file)，则目录下有同名文件将被覆盖。\n如果使用构造器FileOutputStream(file,true)，则目录下的同名文件不会被覆盖，在文件内容末尾追加内容。\n在读取文件时，必须保证该文件已存在，否则报异常。  字节流操作字节，比如：.mp3，.avi，.rmvb，mp4，.jpg，.doc，.ppt\n字符流操作字符，只能操作普通文本文件。最常见的文本文件：.txt，.java，.c，.cpp 等语言的源代码。尤其注意.doc,excel,ppt这些不是文本文件。\n\n1.4 缓冲流\n为了提高数据读写的速度，Java API提供了带缓冲功能的流类，在使用这些流类时，会创建一个内部缓冲区数组，缺省使用8192个字节(8Kb)的缓冲区。 \n\n缓冲流要“套接”在相应的节点流之上，根据数据操作单位可以把缓冲流分为：\nBufferedInputStream 和 BufferedOutputStream\nBufferedReader 和 BufferedWriter \n\n\nBufferedReader br = null;\nBufferedWriter bw = null;\ntry {\n    // 创建缓冲流对象：它是处理流，是对节点流的包装\n    br = new BufferedReader(new FileReader(\"d:\\\\IOTest\\\\source.txt\"));\n    bw = new BufferedWriter(new FileWriter(\"d:\\\\IOTest\\\\dest.txt\"));\n    String str;\n    while ((str = br.readLine()) != null) { // 一次读取字符文本文件的一行字符\n    \tbw.write(str); // 一次写入一行字符串\n    \tbw.newLine(); // 写入行分隔符\n    }\n    bw.flush(); // 刷新缓冲区\n} catch (IOException e) {\n\te.printStackTrace();\n} finally {\n    // 关闭IO流对象\n    try {\n        if (bw != null) {\n        \tbw.close(); // 关闭过滤流时,会自动关闭它所包装的底层节点流\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    try {\n        if (br != null) {\n        \tbr.close();\n        }\n    } catch (IOException e) {\n    \te.printStackTrace();\n    } \n}\n1.5 转换流\n转换流提供了在字节流和字符流之间的转换\nJava API提供了两个转换流：InputStreamReader：将InputStream转换为ReaderOutputStreamWriter：将Writer转换为OutputStream\n字节流中的数据都是字符时，转成字符流操作更高效。 \n很多时候我们使用转换流来处理文件乱码问题。实现编码和解码的功能。\n\nInputStreamReader\n实现将字节的输入流按指定字符集转换为字符的输入流。\n需要和InputStream“套接”。\n构造器public InputStreamReader(InputStream in)public InputSreamReader(InputStream in,String charsetName)如： Reader isr = new InputStreamReader(System.in,”gbk”);\n\nOutputStreamWriter\n\n实现将字符的输出流按指定字符集转换为字节的输出流。\n\n需要和OutputStream“套接”。\n\n构造器 \n\n\n​         public OutputStreamWriter(OutputStream out)\n​         public OutputSreamWriter(OutputStream out,String charsetName)\n\npublic void testMyInput() throws Exception {\n    FileInputStream fis = new FileInputStream(\"dbcp.txt\");\n    FileOutputStream fos = new FileOutputStream(\"dbcp5.txt\");\n    InputStreamReader isr = new InputStreamReader(fis, \"GBK\");\n    OutputStreamWriter osw = new OutputStreamWriter(fos, \"GBK\");\n    BufferedReader br = new BufferedReader(isr);\n    BufferedWriter bw = new BufferedWriter(osw);\n    String str = null;\n    while ((str = br.readLine()) != null) {\n        bw.write(str);\n        bw.newLine();\n        bw.flush();\n    }\n    bw.close();\n    br.close();\n}\n补充：字符编码\n\n编码表的由来计算机只能识别二进制数据，早期由来是电信号。为了方便应用计算机，让它可以识别各个国家的文字。就将各个国家的文字用数字来表示，并一一对应，形成一张表。这就是编码表。\n常见的编码表\n\nASCII：美国标准信息交换码。用一个字节的7位可以表示。\nISO8859-1：拉丁码表。欧洲码表用一个字节的8位表示。\nGB2312：中国的中文编码表。最多两个字节编码所有字符\nGBK：中国的中文编码表升级，融合了更多的中文文字符号。最多两个字节编码\nUnicode：国际标准码，融合了目前人类使用的所有字符。为每个字符分配唯一的字符码。所有的文字都用两个字节来表示。\nUTF-8：变长的编码方式，可用1-4个字节来表示一个字符。\n\n\nUnicode不完美，这里就有三个问题，一个是，我们已经知道，英文字母只用一个字节表示就够了，第二个问题是如何才能区别Unicode和ASCII？计算机怎么知道两个字节表示一个符号，而不是分别表示两个符号呢？第三个，如果和GBK等双字节编码方式一样，用最高位是1或0表示两个字节和一个字节，就少了很多值无法用于表示字符，不够表示所有字符。Unicode在很长一段时间内无法推广，直到互联网的出现。 \n\n面向传输的众多 UTF（UCS Transfer Format）标准出现了，顾名思义，UTF- 8就是每次8个位传输数据，而UTF-16就是每次16个位。这是为传输而设计的编码，并使编码无国界，这样就可以显示全世界上所有文化的字符了。 \nUnicode只是定义了一个庞大的、全球通用的字符集，并为每个字符规定了唯一确定的编号，具体存储成什么样的字节流，取决于字符编码方案。推荐的Unicode编码是UTF-8和UTF-16。\n\n编码：字符串字节数组解码：字节数组字符串转换流的编码应用\n\n可以将字符按指定编码格式存储\n可以对文本数据按指定编码格式来解读\n指定编码表的动作由构造器完成\n\n1.6 标准输入、输出流\nSystem.in和System.out分别代表了系统标准的输入和输出设备\n默认输入设备是：键盘，输出设备是：显示器\nSystem.in的类型是InputStream\nSystem.out的类型是PrintStream，其是OutputStream的子类FilterOutputStream 的子类\n重定向：通过System类的setIn，setOut方法对默认设备进行改变。public static void setIn(InputStream in)public static void setOut(PrintStream out)\n\n例 题\n​        从键盘输入字符串，要求将读取到的整行字符串转成大写输出。然后继续进行输入操作，直至当输入“e”或者“exit”时，退出程序。\nSystem.out.println(\"请输入信息(退出输入e或exit):\");\n// 把\"标准\"输入流(键盘输入)这个字节流包装成字符流,再包装成缓冲流\nBufferedReader br = new BufferedReader(new InputStreamReader(System.in));\nString s = null;\ntry {\n    while ((s = br.readLine()) != null) { // 读取用户输入的一行数据 --&gt; 阻塞程序\n        if (\"e\".equalsIgnoreCase(s) || \"exit\".equalsIgnoreCase(s)) {\n            System.out.println(\"安全退出!!\");\n            break; \n        }\n        // 将读取到的整行字符串转成大写输出\n        System.out.println(\"--&gt;:\" + s.toUpperCase());\n        System.out.println(\"继续输入信息\");\n    }\n} catch (IOException e) {\n\te.printStackTrace();\n} finally {\n    try {\n        if (br != null) {\n            br.close(); // 关闭过滤流时,会自动关闭它包装的底层节点流\n        }\n    } catch (IOException e) {\n    \te.printStackTrace();\n    } \n}\n1.7 打印流\n实现将基本数据类型的数据格式转化为字符串输出\n\n打印流：PrintStream和PrintWriter提供了一系列重载的print()和println()方法，用于多种数据类型的输出PrintStream和PrintWriter的输出不会抛出IOException异常PrintStream和PrintWriter有自动flush功能PrintStream 打印的所有字符都使用平台的默认字符编码转换为字节。在需要写入字符而不是写入字节的情况下，应该使用 PrintWriter 类。 \nSystem.out返回的是PrintStream的实例\n\n\nPrintStream ps = null;\ntry {\n    FileOutputStream fos = new FileOutputStream(new File(\"D:\\\\IO\\\\text.txt\"));\n    // 创建打印输出流,设置为自动刷新模式(写入换行符或字节 '\\n' 时都会刷新输出缓冲区)\n    ps = new PrintStream(fos, true);\n    if (ps != null) {// 把标准输出流(控制台输出)改成文件\n    \tSystem.setOut(ps);\n    }\n    for (int i = 0; i &lt;= 255; i++) { // 输出ASCII字符\n        System.out.print((char) i);\n        if (i % 50 == 0) { // 每50个数据一行\n        \tSystem.out.println(); // 换行\n        } \n    }\n} catch (FileNotFoundException e) {\n\te.printStackTrace();\n} finally {\n    if (ps != null) {\n    \tps.close();\n    } \n}\n1.8 数据流\n为了方便地操作Java语言的基本数据类型和String的数据，可以使用数据流。\n数据流有两个类：(用于读取和写出基本数据类型、String类的数据）DataInputStream 和 DataOutputStream分别“套接”在 InputStream 和 OutputStream 子类的流上 \nDataInputStream中的方法\n\n\nboolean readBoolean()\nbyte readByte()\nchar readChar() \nfloat readFloat()\ndouble readDouble() \nshort readShort()\nlong readLong() \nint readInt()\nString readUTF() \nvoid readFully(byte[] b)\n\n\nDataOutputStream中的方法\n将上述的方法的read改为相应的write即可。\n\nDataOutputStream dos = null;\ntry { // 创建连接到指定文件的数据输出流对象\n    dos = new DataOutputStream(new FileOutputStream(\"destData.dat\"));\n    dos.writeUTF(\"我爱北京天安门\"); // 写UTF字符串\n    dos.writeBoolean(false); // 写入布尔值\n    dos.writeLong(1234567890L); // 写入长整数\n    System.out.println(\"写文件成功!\");\n} catch (IOException e) {\n\te.printStackTrace();\n} finally { // 关闭流对象\n    try {\n        if (dos != null) {\n            // 关闭过滤流时,会自动关闭它包装的底层节点流\n            dos.close();\n        }\n    } catch (IOException e) {\n    \te.printStackTrace();\n    } \n}\nDataInputStream dis = null;\ntry {\n    dis = new DataInputStream(new FileInputStream(\"destData.dat\"));\n    String info = dis.readUTF();\n    boolean flag = dis.readBoolean();\n    long time = dis.readLong();\n    System.out.println(info);\n    System.out.println(flag);\n    System.out.println(time);\n} catch (Exception e) {\n\te.printStackTrace();\n} finally {\n    if (dis != null) {\n        try {\n        \tdis.close();\n        } catch (IOException e) {\n        \te.printStackTrace();\n        } \n    } \n}\n1.9 对象流ObjectInputStream和OjbectOutputSteam\n用于存储和读取基本数据类型数据或对象的处理流。它的强大之处就是可以把Java中的对象写入到数据源中，也能把对象从数据源中还原回来。\n序列化：用ObjectOutputStream类保存基本类型数据或对象的机制\n反序列化：用ObjectInputStream类读取基本类型数据或对象的机制\nObjectOutputStream和ObjectInputStream不能序列化static和transient修饰的成员变量\n\n对象的序列化\n对象序列化机制允许把内存中的Java对象转换成平台无关的二进制流，从而允许把这种二进制流持久地保存在磁盘上，或通过网络将这种二进制流传输到另一个网络节点。//当其它程序获取了这种二进制流，就可以恢复成原来的Java对象\n序列化的好处在于可将任何实现了Serializable接口的对象转化为字节数据，使其在保存和传输时可被还原\n序列化是 RMI（Remote Method Invoke – 远程方法调用）过程的参数和返回值都必须实现的机制，而 RMI 是 JavaEE 的基础。因此序列化机制是JavaEE 平台的基础\n如果需要让某个对象支持序列化机制，则必须让对象所属的类及其属性是可序列化的，为了让某个类是可序列化的，该类必须实现如下两个接口之一。否则，会抛出NotSerializableException异常SerializableExternalizable\n凡是实现Serializable接口的类都有一个表示序列化版本标识符的静态变量：private static final long serialVersionUID;serialVersionUID用来表明类的不同版本间的兼容性。简言之，其目的是以序列化对象\n进行版本控制，有关各版本反序列化时是否兼容。如果类没有显示定义这个静态常量，它的值是Java运行时环境根据类的内部细节自动生成的。若类的实例变量做了修改，serialVersionUID 可能发生变化。故建议，显式声明。\n简单来说，Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来的字节流中的serialVersionUID与本地相应实体类的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常。(InvalidCastException)\n\n使用对象流序列化对象\n若某个类实现了 Serializable 接口，该类的对象就是可序列化的：创建一个 ObjectOutputStream调用 ObjectOutputStream 对象的 writeObject(对象) 方法输出可序列化对象注意写出一次，操作flush()一次\n反序列化创建一个 ObjectInputStream调用 readObject() 方法读取流中的对象\n强调：如果某个类的属性不是基本数据类型或 String 类型，而是另一个引用类型，那么这个引用类型必须是可序列化的，否则拥有该类型的Field 的类也不能序列化\n\n//序列化：将对象写入到磁盘或者进行网络传输。\n//要求对象必须实现序列化\nObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(“data.txt\"));\nPerson p = new Person(\"韩梅梅\", 18, \"中华大街\", new Pet());\noos.writeObject(p);\noos.flush();\noos.close();\n//反序列化：将磁盘中的对象数据源读出。\nObjectInputStream ois = new ObjectInputStream(new FileInputStream(“data.txt\"));\nPerson p1 = (Person)ois.readObject();\nSystem.out.println(p1.toString());\nois.close();\n谈谈你对java.io.Serializable接口的理解，我们知道它用于序列化，是空方法接口，还有其它认识吗？\n\n实现了Serializable接口的对象，可将它们转换成一系列字节，并可在以后完全恢复回原来的样子。这一过程亦可通过网络进行。这意味着序列化机制能自动补偿操作系统间的差异。换句话说，可以先在Windows机器上创建一个对象，对其序列化，然后通过网络发给一台Unix机器，然后在那里准确无误地重新“装配”。不必关心数据在不同机器上如何表示，也不必关心字节的顺序或者其他任何细节。\n由于大部分作为参数的类如String、Integer等都实现了java.io.Serializable的接口，也可以利用多态的性质，作为参数使接口更灵活。\n\n1.10 随机存取文件流RandomAccessFile 类\nRandomAccessFile 声明在java.io包下，但直接继承于java.lang.Object类。并且它实现了DataInput、DataOutput这两个接口，也就意味着这个类既可以读也可以写。\nRandomAccessFile 类支持 “随机访问” 的方式，程序可以直接跳到文件的任意地方来读、写文件\n支持只访问文件的部分内容\n可以向已存在的文件后追加内容\n\n\nRandomAccessFile 对象包含一个记录指针，用以标示当前读写处的位置。RandomAccessFile 类对象可以自由移动记录指针：\nlong getFilePointer()：获取文件记录指针的当前位置\nvoid seek(long pos)：将文件记录指针定位到 pos 位置\n\n\n\nRandomAccessFile 类\n构造器\npublic RandomAccessFile(File file, String mode) \npublic RandomAccessFile(String name, String mode) \n\n\n创建 RandomAccessFile 类实例需要指定一个mode 参数，该参数指定 RandomAccessFile 的访问模式：\nr: 以只读方式打开\nrw：打开以便读取和写入\nrwd:打开以便读取和写入；同步文件内容的更新\nrws:打开以便读取和写入；同步文件内容和元数据的更新\n\n\n如果模式为只读r。则不会创建文件，而是会去读取一个已经存在的文件，如果读取的文件不存在则会出现异常。 如果模式为rw读写。如果文件不存在则会去创建文件，如果存在则不会创建。\n\n​        我们可以用RandomAccessFile这个类，来实现一个多线程断点下载的功能，用过下载工具的朋友们都知道，下载前都会建立两个临时文件，一个是与被下载文件大小相同的空文件，另一个是记录文件指针的位置文件，每次暂停的时候，都会保存上一次的指针，然后断点下载的时候，会继续从上一次的地方下载，从而实现断点下载或上传的功能，有兴趣的朋友们可以自己实现下。\n//读取文件内容\nRandomAccessFile raf = new RandomAccessFile(“test.txt”, “rw”）;\nraf.seek(5);\nbyte [] b = new byte[1024];\nint off = 0;\nint len = 5;\nraf.read(b, off, len);\nString str = new String(b, 0, len);\nSystem.out.println(str);\nraf.close();\n\n//写入文件内容\nRandomAccessFile raf = new RandomAccessFile(\"test.txt\", \"rw\");\nraf.seek(5);\n//先读出来\nString temp = raf.readLine();\nraf.seek(5);\nraf.write(\"xyz\".getBytes());\nraf.write(temp.getBytes());\nraf.close();  \n\nRandomAccessFile raf1 = new RandomAccessFile(\"hello.txt\", \"rw\");\nraf1.seek(5);\n//方式一：\n//StringBuilder info = new StringBuilder((int) file.length());\n//byte[] buffer = new byte[10];\n//int len;\n//while((len = raf1.read(buffer)) != -1){\n////info += new String(buffer,0,len);\n//info.append(new String(buffer,0,len));\n//}\n//方式二：\nByteArrayOutputStream baos = new ByteArrayOutputStream();\nbyte[] buffer = new byte[10];\nint len;\nwhile((len = raf1.read(buffer)) != -1){\nbaos.write(buffer, 0, len);\n}\nraf1.seek(5);\nraf1.write(\"xyz\".getBytes());\nraf1.write(baos.toString().getBytes());\nbaos.close();\nraf1.close();\n流的基本应用小节\n\n流是用来处理数据的。\n处理数据时，一定要先明确数据源，与数据目的地\n数据源可以是文件，可以是键盘。\n数据目的地可以是文件、显示器或者其他设备。\n\n\n而流只是在帮助数据进行传输,并对传输的数据进行处理，比如过滤处理、转换处理等。\n\n1.11 NIO.2中Path、Paths、 Files类的使用​        Java NIO (New IO，Non-Blocking IO)是从Java 1.4版本开始引入的一套新 的IO API，可以替代标准的Java IO API。NIO与原来的IO有同样的作用和目的，但是使用的方式完全不同，NIO支持面向缓冲区的(IO是面向流的)、基于通道的IO操作。NIO将以更加高效的方式进行文件的读写操作。​        Java API中提供了两套NIO，一套是针对标准输入输出NIO，另一套就是网络编程NIO。 \n|——-java.nio.channels.Channel    |——-FileChannel:处理本地文件    |——-SocketChannel：TCP网络编程的客户端的Channel    |——-ServerSocketChannel:TCP网络编程的服务器端的Channel    |——-DatagramChannel：UDP网络编程中发送端和接收端的Channel\n​        随着 JDK 7 的发布，Java对NIO进行了极大的扩展，增强了对文件处理和文件系统特性的支持，以至于我们称他们为 NIO.2。因为 NIO 提供的一些功能，NIO已经成为文件处理中越来越重要的部分\nPath、Paths和Files核心API\n早期的Java只提供了一个File类来访问文件系统，但File类的功能比较有限，所提供的方法性能也不高。而且，大多数方法在出错时仅返回失败，并不会提供异常信息。\nNIO. 2为了弥补这种不足，引入了Path接口，代表一个平台无关的平台路径，描述了目录结构中文件的位置。Path可以看成是File类的升级版本，实际引用的资源也可以不存在。\n在以前IO操作都是这样写的:import java.io.File;File file = new File(“index.html”);\n但在Java7 中，我们可以这样写：import java.nio.file.Path;import java.nio.file.Paths;Path path = Paths.get(“index.html”);\n同时，NIO.2在java.nio.file包下还提供了Files、Paths工具类，Files包含了大量静态的工具方法来操作文件；Paths则包含了两个返回Path的静态工厂方法。\nPaths 类提供的静态 get() 方法用来获取 Path 对象：static Path get(String first, String … more) : 用于将多个字符串串连成路径static Path get(URI uri): 返回指定uri对应的Path路径\n\n\n\n\n\nPath 常用方法\n功能\n\n\n\n\nString toString()\n返回调用 Path 对象的字符串表示形式\n\n\nboolean startsWith(String path)\n判断是否以 path 路径开始\n\n\nboolean endsWith(String path)\n判断是否以 path 路径结束\n\n\nboolean isAbsolute()\n判断是否是绝对路径\n\n\nPath getParent()\n返回Path对象包含整个路径，不包含 Path 对象指定的文件路径\n\n\nPath getRoot()\n返回调用 Path 对象的根路径\n\n\nPath getFileName()\n返回与调用 Path 对象关联的文件名\n\n\nint getNameCount()\n返回Path 根目录后面元素的数量\n\n\nPath getName(int idx)\n返回指定索引位置 idx 的路径名称\n\n\nPath toAbsolutePath()\n作为绝对路径返回调用 Path 对象\n\n\nPath resolve(Path p)\n合并两个路径，返回合并后的路径对应的Path对象\n\n\nFile toFile()\n将Path转化为File类的对象\n\n\n\n\n java.nio.file.Files 用于操作文件或目录的工具类。\n\n\n\n\nFiles常用方法\n\n\n\n\n\nPath copy(Path src, Path dest, CopyOption … how)\n文件的复制\n\n\nPath createDirectory(Path path, FileAttribute&lt;?&gt; … attr)\n创建一个目录\n\n\nPath createFile(Path path, FileAttribute&lt;?&gt; … arr)\n创建一个文件\n\n\nvoid delete(Path path)\n删除一个文件/目录，如果不存在，执行报错\n\n\nvoid deleteIfExists(Path path)\nPath对应的文件/目录如果存在，执行删除\n\n\nPath move(Path src, Path dest, CopyOption…how)\n将 src 移动到 dest 位置\n\n\nlong size(Path path)\n返回 path 指定文件的大小\n\n\n\n\n\n\n\n\nFiles常用方法：用于判断\n\n\n\n\n\nboolean exists(Path path, LinkOption … opts)\n判断文件是否存在\n\n\nboolean isDirectory(Path path, LinkOption … opts)\n判断是否是目录\n\n\nboolean isRegularFile(Path path, LinkOption … opts)\n判断是否是文件\n\n\nboolean isHidden(Path path)\n判断是否是隐藏文件\n\n\nboolean isReadable(Path path)\n判断文件是否可读\n\n\nboolean isWritable(Path path)\n判断文件是否可写\n\n\nboolean notExists(Path path, LinkOption … opts)\n判断文件是否不存在\n\n\n\n\n\n\n\n\nFiles常用方法：用于操作内容\n\n\n\n\n\nSeekableByteChannel newByteChannel(Path path, OpenOption…how)\n获取与指定文件的连接，how 指定打开方式。\n\n\nDirectoryStream newDirectoryStream(Path path)\n打开 path 指定的目录\n\n\nInputStream newInputStream(Path path, OpenOption…how\n获取 InputStream 对象\n\n\nOutputStream newOutputStream(Path path, OpenOption…how)\n获取 OutputStream 对象\n\n\n\n\n2. 网络编程2.1 网络编程概述\n计算机网络：把分布在不同地理区域的计算机与专门的外部设备用通信线路互连成一个规模大、功能强的网络系统，从而使众多的计算机可以方便地互相传递信息、共享硬件、软件、数据信息等资源。\n网络编程的目的：直接或间接地通过网络协议与其它计算机实现数据交换，进行通讯。\n网络编程中有两个主要的问题：如何准确地定位网络上一台或多台主机；定位主机上的特定的应用找到主机后如何可靠高效地进行数据传输\n\n2.2 网络通信要素概述如何实现网络中的主机互相通信?\n\n通信双方地址\nIP\n端口号\n\n\n一定的规则（即：网络通信协议。有两套参考模型）OSI参考模型：模型过于理想化，未能在因特网上进行广泛推广TCP/IP参考模型(或TCP/IP协议)：事实上的国际标准。\n\n\n2.3 通信要素1：IP和端口号IP 地址：InetAddress\n\n唯一的标识 Internet 上的计算机（通信实体）\n本地回环地址(hostAddress)：127.0.0.1 主机名(hostName)：localhost\nIP地址分类方式1：IPV4 和 IPV6IPV4：4个字节组成，4个0-255。大概42亿，30亿都在北美，亚洲4亿。2011年初已经用尽。以点分十进制表示，如192.168.0.1IPV6：128位（16个字节），写成8个无符号整数，每个整数用四个十六进制位表示，数之间用冒号（：）分开，如：3ffe:3201:1401:1280:c8ff:fe4d:db39:1984\nIP地址分类方式2：公网地址(万维网使用)和私有地址(局域网使用)。192.168.开头的就是私有址址，范围即为192.168.0.0—192.168.255.255，专门为组织机构内部使用\n特点：不易记忆\n\n端口号标识正在计算机上运行的进程（程序）\n\n不同的进程有不同的端口号\n被规定为一个 16 位的整数 0~65535。  端口分类：\n公认端口：0~1023。被预先定义的服务通信占用（如：HTTP占用端口80，FTP占用端口21，Telnet占用端口23） \n注册端口：1024~49151。分配给用户进程或应用程序。（如：Tomcat占用端口8080，MySQL占用端口3306，Oracle占用端口1521等）。 \n动态/私有端口：49152~65535。 \n\n\n端口号与IP地址的组合得出一个网络套接字：Socket。\n\nInetAddress类\nInternet上的主机有两种方式表示地址：\n域名(hostName)：www.atguigu.com\nIP 地址(hostAddress)：202.108.35.210\n\n\nInetAddress类主要表示IP地址，两个子类：Inet4Address、Inet6Address。 \nInetAddress 类 对 象 含 有 一 个 Internet 主 机 地 址 的 域 名 和 IP 地 址 ：www.atguigu.com 和 202.108.35.210。 \n域名容易记忆，当在连接网络时输入一个主机的域名后，域名服务器(DNS)负责将域名转化成IP地址，这样才能和主机建立连接。 ———-域名解析\n\nInetAddress类没有提供公共的构造器，而是提供了如下几个静态方法来获取InetAddress实例\n\npublic static InetAddress getLocalHost()\npublic static InetAddress getByName(String host)\n\n\nInetAddress提供了如下几个常用的方法\npublic String getHostAddress()：返回 IP 地址字符串（以文本表现形式）。 \npublic String getHostName()：获取此 IP 地址的主机名\npublic boolean isReachable(int timeout)：测试是否可以达到该地址\n\n\n\nInetAddress address_ 1 = InetAddress.getByName( \"WwW . atguigu. com\");\nSystem.out.println(address_1);\n//获取InetAddress 对象所含的域名\nSystem.out.print1n( address_1.getHostName());\n//获取InetAddress 对象所含的IP地址\nSystem.out.println( address_1.getHostAddress());\n//获取本机的域名和IP地址。\nInetAddress address_2 = InetAddress.getLocalHost();\nSystem.out.println(address_2) ;\n\n2.4 通信要素2：网络协议网络通信协议:计算机网络中实现通信必须有一些约定，即通信协议，对速率、传输代码、代码结构、传输控制步骤、出错控制等制定标准。 \n 问题：网络协议太复杂计算机网络通信涉及内容很多，比如指定源地址和目标地址，加密解密，压缩解压缩，差错控制，流量控制，路由控制，如何实现如此复杂的网络协议呢？ \n​        通信协议分层的思想在制定协议时，把复杂成份分解成一些简单的成份，再将它们复合起来。最常用的复合方式是层次方式，即同层间可以通信、上一层可以调用下一层，而与再下一层不发生关系。各层互不影响，利于系统的开发和扩展。\nTCP/IP协议簇\n传输层协议中有两个非常重要的协议：\n传输控制协议TCP(Transmission Control Protocol)\n用户数据报协议UDP(User Datagram Protocol)。 TCP/IP 以其两个主要协议：传输控制协议(TCP)和网络互联协议(IP)而得名，实际上是一组协议，包括多个具有不同功能且互为关联的协议。\n\n\nIP(Internet Protocol)协议是网络层的主要协议，支持网间互连的数据通信。\nTCP/IP协议模型从更实用的角度出发，形成了高效的四层体系结构，即物理链路层、IP层、传输层和应用层。\n\nTCP 和 UDP\nTCP协议：\n使用TCP协议前，须先建立TCP连接，形成传输数据通道\n传输前，采用“三次握手”方式，点对点通信，是可靠的\nTCP协议进行通信的两个应用进程：客户端、服务端。\n在连接中可进行大数据量的传输\n传输完毕，需释放已建立的连接，效率低\n\n\nUDP协议：\n将数据、源、目的封装成数据包，不需要建立连接\n每个数据报的大小限制在64K内 \n发送不管对方是否准备好，接收方收到也不确认，故是不可靠的 \n可以广播发送\n发送数据结束时无需释放资源，开销小，速度快\n\n\n\n\n\nSocket\n利用套接字(Socket)开发网络应用程序早已被广泛的采用，以至于成为事实上的标准。\n网络上具有唯一标识的IP地址和端口号组合在一起才能构成唯一能识别的标识符套接字。 \n通信的两端都要有Socket，是两台机器间通信的端点。  网络通信其实就是Socket间的通信。\nSocket允许程序把网络连接当成一个流，数据在两个Socket间通过IO传输。\n一般主动发起通信的应用程序属客户端，等待通信请求的为服务端。\nSocket分类：\n流套接字（stream socket）：使用TCP提供可依赖的字节流服务\n数据报套接字（datagram socket）：使用UDP提供“尽力而为”的数据报服务\n\n\n\nSocket类的常用构造器： \n\npublic Socket(InetAddress address,int port)创建一个流套接字并将其连接到指定 IP 地址的指定端口号。 \npublic Socket(String host,int port)创建一个流套接字并将其连接到指定主机上的指定端口号。 \n\nSocket类的常用方法：\n\npublic InputStream getInputStream()返回此套接字的输入流。可以用于接收网络消息\npublic OutputStream getOutputStream()返回此套接字的输出流。可以用于发送网络消息\npublic InetAddress getInetAddress()此套接字连接到的远程 IP 地址；如果套接字是未连接的，则返回 null。 \npublic InetAddress getLocalAddress()获取套接字绑定的本地地址。 即本端的IP地址\npublic int getPort()此套接字连接到的远程端口号；如果尚未连接套接字，则返回 0。 \npublic int getLocalPort()返回此套接字绑定到的本地端口。 如果尚未绑定套接字，则返回 -1。即本端的端口号。\npublic void close()关闭此套接字。套接字被关闭后，便不可在以后的网络连接中使用（即无法重新连接或重新绑定）。需要创建新的套接字对象。 关闭此套接字也将会关闭该套接字的 InputStream 和OutputStream。 \npublic void shutdownInput()如果在套接字上调用 shutdownInput() 后从套接字输入流读取内容，则流将返回 EOF（文件结束符）。 即不能在从此套接字的输入流中接收任何数据。 \npublic void shutdownOutput()禁用此套接字的输出流。对于 TCP 套接字，任何以前写入的数据都将被发送，并且后跟 TCP 的正常连接终止序列。 如果在套接字上调用 shutdownOutput() 后写入套接字输出流，则该流将抛出 IOException。 即不能通过此套接字的输出流发送任何数据。\n\n2.5 TCP网络编程基于Socket的TCP编程\nJava语言的基于套接字编程分为服务端编程和客户端编程，其通信模型如图所示：\n \n客户端Socket的工作过程包含以下四个基本的步骤：\n\n创建 Socket：根据指定服务端的 IP 地址或端口号构造 Socket 类对象。若服务器端响应，则建立客户端到服务器的通信线路。若连接失败，会出现异常。\n打开连接到 Socket 的输入/出流： 使用 getInputStream()方法获得输入流，使用getOutputStream()方法获得输出流，进行数据传输\n按照一定的协议对 Socket 进行读/写操作：通过输入流读取服务器放入线路的信息（但不能读取自己放入线路的信息），通过输出流将信息写入线程。\n关闭 Socket：断开客户端到服务器的连接，释放线路\n\n客户端创建Socket对象\n\n客户端程序可以使用Socket类创建对象，创建的同时会自动向服务器方发起连接。Socket的构造器是： \nSocket(String host,int port)throws UnknownHostException,IOException：向服务器(域名是host。端口号为port)发起TCP连接，若成功，则创建Socket对象，否则抛出异常。\nSocket(InetAddress address,int port)throws IOException：根据InetAddress对象所表示的IP地址以及端口号port发起连接。\n\n\n客户端建立socketAtClient对象的过程就是向服务器发出套接字连接请求\n\nSocket s = new \nSocket(“192.168.40.165”,9999);\nOutputStream out = s.getOutputStream();\nout.write(\" hello\".getBytes());\ns.close();\n服务器程序的工作过程包含以下四个基本的步骤：\n\n调用 ServerSocket(int port) ：创建一个服务器端套接字，并绑定到指定端口上。用于监听客户端的请求。\n调用 accept()：监听连接请求，如果客户端请求连接，则接受连接，返回通信套接字对象。 \n调用 该Socket类对象的 getOutputStream() 和 getInputStream ()：获取输出流和输入流，开始网络数据的发送和接收。\n关闭ServerSocket和Socket对象：客户端访问结束，关闭通信套接字。\n\n服务器建立 ServerSocket 对象\n\nServerSocket 对象负责等待客户端请求建立套接字连接，类似邮局某个窗口中的业务员。也就是说，服务器必须事先建立一个等待客户请求建立套接字连接的ServerSocket对象。\n所谓“接收”客户的套接字请求，就是accept()方法会返回一个 Socket 对象\n\nServerSocket ss = new ServerSocket(9999);\nSocket s = ss.accept ();\nInputStream in = s.getInputStream();\nbyte[] buf = new byte[1024];\nint num = in.read(buf);\nString str = new String(buf,0,num);\nSystem.out.println(s.getInetAddress().toString()+”:”+str);\ns.close();\nss.close();\n2.6 UDP网络编程\n类 DatagramSocket 和 DatagramPacket 实现了基于 UDP 协议网络程序。\nUDP数据报通过数据报套接字 DatagramSocket 发送和接收，系统不保证UDP数据报一定能够安全送到目的地，也不能确定什么时候可以抵达。\nDatagramPacket 对象封装了UDP数据报，在数据报中包含了发送端的IP地址和端口号以及接收端的IP地址和端口号。\nUDP协议中每个数据报都给出了完整的地址信息，因此无须建立发送方和接收方的连接。如同发快递包裹一样。\n\nDatagramSocket 类的常用方法\n\npublic DatagramSocket(int port)创建数据报套接字并将其绑定到本地主机上的指定端口。套接字将被绑定到通配符地址，IP 地址由内核来选择。 \npublic DatagramSocket(int port,InetAddress laddr)创建数据报套接字，将其绑定到指定的本地地址。本地端口必须在 0 到 65535 之间（包括两者）。如果 IP 地址为 0.0.0.0，套接字将被绑定到通配符地址，IP 地址由内核选择。 \npublic void close()关闭此数据报套接字。 \npublic void send(DatagramPacket p)从此套接字发送数据报包。DatagramPacket 包含的信息指示：将要发送的数据、其长度、远程主机的 IP 地址和远程主机的端口号。 \npublic void receive(DatagramPacket p)从此套接字接收数据报包。当此方法返回时，DatagramPacket的缓冲区填充了接收的数据。数据报包也包含发送方的 IP 地址和发送方机器上的端口号。 此方法在接收到数据报前一直阻塞。数据报包对象的 length 字段包含所接收信息的长度。如果信息比包的长度长，该信息将被截短。 \npublic InetAddress getLocalAddress()获取套接字绑定的本地地址。 \npublic int getLocalPort()返回此套接字绑定的本地主机上的端口号。 \npublic InetAddress getInetAddress()返回此套接字连接的地址。如果套接字未连接，则返回 null。 \npublic int getPort()返回此套接字的端口。如果套接字未连接，则返回 -1。\n\nDatagramPacket类的常用方法\n\npublic DatagramPacket(byte[] buf,int length)构造 DatagramPacket，用来接收长度为 length 的数据包。 length 参数必须小于等于 buf.length。 \npublic DatagramPacket(byte[] buf,int length,InetAddress address,int port)构造数据报包，用来将长度为 length 的包发送到指定主机上的指定端口号。length参数必须小于等于 buf.length。 \npublic InetAddress getAddress()返回某台机器的 IP 地址，此数据报将要发往该机器或者是从该机器接收到的。 \npublic int getPort()返回某台远程主机的端口号，此数据报将要发往该主机或者是从该主机接收到的。 \npublic byte[] getData()返回数据缓冲区。接收到的或将要发送的数据从缓冲区中的偏移量 offset 处开始，持续 length 长度。  public int getLength()返回将要发送或接收到的数据的长度。\n\nUDP网络通信流 程：\n\nDatagramSocket与DatagramPacket\n建立发送端，接收端\n建立数据包\n调用Socket的发送、接收方法\n关闭Socket，发送端与接收端是两个独立的运行程序\n\n//发送端\nDatagramSocket ds = null;\ntry {\n    ds = new DatagramSocket();\n    byte[] by = \"hello,atguigu.com\".getBytes();\n    DatagramPacket dp = new DatagramPacket(by, 0, by.length, \n    InetAddress.getByName(\"127.0.0.1\"), 10000);\n    ds.send(dp);\n} catch (Exception e) {\n\te.printStackTrace();\n} finally {\n    if (ds != null)\n    \tds.close();\n}\n\n//接收端 在接收端，要指定监听的端口。\nDatagramSocket ds = null;\ntry {\n    ds = new DatagramSocket(10000);\n    byte[] by = new byte[1024];\n    DatagramPacket dp = new DatagramPacket(by, by.length);\n    ds.receive(dp);\n    String str = new String(dp.getData(), 0, dp.getLength());\n    System.out.println(str + \"--\" + dp.getAddress());\n} catch (Exception e) {\n\te.printStackTrace();\n} finally {\n    if (ds != null)\n    \tds.close();\n}\n2.7 URL编程URL类\nURL(Uniform Resource Locator)：统一资源定位符，它表示 Internet 上某一资源的地址。 \n它是一种具体的URI，即URL可以用来标识一个资源，而且还指明了如何locate这个资源。\n通过 URL 我们可以访问 Internet 上的各种网络资源，比如最常见的 www，ftp 站点。浏览器通过解析给定的 URL 可以在网络上查找相应的文件或其他资源。\nURL的基本结构由5部分组成： &lt;传输协议&gt;://&lt;主机名&gt;:&lt;端口号&gt;/&lt;文件名&gt;#片段名?参数列表例如: http://192.168.1.100:8080/helloworld/index.jsp#a?username=shkstart&amp;password=123片段名：即锚点，例如看小说，直接定位到章节参数列表格式：参数名=参数值&amp;参数名=参数值….\n\nURL类构造器\n\n为了表示URL，java.net 中实现了类 URL。我们可以通过下面的构造器来初始化一个 URL 对象：\npublic URL (String spec)：通过一个表示URL地址的字符串可以构造一个URL对象。例如：URL url = new URL (“http://www. atguigu.com/“); \npublic URL(URL context, String spec)：通过基 URL 和相对 URL 构造一个 URL 对象。例如：URL downloadUrl = new URL(url, “download.html”)\npublic URL(String protocol, String host, String file); 例如：new URL(“http”,“www.atguigu.com”, “download. html”);\npublic URL(String protocol, String host, int port, String file); 例如: URL gamelan = newURL(“http”, “www.atguigu.com”, 80, “download.html”);\nURL类的构造器都声明抛出非运行时异常，必须要对这一异常进行处理，通常是用 try-catch 语句进行捕获。\n\n一个URL对象生成后，其属性是不能被改变的，但可以通过它给定的方法来获取这些属性：\n\npublic String getProtocol( ) 获取该URL的协议名\npublic String getHost( ) 获取该URL的主机名\npublic String getPort( ) 获取该URL的端口号\npublic String getPath( ) 获取该URL的文件路径\npublic String getFile( ) 获取该URL的文件名\npublic String getQuery( ) 获取该URL的查询名\n\nURL url = new URL(\"http://localhost:8080/examples/myTest.txt\");\nSystem.out.println(\"getProtocol() :\"+url.getProtocol());\nSystem.out.println(\"getHost() :\"+url.getHost());\nSystem.out.println(\"getPort() :\"+url.getPort());\nSystem.out.println(\"getPath() :\"+url.getPath());\nSystem.out.println(\"getFile() :\"+url.getFile());\nSystem.out.println(\"getQuery() :\"+url.getQuery());\nURLConnection类\nURL的方法 openStream()：能从网络上读取数据\n若希望输出数据，例如向服务器端的 CGI （公共网关接口-Common Gateway Interface-的简称，是用户浏览器和服务器端的应用程序进行连接的接口）程序发送一些数据，则必须先与URL建立连接，然后才能对其进行读写，此时需要使用URLConnection 。 \nURLConnection：表示到URL所引用的远程对象的连接。当与一个URL建立连接时，首先要在一个 URL 对象上通过方法 openConnection() 生成对应的 URLConnection对象。如果连接过程失败，将产生IOException. \nURL netchinaren = new URL (“http://www.atguigu.com/index.shtml“); \nURLConnectonn u = netchinaren.openConnection( ); \n\n通过URLConnection对象获取的输入流和输出流，即可以与现有的CGI程序进行交互。\n\npublic Object getContent( ) throws IOException\npublic int getContentLength( )\npublic String getContentType( )\npublic long getDate( )\npublic long getLastModified( )\npublic InputStream getInputStream( )throws IOException\npublic OutputSteram getOutputStream( )throws IOException\n\nURI、URL和URN的区别        URI，是uniform resource identifier，统一资源标识符，用来唯一的标识一个资源。而URL是uniform resource locator，统一资源定位符，它是一种具体的URI，即URL可以用来标识一个资源，而且还指明了如何locate这个资源。\n​        而URN，uniform resource name，统一资源命名，是通过名字来标识资源，比如mailto:java-net@java.sun.com。也就是说，URI是以一种抽象的，高层次概念定义统一资源标识，而URL和URN则是具体的资源标识的方式。\n​        URL和URN都是一种URI。 在Java的URI中，一个URI实例可以代表绝对的，也可以是相对的，只要它符合URI的语法规则。而URL类则不仅符合语义，还包含了定位该资源的信息，因此它不能是相对的。\n小 结\n位于网络中的计算机具有唯一的IP地址，这样不同的主机可以互相区分。\n客户端－服务器是一种最常见的网络应用程序模型。服务器是一个为其客户端提供某种特定服务的硬件或软件。客户机是一个用户应用程序，用于访问某台服务器提供的服务。端口号是对一个服务的访问场所，它用于区分同一物理计算机上的多个服务。套接字用于连接客户端和服务器，客户端和服务器之间的每个通信会话使用一个不同的套接字。TCP协议用于实现面向连接的会话。\nJava 中有关网络方面的功能都定义在 java.net 程序包中。Java 用 InetAddress 对象表示 IP 地址，该对象里有两个字段：主机名(String) 和 IP 地址(int)。 类 Socket 和 ServerSocket 实现了基于TCP协议的客户端－服务器程序。Socket是客户端和服务器之间的一个连接，连接创建的细节被隐藏了。这个连接提供了一个安全的数据传输通道，这是因为 TCP 协议可以解决数据在传送过程中的丢失、损坏、重复、乱序以及网络拥挤等问题，它保证数据可靠的传送。\n类 URL 和 URLConnection 提供了最高级网络应用。URL 的网络资源的位置来同一表示Internet 上各种网络资源。通过URL对象可以创建当前应用程序和 URL 表示的网络资源之间的连接，这样当前程序就可以读取网络资源数据，或者把自己的数据传送到网络上去。\n\n","slug":"J5-IO、网络编程","date":"2021-11-12T02:31:03.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"a3bf914b929f60f3127d07c8144366a0","title":"集合、泛型","content":"1. Java集合1.1 Java集合框架概述Java 集合可分为 Collection 和 Map 两种体系\n\nCollection接口：单列数据，定义了存取一组对象的方法的集合List：元素有序、可重复的集合Set：元素无序、不可重复的集合\nMap接口：双列数据，保存具有映射关系“key-value对”的集合\n\n\n\n1.2 Collection接口方法​        Collection 接口是 List、Set 和 Queue 接口的父接口，该接口里定义的方法既可用于操作 Set 集合，也可用于操作 List 和 Queue 集合。\n\n\n\n\nCollection接口方法\n描述\n\n\n\n\nadd(Object obj)addAll(Collection coll)\n添加\n\n\nint size()\n计算个数\n\n\nvoid clear()\n清空集合\n\n\nboolean isEmpty()\n判断是否为空\n\n\nboolean contains(Object obj)\nequals方法判断是否是同一对象\n\n\nboolean containsAll(Collection c)\n拿两个集合的元素挨个比较。\n\n\nboolean remove(Object obj)\nequals方法删除第一个相同元素\n\n\nboolean removeAll(Collection coll)\n取当前集合的差集\n\n\nboolean retainAll(Collection c)\n取两个集合的交集\n\n\nboolean equals(Object obj)\n集合是否相等\n\n\nObject[] toArray()\n转成对象数组\n\n\nhashCode()\n获取集合对象的哈希值\n\n\niterator()：\n遍历\n\n\n\n\n1.3 Iterator迭代器接口 Iterator对象称为迭代器(设计模式的一种)，主要用于遍历 Collection 集合中的元素。\n\n\n\n\nIterator接口的方法\n\n\n\n\n\nhasNext()\n是否还有更多元素\n\n\nnext()\n返回下一个元素\n\n\nremove()\n删除元素\n\n\n\n\n​        在调用it.next()方法之前必须要调用it.hasNext()进行检测。若不调用，且下一条记录无效，直接调用it.next()会抛出NoSuchElementException异常。\nIterator iterator = coll.iterator();\n//hasNext():判断是否还有下一个元素\nwhile(iterator.hasNext()){\n\t//next():①指针下移 ②将下移以后集合位置上的元素返回\n\tSystem.out.println(iterator.next());\n}\n\nIterator iter = coll.iterator();//回到起点\nwhile(iter.hasNext()){\n\tObject obj = iter.next();\n\tif(obj.equals(\"Tom\")){\n\t\titer.remove();\n\t} \n}\n​        如果还未调用next()或在上一次调用 next 方法之后已经调用了 remove 方法，再调用remove都会报IllegalStateException。\n使用 foreach 循环遍历集合元素Java 5.0 提供了 foreach 循环迭代访问 Collection和数组。\n遍历操作不需获取Collection或数组的长度，无需使用索引访问元素。\n遍历集合的底层调用Iterator完成操作。\nforeach还可以用来遍历数组。\nfor(Person : persons){\n    ...;\n}\n1.4 Collection子接口一：ListList集合类中元素有序、且可重复，集合中的每个元素都有其对应的顺序索引。\nList容器中的元素都对应一个整数型的序号记载其在容器中的位置，可以根据序号存取容器中的元素。\nJDK API中List接口的实现类常用的有：ArrayList、LinkedList和Vector。\nList接口方法\n\n\n\n些根据索引来操作集合元素的方法\n\n\n\n\n\nvoid add(int index, Object ele)\n在index位置插入ele元素\n\n\nboolean addAll(int index, Collection eles)\nindex位置开始将eles中的所有元素添加\n\n\nObject get(int index)\n获取指定index位置的元素\n\n\nint indexOf(Object obj)\n返回obj在集合中首次出现的位置\n\n\nint lastIndexOf(Object obj)\n返回obj在当前集合中末次出现的位置\n\n\nObject remove(int index)\n移除指定index位置的元素，并返回此元素\n\n\nObject set(int index, Object ele)\n设置指定index位置的元素为ele\n\n\nList subList(int fromIndex, int toIndex)\n返回从fromIndex到toIndex位置的子集合\n\n\n\n\nArrayList本质上，ArrayList是对象引用的一个”变长”数组\nList list = new ArrayList();\nlist.add(1);\nlist.add(2);\nlist.add(3);\nlist.remove(2);\nSystem.out.println(list);\nLinkedList双向链表，内部没有声明数组，而是定义了Node类型的first和last，用于记录首末元素。同时，定义内部类Node，作为LinkedList中保存数据的基本结构。Node除了保存数据，还定义了两个变量：\n prev变量记录前一个元素的位置\n next变量记录下一个元素的位置\n对于频繁的插入或删除元素的操作，建议使用LinkedList类，效率较高。\nprivate static class Node&lt;E&gt; {\n    E item;\n    Node&lt;E&gt; next;\n    Node&lt;E&gt; prev;\n    Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) {\n        this.item = element;\n        this.next = next;\n        this.prev = prev;\n    } \n}\n\n\n\n\n新增方法\n描述\n\n\n\n\nvoid addFirst(Object obj)\n头插\n\n\nvoid addLast(Object obj)\n尾插\n\n\nObject getFirst()\n得到头节点\n\n\nObject getLast()\n得到尾节点\n\n\nObject removeFirst()\n删除头节点\n\n\nObject removeLast()\n删除尾节点\n\n\n\n\nVectorVector 是一个古老的集合，JDK1.0就有了。大多数操作与ArrayList相同，区别之处在于Vector是线程安全的。\n\n\n\n\n新增方法\n\n\n\n\n\nvoid addElement(Object obj)\n\n\n\nvoid insertElementAt(Object obj,int index)\n\n\n\nvoid setElementAt(Object obj,int index)\n\n\n\nvoid removeElement(Object obj)\n\n\n\nvoid removeAllElements()\n\n\n\n\n面试题​        请问ArrayList/LinkedList/Vector的异同？谈谈你的理解？ArrayList底层是什么？扩容机制？Vector和ArrayList的最大区别? \n\nArrayList和LinkedList的异同\n\n​        二者都线程不安全，相对线程安全的Vector，执行效率高。此外，ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。对于随机访问get和set，ArrayList觉得优于LinkedList，因为LinkedList要移动指针。对于新增和删除操作add(特指插入)和remove，LinkedList比较占优势，因为ArrayList要移动数据。\n\nArrayList和Vector的区别\n\n​        Vector和ArrayList几乎是完全相同的,唯一的区别在于Vector是同步类(synchronized)，属于强同步类。因此开销就比ArrayList要大，访问要慢。正常情况下,大多数的Java程序员使用ArrayList而不是Vector,因为同步完全可以由程序员自己来控制。Vector每次扩容请求其大小的2倍空间，而ArrayList是1.5倍。Vector还有一个子类Stack。\n1.5 Collection子接口二：SetSet接口是Collection的子接口，set接口没有提供额外的方法。\nSet 集合不允许包含相同的元素，如果试把两个相同的元素加入同一个Set 集合中，则添加操作失败。\nSet 判断两个对象是否相同不是使用 == 运算符，而是根据 equals() 方法。\nHashSet​        HashSet 按 Hash 算法来存储集合中的元素，因此具有很好的存取、查找、删除性能。 \nHashSet具有以下特点：\n\n不能保证元素的排列顺序\nHashSet 不是线程安全的\n集合元素可以是 null\n\nHashSet集合判断两个元素相等的标准：两个对象通过 hashCode() 方法比较相等，并且两个对象的 equals() 方法返回值也相等。 \n对于存放在Set容器中的对象，对应的类一定要重写equals()和hashCode(Objectobj)方法，以实现对象相等规则。即：“相等的对象必须具有相等的散列码”。\n向HashSet中添加元素的过程：\n​        当向 HashSet 集合中存入一个元素时，HashSet 会调用该对象的 hashCode() 方法来得到该对象的 hashCode 值，然后根据 hashCode 值，通过某种散列函数决定该对象在 HashSet 底层数组中的存储位置。（这个散列函数会与底层数组的长度相计算得到在数组中的下标，并且这种散列函数计算还尽可能保证能均匀存储元素，越是散列分布，该散列函数设计的越好） \n​        如果两个元素的hashCode()值相等，会再继续调用equals方法，如果equals方法结果为true，添加失败；如果为false，那么会保存该元素，但是该数组的位置已经有元素了，那么会通过链表的方式继续链接。 \n​        如果两个元素的 equals() 方法返回 true，但它们的 hashCode() 返回值不相等，hashSet 将会把它们存储在不同的位置，但依然可以添加成功。\n重写 hashCode() 方法的基本原则\n\n在程序运行时，同一个对象多次调用 hashCode() 方法应该返回相同的值。\n当两个对象的 equals() 方法比较返回 true 时，这两个对象的 hashCode() 方法的返回值也应相等。\n对象中用作 equals() 方法比较的 Field，都应该用来计算 hashCode 值。\n\n以自定义的Customer类为例，何时需要重写equals()？\n\n当一个类有自己特有的“逻辑相等”概念,当改写equals()的时候，总是要改写hashCode()，根据一个类的equals方法（改写后），两个截然不同的实例有可能在逻辑上是相等的，但是，根据Object.hashCode()方法，它们仅仅是两个对象。\n因此，违反了“相等的对象必须具有相等的散列码”。\n结论：复写equals方法的时候一般都需要同时复写hashCode方法。通常参与计算hashCode的对象的属性也应该参与到equals()中进行计算。\n\n以Eclipse/IDEA为例，在自定义类中可以调用工具自动重写equals和hashCode。问题：为什么用Eclipse/IDEA复写hashCode方法，有31这个数字？\n\n选择系数的时候要选择尽量大的系数。因为如果计算出来的hash地址越大，所谓的“冲突”就越少，查找起来效率也会提高。（减少冲突）\n并且31只占用5bits,相乘造成数据溢出的概率较小。\n31可以 由i*31== (i&lt;&lt;5)-1来表示,现在很多虚拟机里面都有做相关优化。（提高算法效率）\n31是一个素数，素数作用就是如果我用一个数字来乘以这个素数，那么最终出来的结果只能被素数本身和被乘数还有1来整除！(减少冲突)\n\nLinkedHashSet​        LinkedHashSet 根据元素的 hashCode 值来决定元素的存储位置，但它同时使用双向链表维护元素的次序，这使得元素看起来是以插入顺序保存的。\n​        LinkedHashSet插入性能略低于 HashSet，但在迭代访问 Set 里的全部元素时有很好的性能。\n​        LinkedHashSet 不允许集合元素重复。\nSet set = new LinkedHashSet();\nset.add(new String(\"AA\"));\nset.add(456);\nset.add(456);\nset.add(new Customer(\"刘德华\", 1001));\nTreeSet​        TreeSet 是 SortedSet 接口的实现类，TreeSet 可以确保集合元素处于排序状态。\n​        TreeSet底层使用红黑树结构存储数据\n\n\n\n\n新增的方法\n\n\n\n\n\nComparator comparator()\n\n\n\nObject first()\n\n\n\nObject last()\n\n\n\nObject lower(Object e)\n\n\n\nObject higher(Object e)\n\n\n\nSortedSet subSet(fromElement, toElement)\n\n\n\nSortedSet headSet(toElement)\n\n\n\nSortedSet tailSet(fromElement)\n\n\n\n\nTreeSet 两种排序方法：自然排序和定制排序。默认情况下，TreeSet 采用自然排序。\n自然排序：TreeSet 会调用集合元素的 compareTo(Object obj) 方法来比较元素之间的大小关系，然后将集合元素按升序(默认情况)排列。如果试图把一个对象添加到 TreeSet 时，则该对象的类必须实现 Comparable 接口。\nComparable 的典型实现：\n\nBigDecimal、BigInteger 以及所有的数值型对应的包装类：按它们对应的数值大小进行比较\nCharacter：按字符的 unicode值来进行比较\nBoolean：true 对应的包装类实例大于 false 对应的包装类实例\nString：按字符串中字符的 unicode 值进行比较\nDate、Time：后边的时间、日期比前面的时间、日期大\n\n​        当需要把一个对象放入 TreeSet 中，重写该对象对应的 equals() 方法时，应保证该方法与 compareTo(Object obj) 方法有一致的结果：如果两个对象通过equals() 方法比较返回 true，则通过 compareTo(Object obj) 方法比较应返回 0。否则，让人难以理解。\n定制排序：不希望按照升序(默认情况)的方式排列元素或希望按照其它属性大小进行排序，则考虑使用定制排序。定制排序，通过Comparator接口来实现。需要重写compare(T o1,T o2)方法。 如果方法返回正整数，则表示o1大于o2；如果返回0，表示相等；返回负整数，表示o1小于o2。 \n面试题HashSet set = new HashSet();\nPerson p1 = new Person(1001,\"AA\");\nPerson p2 = new Person(1002,\"BB\");\nset.add(p1);\nset.add(p2);\np1.name = \"CC\";\nset.remove(p1);\nSystem.out.println(set);\nset.add(new Person(1001,\"CC\"));\nSystem.out.println(set);\nset.add(new Person(1001,\"AA\"));\nSystem.out.println(set);\n1.6 Map接口\nMap与Collection并列存在。用于保存具有映射关系的数据:key-value\nMap 中的 key 和 value 都可以是任何引用类型的数据\nMap 中的 key 用Set来存放，不允许重复，即同一个 Map 对象所对应的类，须重写hashCode()和equals()方法\n常用String类作为Map的“键”\nkey 和 value 之间存在单向一对一关系，即通过指定的 key 总能找到唯一的、确定的 value\nMap接口的常用实现类：HashMap、TreeMap、LinkedHashMap和Properties。其中，HashMap是 Map 接口使用频率最高的实现类\n\nMap接口：常用方法\n\n\n\n添加、删除、修改操作\n\n\n\n\n\nObject put(Object key,Object value)\n将指定key-value添加到(或修改)当前map对象中\n\n\nvoid putAll(Map m)\n将m中的所有key-value对存放到当前map中\n\n\nObject remove(Object key)\n移除指定key的key-value对，并返回value\n\n\nvoid clear()\n清空当前map中的所有数据\n\n\n\n\n\n\n\n\n元素查询的操作\n\n\n\n\n\nObject get(Object key)\n获取指定key对应的value\n\n\nboolean containsKey(Object key)\n是否包含指定的key\n\n\nboolean containsValue(Object value)\n是否包含指定的value\n\n\nint size()\n返回map中key-value对的个数\n\n\nboolean isEmpty()\n判断当前map是否为空\n\n\nboolean equals(Object obj)\n判断当前map和参数对象obj是否相等\n\n\n\n\n\n\n\n\n元视图操作的方法\n\n\n\n\n\nSet keySet()\n返回所有key构成的Set集合\n\n\nCollection values()\n返回所有value构成的Collection集合\n\n\nSet entrySet()\n返回所有key-value对构成的Set集合\n\n\n\n\nMap map = new HashMap();\n//map.put(..,..)省略\nSystem.out.println(\"map的所有key:\");\nSet keys = map.keySet();// HashSet\nfor (Object key : keys) {\n\tSystem.out.println(key + \"-&gt;\" + map.get(key));\n}\nSystem.out.println(\"map的所有的value：\");\nCollection values = map.values();\nIterator iter = values.iterator();\nwhile (iter.hasNext()) {\n\tSystem.out.println(iter.next());\n}\nSystem.out.println(\"map所有的映射关系：\");\n// 映射关系的类型是Map.Entry类型，它是Map接口的内部接口\nSet mappings = map.entrySet();\nfor (Object mapping : mappings) {\n\tMap.Entry entry = (Map.Entry) mapping;\n\tSystem.out.println(\"key是：\" + entry.getKey() + \"，value是：\" + entry.getValue());\n}\nHashMap\nHashMap是 Map 接口使用频率最高的实现类。\n允许使用null键和null值，与HashSet一样，不保证映射的顺序。\n所有的key构成的集合是Set:无序的、不可重复的。所以，key所在的类要重写：equals()和hashCode()\n所有的value构成的集合是Collection:无序的、可以重复的。所以，value所在的类要重写：equals()\n一个key-value构成一个entry\n所有的entry构成的集合是Set:无序的、不可重复的\nHashMap 判断两个 key 相等的标准是：两个 key 通过 equals() 方法返回 true，hashCode 值也相等。\nHashMap 判断两个 value**相等的标准**是：两个 value 通过 equals() 方法返回 true。\n\nHashMap的存储结构\nJDK 7及以前版本：HashMap是数组+链表结构(即为链地址法)\nJDK 8版本发布以后：HashMap是数组+链表+红黑树实现。\nHashMap源码中的重要常量\n\nDEFAULT_INITIAL_CAPACITY : HashMap的默认容量，16\nMAXIMUM_CAPACITY ： HashMap的最大支持容量，2^30\nDEFAULT_LOAD_FACTOR：HashMap的默认加载因子\nTREEIFY_THRESHOLD：Bucket中链表长度大于该默认值，转化为红黑树\nUNTREEIFY_THRESHOLD：Bucket中红黑树存储的Node小于该默认值，转化为链表\nMIN_TREEIFY_CAPACITY：桶中的Node被树化时最小的hash表容量。（当桶中Node的数量大到需要变红黑树时，若hash表容量小于MIN_TREEIFY_CAPACITY时，此时应执行resize扩容操作这MIN_TREEIFY_CAPACITY的值至少是TREEIFY_THRESHOLD的4倍。）\ntable：存储元素的数组，总是2的n次幂\nentrySet：存储具体元素的集\nsize：HashMap中存储的键值对的数量\nmodCount：HashMap扩容和结构改变的次数。\nthreshold：扩容的临界值，=容量*填充因子\nloadFactor：填充因子\n\nHashMap的存储结构：JDK 1.8之前\n\nHashMap的内部存储结构其实是数组和链表的结合。当实例化一个HashMap时，系统会创建一个长度为Capacity的Entry数组，这个长度在哈希表中被称为容量(Capacity)，在这个数组中可以存放元素的位置我们称之为“桶”(bucket)，每个bucket都有自己的索引，系统可以根据索引快速的查找bucket中的元素。 \n\n每个bucket中存储一个元素，即一个Entry对象，但每一个Entry对象可以带一个引用变量，用于指向下一个元素，因此，在一个桶中，就有可能生成一个Entry链。而且新添加的元素作为链表的head。 \n\n添加元素的过程：\n向HashMap中添加entry1(key，value)，需要首先计算entry1中key的哈希值(根据key所在类的hashCode()计算得到)，此哈希值经过处理以后，得到在底层Entry[]数组中要存储的位置i。如果位置i上没有元素，则entry1直接添加成功。如果位置i上已经存在entry2(或还有链表存在的entry3，entry4)，则需要通过循环的方法，依次比较entry1中key和其他的entry。如果彼此hash值不同，则直接添加成功。如果hash值不同，继续比较二者是否equals。如果返回值为true，则使用entry1的value去替换equals为true的entry的value。如果遍历一遍以后，发现所有的equals返回都为false,则entry1仍可添加成功。entry1指向原有的entry元素。\n\nHashMap的扩容\n当HashMap中的元素越来越多的时候，hash冲突的几率也就越来越高，因为数组的长度是固定的。所以为了提高查询的效率，就要对HashMap的数组进行扩容，而在HashMap数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是resize。\n\n那么HashMap什么时候进行扩容呢？\n当HashMap中的元素个数超过数组大小(数组总大小length,不是数组中个数size)*loadFactor 时 ， 就 会 进 行 数 组 扩 容 ， loadFactor 的默认 值 (DEFAULT_LOAD_FACTOR)为0.75，这是一个折中的取值。也就是说，默认情况下，数组大小(DEFAULT_INITIAL_CAPACITY)为16，那么当HashMap中元素个数超过16*0.75=12（这个值就是代码中的threshold值，也叫做临界值）的时候，就把数组的大小扩展为 2*16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以如果我们已经预知HashMap中元素的个数，那么预设元素的个数能够有效的提高HashMap的性能。\n\nHashMap的内部存储结构其实是数组+链表+树的结合。当实例化一个HashMap时，会初始化initialCapacity和loadFactor，在put第一对映射关系时，系统会创建一个长度为initialCapacity的Node数组，这个长度在哈希表中被称为容量(Capacity)，在这个数组中可以存放元素的位置我们称之为“桶”(bucket)，每个bucket都有自己的索引，系统可以根据索引快速的查找bucket中的元素。 \n\n每个bucket中存储一个元素，即一个Node对象，但每一个Node对象可以带一个引用变量next，用于指向下一个元素，因此，在一个桶中，就有可能生成一个Node链。也可能是一个一个TreeNode对象，每一个TreeNode对象可以有两个叶子结点left和right，因此，在一个桶中，就有可能生成一个TreeNode树。而新添加的元素作为链表的last，或树的叶子结点。\n\n那么HashMap什么时候进行扩容和树形化呢？ \n​        当HashMap中的元素个数超过数组大小(数组总大小length,不是数组中个数size)*loadFactor 时 ， 就会进行数组扩容 ， loadFactor 的默认 值 (DEFAULT_LOAD_FACTOR)为0.75，这是一个折中的取值。也就是说，默认情况下，数组大小(DEFAULT_INITIAL_CAPACITY)为16，那么当HashMap中元素个数超过16*0.75=12（这个值就是代码中的threshold值，也叫做临界值）的时候，就把数组的大小扩展为 2*16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以如果我们已经预知HashMap中元素的个数，那么预设元素的个数能够有效的提高HashMap的性能。\n​        当HashMap中的其中一个链的对象个数如果达到了8个，此时如果capacity没有达到64，那么HashMap会先扩容解决，如果已经达到了64，那么这个链会变成树，结点类型由Node变成TreeNode类型。当然，如果当映射关系被移除后，下次resize方法时判断树的结点个数低于6个，也会把树再转为链表。\n关于映射关系的key是否可以修改？answer：不要修改\n​        映射关系存储到HashMap中会存储key的hash值，这样就不用在每次查找时重新计算每一个Entry或Node（TreeNode）的hash值了，因此如果已经put到Map中的映射关系，再修改key的属性，而这个属性又参与hashcode值的计算，那么会导致匹配不上。\n总结：JDK1.8相较于之前的变化：\n\nHashMap map = new HashMap();//默认情况下，先不创建长度为16的数组\n当首次调用map.put()时，再创建长度为16的数组\n数组为Node类型，在jdk7中称为Entry类型\n形成链表结构时，新添加的key-value对在链表的尾部（七上八下）\n当数组指定索引位置的链表长度&gt;8时，且map中的数组的长度&gt; 64时，此索引位置上的所有key-value对使用红黑树进行存储。\n\n面试题谈谈你对HashMap中put/get方法的认识？如果了解再谈谈HashMap的扩容机制？默认大小是多少？什么是负载因子(或填充比)？什么是吞吐临界值(或阈值、threshold)？\n负载因子值的大小，对HashMap有什么影响\n\n负载因子的大小决定了HashMap的数据密度。\n负载因子越大密度越大，发生碰撞的几率越高，数组中的链表越容易长,造成查询或插入时的比较次数增多，性能会下降。\n负载因子越小，就越容易触发扩容，数据密度也越小，意味着发生碰撞的几率越小，数组中的链表也就越短，查询和插入时比较的次数也越小，性能会更高。但是会浪费一定的内容空间。而且经常扩容也会影响性能，建议初始化预设大一点的空间。\n按照其他语言的参考及研究经验，会考虑将负载因子设置为0.7~0.75，此时平均检索长度接近于常数。\n\nLinkedHashMap\nLinkedHashMap 是 HashMap 的子类\n在HashMap存储结构的基础上，使用了一对双向链表来记录添加元素的顺序\n与LinkedHashSet类似，LinkedHashMap 可以维护 Map 的迭代顺序：迭代顺序与 Key-Value 对的插入顺序一致\n\n//HashMap中的内部类：Node\nstatic class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; {\n    final int hash;\n    final K key;\n    V value;\n    Node&lt;K,V&gt; next; \n}\n//LinkedHashMap中的内部类：Entry\nstatic class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; {\n    Entry&lt;K,V&gt; before, after;\n    Entry(int hash, K key, V value, Node&lt;K,V&gt; next) {\n    \tsuper(hash, key, value, next);\n    } \n}\nTreeMap\nTreeMap存储 Key-Value 对时，需要根据 key-value 对进行排序。TreeMap 可以保证所有的 Key-Value 对处于有序状态。 \nTreeSet底层使用红黑树结构存储数据\nTreeMap 的 Key 的排序：\n自然排序：TreeMap 的所有的 Key 必须实现 Comparable 接口，而且所有的 Key 应该是同一个类的对象，否则将会抛出 ClasssCastException\n定制排序：创建 TreeMap 时，传入一个 Comparator 对象，该对象负责对TreeMap 中的所有 key 进行排序。此时不需要 Map 的 Key 实现Comparable 接口\n\n\nTreeMap判断两个key相等的标准：两个key通过compareTo()方法或者compare()方法返回0。\n\nHashtable\nHashtable是个古老的 Map 实现类，JDK1.0就提供了。不同于HashMap，Hashtable是线程安全的。\nHashtable实现原理和HashMap相同，功能相同。底层都使用哈希表结构，查询速度快，很多情况下可以互用。 \n与HashMap不同，Hashtable 不允许使用 null 作为 key 和 value\n与HashMap一样，Hashtable 也不能保证其中 Key-Value 对的顺序\nHashtable判断两个key相等、两个value相等的标准，与HashMap一致。\n\nProperties\nProperties 类是 Hashtable 的子类，该对象用于处理属性文件\n由于属性文件里的 key、value 都是字符串类型，所以 Properties 里的 key 和 value 都是字符串类型\n存取数据时，建议使用setProperty(String key,String value)方法和getProperty(String key)方法\n\nProperties pros = new Properties();\npros.load(new FileInputStream(\"jdbc.properties\"));\nString user = pros.getProperty(\"user\");\nSystem.out.println(user);\n1.7 Collections工具类​        Collections 是一个操作 Set、List 和 Map 等集合的工具类​        Collections 中提供了一系列静态的方法对集合元素进行排序、查询和修改等操作，还提供了对集合对象设置不可变、对集合对象实现同步控制等方法\n\n\n\n\n排序操作\n（均为static方法）\n\n\n\n\nreverse(List)\n反转 List 中元素的顺序\n\n\nshuffle(List)\n对 List 集合元素进行随机排序\n\n\nsort(List)\n根据元素的自然顺序对指定 List 集合元素按升序排序\n\n\nsort(List，Comparator)\n根据指定的 Comparator 产生的顺序对 List 集合元素进行排序\n\n\nswap(List，int， int)\n将指定 list 集合中的 i 处元素和 j 处元素进行交换\n\n\n\n\n\n\n\n\n查找、替换\n\n\n\n\n\nObject max(Collection)\n根据元素的自然顺序，返回给定集合中的最大元素\n\n\nObject max(Collection，Comparator)\n根据 Comparator 指定的顺序，返回给定集合中的最大元素\n\n\nObject min(Collection)\n\n\n\nObject min(Collection，Comparator)\n\n\n\nint frequency(Collection，Object)\n返回指定集合中指定元素的出现次数\n\n\nvoid copy(List dest,List src)\n将src中的内容复制到dest中\n\n\nboolean replaceAll(List list，Object oldVal，Object newVal)\n使用新值替换List 对象的所有旧值\n\n\n\n\n同步控制:Collections 类中提供了多个 synchronizedXxx() 方法，该方法可使将指定集合包装成线程同步的集合，从而可以解决多线程并发访问集合时的线程安全问题。\n补充：Enumeration：Enumeration 接口是 Iterator 迭代器的 “古老版本”\nEnumeration stringEnum = new StringTokenizer(\"a-b*c-d-e-g\", \"-\");\nwhile(stringEnum.hasMoreElements()){\n    Object obj = stringEnum.nextElement();\n    System.out.println(obj); \n}\n2. 泛型​        集合容器类在设计阶段/声明阶段不能确定这个容器到底实际存的是什么类型的对象，所以在JDK1.5之前只能把元素类型设计为Object，JDK1.5之后使用泛型来解决。因为这个时候除了元素的类型不确定，其他的部分是确定的，例如关于这个元素如何保存，如何管理等是确定的，因此此时把元素的类型设计成一个参数，这个类型参数叫做泛型。Collection，List，ArrayList 这个就是类型参数，即泛型。\n2.1 为什么要有泛型泛型的概念\n​        所谓泛型，就是允许在定义类、接口时通过一个标识表示类中某个属性的类型或者是某个方法的返回值及参数类型。这个类型参数将在使用时（例如，继承或实现这个接口，用这个类型声明变量、创建对象时）确定（即传入实际的类型参数，也称为类型实参）。\n那么为什么要有泛型呢，直接Object不是也可以存储数据吗？\n\n解决元素存储的安全性问题，好比商品、药品标签，不会弄错。\n解决获取数据元素时，需要类型强制转换的问题，好比不用每回拿商品、药品都要辨别。\n\n2.2 在集合中使用泛型ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;();//类型推断\nlist.add(78);\nlist.add(88);\nlist.add(77);\nlist.add(66);\n//遍历方式一：\n//for(Integer i : list){\n    //不需要强转\n    //System.out.println(i);\n//}\n//遍历方式二：\nIterator&lt;Integer&gt; iterator = list.iterator();\nwhile(iterator.hasNext()){\n\tSystem.out.println(iterator.next());\n}\nMap&lt;String,Integer&gt; map = new HashMap&lt;String,Integer&gt;();\nmap.put(\"Tom1\",34);\nmap.put(\"Tom2\",44);\nmap.put(\"Tom3\",33);\nmap.put(\"Tom4\",32);\n//添加失败\n//map.put(33, \"Tom\");\nSet&lt;Entry&lt;String,Integer&gt;&gt; entrySet = map.entrySet();\nIterator&lt;Entry&lt;String,Integer&gt;&gt; iterator = entrySet.iterator();\nwhile(iterator.hasNext()){\n    Entry&lt;String,Integer&gt; entry = iterator.next();\n    System.out.println(entry.getKey() + \"---&gt;\" + entry.getValue());\n}\n2.3 自定义泛型结构自定义泛型结构1.泛型的声明\n​        interface List 和 class GenTest 其中，T,K,V不代表值，而是表示类型。这里使用任意字母都可以。常用T表示，是Type的缩写。\n2.泛型的实例化：\n一定要在类名后面指定类型参数的值（类型）。如：List strList = new ArrayList();Iterator iterator = customers.iterator();\n\nT只能是类，不能用基本数据类型填充。但可以使用包装类填充\n把一个集合中的内容限制为一个特定的数据类型，这就是generics背后的核心思想\n\n泛型类、泛型接口\n泛型类可能有多个参数，此时应将多个参数一起放在尖括号内。比如：\n泛型类的构造器如下：public GenericClass(){}。而下面是错误的：public GenericClass(){}\n实例化后，操作原来泛型位置的结构必须与指定的泛型类型一致。\n泛型不同的引用不能相互赋值。尽管在编译时ArrayList和ArrayList是两种类型，但是，在运行时只有一个ArrayList被加载到JVM中。\n\n泛型如果不指定，将被擦除，泛型对应的类型均按照Object处理，但不等价于Object。经验：泛型要使用一路都用。要不用，一路都不要用。\n\n如果泛型结构是一个接口或抽象类，则不可创建泛型类的对象。\njdk1.7，泛型的简化操作：ArrayList flist = new ArrayList&lt;&gt;();\n泛型的指定中不能使用基本数据类型，可以使用包装类替换。\n在类/接口上声明的泛型，在本类或本接口中即代表某种类型，可以作为非静态属性的类型、非静态方法的参数类型、非静态方法的返回值类型。但在静态方法中不能使用类的泛型。\n异常类不能是泛型的\n不能使用new E[]。但是可以：E[] elements = (E[])new Object[capacity];参考：ArrayList源码中声明：Object[] elementData，而非泛型参数类型数组。\n父类有泛型，子类可以选择保留泛型也可以选择指定泛型类型：\n子类不保留父类的泛型：按需实现 没有类型 擦除 具体类型\n子类保留父类的泛型：泛型子类 全部保留 部分保留\n结论：子类必须是“富二代”，子类除了指定或保留父类的泛型，还可以增加自己的泛型\n\n\n\nclass Father&lt;T1, T2&gt; {\n}\n// 子类不保留父类的泛型\n// 1)没有类型 擦除\nclass Son1 extends Father {// 等价于class Son extends Father&lt;Object,Object&gt;{\n}\n// 2)具体类型\nclass Son2 extends Father&lt;Integer, String&gt; {\n}\n// 子类保留父类的泛型\n// 1)全部保留\nclass Son3&lt;T1, T2&gt; extends Father&lt;T1, T2&gt; {\n}\n// 2)部分保留\nclass Son4&lt;T2&gt; extends Father&lt;Integer, T2&gt; {\n}\n\nclass Person&lt;T&gt; {\n    // 使用T类型定义变量\n    private T info;\n    // 使用T类型定义一般方法\n    public T getInfo() {\n    \treturn info; \n    }\n    public void setInfo(T info) {\n    \tthis.info = info; \n    }\n    // 使用T类型定义构造器\n    public Person() {\n    }\n    public Person(T info) {\n    \tthis.info = info; \n    }\n    // static的方法中不能声明泛型\n    //public static void show(T t) {\n    //\n    //}\n    // 不能在try-catch中使用泛型定义\n    //public void test() {\n    //try {\n    //\n    //} catch (MyException&lt;T&gt; ex) {\n    //\n    //}\n    //}\n}\n泛型方法泛型方法的格式：[访问权限] &lt;泛型&gt; 返回类型 方法名([泛型标识 参数名称]) 抛出的异常\npublic class DAO {\n    public &lt;E&gt; E get(int id, E e) {\n        E result = null;\n        return result; \n    } \n}\npublic static &lt;T&gt; void fromArrayToCollection(T[] a, Collection&lt;T&gt; c) {\n    for (T o : a) {\n    \tc.add(o);\n    } \n}\npublic static void main(String[] args) {\n    Object[] ao = new Object[100];\n    Collection&lt;Object&gt; co = new ArrayList&lt;Object&gt;();\n    fromArrayToCollection(ao, co);\n    String[] sa = new String[20];\n    Collection&lt;String&gt; cs = new ArrayList&lt;&gt;();\n    fromArrayToCollection(sa, cs);\n    Collection&lt;Double&gt; cd = new ArrayList&lt;&gt;();\n    // 下面代码中T是Double类，但sa是String类型，编译错误。\n    // fromArrayToCollection(sa, cd);\n    // 下面代码中T是Object类型，sa是String类型，可以赋值成功。\n    fromArrayToCollection(sa, co);\n}\nclass Creature{}\nclass Person extends Creature{}\nclass Man extends Person{}\nclass PersonTest {\n    public static &lt;T extends Person&gt; void test(T t){\n    \tSystem.out.println(t);\n    }\n    public static void main(String[] args) {\n        test(new Person());\n        test(new Man());\n        //The method test(T) in the type PersonTest is not \n        //applicable for the arguments (Creature)\n        test(new Creature());\n    } \n}\n2.4 泛型在继承上的体现如果B是A的一个子类型（子类或者子接口），而G是具有泛型声明的类或接口，G并不是G的子类型！比如：String是Object的子类，但是List并不是List的子类。\npublic void testGenericAndSubClass() {\n    Person[] persons = null;\n    Man[] mans = null;\n    // 而 Person[] 是 Man[] 的父类.\n    persons = mans;\n    Person p = mans[0];\n    // 在泛型的集合上\n    List&lt;Person&gt; personList = null;\n    List&lt;Man&gt; manList = null;\n    // personList = manList;(报错) \n}\n2.5 通配符的使用\n使用类型通配符：？比如：List&lt;?&gt; ，Map&lt;?,?&gt;List&lt;?&gt;是List、List等各种泛型List的父类。\n读取List&lt;?&gt;的对象list中的元素时，永远是安全的，因为不管list的真实类型是什么，它包含的都是Object。\n写入list中的元素时，不行。因为我们不知道c的元素类型，我们不能向其中添加对象。 唯一的例外是null，它是所有类型的成员。\n\n\n将任意元素加入到其中不是类型安全的：Collection&lt;?&gt; c = new ArrayList();c.add(new Object()); // 编译时错误因为我们不知道c的元素类型，我们不能向其中添加对象。add方法有类型参数E作为集合的元素类型。我们传给add的任何参数都必须是一个未知类型的子类。因为我们不知道那是什么类型，所以我们无法传任何东西进去。\n唯一的例外的是null，它是所有类型的成员。\n另一方面，我们可以调用get()方法并使用其返回值。返回值是一个未知的类型，但是我们知道，它总是一个Object。\n\npublic static void main(String[] args) {\n    List&lt;?&gt; list = null;\n    list = new ArrayList&lt;String&gt;();\n    list = new ArrayList&lt;Double&gt;();\n    // list.add(3);//编译不通过\n    list.add(null);\n    List&lt;String&gt; l1 = new ArrayList&lt;String&gt;();\n    List&lt;Integer&gt; l2 = new ArrayList&lt;Integer&gt;();\n    l1.add(\"尚硅谷\");\n    l2.add(15);\n    read(l1);\n    read(l2);\n}\npublic static void read(List&lt;?&gt; list) {\n    for (Object o : list) {\n    \tSystem.out.println(o);\n    } \n}\n//注意点1：编译错误：不能用在泛型方法声明上，返回值类型前面&lt;&gt;不能使用?\npublic static &lt;?&gt; void test(ArrayList&lt;?&gt; list){\n}\n//注意点2：编译错误：不能用在泛型类的声明上\nclass GenericTypeClass&lt;?&gt;{\n}\n//注意点3：编译错误：不能用在创建对象上，右边属于创建集合对象\nArrayList&lt;?&gt; list2 = new ArrayList&lt;?&gt;();\n有限制的通配符\n\n&lt;?&gt;允许所有泛型的引用调用\n\n通配符指定上限上限extends：使用时指定的类型必须是继承某个类，或者实现某个接口，即&lt;= \n\n通配符指定下限下限super：使用时指定的类型不能小于操作的类，即&gt;=\n\n举例： \n\n&lt;? extends Number&gt; (无穷小 , Number]只允许泛型为Number及Number子类的引用调用\n\n&lt;? super Number&gt; [Number , 无穷大) \n只允许泛型为Number及Number父类的引用调用\n\n&lt;? extends Comparable&gt;只允许泛型为实现Comparable接口的实现类的引用调用\n\n\n\n\npublic static void printCollection3(Collection&lt;? extends Person&gt; coll) {\n    //Iterator只能用Iterator&lt;?&gt;或Iterator&lt;? extends Person&gt;.why?\n    Iterator&lt;?&gt; iterator = coll.iterator();\n    while (iterator.hasNext()) {\n    \tSystem.out.println(iterator.next());\n    } \n}\npublic static void printCollection4(Collection&lt;? super Person&gt; coll) {\n    //Iterator只能用Iterator&lt;?&gt;或Iterator&lt;? super Person&gt;.why?\n    Iterator&lt;?&gt; iterator = coll.iterator();\n    while (iterator.hasNext()) {\n    \tSystem.out.println(iterator.next());\n    } \n}\n2.6 泛型应用举例public static void main(String[] args) {\n    HashMap&lt;String, ArrayList&lt;Citizen&gt;&gt; map = new HashMap&lt;String, ArrayList&lt;Citizen&gt;&gt;();\n    ArrayList&lt;Citizen&gt; list = new ArrayList&lt;Citizen&gt;();\n    list.add(new Citizen(\"刘恺威\"));\n    list.add(new Citizen(\"杨幂\"));\n    list.add(new Citizen(\"小糯米\"));\n    map.put(\"刘恺威\", list);\n    Set&lt;Entry&lt;String, ArrayList&lt;Citizen&gt;&gt;&gt; entrySet = map.entrySet();\n    Iterator&lt;Entry&lt;String, ArrayList&lt;Citizen&gt;&gt;&gt; iterator = entrySet.iterator();\n    while (iterator.hasNext()) {\n        Entry&lt;String, ArrayList&lt;Citizen&gt;&gt; entry = iterator.next();\n        String key = entry.getKey();\n        ArrayList&lt;Citizen&gt; value = entry.getValue();\n        System.out.println(\"户主：\" + key);\n        System.out.println(\"家庭成员：\" + value);\n    } \n}\n","slug":"J4-集合、泛型","date":"2021-11-12T02:30:35.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"d0396f84e2a9cf009e1f2a35f0f836dd","title":"常用类、枚举、注解","content":"1. Java常用类0. Wapper包装类ing\n1.1 字符串相关的类StringString的特性\n\nString类：代表字符串。Java 程序中的所有字符串字面值（如 “abc” ）都作为此类的实例实现。\nString是一个final类，代表不可变的字符序列。\n字符串是常量，用双引号引起来表示。它们的值在创建之后不能更改。\nString对象的字符内容是存储在一个字符数组value[]中的。\n\nString对象的创建\nString str = \"hello\";\n\n//本质上this.value = new char[0];\nString s1 = new String(); \n\n//this.value = original.value;\nString s2 = new String(String original); \n\n//this.value = Arrays.copyOf(value, value.length);\nString s3 = new String(char[] a); \nString s4 = new String(char[] a,int startIndex,int count);\n字符串对象是如何存储的结论：\n\n常量与常量的拼接结果在常量池。且常量池中不会存在相同内容的常量。\n只要其中有一个是变量，结果就在堆中\n如果拼接的结果调用intern()方法，返回值就在常量池中\n\nString使用陷阱\n\nString s1 = “a”;说明：在字符串常量池中创建了一个字面量为”a”的字符串。\ns1 = s1 + “b”;说明：实际上原来的“a”字符串对象已经丢弃了，现在在堆空间中产生了一个字符串s1+”b”（也就是”ab”)。如果多次执行这些改变串内容的操作，会导致大量副本字符串对象存留在内存中，降低效率。如果这样的操作放到循环中，会极大影响程序的性能。\nString s2 = “ab”;说明：直接在字符串常量池中创建一个字面量为”ab”的字符串。\nString s3 = “a” + “b”;说明：s3指向字符串常量池中已经创建的”ab”的字符串。String s4 = s1.intern();说明：堆空间的s1对象在调用intern()之后，会将常量池中已经存在的”ab”字符串赋值给s4。\n\nString常用方法\nint length()：返回字符串的长度： return value.length\nchar charAt(int index)： 返回某索引处的字符return value[index]\nboolean isEmpty()：判断是否是空字符串：return value.length == 0\nString toLowerCase()：使用默认语言环境，将 String 中的所有字符转换为小写\nString toUpperCase()：使用默认语言环境，将 String 中的所有字符转换为大写\nString trim()：返回字符串的副本，忽略前导空白和尾部空白\nboolean equals(Object obj)：比较字符串的内容是否相同\nboolean equalsIgnoreCase(String anotherString)：与equals方法类似，忽略大小写\nString concat(String str)：将指定字符串连接到此字符串的结尾。 等价于用“+”\nint compareTo(String anotherString)：比较两个字符串的大小\nString substring(int beginIndex)：返回一个新的字符串，它是此字符串的从beginIndex开始截取到最后的一个子字符串。\nString substring(int beginIndex, int endIndex) ：返回一个新字符串，它是此字符串从beginIndex开始截取到endIndex(不包含)的一个子字符串。\nboolean endsWith(String suffix)：测试此字符串是否以指定的后缀结束\nboolean startsWith(String prefix)：测试此字符串是否以指定的前缀开始\nboolean startsWith(String prefix, int toffset)：测试此字符串从指定索引开始的子字符串是否以指定前缀开始\nboolean contains(CharSequence s)：当且仅当此字符串包含指定的 char 值序列时，返回 true\nint indexOf(String str)：返回指定子字符串在此字符串中第一次出现处的索引\nint indexOf(String str, int fromIndex)：返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始\nint lastIndexOf(String str)：返回指定子字符串在此字符串中最右边出现处的索引\nint lastIndexOf(String str, int fromIndex)：返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索\nString replace(char oldChar, char newChar)：返回一个新的字符串，它是通过用newChar 替换此字符串中出现的所有oldChar 得到的。\nString replace(CharSequence target, CharSequence replacement)：使用指定的字面值替换序列替换此字符串所有匹配字面值目标序列的子字符串。\nString replaceAll(String regex, String replacement) ： 使用 给 定 的replacement 替换此字符串所有匹配给定的正则表达式的子字符串。\nString replaceFirst(String regex, String replacement) ：使用给定的replacement 替换此字符串匹配给定的正则表达式的第一个子字符串。\nboolean matches(String regex)：告知此字符串是否匹配给定的正则表达式。\nString[] split(String regex)：根据给定正则表达式的匹配拆分此字符串。\nString[] split(String regex, int limit)：根据匹配给定的正则表达式来拆分此字符串，最多不超过limit个，如果超过了，剩下的全部都放到最后一个元素中。\n\nString与字符数组转换\n字符数组-&gt;字符串    String 类的构造器：String(char[]) 和 String(char[]，int offset，int length) 分别用字符数组中的全部字符和部分字符创建字符串对象。\n\n字符串-&gt;字符数组    public char[] toCharArray()：将字符串中的全部字符存放在一个字符数组中的方法。    public void getChars(int srcBegin, int srcEnd, char[] dst,int dstBegin)：提供了将指定索引范围内的字符串存放到数组中的方法。\n\n字节数组-&gt;字符串    String(byte[])：通过使用平台的默认字符集解码指定的 byte 数组，构造一个新的 String。    String(byte[]，int offset，int length) ：用指定的字节数组的一部分，即从数组起始位置offset开始取length个字节构造一个字符串对象。\n\n字符串-&gt;字节数组    public byte[] getBytes() ：使用平台的默认字符集将此 String 编码为byte 序列，并将结果存储到一个新的 byte 数组中。    public byte[] getBytes(String charsetName) ：使用指定的字符集将此 String 编码到 byte 序列，并将结果存储到新的 byte 数组。\n\n\nStringBuffer类\njava.lang.StringBuffer代表可变的字符序列，JDK1.0中声明，可以对字符串内容进行增删，此时不会产生新的对象。\n很多方法与String相同。\n作为参数传递时，方法内部可以改变值。\n\nStringBuffer类不同于String，其对象必须使用构造器生成。有三个构造器：    StringBuffer()：初始容量为16的字符串缓冲区    StringBuffer(int size)：构造指定容量的字符串缓冲区    StringBuffer(String str)：将内容初始化为指定字符串内容\nStringBuffer类的常用方法\nStringBuffer append(xxx)：提供了很多的append()方法，用于进行字符串拼接\nStringBuffer delete(int start,int end)：删除指定位置的内容\nStringBuffer replace(int start, int end, String str)：把[start,end)位置替换为str \nStringBuffer insert(int offset, xxx)：在指定位置插入xxx\nStringBuffer reverse() ：把当前字符序列逆转\npublic int indexOf(String str)\npublic String substring(int start,int end) public int length()\npublic char charAt(int n )\npublic void setCharAt(int n ,char ch)\n\nStringBuilder类StringBuilder 和 StringBuffer 非常类似，均代表可变的字符序列，而且提供相关功能的方法也一样面试题：对比String、StringBuffer、StringBuilder⭐⭐\n    String(JDK1.0)：不可变字符序列    StringBuffer(JDK1.0)：可变字符序列、效率低、线程安全    StringBuilder(JDK 5.0)：可变字符序列、效率高、线程不安全\n注意：作为参数传递的话，方法内部String不会改变其值，StringBuffer和StringBuilder会改变其值。\n1.2 JDK 8之前的日期时间API1. java.lang.System类System类提供的public static long currentTimeMillis()用来返回当前时间与1970年1月1日0时0分0秒之间以毫秒为单位的时间差。\n此方法适于计算时间差。\n\n计算世界时间的主要标准有：    UTC(Coordinated Universal Time)    GMT(Greenwich Mean Time)    CST(Central Standard Time)\n\n2. java.util.Date类表示特定的瞬间，精确到毫秒\n\n构造器：    Date()：使用无参构造器创建的对象可以获取本地当前时间。    Date(long date)\n常用方法    getTime():返回自 1970 年 1 月 1 日 00:00:00 GMT 以来此 Date 对象表示的毫秒数。    toString():把此 Date 对象转换为以下形式的 String： dow mon dd hh:mm:ss zzz yyyy 其中： dow 是一周中的某一天 (Sun, Mon, Tue, Wed, Thu, Fri, Sat)，zzz是时间标准。    其它很多方法都过时了。\n\n3. java.text.SimpleDateFormat类\nDate类的API不易于国际化，大部分被废弃了，java.text.SimpleDateFormat类是一个不与语言环境有关的方式来格式化和解析日期的具体类。\n\n它允许进行格式化：日期文本、解析：文本日期\n\n格式化\n    SimpleDateFormat() ：默认的模式和语言环境创建对象    public SimpleDateFormat(String pattern)：该构造方法可以用参数pattern指定的格式创建一个对象，该对象调用：    public String format(Date date)：方法格式化时间对象date\n\n解析：public Date parse(String source)：从给定字符串的开始解析文本，以生成一个日期。\n\n\n\n4. java.util.Calendar(日历)类\nCalendar是一个抽象基类，主用用于完成日期字段之间相互操作的功能。\n获取Calendar实例的方法    使用Calendar.getInstance()方法    调用它的子类GregorianCalendar的构造器。\n一个Calendar的实例是系统时间的抽象表示，通过get(int field)方法来取得想要的时间信息。比如YEAR、MONTH、DAY_OF_WEEK、HOUR_OF_DAY 、 MINUTE、SECOND    public void set(int field,int value)    public void add(int field,int amount)    public final Date getTime()    public final void setTime(Date date)\n注意:    获取月份时：一月是0，二月是1，以此类推，12月是11    获取星期时：周日是1，周二是2 ， 。。。。周六是7\n\n1.3 JDK 8中新日期时间API新时间日期API\n    第三次引入的API是成功的，并且Java 8中引入的java.time API 已经纠正了过去的缺陷，将来很长一段时间内它都会为我们服务。    Java 8 吸收了 Joda-Time 的精华，以一个新的开始为 Java 创建优秀的API。新的 java.time 中包含了所有关于本地日期（LocalDate）、本地时间（LocalTime）、本地日期时间（LocalDateTime）、时区（ZonedDateTime）和持续时间（Duration）的类。历史悠久的 Date 类新增了 toInstant() 方法，用于把 Date 转换成新的表示形式。这些新增的本地化时间日期API 大大简化了日期时间和本地化的管理。\n\njava.time – 包含值对象的基础包\njava.time.chrono – 提供对不同的日历系统的访问\njava.time.format – 格式化和解析时间和日期\njava.time.temporal – 包括底层框架和扩展特性\njava.time.zone – 包含时区支持的类\n\n    LocalDate、LocalTime、LocalDateTime 类是其中较重要的几个类，它们的实例是不可变的对象，分别表示使用 ISO-8601日历系统的日期、时间、日期和时间。它们提供了简单的本地日期或时间，并不包含当前的时间信息，也不包含与时区相关的信息。    LocalDate代表IOS格式（yyyy-MM-dd）的日期,可以存储 生日、纪念日等日期。    LocalTime表示一个时间，而不是日期。    LocalDateTime是用来表示日期和时间的，这是一个最常用的类之一。\n\n\n\n\n方法\n描述\n\n\n\n\nnow() / *  now(ZoneId zone)\n静态方法，根据当前时间创建对象/指定时区的对象\n\n\nof()\n静态方法，根据指定日期/时间创建对象\n\n\ngetDayOfMonth()/getDayOfYear()\n获得月份天数(1-31) /获得年份天数(1-366)\n\n\ngetDayOfWeek()\n获得星期几(返回一个 DayOfWeek 枚举值)\n\n\ngetMonth()\n获得月份, 返回一个 Month 枚举值\n\n\ngetMonthValue() / getYear()\n获得月份(1-12) /获得年份\n\n\ngetHour()/getMinute()/getSecond()\n获得当前对象对应的小时、分钟、秒\n\n\nwithDayOfMonth()/withDayOfYear()/withMonth()/withYear()\n将月份天数、年份天数、月份、年份修改为指定的值并返回新的对象\n\n\nplusDays(), plusWeeks(), plusMonths(), plusYears(),plusHours()\n向当前对象添加几天、几周、几个月、几年、几小时\n\n\nminusMonths() / minusWeeks()/minusDays()/minusYears()/minusHours()\n从当前对象减去几月、几周、几天、几年、几小时\n\n\n\n\n瞬时：Instant    Instant：时间线上的一个瞬时点。 这可能被用来记录应用程序中的事件时间戳。    在处理时间和日期的时候，我们通常会想到年,月,日,时,分,秒。然而，这只是时间的一个模型，是面向人类的。第二种通用模型是面向机器的，或者说是连续的。在此模型中，时间线中的一个点表示为一个很大的数，这有利于计算机处理。在UNIX中，这个数从1970年开始，以秒为的单位；同样的，在Java中，也是从1970年开始，但以毫秒为单位。    java.time包通过值类型Instant提供机器视图，不提供处理人类意义上的时间单位。Instant表示时间线上的一点，而不需要任何上下文信息，例如，时区。概念上讲，它只是简单的表示自1970年1月1日0时0分0秒（UTC）开始的秒数。因为java.time包是基于纳秒计算的，所以Instant的精度可以达到纳秒级。    (1 ns = 10-9 s)   1秒 = 1000毫秒 =10^6微秒=10^9纳秒\n\n\n\n\n方法\n描述\n\n\n\n\nnow()\n静态方法，返回默认UTC时区的Instant类的对象\n\n\nofEpochMilli(long epochMilli)\n静态方法，返回在1970-01-01 00:00:00基础上加上指定毫秒数之后的Instant类的对象\n\n\natOffset(ZoneOffset offset)\n结合即时的偏移来创建一个OffsetDateTime\n\n\ntoEpochMilli()\n返回1970-01-01 00:00:00到当前时间的毫秒数，即为时间戳\n\n\n\n\n时间戳是指格林威治时间1970年01月01日00时00分00秒(北京时间1970年01月01日08时00分00秒)起至现在的总秒数。\n格式化与解析日期或时间java.time.format.DateTimeFormatter 类：该类提供了三种格式化方法：\n    预定义的标准格式。如：ISO_LOCAL_DATE_TIME;ISO_LOCAL_DATE;ISO_LOCAL_TIME    本地化相关的格式。如：ofLocalizedDateTime(FormatStyle.LONG)    自定义的格式。如：ofPattern(“yyyy-MM-dd hh:mm:ss”)\n\n\n\n\n方 法\n描 述\n\n\n\n\nofPattern(String pattern)\n静态方法 ， 返 回 一 个 指 定 字 符 串 格 式 的DateTimeFormatter\n\n\nformat(TemporalAccessor t)\n格式化一个日期、时间，返回字符串\n\n\nparse(CharSequence text)\n将指定格式的字符序列解析为一个日期、时间\n\n\n\n\n其它API\nZoneId：该类中包含了所有的时区信息，一个时区的ID，如 Europe/Paris\nZonedDateTime：一个在ISO-8601日历系统时区的日期时间，如 2007-12-03T10:15:30+01:00 Europe/Paris。\n其中每个时区都对应着ID，地区ID都为“{区域}/{城市}”的格式，例如：Asia/Shanghai等\nClock：使用时区提供对当前即时、日期和时间的访问的时钟。\n持续时间：Duration，用于计算两个“时间”间隔\n日期间隔：Period，用于计算两个“日期”间隔\nTemporalAdjuster : 时间校正器。有时我们可能需要获取例如：将日期调整到“下一个工作日”等操作。\nTemporalAdjusters : 该类通过静态方法(firstDayOfXxx()/lastDayOfXxx()/nextXxx())提供了大量的常用TemporalAdjuster 的实现。\n\n与传统日期处理的转换\n\n\n\n类\nTo 遗留类\nFrom 遗留类\n\n\n\n\njava.time.Instant与java.util.Date\nDate.from(instant)\ndate.toInstant()\n\n\njava.time.Instant与java.sql.Timestamp\nTimestamp.from(instant)\ntimestamp.toInstant()\n\n\njava.time.ZonedDateTime与java.util.GregorianCalendar\nGregorianCalendar.from(zonedDateTime)\ncal.toZonedDateTime()\n\n\njava.time.LocalDate与java.sql.Time\nDate.valueOf(localDate)\ndate.toLocalDate()\n\n\njava.time.LocalTime与java.sql.Time\nDate.valueOf(localDate)\ndate.toLocalTime()\n\n\njava.time.LocalDateTime与java.sql.Timestamp\nTimestamp.valueOf(localDateTime)\ntimestamp.toLocalDateTime()\n\n\njava.time.ZoneId与java.util.TimeZone\nTimezone.getTimeZone(id)\ntimeZone.toZoneId()\n\n\njava.time.format.DateTimeFormatter与java.text.DateFormat\nformatter.toFormat()\n无\n\n\n\n\n1.4 Java比较器在Java中经常会涉及到对象数组的排序问题，那么就涉及到对象之间的比较问题。Java实现对象排序的方式有两种：\n\n自然排序：java.lang.Comparable\n定制排序：java.util.Comparator\n\n自然排序：java.lang.Comparable    Comparable接口强行对实现它的每个类的对象进行整体排序。这种排序被称为类的自然排序。    实现 Comparable 的类必须实现 compareTo(Object obj) 方法，两个对象即通过 compareTo(Object obj) 方法的返回值来比较大小。如果当前对象this大于形参对象obj，则返回正整数，如果当前对象this小于形参对象obj，则返回负整数，如果当前对象this等于形参对象obj，则返回零。    实现Comparable接口的对象列表（和数组）可以通过 Collections.sort 或Arrays.sort进行自动排序。实现此接口的对象可以用作有序映射中的键或有序集合中的元素，无需指定比较器。    对于类 C 的每一个 e1 和 e2 来说，当且仅当 e1.compareTo(e2) == 0 与e1.equals(e2) 具有相同的 boolean 值时，类 C 的自然排序才叫做与 equals一致。建议（虽然不是必需的）最好使自然排序与 equals 一致。\nComparable 的典型实现：(默认都是从小到大排列的)    String：按照字符串中字符的Unicode值进行比较    Character：按照字符的Unicode值来进行比较    数值类型对应的包装类以及BigInteger、BigDecimal：按照它们对应的数值大小进行比较    Boolean：true 对应的包装类实例大于 false 对应的包装类实例    Date、Time等：后面的日期时间比前面的日期时间大\n定制排序：java.util.Comparator    当元素的类型没有实现java.lang.Comparable接口而又不方便修改代码，或者实现了java.lang.Comparable接口的排序规则不适合当前的操作，那么可以考虑使用 Comparator 的对象来排序，强行对多个对象进行整体排序的比较。    重写compare(Object o1,Object o2)方法，比较o1和o2的大小：如果方法返回正整数，则表示o1大于o2；如果返回0，表示相等；返回负整数，表示o1小于o2。    可以将 Comparator 传递给 sort 方法（如 Collections.sort 或Arrays.sort），从而允许在排序顺序上实现精确控制。    还可以使用 Comparator 来控制某些数据结构（如有序 set或有序映射）的顺序，或者为那些没有自然顺序的对象 collection 提供排序。\n1.5 System类    System类代表系统，系统级的很多属性和控制方法都放置在该类的内部。该类位于java.lang包。    由于该类的构造器是private的，所以无法创建该类的对象，也就是无法实例化该类。其内部的成员变量和成员方法都是static的，所以也可以很方便的进行调用。成员变量    System类内部包含in、out和err三个成员变量，分别代表标准输入流(键盘输入)，标准输出流(显示器)和标准错误输出流(显示器)。成员方法    native long currentTimeMillis()：该方法的作用是返回当前的计算机时间，时间的表达格式为当前计算机时间和GMT时间(格林威治时间)1970年1月1号0时0分0秒所差的毫秒数。    void exit(int status)：该方法的作用是退出程序。其中status的值为0代表正常退出，非零代表异常退出。使用该方法可以在图形界面编程中实现程序的退出功能等。\n\nvoid gc()：该方法的作用是请求系统进行垃圾回收。至于系统是否立刻回收，则取决于系统中垃圾回收算法的实现以及系统执行时的情况。\nString getProperty(String key)：该方法的作用是获得系统中属性名为key的属性对应的值。系统中常见的属性名以及属性的作用如下表所示：\n\n1.6 Math类ava.lang.Math提供了一系列静态方法用于科学计算。其方法的参数和返回值类型一般为double型。\n\n\n\n\n\n\n\n\n\n\nabs\n绝对值\n\n\nacos,asin,atan,cos,sin,tan\n三角函数\n\n\nsqrt\n平方根\n\n\npow(double a,doble b)\na的b次幂\n\n\nlog\n自然对数\n\n\nexp\ne为底指数\n\n\nmax(double a,double b)\n\n\n\nmin(double a,double b)\n\n\n\nrandom()\n返回0.0到1.0的随机数\n\n\nlong round(double a)\ndouble型数据a转换为long型（四舍五入）\n\n\ntoDegrees(double angrad)\n弧度—&gt;角度\n\n\ntoRadians(double angdeg)\n角度—&gt;弧度\n\n\n\n\n1.7 BigInteger与BigDecimalBigInteger类    Integer类作为int的包装类，能存储的最大整型值为231-1，Long类也是有限的，最大为263-1。如果要表示再大的整数，不管是基本数据类型还是他们的包装类都无能为力，更不用说进行运算了。    java.math包的BigInteger可以表示不可变的任意精度的整数。BigInteger 提供所有 Java 的基本整数操作符的对应物，并提供 java.lang.Math 的所有相关方法。另外，BigInteger 还提供以下运算：模算术、GCD 计算、质数测试、素数生成、位操作以及一些其他操作。    构造器：BigInteger(String val)：根据字符串构建BigInteger对象\n常用方法\n\npublic BigInteger abs()：返回此 BigInteger 的绝对值的 BigInteger。\nBigInteger add(BigInteger val) ：返回其值为 (this + val) 的 BigInteger\nBigInteger subtract(BigInteger val) ：返回其值为 (this - val) 的 BigInteger\nBigInteger multiply(BigInteger val) ：返回其值为 (this * val) 的 BigInteger\nBigInteger divide(BigInteger val) ：返回其值为 (this / val) 的 BigInteger。整数相除只保留整数部分。\nBigInteger remainder(BigInteger val) ：返回其值为 (this % val) 的 BigInteger。\nBigInteger[] divideAndRemainder(BigInteger val)：返回包含 (this / val) 后跟(this % val) 的两个 BigInteger 的数组。\nBigInteger pow(int exponent) ：返回其值为 (thisexponent) 的 BigInteger。\n\nBigDecimal类​        一般的Float类和Double类可以用来做科学计算或工程计算，但在商业计算中，要求数字精度比较高，故用到java.math.BigDecimal类。​        BigDecimal类支持不可变的、任意精度的有符号十进制定点数。构造器    public BigDecimal(double val)    public BigDecimal(String val)常用方法    public BigDecimal add(BigDecimal augend)    public BigDecimal subtract(BigDecimal subtrahend)    public BigDecimal multiply(BigDecimal multiplicand)    public BigDecimal divide(BigDecimal divisor, int scale, int roundingMode)\n2. 枚举类与注解枚举类的实现    JDK1.5之前需要自定义枚举类    JDK 1.5 新增的 enum 关键字用于定义枚举类\n\n若枚举只有一个对象, 则可以作为一种单例模式的实现方式。\n枚举类的属性    枚举类对象的属性不应允许被改动, 所以应该使用 private final 修饰    枚举类的使用 private final 修饰的属性应该在构造器中为其赋值    若枚举类显式的定义了带参数的构造器, 则在列出枚举值时也必须对应的传入参数\n\n2.1 枚举类的使用自定义枚举类\n私有化类的构造器，保证不能在类的外部创建其对象\n在类的内部创建枚举类的实例。声明为：public static final\n对象如果有实例变量，应该声明为private final，并在构造器中初始化\n\n使用关键字enum定义枚举类使用说明    使用 enum 定义的枚举类默认继承了 java.lang.Enum类，因此不能再继承其他类    枚举类的构造器只能使用 private 权限修饰符    枚举类的所有实例必须在枚举类中显式列出(, 分隔 ; 结尾)。列出的实例系统会自动添加 public static final 修饰    必须在枚举类的第一行声明枚举类对象\n\nJDK 1.5 中可以在 switch 表达式中使用Enum定义的枚举类的对象作为表达式, case 子句可以直接使用枚举值的名字, 无需添加枚举类作为限定。\n\nEnum类的主要方法    values()方法：返回枚举类型的对象数组。该方法可以很方便地遍历所有的枚举值。    valueOf(String str)：可以把一个字符串转为对应的枚举类对象。要求字符串必须是枚举类对象的“名字”。如不是，会有运行时异常：IllegalArgumentException。    toString()：返回当前枚举类对象常量的名称\n\n实现接口的枚举类\n和普通 Java 类一样，枚举类可以实现一个或多个接口\n若每个枚举值在调用实现的接口方法呈现相同的行为方式，则只要统一实现该方法即可。\n若需要每个枚举值在调用实现的接口方法呈现出不同的行为方式,则可以让每个枚举值分别来实现该方法\n\n2.2 注解的使用注解(Annotation)概述    从 JDK 5.0 开始, Java 增加了对元数据(MetaData) 的支持, 也就是Annotation(注解)    Annotation 其实就是代码里的特殊标记, 这些标记可以在编译, 类加载, 运行时被读取, 并执行相应的处理。通过使用Annotation, 程序员可以在不改变原有逻辑的情况下, 在源文件中嵌入一些补充信息。代码分析工具、开发工具和部署工具可以通过这些补充信息进行验证或者进行部署。    Annotation 可以像修饰符一样被使用, 可用于修饰包,类, 构造器, 方法, 成员变量, 参数, 局部变量的声明, 这些信息被保存在Annotation的 “name=value” 对中。\n    在JavaSE中，注解的使用目的比较简单，例如标记过时的功能，忽略警告等。在JavaEE/Android中注解占据了更重要的角色，例如用来配置应用程序的任何切面，代替JavaEE旧版中所遗留的繁冗代码和XML配置等。    未来的开发模式都是基于注解的，JPA是基于注解的，Spring2.5以上都是基于注解的，Hibernate3.x以后也是基于注解的，现在的Struts2有一部分也是基于注解的了，注解是一种趋势，一定程度上可以说：框架 = 注解 + 反射 + 设计模式。\n常见的Annotation示例    使用Annotation 时要在其前面增加 @ 符号, 并把该Annotation 当成一个修饰符使用。用于修饰它支持的程序元素    示例一：生成文档相关的注解\n@author 标明开发该类模块的作者，多个作者之间使用,分割\n@version 标明该类模块的版本\n@see 参考转向，也就是相关主题\n@since 从哪个版本开始增加的\n@param 对方法中某参数的说明，如果没有参数就不能写\n@return 对方法返回值的说明，如果方法的返回值类型是void就不能写\n@exception 对方法可能抛出的异常进行说明，如果方法没有用throws显式抛出的异常就不能写其中\n\n@param @return 和@exception 这三个标记都是只用于方法的。 @param的格式要求：@param 形参名形参类型 形参说明\n@return 的格式要求：@return 返回值类型返回值说明\n@exception的格式要求：@exception 异常类型异常说明\n@param和@exception可以并列多个\n\n示例二：在编译时进行格式检查(JDK内置的三个基本注解)    @Override: 限定重写父类方法, 该注解只能用于方法    @Deprecated: 用于表示所修饰的元素(类, 方法等)已过时。通常是因为所修饰的结构危险或存在更好的选择    @SuppressWarnings: 抑制编译器警告\n自定义Annotation\n定义新的Annotation 类型使用@interface 关键字\n自定义注解自动继承了java.lang.annotation.Annotation接口\nAnnotation 的成员变量在 Annotation 定义中以无参数方法的形式来声明。其方法名和返回值定义了该成员的名字和类型。我们称为配置参数。类型只能是八种基本数据类型、String类型、Class类型、enum类型、Annotation类型、以上所有类型的数组。\n可以在定义 Annotation 的成员变量时为其指定初始值, 指定成员变量的初始值可使用default 关键字\n如果只有一个参数成员，建议使用参数名为value\n如果定义的注解含有配置参数，那么使用时必须指定参数值，除非它有默认值。格式是“参数名 = 参数值”，如果只有一个参数成员，且名称为value，可以省略“value=”\n没有成员定义的 Annotation 称为标记; 包含成员变量的 Annotation 称为元数据Annotation\n\nJDK中的元注解JDK 的元Annotation 用于修饰其他Annotation 定义\nJDK5.0提供了4个标准的meta-annotation类型，分别是：    Retention    Target    Documented    Inherited\n\n@Retention: 只能用于修饰一个Annotation 定义, 用于指定该Annotation 的生命周期, @Rentention 包含一个 RetentionPolicy 类型的成员变量, 使用@Rentention 时必须为该 value 成员变量指定值:    RetentionPolicy.SOURCE:在源文件中有效（即源文件保留），编译器直接丢弃这种策略的注释    RetentionPolicy.CLASS:在class文件中有效（即class保留） ， 当运行 Java 程序时, JVM不会保留注解。 这是默认值    RetentionPolicy.RUNTIME:在运行时有效（即运行时保留），当运行 Java 程序时, JVM 会保留注释。程序可以通过反射获取该注释。\n\n@Target: 用于修饰Annotation 定义, 用于指定被修饰的Annotation 能用于修饰哪些程序元素。 @Target 也包含一个名为 value 的成员变量。\n\n\n@Documented: 用于指定被该元Annotation 修饰的Annotation 类将被javadoc 工具提取成文档。默认情况下，javadoc是不包括注解的。    定义为Documented的注解必须设置Retention值为RUNTIME。\n\n@Inherited: 被它修饰的Annotation 将具有继承性。如果某个类使用了被@Inherited 修饰的Annotation, 则其子类将自动具有该注解。    比如：如果把标有@Inherited注解的自定义的注解标注在类级别上，子类则可以继承父类类级别的注解    实际应用中，使用较少\n\n\n利用反射获取注解信息（在反射部分涉及）\nJDK 5.0 在 java.lang.reflect 包下新增了 AnnotatedElement 接口, 该接口代表程序中可以接受注解的程序元素\n当一个Annotation 类型被定义为运行时Annotation 后, 该注解才是运行时可见, 当 class 文件被载入时保存在 class 文件中的Annotation 才会被虚拟机读取\n程序可以调用AnnotatedElement对象的如下方法来访问Annotation 信息\n\nJDK 8中注解的新特性Java 8对注解处理提供了两点改进：可重复的注解及可用于类型的注解。此外，反射也得到了加强，在Java8中能够得到方法参数的名称。这会简化标注在方法参数上的注解。\n类型注解：\n\nJDK1.8之后，关于元注解@Target的参数类型ElementType枚举值多了两个：TYPE_PARAMETER,TYPE_USE。\n在Java 8之前，注解只能是在声明的地方所使用，Java8开始，注解可以应用在任何地方。    ElementType.TYPE_PARAMETER 表示该注解能写在类型变量的声明语句中（如：泛型声明）。    ElementType.TYPE_USE 表示该注解能写在使用类型的任何语句中。 \n\n","slug":"J3-常用类、枚举、注解","date":"2021-11-12T02:30:10.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"8dfa68efe4979ccc3a00434835ec3a9a","title":"异常与多线程","content":"1. 异常处理1.1 异常概述与异常体系结构异常：在Java语言中，将程序执行中发生的不正常情况\n异常事件可分为两类：\n\nError：Java虚拟机无法解决的严重问题。如：JVM系统内部错误、资源耗尽等严重情况。比如：StackOverflowError和OOM。一般不编写针对性的代码进行处理。\nException: 其它因编程错误或偶然的外在因素导致的一般性问题，可以使用针对性的代码进行处理。例如：    空指针访问    试图读取不存在的文件    网络连接中断    数组角标越界\n\n错误的处理方法\n\n遇到错误就终止程序的运行。\n由程序员在编写程序时，就考虑到错误的检测、错误消息的提示，以及错误的处理。\n\n\n捕获错误最理想的是在编译期间，但有的错误只有在运行时才会发生。比如：除数为0，数组下标越界等\n分类：编译时异常和运行时异常\n\n运行时异常    是指编译器不要求强制处置的异常。一般是指编程时的逻辑错误，是程序员应该积极避免其出现的异常。java.lang.RuntimeException类及它的子类都是运行时异常。    对于这类异常，可以不作处理，因为这类异常很普遍，若全处理可能会对程序的可读性和运行效率产生影响。\n编译时异常    是指编译器要求必须处置的异常。即程序在运行时由于外界因素造成的一般性异常。编译器要求Java程序必须捕获或声明所有编译时异常。    对于这类异常，如果程序不处理，可能会带来意想不到的结果。\n1.2 常见异常\n1.3 异常处理机制一：try-catch-finallyJava程序的执行过程中如出现异常，会生成一个异常类对象，该异常对象将被提交给Java运行时系统，这个过程称为抛出(throw)异常。\n异常对象的生成\n\n由虚拟机自动生成：程序运行过程中，虚拟机检测到程序发生了问题，如果在当前代码中没有找到相应的处理程序，就会在后台自动创建一个对应异常类的实例对象并抛出——自动抛出\n由开发人员手动创建：Exception exception = new ClassCastException();——创建好的异常对象不抛出对程序没有任何影响，和创建一个普通对象一样\n\n异常抛出流程\n\n如果一个方法内抛出异常，该异常对象会被抛给调用者方法中处理。如果异常没有在调用者方法中处理，它继续被抛给这个调用方法的上层方法。这个过程将一直继续下去，直到异常被处理。这一过程称为捕获(catch)异常。\n如果一个异常回到main()方法，并且main()也不处理，则程序运行终止。\n程序员通常只能处理Exception，而对Error无能为力。\n\n异常处理是通过try-catch-finally语句实现的。\ntry{\n//可能产生异常的代码\n}\ncatch( ExceptionName1 e ){\n//当产生ExceptionName1型异常时的处置措施\n}\ncatch( ExceptionName2 e ){\n//当产生ExceptionName2型异常时的处置措施\n}\n[ finally{\n.. //无论是否发生异常，都无条件执行的语句\n}]\n\ntry捕获异常的第一步是用try{…}语句块选定捕获异常的范围，将可能出现异常的代码放在try语句块中。catch (Exceptiontype e)在catch语句块中是对异常对象进行处理的代码。每个try语句块可以伴随一个或多个catch语句，用于处理可能产生的不同类型的异常对象。\n\n如果明确知道产生的是何种异常，可以用该异常类作为catch的参数；也可以用其父类作为catch的参数。\n\n捕获异常的有关信息：\ngetMessage() 获取异常信息，返回字符串printStackTrace()  获取异常类名和异常信息，以及异常出现在程序中的位置。返回值void。\nfinally\n\n捕获异常的最后一步是通过finally语句为异常处理提供一个统一的出口，使得在控制流转到程序的其它部分以前，能够对程序的状态作统一的管理。\n不论在try代码块中是否发生了异常事件，catch语句是否执行，catch语句是否有异常，catch语句中是否有return， finally块中的语句都会被执行。\nfinally语句和catch语句是任选的\n\n1.4 异常处理机制二：throws​        如果一个方法(中的语句执行时)可能生成某种异常，但是并不能确定如何处理这种异常，则此方法应显示地声明抛出异常，表明该方法将不对这些异常进行处理，而由该方法的调用者负责处理。在方法声明中用throws语句可以声明抛出异常的列表，throws后面的异常类型可以是方法中产生的异常类型，也可以是它的父类。\npublic void readFile(String file) throws FileNotFoundException {\n    //读文件的操作可能产生FileNotFoundException类型的异常\n    FilelnputStream fis = new FilelnputStream(file);\n}\n重写方法声明抛出异常的原则\n重写方法不能抛出比被重写方法范围更大的异常类型。在多态的情况下，对methodA()方法的调用-异常的捕获按父类声明的异常处理。\n1.5 手动抛出异常：throw​        Java异常类对象除在程序执行过程中出现异常时由系统自动生成并抛出，也可根据需要使用人工创建并抛出。首先要生成异常类对象，然后通过throw语句实现抛出操作(提交给Java运行环境)。\nIOException e = new IOException(); throw e;\n可以抛出的异常必须是Throwable或其子类的实例。下面的语句在编译时将会产生语法错误：\nthrow new String(\"want to throw\");\n1.6 用户自定义异常类\n一般地，用户自定义异常类都是RuntimeException的子类。\n自定义异常类通常需要编写几个重载的构造器。\n自定义异常需要提供serialVersionUID\n自定义的异常通过throw抛出。\n自定义异常最重要的是异常类的名字，当异常出现时，可以根据名字判断异常类型。\n\n2 多线程2.1 基本概念：程序、进程、线程程序(program)是为完成特定任务、用某种语言编写的一组指令的集合。即指一段静态的代码，静态对象。进程(process)是程序的一次执行过程，或是正在运行的一个程序。是一个动态的过程：有它自身的产生、存在和消亡的过程。——生命周期    如：运行中的QQ，运行中的MP3播放器    程序是静态的，进程是动态的    进程作为资源分配的单位，系统在运行时会为每个进程分配不同的内存区域线程(thread)，进程可进一步细化为线程，是一个程序内部的一条执行路径。    若一个进程同一时间并行执行多个线程，就是支持多线程的    线程作为调度和执行的单位，每个线程拥有独立的运行栈和程序计数器(pc)，线程切换的开销小    一个进程中的多个线程共享相同的内存单元/内存地址空间它们从同一堆中分配对象，可以访问相同的变量和对象。这就使得线程间通信更简便、高效。但多个线程操作共享的系统资源可能就会带来安全的隐患。\n并行与并发    并行：多个CPU同时执行多个任务。比如：多个人同时做不同的事。    并发：一个CPU(采用时间片)同时执行多个任务。比如：秒杀、多个人做同一件事。\n多线程程序的优点：\n\n提高应用程序的响应。对图形化界面更有意义，可增强用户体验。\n提高计算机系统CPU的利用率\n改善程序结构。将既长又复杂的进程分为多个线程，独立运行，利于理解和修改\n\n2.2 线程的创建和使用⭐⭐Thread类的特性    每个线程都是通过某个特定Thread对象的run()方法来完成操作的，经常把run()方法的主体称为线程体    通过该Thread对象的start()方法来启动这个线程，而非直接调用run()\nThread类构造器    Thread()：创建新的Thread对象    Thread(String threadname)：创建线程并指定线程实例名    Thread(Runnable target)：指定创建线程的目标对象，它实现了Runnable接口中的run方法    Thread(Runnable target, String name)：创建新的Thread对象\nAPI中创建线程的两种方式方式一：继承Thread类\n1)    定义子类继承Thread类。2)    子类中重写Thread类中的run方法。3)    创建Thread子类对象，即创建了线程对象。4)    调用线程对象start方法：启动线程，调用run方法。\n注意点：\n\n如果自己手动调用run()方法，那么就只是普通方法，没有启动多线程模式。\nrun()方法由JVM调用，什么时候调用，执行的过程控制都有操作系统的CPU调度决定。\n想要启动多线程，必须调用start方法。\n一个线程对象只能调用一次start()方法启动，如果重复调用了，则将抛出以上的异常“IllegalThreadStateException”。\n\n方式二：实现Runnable接口\n1)    定义子类，实现Runnable接口。2)    子类中重写Runnable接口中的run方法。3)    通过Thread类含参构造器创建线程对象。4)    将Runnable接口的子类对象作为实际参数传递给Thread类的构造器中。5)    调用Thread类的start方法：开启线程，调用Runnable子类接口的run方法。\n继承方式和实现方式的联系与区别\n区别    继承Thread：线程代码存放Thread子类run方法中。    实现Runnable：线程代码存在接口的子类的run方法。实现方式的好处    避免了单继承的局限性    多个线程可以共享同一个接口实现类的对象，非常适合多个相同线程来处理同一份资源。\nThread类的有关方法\nvoid start():  启动线程，并执行对象的run()方法\nrun():  线程在被调度时执行的操作\nString getName():  返回线程的名称\nvoid setName(String name):设置该线程名称\nstatic Thread currentThread(): 返回当前线程。在Thread子类中就是this，通常用于主线程和Runnable实类\nstatic  void  yield()：线程让步    暂停当前正在执行的线程，把执行机会让给优先级相同或更高的线程    若队列中没有同优先级的线程，忽略此方法\njoin() ：当某个程序执行流中调用其他线程的 join() 方法时，调用线程将被阻塞，直到 join() 方法加入的 join 线程执行完为止    低优先级的线程也可以获得执行\nstatic  void  sleep(long millis)：(指定时间:毫秒)    令当前活动线程在指定时间段内放弃对CPU控制,使其他线程有机会被执行,时间到后重排队。    抛出InterruptedException异常\nstop(): 强制线程生命期结束，不推荐使用\nboolean isAlive()：返回boolean，判断线程是否还活着\n\n线程的调度Java的调度方法    同优先级线程组成先进先出队列（先到先服务），使用时间片策略    对高优先级，使用优先调度的抢占式策略\n线程的优先级线程的优先级等级    MAX_PRIORITY：10    MIN _PRIORITY：1    NORM_PRIORITY：5涉及的方法    getPriority() ：返回线程优先值    setPriority(int newPriority) ：改变线程的优先级说明    线程创建时继承父线程的优先级    低优先级只是获得调度的概率低，并非一定是在高优先级线程之后才被调用\n线程的分类Java中的线程分为两类：一种是守护线程，一种是用户线程。\n\n它们在几乎每个方面都是相同的，唯一的区别是判断JVM何时离开。\n守护线程是用来服务用户线程的，通过在start()方法前调用thread.setDaemon(true)可以把一个用户线程变成一个守护线程。\nJava垃圾回收就是一个典型的守护线程。\n若JVM中都是守护线程，当前JVM将退出。\n形象理解：兔死狗烹，鸟尽弓藏\n\n2.3 线程的生命周期JDK中用Thread.State类定义了线程的几种状态\n新建： 当一个Thread类或其子类的对象被声明并创建时，新生的线程对象处于新建状态就绪：处于新建状态的线程被start()后，将进入线程队列等待CPU时间片，此时它已具备了运行的条件，只是没分配到CPU资源运行：当就绪的线程被调度并获得CPU资源时,便进入运行状态， run()方法定义了线程的操作和功能阻塞：在某种特殊情况下，被人为挂起或执行输入输出操作时，让出 CPU 并临时中止自己的执行，进入阻塞状态死亡：线程完成了它的全部工作或线程被提前强制性地中止或出现异常导致结束\n\n2.4 线程的同步问题的提出    多个线程执行的不确定性引起执行结果的不稳定    多个线程对账本的共享，会造成操作的不完整性，会破坏数据。\nSynchronized的使用方法对多条操作共享数据的语句，只能让一个线程都执行完，在执行过程中，其他线程不可以参与执行。\n\n同步代码块：\n\nsynchronized (对象){\n// 需要被同步的代码；\n}\n\nsynchronized还可以放在方法声明中，表示整个方法为同步方法。例如：\n\npublic synchronized void show (String name){ \n...\n}\n同步机制中的锁当资源被一个任务使用时，在其上加锁。第一个访问某项资源的任务必须锁定这项资源，使其他任务在其被解锁之前，就无法访问它了，而在其被解锁之时，另一个任务就可以锁定并使用它了。\nsynchronized的锁\n\n任意对象都可以作为同步锁。所有对象都自动含有单一的锁（监视器）。\n同步方法的锁：静态方法（类名.class）、非静态方法（this）\n同步代码块：自己指定，很多时候也是指定为this或类名.class\n必须确保使用同一个资源的多个线程共用一把锁，这个非常重要，否则就无法保证共享资源的安全\n一个线程类中的所有静态方法共用同一把锁（类名.class），所有非静态方法共用同一把锁（this），同步代码块（指定需谨慎）\n\n同步的范围1、如何找问题，即代码是否存在线程安全？（非常重要）\n（1）    明确哪些代码是多线程运行的代码（2）    明确多个线程是否有共享数据（3）    明确多线程运行代码中是否有多条语句操作共享数据\n2、如何解决呢？（非常重要）对多条操作共享数据的语句，只能让一个线程都执行完，在执行过程中，其他线程不可以参与执行。即所有操作共享数据的这些语句都要放在同步范围中3、切记：\n    范围太小：没锁住所有有安全问题的代码    范围太大：没发挥多线程的功能。\n释放锁的操作    当前线程的同步方法、同步代码块执行结束。    当前线程在同步代码块、同步方法中遇到break、return终止了该代码块、该方法的继续执行。    当前线程在同步代码块、同步方法中出现了未处理的Error或Exception，导致异常结束。    当前线程在同步代码块、同步方法中执行了线程对象的wait()方法，当前线程暂停，并释放锁。\n不会释放锁的操作\n线程执行同步代码块或同步方法时，程序调用Thread.sleep()、 Thread.yield()方法暂停当前线程的执行\n线程执行同步代码块时，其他线程调用了该线程的suspend()方法将该线程挂起，该线程不会释放锁（同步监视器）。    应尽量避免使用suspend()和resume()来控制线程\n\n线程的死锁问题死锁:不同的线程分别占用对方需要的同步资源不放弃，都在等待对方放弃自己需要的同步资源，就形成了线程的死锁,出现死锁后，不会出现异常，不会出现提示，只是所有的线程都处于阻塞状态，无法继续。解决方法    专门的算法、原则    尽量减少同步资源的定义    尽量避免嵌套同步\nLock(锁)    从JDK 5.0开始，Java提供了更强大的线程同步机制——通过显式定义同步锁对象来实现同步。同步锁使用Lock对象充当。    java.util.concurrent.locks.Lock接口是控制多个线程对共享资源进行访问的工具。锁提供了对共享资源的独占访问，每次只能有一个线程对Lock对象加锁，线程开始访问共享资源之前应先获得Lock对象。    ReentrantLock 类实现了 Lock ，它拥有与 synchronized 相同的并发性和内存语义，在实现线程安全的控制中，比较常用的是ReentrantLock，可以显式加锁、释放锁。\nclass A{\n    private final ReentrantLock lock = new ReenTrantLock();\n    public void m(){\n        lock.lock();\n        try{\n        \t11保证线程安全的代码;\n        }\n        finally{\n        \tlock.unlock();\n        }\n    }\n}\n//注意：如果同步代码有异常，要将unlock()写入finally语句块\nsynchronized 与 Lock 的对比\nLock是显式锁（手动开启和关闭锁，别忘记关闭锁），synchronized是隐式锁，出了作用域自动释放\nLock只有代码块锁，synchronized有代码块锁和方法锁\n使用Lock锁，JVM将花费较少的时间来调度线程，性能更好。并且具有更好的扩展性（提供更多的子类）\n\n2.5 线程的通信\nwait()：令当前线程挂起并放弃CPU、同步资源并等待，使别的线程可访问并修改共享资源，而当前线程排队等候其他线程调用notify()或notifyAll()方法唤醒，唤醒后等待重新获得对监视器的所有权后才能继续执行。\nnotify()：唤醒正在排队等待同步资源的线程中优先级最高者结束等待\nnotifyAll ()：唤醒正在排队等待资源的所有线程结束等待.\n\n    这三个方法只有在synchronized方法或synchronized代码块中才能使用，否则会报java.lang.IllegalMonitorStateException异常。因为这三个方法必须有锁对象调用，而任意对象都可以作为synchronized的同步锁，因此这三个方法只能在Object类中声明。\nwait() 方法    在当前线程中调用方法： 对象名.wait()    使当前线程进入等待（某对象）状态 ，直到另一线程对该对象发出 notify(或notifyAll) 为止。    调用方法的必要条件：当前线程必须具有对该对象的监控权（加锁）    调用此方法后，当前线程将释放对象监控权 ，然后进入等待    在当前线程被notify后，要重新获得监控权，然后从断点处继续代码的执行。\nnotify()/notifyAll()    在当前线程中调用方法： 对象名.notify()    功能：唤醒等待该对象监控权的一个/所有线程。    调用方法的必要条件：当前线程必须具有对该对象的监控权（加锁）\n2.6 JDK5.0新增线程创建方式新增方式一：实现Callable接口与使用Runnable相比， Callable功能更强大些    相比run()方法，可以有返回值    方法可以抛出异常    支持泛型的返回值    需要借助FutureTask类，比如获取返回结果\nFuture接口    可以对具体Runnable、Callable任务的执行结果进行取消、查询是否完成、获取结果等。    FutrueTask是Futrue接口的唯一的实现类    FutureTask 同时实现了Runnable, Future接口。它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值\n新增方式二：使用线程池背景：经常创建和销毁、使用量特别大的资源，比如并发情况下的线程，对性能影响很大。思路：提前创建好多个线程，放入线程池中，使用时直接获取，使用完放回池中。可以避免频繁创建销毁、实现重复利用。类似生活中的公共交通工具。好处：    提高响应速度（减少了创建新线程的时间）    降低资源消耗（重复利用线程池中线程，不需要每次都创建）    便于线程管理            corePoolSize：核心池的大小            maximumPoolSize：最大线程数            keepAliveTime：线程没有任务时最多保持多长时间后会终止\n线程池相关APIJDK 5.0起提供了线程池相关API：ExecutorService 和 ExecutorsExecutorService：真正的线程池接口。常见子类ThreadPoolExecutor    void execute(Runnable command) ：执行任务/命令，没有返回值，一般用来执行Runnable     Future submit(Callable task)：执行任务，有返回值，一般又来执行Callable    void shutdown() ：关闭连接池Executors：工具类、线程池的工厂类，用于创建并返回不同类型的线程池    Executors.newCachedThreadPool()：创建一个可根据需要创建新线程的线程池    Executors.newFixedThreadPool(n); 创建一个可重用固定线程数的线程池    Executors.newSingleThreadExecutor() ：创建一个只有一个线程的线程池    Executors.newScheduledThreadPool(n)：创建一个线程池，它可安排在给定延迟后运行命令或者定期地执行。\n","slug":"J2-异常与多线程","date":"2021-11-12T02:28:44.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"3c25c67d0781653798d002eeefd584b0","title":"环境与包管理","content":"1. Pip对 Python 包的查找、下载、安装、卸载的功能。\n1.1 pip常用命令pip的更新\npython -m pip install --upgrade pip\npip安装包\npip install 包名==版本号\npip更新包\npip install --upgrade 要更新的包名\npip删除包\npip uninstall 要卸载的包名\n1.2 pip镜像配置临时使用pip安装包的时候加参数-i 镜像源地址\npip install bs4 -i https://pypi.tuna.tsinghua.edu.cn/simple\n清华：https://pypi.tuna.tsinghua.edu.cn/simple阿里云：http://mirrors.aliyun.com/pypi/simple/中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/华中理工大学：http://pypi.hustunique.com/山东理工大学：http://pypi.sdutlinux.org/豆瓣：http://pypi.douban.com/simple/\n永久更改Linux：修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹)\n[global] \nindex-url = https://pypi.tuna.tsinghua.edu.cn/simple\n[install]\ntrusted-host=mirrors.aliyun.com\nwindows直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，然后新建文件pip.ini，即 %HOMEPATH%\\pip\\pip.ini，在pip.ini文件中输入以下内容（以豆瓣镜像为例）：\n[global]\nindex-url = http://pypi.douban.com/simple\n[install]\ntrusted-host = pypi.douban.com\n1.3 pip依赖迁移导出依赖\npip freeze &gt;  requirements.txt\n安装依赖\npip install -r requirements.txt\n2. Anacondaconda is a tool for managing and deploying applications, environments and packages.\n2.1 下载安装官网下载：Anaconda | The World’s Most Popular Data Science Platform\n下一步、下一步点点点，安装完成\n2.2 conda环境管理conda clean ：净化Anaconda\n# 删除索引缓存，锁定文件，未使用的缓存包，和包。\nconda clean -a\nconda compare：比较conda环境之间的包。\nconda config：配置环境的配置信息。\nconda create：创建新的conda环境。⭐⭐\n# 创建环境：conda create -n 环境名称 [python]\nconda create -n env_name python=3.8\n\n# 用 environment.yml 配置文件创建环境\nconda env create -f nvironment.yml\n\n# 导出environment.yml环境文件\nconda env_name export &gt; environment.yml \n\n\n\n\n\n\n\n\n\n坑爹：\n装新环境必须删除原镜像配置\nconda info：显示有关当前conda安装的信息。\nconda activate env_name：激活环境。⭐⭐\nconda deactivate env_name：停用环境。\nconda env remove —n env_name：删除环境\n构建相同的conda环境（不同机器间的环境复制）\n# 激活需要导出配置文件的环境\nconda list --explicit &gt; files.txt\n# 在同系统的不同机器执行\nconda create --name env_name -f files.txt\n克隆环境（同一台机器的环境复制\nconda create --name clone_env_name --clone env_name\nconda env list：查看环境列表\n2.3 conda包管理conda search [包名]：查看特定包\nconda install 包名[=版本号]：安装指定包\nconda update 包名：更新包\nconda update python：更新python\nconda list：查看当前环境的包\nconda remove 包名：删除包\n2.4 conda镜像修改conda config —remove-key channels恢复默认镜像\nconda config —add 镜像地址\n# 清华大学镜像\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge \nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/\nconda config —show：在channel里查看配置的镜像地址\n3. Jupyter notebook3.1 设置代码提示conda activate base\n\npip install jupyter_contrib_nbextensions\n\njupyter contrib nbextension install --user\n\npip install jupyter_nbextensions_configurator\n\njupyter nbextensions_configurator enable --user\n\n\n3.2 jupyter中添加conda环境查看JupyterNotebook的kernel及存放位置\nipython kernelspec list\n添加kernel\n# 首先安装ipykernel：\nconda install ipykernel\n\n# 在虚拟环境下创建kernel文件：\nconda install -n 环境名称 ipykernel\n\n# 激活conda环境，将环境写入notebook的kernel中\n\npython -m ipykernel install --user --name 环境名称 --display-name \"Python (环境名称)\"\n\n# 打开notebook服务器：jupyter notebook\n\n# 删除kernel环境：\n\njupyter kernelspec remove 环境名称\n","slug":"P3-Anaconda与jupyter notebook","date":"2021-11-11T10:25:42.000Z","categories_index":"Python","tags_index":"","author_index":"YFR718"},{"id":"4a207b7e885fdbcca8129599135e5f2e","title":"MySQL基础","content":"1 MySQL概述1.1 数据库概述数据库的好处持久化数据到本地可以实现结构化查询，方便管理\n数据库相关概念DB：数据库，保存一组有组织的数据的容器DBMS：数据库管理系统，又称为数据库软件（产品），用于管理DB中的数据SQL:结构化查询语言，用于和DBMS通信的语言\n数据库存储数据的特点\n将数据放到表中，表再放到库中\n一个数据库中可以有多个表，每个表都有一个的名字，用来标识自己。表名具有唯一性。\n表具有一些特性，这些特性定义了数据在表中如何存储，类似java中 “类”的设计。\n表由列组成，我们也称为字段。所有表都是由一个或多个列组成的，每一列类似java 中的”属性”\n表中的数据是按行存储的，每一行类似于java中的“对象”。\n\n1.2 MySQL的背景前身属于瑞典的一家公司，MySQL AB\n08年被sun公司收购\n09年sun被oracle收购\nMySQL优点\n1、开源、免费、成本低\n2、性能高、移植性也好\n3、体积小，便于安装\n1.3 MySQL产品的介绍和安装MySQL服务的启动和停止方式一：计算机——右击管理——服务方式二：通过管理员身份运行\nnet start 服务名（启动服务）\nnet stop 服务名（停止服务）\nMySQL服务的登录和退出  方式一：通过mysql自带的客户端（只限于root用户）\n  方式二：通过windows自带的客户端\n#登录：\nmysql 【-h主机名 -P端口号 】-u用户名 -p密码\n#退出：\nexit或ctrl+C\n1.4SQL语言的规则与规范基本规则\nSQL 可以写在一行或者多行。为了提高可读性，各子句分行写，必要时使用缩进\n每条命令以 ; 或 \\g 或 \\G 结束\n关键字不能被缩写也不能分行\n关于标点符号\n必须保证所有的()、单引号、双引号是成对结束的\n必须使用英文状态下的半角输入方式\n字符串型和日期时间类型的数据可以使用单引号（’ ‘）表示\n列的别名，尽量使用双引号（” “），而且不建议省略as\n\n\n\nSQL大小写规范 （建议遵守）\nMySQL 在 Windows 环境下是大小写不敏感的\nMySQL 在 Linux 环境下是大小写敏感的\n数据库名、表名、表的别名、变量名是严格区分大小写的\n关键字、函数名、列名(或字段名)、列的别名(字段的别名) 是忽略大小写的。\n\n\n推荐采用统一的书写规范：\n数据库名、表名、表别名、字段名、字段别名等都小写\nSQL 关键字、函数名、绑定变量等都大写\n\n\n\n注 释可以使用如下格式的注释结构\n单行注释：#注释文字(MySQL特有的方式)\n单行注释：-- 注释文字(--后面必须包含一个空格。)\n多行注释：/* 注释文字  */\n命名规则（暂时了解）\n数据库、表名不得超过30个字符，变量名限制为29个\n必须只能包含 A–Z, a–z, 0–9, _共63个字符\n数据库名、表名、字段名等对象名中间不要包含空格\n同一个MySQL软件中，数据库不能同名；同一个库中，表不能重名；同一个表中，字段不能重名\n必须保证你的字段没有和保留字、数据库系统或常用方法冲突。如果坚持使用，请在SQL语句中使用`（着重号）引起来\n保持字段名和类型的一致性，在命名字段并为其指定数据类型的时候一定要保证一致性。假如数据类型在一个表里是整数，那在另一个表里可就别变成字符型了\n\n举例：\n#以下两句是一样的，不区分大小写\nshow databases;\nSHOW DATABASES;\n\n#创建表格\n#create table student info(...); #表名错误，因为表名有空格\ncreate table student_info(...); \n\n#其中order使用``飘号，因为order和系统关键字或系统函数名等预定义标识符重名了\nCREATE TABLE `order`(\n    id INT,\n    lname VARCHAR(20)\n);\n\nselect id as \"编号\", `name` as \"姓名\" from t_stu; #起别名时，as都可以省略\nselect id as 编号, `name` as 姓名 from t_stu; #如果字段别名中没有空格，那么可以省略\"\"\nselect id as 编 号, `name` as 姓 名 from t_stu; #错误，如果字段别名中有空格，那么不能省略\"\"\n2. 基本操作存储过程：创建数据库=&gt;确认字段=&gt;创建数据表=&gt;插入数据\n创建数据库# 展示数据库\nmysql&gt; SHOW DATABASES;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| sakila             |\n| sys                |\n| world              |\n+--------------------+\n# 打开指定的库\nmysql&gt; use 库名\n# 创建数据库\nmysql&gt; CREATE DATABASE demo;\n# 删除数据库\nmysql&gt; DROP DATABASE demo;\n# 查看服务器的版本\n#方式一：登录到mysql服务端\nmysql&gt; select version();\n#方式二：没有登录到mysql服务端\nmysql --version\n或\nmysql --V\n●”information_schema”是MySQL系统自带的数据库,主要保存MySQL数据库服务器的系统信息，比如数据库的名称、数据表的名称、字段名称、存取权限、数据文件所在的文件夹和系统使用的文件夹，等等。●”performance_ schema”是MySQL系统自带的数据库，可以用来监控MySQL的各类性能指标。●“sys” 数据库是MySQL系统自带的数据库，主要作用是，以一种更容易被理解的方式展示MySQL数据库服务器的各类性能指标，帮助系统管理员和开发人员监控MySQL的技术性能。●”mysql”数据库保存了MySQL数据库服务器运行时需要的系统信息，比如数据文件夹、当前使用的字符集、约束检查信息，等等。\n确认字段MySQL数据表由行与列组成，一行就是一条数据记录， 每一条数据记录都被分成许多列，一列就叫一个字段。每个字段都需要定义数据类型，这个数据类型叫做字段类型。\n创建数据表#展示数据表\nmysql&gt; SHOW TABLES;\n+----------------+\n| Tables_in_demo |\n+----------------+\n| test           |\n+----------------+\n# 查看其它库的所有表\nmysql&gt; show tables from 库名;\n#创建数据表\nmysql&gt; CREATE TABLE demo.test(barcode text,goodname text,price int);\n# 查看表结构\nmysql&gt; desc 表名;\nmysql&gt; show columns from actor;\nmysql&gt; DESCRIBE demo.test;\n+----------+---------+------+-----+---------+-------+\n| Field    | Type    | Null | Key | Default | Extra |\n+----------+---------+------+-----+---------+-------+\n| barcode  | text    | YES  |     | NULL    |       |\n| goodname | text    | YES  |     | NULL    |       |\n| price    | int(11) | YES  |     | NULL    |       |\n+----------+---------+------+-----+---------+-------+\n●Field: 表示字段名称。●Type: 表示字段类型。●Null: 表示这个字段是否允许是空值(NULL) 。在MySQL里面,空值不等于空字符串。一个空字符串的长度是0,而一个空值的长度是空。而且，在MySQL里面，空值是占用空间的。●Key:我们暂时把它叫做键。●Default: 表示默认值。我们导入的表的所有的字段都允许是空，默认值都是NULL。●Extra:表示附加信息。\n#其他show语句\n❑ SHOW STATUS，用于显示广泛的服务器状态信息；\n❑ SHOW CREATE DATABASE和SHOW CREATE TABLE，分别用来显示创建特定数据库或表的MySQL语句；\n❑ SHOW GRANTS，用来显示授予用户（所有用户或特定用户）的安全权限；\n❑ SHOW ERRORS 和 SHOW WARNINGS，用来显示服务器错误或警告消息。\n插入数据#添加主键\nALTER TABLE demo. test\nADD COLUMN itemnumber int PRIMARY KEY AUTO_INCREMENT ;\n#向表中添加数据\nINSERT INTO demo.test\n(barcode, goodsname,price)\nVALUES ('0001','本' ,3);\n\nSQL的语言分类SQL语言在功能上主要分为如下3大类：\n\nDDL（Data Definition Languages、数据定义语言），这些语句定义了不同的数据库、表、视图、索引等数据库对象，还可以用来创建、删除、修改数据库和数据表的结构。\n\n主要的语句关键字包括CREATE、DROP、ALTER等。\n\n\nDML（Data Manipulation Language、数据操作语言），用于添加、删除、更新和查询数据库记录，并检查数据完整性。\n\n主要的语句关键字包括INSERT、DELETE、UPDATE、SELECT等。\nSELECT是SQL语言的基础，最为重要。\n\n\nDCL（Data Control Language、数据控制语言），用于定义数据库、表、字段、用户的访问权限和安全级别。\n\n主要的语句关键字包括GRANT、REVOKE、COMMIT、ROLLBACK、SAVEPOINT等。\n\n\n\n\n\n\n\n\n\n\n\n\n因为查询语句使用的非常的频繁，所以很多人把查询语句单拎出来一类：DQL（数据查询语言）。\n还有单独将COMMIT、ROLLBACK 取出来称为TCL （Transaction Control Language，事务控制语言）。\n3. DQL语言数据查询语言：select \n/*\n类似于Java中 :System.out.println(要打印的东西);\n特点：\n①通过select查询完的结果 ，是一个虚拟的表格，不是真实存在\n② 要查询的东西 可以是常量值、可以是表达式、可以是字段、可以是函数*/\n\n#检索单个列\nmysql&gt; SELECT 列名 FROM 表名;\n#检索多个列\nmysql&gt; SELECT 列名,列名... FROM 表名;\n#检索所有列\nmysql&gt; SELECT * FROM 表名;\n#使用完全限定的名字\nmysql&gt; SELECT 表名.列名 FROM 数据库.表名;\n#拼接计算字段\nmysql&gt; SELECT CONCAT(列名，列名) FROM 表名;\n#计算字段运算\nmysql&gt; SELECT 列名*5 FROM 表名;\n\n\n\n\n子句\n功能\n示例\n\n\n\n\nDISTINCT\n去重\nSELECT DISTINCT 列名 FROM 表名;\n\n\nLIMIT\n限制行数\nSELECT 列名 FROM 表名 LIMIT 10;SELECT 列名 FROM 表名 LIMIT 10,3; #从行10开始取3行\n\n\nORDER BY\n排序\nSELECT 列名 FROM 表名 ORDER BY 列名 LIMIT 10;   //先排序再取10SELECT 列名 FROM 表名 ORDER BY 列名,列名..;     //按多个列排序\n\n\nDESC\n降序排\nSELECT 列名 FROM 表名 ORDER BY 列名 DESC;\n\n\nASC\n升序排\n与不加相同\n\n\nWHERE\n数据过滤\nSELECT 列名 FROM 表名 WHERE 条件;\n\n\nGROUP BY\n分组数据\nSELECT 列名 FROM 表名 GROUP BY 列名;\n\n\nHAVING\n过滤基于分组值\nSELECT 列名 FROM 表名 GROUP BY 列名 HAVING COUNT(*)&gt;=2;;\n\n\nUNION (ALL)\n组合查询\n执行多条SELECT语句，并将结果作为单个查询结果集返回\n\n\n\n\nSELECT子句顺序\n\n\n\n子句\n说明\n是否必须使用\n\n\n\n\nSELECT\n要返回的列\n是\n\n\nFROM\n要检索的表\n仅在表中选择数据\n\n\nWHERE\n行过滤\n否\n\n\nGROUP BY\n分组说明\n仅在按组计算聚集时\n\n\nHAVING\n组过滤\n否\n\n\nORDER BY\n排序\n否\n\n\nLIMIT\n要检索的行数\n否\n\n\n\n\nSELECT 语句的执行顺序**（在 MySQL 和 Oracle 中，SELECT 执行顺序基本相同）：\nFROM -&gt; WHERE -&gt; GROUP BY -&gt; HAVING -&gt; SELECT 的字段 -&gt; DISTINCT -&gt; ORDER BY -&gt; LIMIT\n比如你写了一个 SQL 语句，那么它的关键字顺序和执行顺序是下面这样的：\nSELECT DISTINCT player_id, player_name, count(*) as num # 顺序 5\nFROM player JOIN team ON player.team_id = team.team_id # 顺序 1\nWHERE height &gt; 1.80 # 顺序 2\nGROUP BY player.team_id # 顺序 3\nHAVING num &gt; 2 # 顺序 4\nORDER BY num DESC # 顺序 6\nLIMIT 2 # 顺序 7\n在 SELECT 语句执行这些步骤的时候，每个步骤都会产生一个虚拟表，然后将这个虚拟表传入下一个步骤中作为输入。需要注意的是，这些步骤隐含在 SQL 的执行过程中，对于我们来说是不可见的。\nSQL 的执行原理SELECT 是先执行 FROM 这一步的。在这个阶段，如果是多张表联查，还会经历下面的几个步骤：\n\n首先先通过 CROSS JOIN 求笛卡尔积，相当于得到虚拟表 vt（virtual table）1-1；\n通过 ON 进行筛选，在虚拟表 vt1-1 的基础上进行筛选，得到虚拟表 vt1-2；\n添加外部行。如果我们使用的是左连接、右链接或者全连接，就会涉及到外部行，也就是在虚拟表 vt1-2 的基础上增加外部行，得到虚拟表 vt1-3。\n\n当然如果我们操作的是两张以上的表，还会重复上面的步骤，直到所有表都被处理完为止。这个过程得到是我们的原始数据。\n当我们拿到了查询数据表的原始数据，也就是最终的虚拟表 vt1，就可以在此基础上再进行 WHERE 阶段。在这个阶段中，会根据 vt1 表的结果进行筛选过滤，得到虚拟表 vt2。\n然后进入第三步和第四步，也就是 GROUP 和 HAVING 阶段。在这个阶段中，实际上是在虚拟表 vt2 的基础上进行分组和分组过滤，得到中间的虚拟表 vt3 和 vt4。\n当我们完成了条件筛选部分之后，就可以筛选表中提取的字段，也就是进入到 SELECT 和 DISTINCT 阶段。\n首先在 SELECT 阶段会提取想要的字段，然后在 DISTINCT 阶段过滤掉重复的行，分别得到中间的虚拟表 vt5-1 和 vt5-2。\n当我们提取了想要的字段数据之后，就可以按照指定的字段进行排序，也就是 ORDER BY 阶段，得到虚拟表 vt6。\n最后在 vt6 的基础上，取出指定行的记录，也就是 LIMIT 阶段，得到最终的结果，对应的是虚拟表 vt7。\n当然我们在写 SELECT 语句的时候，不一定存在所有的关键字，相应的阶段就会省略。\n同时因为 SQL 是一门类似英语的结构化查询语言，所以我们在写 SELECT 语句的时候，还要注意相应的关键字顺序，所谓底层运行的原理，就是我们刚才讲到的执行顺序。\nWHERE子句操作符\n\n\n\n操作符\n说明\n\n\n\n\n=\n等于\n\n\n&lt;&gt;或!=\n不等于\n\n\n&lt;,&lt;=,&gt;,&gt;=\n小于大于\n\n\nNOT、！\n与\n\n\nAND、&amp;&amp;\n或\n\n\nOR、\\\n\\\n\n非\n\n\nXOR\n异或\n\n\n&amp;、\\\n、^、~、&gt;&gt;、&lt;&lt;\n位运算\n\n\n\n\n优先级：AND&gt;OR\n安全等于运算符（&lt;=&gt;）与等于运算符（=）的作用是相似的，唯一的区别是‘&lt;=&gt;’可以用来对NULL进行判断。在两个操作数均为NULL时，其返回值为1，而不为NULL；当一个操作数为NULL时，其返回值为0，而不为NULL。\nmysql&gt; SELECT 1 &lt;=&gt; '1', 1 &lt;=&gt; 0, 'a' &lt;=&gt; 'a', (5 + 3) &lt;=&gt; (2 + 6), '' &lt;=&gt; NULL,NULL &lt;=&gt; NULL FROM dual;\n+-----------+---------+-------------+---------------------+-------------+---------------+\n| 1 &lt;=&gt; '1' | 1 &lt;=&gt; 0 | 'a' &lt;=&gt; 'a' | (5 + 3) &lt;=&gt; (2 + 6) | '' &lt;=&gt; NULL | NULL &lt;=&gt; NULL |\n+-----------+---------+-------------+---------------------+-------------+---------------+\n|         1 |       0 |           1 |                   1 |           0 |             1 |\n+-----------+---------+-------------+---------------------+-------------+---------------+\n1 row in set (0.00 sec)\n#查询commission_pct等于0.40\nSELECT employee_id,commission_pct FROM employees WHERE commission_pct = 0.40;\n\nSELECT employee_id,commission_pct FROM employees WHERE commission_pct &lt;=&gt; 0.40;\n\n#如果把0.40改成 NULL 呢？\n\n\n\n\n非符号类型的运算符\n名称\n作用\n\n\n\n\nIS NULL\n为空运算符\n判断值、字符串或表达式是否为空\n\n\nIS NOTNULL\n不为空运算符\n判断值、字符串或表达式是否不为空\n\n\nLEAST\n最小值运算符\n在多个值中返回最小值\n\n\nGREATEST\n最大值运算符\n在多个值中返回最大值\n\n\nBETWEEN AND\n两值之间的运算符\n判断一个值是否在两个值之间\n\n\nISNULL\n为空运算符\n判断一个值、字符串或表达式是否为空\n\n\nIN\n属于运算符\n判断一个值是否为列表中的任意一个值\n\n\nNOT IN\n不属于运算符\n判断一个值是否不是一个列表中的任意一个值\n\n\nLIKE\n模糊匹配运算符\n判断一个值是否符合模糊匹配规则\n\n\nREGEXP\n正则表达式运算符\n判断一个值是否符合正则表达式的规则\n\n\nRLIKE\n正则表达式运算符\n判断一个值是否符合正则表达式的规则\n\n\n\n\n通配符LIKE用法\n\n\n\n通配符\n含义\n示例\n\n\n\n\n%\n任何字符出现任意次数\nWHERE 列名 LIKE ‘yfr%’：\n\n\n_\n只匹配单个字符\nWHERE 列名 LIKE ‘yfr_’：\n\n\n\n\n正则表达式\n\n\n\n关键字\n含义\n示例\n\n\n\n\nREGEXP\n包含\nwhere 列名 REGEXP ‘400’;\n\n\nBINARY\n不区分大小匹配\nwhere 列名 REGEXP BINARY  ‘AS’;\n\n\n\\\n\nOR匹配\nwhere 列名 REGEXP ‘400\\\n500’;\n\n\n[123]\n匹配几个字符之一\nwhere 列名 REGEXP ‘[123]400’;\n\n\n123\n匹配几个字符之外所有\nwhere 列名 REGEXP ‘[\\^123]400’;\n\n\n[0-9]\n范围匹配\nwhere 列名 REGEXP ‘[0-9]400’;\n\n\n\\\\\n匹配特殊字符．、[]、\n和-where 列名 REGEXP ‘\\400’;\n\n\n字符类\n匹配字符类\n[ :alpha: ] [:blank:]等等\n\n\n{n,m}\n匹配数目范围\nwhere 列名 REGEXP ‘[0-9]{1,}’;匹配一次以上\n\n\n[[:&lt;:]]、[[:&gt;:]]、$、^\n定位符：词的开始/尾，文本的开始/尾\nwhere 列名 REGEXP ‘^[0-9]’;\n\n\n\n\n单行函数基本函数\n\n\n\n函数\n用法\n\n\n\n\nABS(x)\n返回x的绝对值\n\n\nSIGN(X)\n返回X的符号。正数返回1，负数返回-1，0返回0\n\n\nPI()\n返回圆周率的值\n\n\nCEIL(x)，CEILING(x)\n返回大于或等于某个值的最小整数\n\n\nFLOOR(x)\n返回小于或等于某个值的最大整数\n\n\nLEAST(e1,e2,e3…)\n返回列表中的最小值\n\n\nGREATEST(e1,e2,e3…)\n返回列表中的最大值\n\n\nMOD(x,y)\n返回X除以Y后的余数\n\n\nRAND()\n返回0~1的随机值\n\n\nRAND(x)\n返回0~1的随机值，其中x的值用作种子值，相同的X值会产生相同的随机数\n\n\nROUND(x)\n返回一个对x的值进行四舍五入后，最接近于X的整数\n\n\nROUND(x,y)\n返回一个对x的值进行四舍五入后最接近X的值，并保留到小数点后面Y位\n\n\nTRUNCATE(x,y)\n返回数字x截断为y位小数的结果\n\n\nSQRT(x)\n返回x的平方根。当X的值为负数时，返回NULL\n\n\n\n\n字符串函数\n\n\n\n函数\n用法\n\n\n\n\nASCII(S)\n返回字符串S中的第一个字符的ASCII码值\n\n\nCHAR_LENGTH(s)\n返回字符串s的字符数。作用与CHARACTER_LENGTH(s)相同\n\n\nLENGTH(s)\n返回字符串s的字节数，和字符集有关\n\n\nCONCAT(s1,s2,……,sn)\n连接s1,s2,……,sn为一个字符串\n\n\nCONCAT_WS(x, s1,s2,……,sn)\n同CONCAT(s1,s2,…)函数，但是每个字符串之间要加上x\n\n\nINSERT(str, idx, len, replacestr)\n将字符串str从第idx位置开始，len个字符长的子串替换为字符串replacestr\n\n\nREPLACE(str, a, b)\n用字符串b替换字符串str中所有出现的字符串a\n\n\nUPPER(s) 或 UCASE(s)\n将字符串s的所有字母转成大写字母\n\n\nLOWER(s)  或LCASE(s)\n将字符串s的所有字母转成小写字母\n\n\nLEFT(str,n)\n返回字符串str最左边的n个字符\n\n\nRIGHT(str,n)\n返回字符串str最右边的n个字符\n\n\nLPAD(str, len, pad)\n用字符串pad对str最左边进行填充，直到str的长度为len个字符\n\n\nRPAD(str ,len, pad)\n用字符串pad对str最右边进行填充，直到str的长度为len个字符\n\n\nLTRIM(s)\n去掉字符串s左侧的空格\n\n\nRTRIM(s)\n去掉字符串s右侧的空格\n\n\nTRIM(s)\n去掉字符串s开始与结尾的空格\n\n\nTRIM(s1 FROM s)\n去掉字符串s开始与结尾的s1\n\n\nTRIM(LEADING s1 FROM s)\n去掉字符串s开始处的s1\n\n\nTRIM(TRAILING s1 FROM s)\n去掉字符串s结尾处的s1\n\n\nREPEAT(str, n)\n返回str重复n次的结果\n\n\nSPACE(n)\n返回n个空格\n\n\nSTRCMP(s1,s2)\n比较字符串s1,s2的ASCII码值的大小\n\n\nSUBSTR(s,index,len)\n返回从字符串s的index位置其len个字符，作用与SUBSTRING(s,n,len)、MID(s,n,len)相同\n\n\nLOCATE(substr,str)\n返回字符串substr在字符串str中首次出现的位置，作用于POSITION(substr IN str)、INSTR(str,substr)相同。未找到，返回0\n\n\nELT(m,s1,s2,…,sn)\n返回指定位置的字符串，如果m=1，则返回s1，如果m=2，则返回s2，如果m=n，则返回sn\n\n\nFIELD(s,s1,s2,…,sn)\n返回字符串s在字符串列表中第一次出现的位置\n\n\nFIND_IN_SET(s1,s2)\n返回字符串s1在字符串s2中出现的位置。其中，字符串s2是一个以逗号分隔的字符串\n\n\nREVERSE(s)\n返回s反转后的字符串\n\n\nNULLIF(value1,value2)\n比较两个字符串，如果value1与value2相等，则返回NULL，否则返回value1\n\n\n\n\n\n\n\n\n\n\n\n\n\n注意：MySQL中，字符串的位置是从1开始的。\n日期时间处理函数获取日期、时间\n\n\n\n函数\n用法\n\n\n\n\nCURDATE() ，CURRENT_DATE()\n返回当前日期，只包含年、月、日\n\n\nCURTIME() ， CURRENT_TIME()\n返回当前时间，只包含时、分、秒\n\n\nNOW() / SYSDATE() / CURRENT_TIMESTAMP() / LOCALTIME() / LOCALTIMESTAMP()\n返回当前系统日期和时间\n\n\nUTC_DATE()\n返回UTC（世界标准时间）日期\n\n\nUTC_TIME()\n返回UTC（世界标准时间）时间\n\n\n\n\n日期与时间戳的转换\n\n\n\n函数\n用法\n\n\n\n\nUNIX_TIMESTAMP()\n以UNIX时间戳的形式返回当前时间。SELECT UNIX_TIMESTAMP() -&gt;1634348884\n\n\nUNIX_TIMESTAMP(date)\n将时间date以UNIX时间戳的形式返回。\n\n\nFROM_UNIXTIME(timestamp)\n将UNIX时间戳的时间转换为普通格式的时间\n\n\n\n\n获取月份、星期、星期数、天数等函数\n\n\n\n函数\n用法\n\n\n\n\nYEAR(date) / MONTH(date) / DAY(date)\n返回具体的日期值\n\n\nHOUR(time) / MINUTE(time) / SECOND(time)\n返回具体的时间值\n\n\nMONTHNAME(date)\n返回月份：January，…\n\n\nDAYNAME(date)\n返回星期几：MONDAY，TUESDAY…..SUNDAY\n\n\nWEEKDAY(date)\n返回周几，注意，周1是0，周2是1，。。。周日是6\n\n\nQUARTER(date)\n返回日期对应的季度，范围为1～4\n\n\nWEEK(date) ， WEEKOFYEAR(date)\n返回一年中的第几周\n\n\nDAYOFYEAR(date)\n返回日期是一年中的第几天\n\n\nDAYOFMONTH(date)\n返回日期位于所在月份的第几天\n\n\nDAYOFWEEK(date)\n返回周几，注意：周日是1，周一是2，。。。周六是7\n\n\n\n\n日期的操作函数\n\n\n\n函数\n用法\n\n\n\n\nEXTRACT(type FROM date)\n返回指定日期中特定的部分，type指定返回的值\n\n\n\n\n\n\n\n\ntype取值\n含义\n\n\n\n\nMICROS ECONDtype取值\n返回毫秒数\n\n\nSECONDMICROS ECOND\n返回秒数\n\n\nMINUTE .SECOND\n返回分钟数\n\n\nHOURMINUTE\n返回小时数\n\n\nDAYHOUR\n返回天数.\n\n\nWEEKDAY\n返回日期在一年中的第几个星期\n\n\nMONTHWEEK\n返回日期在一年中的第几个月\n\n\nQUARTERMONTH\n返回日期在一年中的第几个季度\n\n\nYEARQUARTER\n返回日期的年份\n\n\nSECOND MICROSECONDYEAR\n返回秒和毫秒值\n\n\nMINUTE MICROSECONDSECOND MICROSECOND\n返回分钟和毫秒值\n\n\nMINUTE SECONDMINUTE MICROSECOND\n返回分钟和秒值\n\n\nHOUR MICROSECONDMINUTE SECOND\n返回小时和毫秒值\n\n\nHOUR SECONDHOUR MICROSECOND\n返回小时和秒值\n\n\nHOUR_ MINUTEHOUR SECOND\n返回小时和分钟值\n\n\nDAY MICROSECONDHOUR_ MINUTE\n返回天和毫秒值\n\n\nDAY_ SECONDDAY_ MICROSECOND\n返回天和秒值\n\n\nDAY_ MINUTEDAY_ SECOND\n返回天和分钟值\n\n\nDAY_ HOURDAY_ MINUTE\n返回天和小时\n\n\nYEAR MONTHDAY_ HOUR\n返回年和月\n\n\n\n\nSELECT EXTRACT(MINUTE FROM NOW()),EXTRACT( WEEK FROM NOW()),\nEXTRACT( QUARTER FROM NOW()),EXTRACT( MINUTE_SECOND FROM NOW())\nFROM DUAL;\n时间和秒钟转换的函数\n\n\n\n函数\n用法\n\n\n\n\nTIME_TO_SEC(time)\n将 time 转化为秒并返回结果值。转化的公式为：小时*3600+分钟*60+秒\n\n\nSEC_TO_TIME(seconds)\n将 seconds 描述转化为包含小时、分钟和秒的时间\n\n\n\n\n计算日期和时间的函数第1组：\n\n\n\n\n函数\n用法\n\n\n\n\nDATE_ADD(datetime, INTERVAL  expr type)，ADDDATE(date,INTERVAL expr type)\n返回与给定日期时间相差INTERVAL时间段的日期时间\n\n\nDATE_SUB(date,INTERVAL expr type)，SUBDATE(date,INTERVAL expr type)\n返回与date相差INTERVAL时间间隔的日期\n\n\n\n\n第2组：\n\n\n\n\n函数\n用法\n\n\n\n\nADDTIME(time1,time2)\n返回time1加上time2的时间。当time2为一个数字时，代表的是秒，可以为负数\n\n\nSUBTIME(time1,time2)\n返回time1减去time2后的时间。当time2为一个数字时，代表的是秒，可以为负数\n\n\nDATEDIFF(date1,date2)\n返回date1 - date2的日期间隔天数\n\n\nTIMEDIFF(time1, time2)\n返回time1 - time2的时间间隔\n\n\nFROM_DAYS(N)\n返回从0000年1月1日起，N天以后的日期\n\n\nTO_DAYS(date)\n返回日期date距离0000年1月1日的天数\n\n\nLAST_DAY(date)\n返回date所在月份的最后一天的日期\n\n\nMAKEDATE(year,n)\n针对给定年份与所在年份中的天数返回一个日期\n\n\nMAKETIME(hour,minute,second)\n将给定的小时、分钟和秒组合成时间并返回\n\n\nPERIOD_ADD(time,n)\n返回time加上n后的时间\n\n\n\n\n日期的格式化与解析\n\n\n\n函数\n用法\n\n\n\n\nDATE_FORMAT(date,fmt)\n按照字符串fmt格式化日期date值\n\n\nTIME_FORMAT(time,fmt)\n按照字符串fmt格式化时间time值\n\n\nGET_FORMAT(date_type,format_type)\n返回日期字符串的显示格式\n\n\nSTR_TO_DATE(str, fmt)\n按照字符串fmt对str进行解析，解析为一个日期\n\n\n\n\n上述非GET_FORMAT函数中fmt参数常用的格式符：\n\n\n\n\n格式符\n说明\n格式符\n说明\n\n\n\n\n%Y\n4位数字表示年份\n%y\n表示两位数字表示年份\n\n\n%M\n月名表示月份（January,….）\n%m\n两位数字表示月份（01,02,03。。。）\n\n\n%b\n缩写的月名（Jan.，Feb.，….）\n%c\n数字表示月份（1,2,3,…）\n\n\n%D\n英文后缀表示月中的天数（1st,2nd,3rd,…）\n%d\n两位数字表示月中的天数(01,02…)\n\n\n%e\n数字形式表示月中的天数（1,2,3,4,5…..）\n\n\n\n\n%H\n两位数字表示小数，24小时制（01,02..）\n%h和%I\n两位数字表示小时，12小时制（01,02..）\n\n\n%k\n数字形式的小时，24小时制(1,2,3)\n%l\n数字形式表示小时，12小时制（1,2,3,4….）\n\n\n%i\n两位数字表示分钟（00,01,02）\n%S和%s\n两位数字表示秒(00,01,02…)\n\n\n%W\n一周中的星期名称（Sunday…）\n%a\n一周中的星期缩写（Sun.，Mon.,Tues.，..）\n\n\n%w\n以数字表示周中的天数(0=Sunday,1=Monday….)\n\n\n\n\n%j\n以3位数字表示年中的天数(001,002…)\n%U\n以数字表示年中的第几周，（1,2,3。。）其中Sunday为周中第一天\n\n\n%u\n以数字表示年中的第几周，（1,2,3。。）其中Monday为周中第一天\n\n\n\n\n%T\n24小时制\n%r\n12小时制\n\n\n%p\nAM或PM\n%%\n表示%\n\n\n\n\n三角函数\n\n\n\n函数\n用法\n\n\n\n\nSIN(x)\n返回x的正弦值，其中，参数x为弧度值\n\n\nASIN(x)\n返回x的反正弦值，即获取正弦为x的值。如果x的值不在-1到1之间，则返回NULL\n\n\nCOS(x)\n返回x的余弦值，其中，参数x为弧度值\n\n\nACOS(x)\n返回x的反余弦值，即获取余弦为x的值。如果x的值不在-1到1之间，则返回NULL\n\n\nTAN(x)\n返回x的正切值，其中，参数x为弧度值\n\n\nATAN(x)\n返回x的反正切值，即返回正切值为x的值\n\n\nATAN2(m,n)\n返回两个参数的反正切值\n\n\nCOT(x)\n返回x的余切值，其中，X为弧度值\n\n\n\n\n指数与对数\n\n\n\n函数\n用法\n\n\n\n\nPOW(x,y)，POWER(X,Y)\n返回x的y次方\n\n\nEXP(X)\n返回e的X次方，其中e是一个常数，2.718281828459045\n\n\nLN(X)，LOG(X)\n返回以e为底的X的对数，当X &lt;= 0 时，返回的结果为NULL\n\n\nLOG10(X)\n返回以10为底的X的对数，当X &lt;= 0 时，返回的结果为NULL\n\n\nLOG2(X)\n返回以2为底的X的对数，当X &lt;= 0 时，返回NULL\n\n\n\n\n进制间的转换\n\n\n\n函数\n用法\n\n\n\n\nBIN(x)\n返回x的二进制编码\n\n\nHEX(x)\n返回x的十六进制编码\n\n\nOCT(x)\n返回x的八进制编码\n\n\nCONV(x,f1,f2)\n返回f1进制数变成f2进制数\n\n\n\n\n流程控制函数流程处理函数可以根据不同的条件，执行不同的处理流程，可以在SQL语句中实现不同的条件选择。MySQL中的流程处理函数主要包括IF()、IFNULL()和CASE()函数。\n\n\n\n\n函数\n用法\n\n\n\n\nIF(value,value1,value2)\n如果value的值为TRUE，返回value1，否则返回value2\n\n\nIFNULL(value1, value2)\n如果value1不为NULL，返回value1，否则返回value2\n\n\nCASE WHEN 条件1 THEN 结果1 WHEN 条件2 THEN 结果2 …. [ELSE resultn] END\n相当于Java的if…else if…else…\n\n\nCASE  expr WHEN 常量值1 THEN 值1 WHEN 常量值1 THEN 值1 …. [ELSE 值n] END\n相当于Java的switch…case…\n\n\n\n\n加密与解密函数加密与解密函数主要用于对数据库中的数据进行加密和解密处理，以防止数据被他人窃取。这些函数在保证数据库安全时非常有用。\n\n\n\n\n函数\n用法\n\n\n\n\nPASSWORD(str)\n返回字符串str的加密版本，41位长的字符串。加密结果不可逆，常用于用户的密码加密\n\n\nMD5(str)\n返回字符串str的md5加密后的值，也是一种加密方式。若参数为NULL，则会返回NULL\n\n\nSHA(str)\n从原明文密码str计算并返回加密后的密码字符串，当参数为NULL时，返回NULL。SHA加密算法比MD5更加安全。\n\n\nENCODE(value,password_seed)\n返回使用password_seed作为加密密码加密value\n\n\nDECODE(value,password_seed)\n返回使用password_seed作为加密密码解密value\n\n\n\n\n可以看到，ENCODE(value,password_seed)函数与DECODE(value,password_seed)函数互为反函数。\nMySQL信息函数MySQL中内置了一些可以查询MySQL信息的函数，这些函数主要用于帮助数据库开发或运维人员更好地对数据库进行维护工作。\n\n\n\n\n函数\n用法\n\n\n\n\nVERSION()\n返回当前MySQL的版本号\n\n\nCONNECTION_ID()\n返回当前MySQL服务器的连接数\n\n\nDATABASE()，SCHEMA()\n返回MySQL命令行当前所在的数据库\n\n\nUSER()，CURRENT_USER()、SYSTEM_USER()，SESSION_USER()\n返回当前连接MySQL的用户名，返回结果格式为“主机名@用户名”\n\n\nCHARSET(value)\n返回字符串value自变量的字符集\n\n\nCOLLATION(value)\n返回字符串value的比较规则\n\n\n\n\n其他函数MySQL中有些函数无法对其进行具体的分类，但是这些函数在MySQL的开发和运维过程中也是不容忽视的。\n\n\n\n\n函数\n用法\n\n\n\n\nFORMAT(value,n)\n返回对数字value进行格式化后的结果数据。n表示四舍五入后保留到小数点后n位\n\n\nCONV(value,from,to)\n将value的值进行不同进制之间的转换\n\n\nINET_ATON(ipvalue)\n将以点分隔的IP地址转化为一个数字\n\n\nINET_NTOA(value)\n将数字形式的IP地址转化为以点分隔的IP地址\n\n\nBENCHMARK(n,expr)\n将表达式expr重复执行n次。用于测试MySQL处理expr表达式所耗费的时间\n\n\nCONVERT(value USING char_code)\n将value所使用的字符编码修改为char_code\n\n\n\n\nSQL聚集函数\n\n\n\n函数\n\n\n\n\n\nAVG()\nmysql&gt; SELECT AVG(列名) from 表名;\n\n\nCOUNT()\nmysql&gt; SELECT COUNT(列名) from 表名;\n\n\nMAX()\nmysql&gt; SELECT MAX(列名) from 表名;\n\n\nMIN()\nmysql&gt; SELECT MIN(列名) from 表名;\n\n\nSUM()\nmysql&gt; SELECT SUM(列名) from 表名;\n\n\n\n\n特点：\n1、以上五个分组函数都忽略null值，除了count(*)\n2、sum和avg一般用于处理数值型\n    max、min、count可以处理任何数据类型\n3、都可以搭配distinct使用，用于统计去重后的结果\n4、count的参数可以支持：\n    字段、*、常量值，一般放1\n\n   建议使用 count(*)\nGROUP BY可以使用GROUP BY子句将表中的数据分成若干组\nSELECT column, group_function(column)\nFROM table\n[WHERE\tcondition]\n[GROUP BY\tgroup_by_expression]\n[ORDER BY\tcolumn];\n使用多个列分组\nSELECT   department_id dept_id, job_id, SUM(salary)\nFROM     employees\nGROUP BY department_id, job_id ;\nHAVING过滤分组：HAVING子句\n\n行已经被分组。\n使用了聚合函数。\n满足HAVING 子句中条件的分组将被显示。\nHAVING 不能单独使用，必须要跟 GROUP BY 一起使用。\n\nSELECT   department_id, MAX(salary)\nFROM     employees\nGROUP BY department_id\nHAVING   MAX(salary)&gt;10000 ;\nWHERE和HAVING的对比区别1：WHERE 可以直接使用表中的字段作为筛选条件，但不能使用分组中的计算函数作为筛选条件；HAVING 必须要与 GROUP BY 配合使用，可以把分组计算的函数和分组字段作为筛选条件。 \n这决定了，在需要对数据进行分组统计的时候，HAVING 可以完成 WHERE 不能完成的任务。这是因为，在查询语法结构中，WHERE 在 GROUP BY 之前，所以无法对分组结果进行筛选。HAVING 在 GROUP BY 之后，可以使用分组字段和分组中的计算函数，对分组的结果集进行筛选，这个功能是 WHERE 无法完成的。另外，WHERE排除的记录不再包括在分组中。\n区别2：如果需要通过连接从关联表中获取需要的数据，WHERE 是先筛选后连接，而 HAVING 是先连接后筛选。 这一点，就决定了在关联查询中，WHERE 比 HAVING 更高效。因为 WHERE 可以先筛选，用一个筛选后的较小数据集和关联表进行连接，这样占用的资源比较少，执行效率也比较高。HAVING 则需要先把结果集准备好，也就是用未被筛选的数据集进行关联，然后对这个大的数据集进行筛选，这样占用的资源就比较多，执行效率也较低。 \n小结如下：\n\n\n\n\n\n优点\n缺点\n\n\n\n\nWHERE\n先筛选数据再关联，执行效率高\n不能使用分组中的计算函数进行筛选\n\n\nHAVING\n可以使用分组中的计算函数\n在最后的结果集中进行筛选，执行效率较低\n\n\n\n\n开发中的选择：\nWHERE 和 HAVING 也不是互相排斥的，我们可以在一个查询里面同时使用 WHERE 和 HAVING。包含分组统计函数的条件用 HAVING，普通条件用 WHERE。这样，我们就既利用了 WHERE 条件的高效快速，又发挥了 HAVING 可以使用包含分组统计函数的查询条件的优点。当数据量特别大的时候，运行效率会有很大的差别。\n连接表jion联结是一种机制，用来在一条SELECT语句中关联表，因此称之为联结。使用特殊的语法，可以联结多个表返回一组输出，联结在运行时关联表中正确的行。\n#等值联结（equijoin）\nSELECT *\nFROM A,B\nWHERE A.c=B.c;\n\n1.等值连接的结果 = 多个表的交集\n2.n表连接，至少需要n-1个连接条件\n3.多个表不分主次，没有顺序要求\n4.一般为表起别名，提高阅读性和性能\n\n#内部联结\nSELECT *\nFROM A INNER JOIN B\nWHERE A.c=B.c;\n#自然联结\nSELECT *\nFROM A AS a,A AS b\nWHERE a.c=b.c;\n#外部联结\nSELECT *\nFROM A OUTER JOIN B\nWHERE A.c=B.c;\n性能考虑 MySQL在运行时关联指定的每个表以处理联结。这种处理可能是非常耗费资源的，因此应该仔细，不要联结不必要的表。联结的表越多，性能下降越厉害。\n\n自然联结：无论何时对表进行联结，应该至少有一个列出现在不止一个表中（被联结的列）。自然联结排除多次出现，使每个列只返回一次。\n外部联结：许多联结将一个表中的行与另一个表中的行相关联。但有时候会需要包含没有关联行的那些行。\n\nsql99语法：通过join关键字实现连接含义：1999年推出的sql语法\n支持：\n等值连接、非等值连接 （内连接）\n外连接\n交叉连接\n\n语法：\n\nselect 字段，...\nfrom 表1\n【inner|left outer|right outer|cross】join 表2 on  连接条件\n【inner|left outer|right outer|cross】join 表3 on  连接条件\n【where 筛选条件】\n【group by 分组字段】\n【having 分组后的筛选条件】\n【order by 排序的字段或表达式】\n\n好处：语句上，连接条件和筛选条件实现了分离，简洁明了！\n别名AS:别名简化编码\nSELECT name as n\nFROM papa as p,Baba as b\nWHERE p.c=b.c;\n满外连接(FULL OUTER JOIN)\n满外连接的结果 = 左右表匹配的数据 + 左表没有匹配到的数据 + 右表没有匹配到的数据。\nSQL99是支持满外连接的。使用FULL JOIN 或 FULL OUTER JOIN来实现。\n需要注意的是，MySQL不支持FULL JOIN，但是可以用 LEFT JOIN UNION RIGHT join代替。\n\nUNION的使用合并查询结果利用UNION关键字，可以给出多条SELECT语句，并将它们的结果组合成单个结果集。合并时，两个表对应的列数和数据类型必须相同，并且相互对应。各个SELECT语句之间使用UNION或UNION ALL关键字分隔。\n语法格式：\nSELECT column,... FROM table1\nUNION [ALL]\nSELECT column,... FROM table2\nUNION 操作符返回两个查询的结果集的并集，去除重复记录。\nUNION ALL操作符返回两个查询的结果集的并集。对于两个结果集的重复部分，不去重。\n\n代码实现#中图：内连接 A∩B\nSELECT employee_id,last_name,department_name\nFROM employees e JOIN departments d\nON e.`department_id` = d.`department_id`;\n#左上图：左外连接\nSELECT employee_id,last_name,department_name\nFROM employees e LEFT JOIN departments d\nON e.`department_id` = d.`department_id`;\n#右上图：右外连接\nSELECT employee_id,last_name,department_name\nFROM employees e RIGHT JOIN departments d\nON e.`department_id` = d.`department_id`;\n#左中图：A - A∩B\nSELECT employee_id,last_name,department_name\nFROM employees e LEFT JOIN departments d\nON e.`department_id` = d.`department_id`\nWHERE d.`department_id` IS NULL\n#右中图：B-A∩B\nSELECT employee_id,last_name,department_name\nFROM employees e RIGHT JOIN departments d\nON e.`department_id` = d.`department_id`\nWHERE e.`department_id` IS NULL\n#左下图：满外连接\n# 左中图 + 右上图  A∪B\nSELECT employee_id,last_name,department_name\nFROM employees e LEFT JOIN departments d\nON e.`department_id` = d.`department_id`\nWHERE d.`department_id` IS NULL\nUNION ALL  #没有去重操作，效率高\nSELECT employee_id,last_name,department_name\nFROM employees e RIGHT JOIN departments d\nON e.`department_id` = d.`department_id`;\n#右下图\n#左中图 + 右中图  A ∪B- A∩B 或者 (A -  A∩B) ∪ （B - A∩B）\nSELECT employee_id,last_name,department_name\nFROM employees e LEFT JOIN departments d\nON e.`department_id` = d.`department_id`\nWHERE d.`department_id` IS NULL\nUNION ALL\nSELECT employee_id,last_name,department_name\nFROM employees e RIGHT JOIN departments d\nON e.`department_id` = d.`department_id`\nWHERE e.`department_id` IS NULL\nSQL99语法新特性自然连接SQL99 在 SQL92 的基础上提供了一些特殊语法，比如 NATURAL JOIN 用来表示自然连接。我们可以把自然连接理解为 SQL92 中的等值连接。它会帮你自动查询两张连接表中所有相同的字段，然后进行等值连接。\n在SQL92标准中：\nSELECT employee_id,last_name,department_name\nFROM employees e JOIN departments d\nON e.`department_id` = d.`department_id`\nAND e.`manager_id` = d.`manager_id`;\n在 SQL99 中你可以写成：\nSELECT employee_id,last_name,department_name\nFROM employees e NATURAL JOIN departments d;\nUSING连接当我们进行连接的时候，SQL99还支持使用 USING 指定数据表里的同名字段进行等值连接。但是只能配合JOIN一起使用。比如：\nSELECT employee_id,last_name,department_name\nFROM employees e JOIN departments d\nUSING (department_id);\n你能看出与自然连接 NATURAL JOIN 不同的是，USING 指定了具体的相同的字段名称，你需要在 USING 的括号 () 中填入要指定的同名字段。同时使用 JOIN...USING 可以简化 JOIN ON 的等值连接。它与下面的 SQL 查询结果是相同的：\nSELECT employee_id,last_name,department_name\nFROM employees e ,departments d\nWHERE e.department_id = d.department_id;\n 分页查询Limit应用场景：实际的web项目中需要根据用户的需求提交对应的分页查询的sql语句\nselect 字段|表达式,...\nfrom 表\n【where 条件】\n【group by 分组字段】\n【having 条件】\n【order by 排序的字段】\nlimit 【起始的条目索引，】条目数;\n特点：\n1.起始条目索引从0开始\n\n2.limit子句放在查询语句的最后\n\n3.公式：select * from  表 limit （page-1）*sizePerPage,sizePerPage\n假如:\n每页显示条目数sizePerPage\n要显示的页数 page\n子查询子查询指一个查询语句嵌套在另一个查询语句内部的查询，这个特性从MySQL 4.1开始引入。\nSQL 中子查询的使用大大增强了 SELECT 查询的能力，因为很多时候查询需要从结果集中获取数据，或者需要从同一个表中先计算得出一个数据结果，然后与这个数据结果（可能是某个标量，也可能是某个集合）进行比较。\nSELECT cust_id\nFROM orders\nWHERE order_num IN (SELECT order_num\n                     FROM orderitems\n                     WHERE prod_id = 'TNT2');\n/*\n1、子查询都放在小括号内\n2、子查询可以放在from后面、select后面、where后面、having后面，但一般放在条件的右侧\n3、子查询优先于主查询执行，主查询使用了子查询的执行结果\n4、子查询根据查询结果的行数不同分为以下两类：\n① 单行子查询\n\t结果集只有一行\n\t一般搭配单行操作符使用：&gt; &lt; = &lt;&gt; &gt;= &lt;= \n\t非法使用子查询的情况：\n\ta、子查询的结果为一组值\n\tb、子查询的结果为空\n\t\n② 多行子查询\n\t结果集有多行\n\t一般搭配多行操作符使用：any、all、in、not in\n\tin： 属于子查询结果中的任意一个就行\n\tany和all往往可以用其他查询代替\n\t*/\n单行子查询\n\n\n\n单行比较操作符\n含义\n\n\n\n\n=\nequal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&lt;&gt;\nnot equal to\n\n\n\n\n返回job_id与141号员工相同，salary比143号员工多的员工姓名，job_id和工资**\n\n```sql\nSELECT last_name, job_id, salary\nFROM   employees\nWHERE  job_id =  \n                (SELECT job_id\n                 FROM   employees\n                 WHERE  employee_id = 141)\nAND    salary &gt;\n                (SELECT salary\n                 FROM   employees\n                 WHERE  employee_id = 143);\n```\n\n\n多行子查询\n也称为集合比较子查询\n内查询返回多行\n使用多行比较操作符\n\n \n\n\n\n多行比较操作符\n含义\n\n\n\n\nIN\n等于列表中的任意一个\n\n\nANY\n需要和单行比较操作符一起使用，和子查询返回的某一个值比较\n\n\nALL\n需要和单行比较操作符一起使用，和子查询返回的所有值比较\n\n\nSOME\n实际上是ANY的别名，作用相同，一般常使用ANY\n\n\n\n\n\n\n\n\n\n\n\n\n\n体会 ANY 和 ALL 的区别\n查询平均工资最低的部门id**\n#方式1：\nSELECT department_id\nFROM employees\nGROUP BY department_id\nHAVING AVG(salary) = (\n\t\t\tSELECT MIN(avg_sal)\n\t\t\tFROM (\n\t\t\t\tSELECT AVG(salary) avg_sal\n\t\t\t\tFROM employees\n\t\t\t\tGROUP BY department_id\n\t\t\t\t) dept_avg_sal\n\t\t\t)\n#方式2：\nSELECT department_id\nFROM employees\nGROUP BY department_id\nHAVING AVG(salary) &lt;= ALL (\n\t\t\t\tSELECT AVG(salary) avg_sal\n\t\t\t\tFROM employees\n\t\t\t\tGROUP BY department_id\n)\n相关子查询相关子查询执行流程如果子查询的执行依赖于外部查询，通常情况下都是因为子查询中的表用到了外部的表，并进行了条件关联，因此每执行一次外部查询，子查询都要重新计算一次，这样的子查询就称之为关联子查询。\n相关子查询按照一行接一行的顺序执行，主查询的每一行都执行一次子查询。\n\n\n说明：子查询中使用主查询中的列\n代码示例题目：查询员工中工资大于本部门平均工资的员工的last_name,salary和其department_id\n方式一：相关子查询\n\n方式二：在 FROM 中使用子查询\nSELECT last_name,salary,e1.department_id\nFROM employees e1,(SELECT department_id,AVG(salary) dept_avg_sal FROM employees GROUP BY department_id) e2\nWHERE e1.`department_id` = e2.department_id\nAND e2.dept_avg_sal &lt; e1.`salary`;\n\n\n\n\n\n\n\n\n\nfrom型的子查询：子查询是作为from的一部分，子查询要用()引起来，并且要给这个子查询取别名，把它当成一张“临时的虚拟的表”来使用。\n在ORDER BY 中使用子查询：\n题目：查询员工的id,salary,按照department_name 排序\nSELECT employee_id,salary\nFROM employees e\nORDER BY (\n\t  SELECT department_name\n\t  FROM departments d\n\t  WHERE e.`department_id` = d.`department_id`\n\t);\n题目：若employees表中employee_id与job_history表中employee_id相同的数目不小于2，输出这些相同id的员工的employee_id,last_name和其job_id\nSELECT e.employee_id, last_name,e.job_id\nFROM   employees e \nWHERE  2 &lt;= (SELECT COUNT(*)\n             FROM   job_history \n             WHERE  employee_id = e.employee_id);\nEXISTS 与 NOT EXISTS关键字\n关联子查询通常也会和 EXISTS操作符一起来使用，用来检查在子查询中是否存在满足条件的行。\n如果在子查询中不存在满足条件的行：\n条件返回 FALSE\n继续在子查询中查找\n\n\n如果在子查询中存在满足条件的行：\n不在子查询中继续查找\n条件返回 TRUE\n\n\nNOT EXISTS关键字表示如果不存在某种条件，则返回TRUE，否则返回FALSE。\n\n题目：查询公司管理者的employee_id，last_name，job_id，department_id信息\n方式一：\nSELECT employee_id, last_name, job_id, department_id\nFROM   employees e1\nWHERE  EXISTS ( SELECT *\n                 FROM   employees e2\n                 WHERE  e2.manager_id = \n                        e1.employee_id);\n方式二：自连接\nSELECT DISTINCT e1.employee_id, e1.last_name, e1.job_id, e1.department_id\nFROM   employees e1 JOIN employees e2\nWHERE e1.employee_id = e2.manager_id;\n方式三：\nSELECT employee_id,last_name,job_id,department_id\nFROM employees\nWHERE employee_id IN (\n\t\t     SELECT DISTINCT manager_id\n\t\t     FROM employees\n\t\t     \n\t\t     );\n题目：查询departments表中，不存在于employees表中的部门的department_id和department_name\nSELECT department_id, department_name\nFROM departments d\nWHERE NOT EXISTS (SELECT 'X'\n                  FROM   employees\n                  WHERE  department_id = d.department_id);\n\n相关更新UPDATE table1 alias1\nSET    column = (SELECT expression\n                 FROM   table2 alias2\n                 WHERE  alias1.column = alias2.column);\n使用相关子查询依据一个表中的数据更新另一个表的数据。\n题目：在employees中增加一个department_name字段，数据为员工对应的部门名称\n# 1）\nALTER TABLE employees\nADD(department_name VARCHAR2(14));\n\n# 2）\nUPDATE employees e\nSET department_name =  (SELECT department_name \n\t                       FROM   departments d\n\t                       WHERE  e.department_id = d.department_id);\n\n相关删除DELETE FROM table1 alias1\nWHERE column operator (SELECT expression\n                       FROM   table2 alias2\n                       WHERE  alias1.column = alias2.column);\n使用相关子查询依据一个表中的数据删除另一个表的数据。\n题目：删除表employees中，其与emp_history表皆有的数据\nDELETE FROM employees e\nWHERE employee_id in  \n           (SELECT employee_id\n            FROM   emp_history \n            WHERE  employee_id = e.employee_id);\n 4. DML语言数据操作语言：insert、update、delete\n4.1 插入语法：\ninsert into 表名(字段名，...)\nvalues(值1，...);\n\n# 支持一次插入多行，语法如下：\ninsert into 表名【(字段名,..)】 values(值，..),(值，...),...;\n\n#支持子查询，语法如下：\ninsert into 表名 查询语句;\n\ninsert into 表名 set 字段=值,字段=值,...;\n特点：\n\n字段类型和值类型一致或兼容，而且一一对应\n可以为空的字段，可以不用插入值，或用null填充\n不可以为空的字段，必须插入值\n字段个数和值的个数必须一致\n字段可以省略，但默认所有字段，并且顺序和表中的存储顺序一致\n\n4.2 更新数据修改单表语法：\nupdate 表名 set 字段=新值,字段=新值\n【where 条件】\n修改多表语法：\nupdate 表1 别名1,表2 别名2\nset 字段=新值，字段=新值\nwhere 连接条件\nand 筛选条件\n4.3 删除方式1：delete语句单表的删除： ★\ndelete from 表名 【where 筛选条件】\n多表的删除：    delete 别名1，别名2    from 表1 别名1，表2 别名2    where 连接条件    and 筛选条件;\n方式2：truncate语句truncate table 表名\n两种方式的区别【面试题】\n#1.truncate不能加where条件，而delete可以加where条件\n\n#2.truncate的效率高一丢丢\n\n#3.truncate 删除带自增长的列的表后，如果再插入数据，数据从1开始\n#delete 删除带自增长列的表后，如果再插入数据，数据从上一次的断点处开始\n\n#4.truncate删除不能回滚，delete删除可以回滚\nMySQL8新特性：计算列什么叫计算列呢？简单来说就是某一列的值是通过别的列计算得来的。例如，a列值为1、b列值为2，c列不需要手动插入，定义a+b的结果为c的值，那么c就是计算列，是通过别的列计算得来的。\n在MySQL 8.0中，CREATE TABLE 和 ALTER TABLE 中都支持增加计算列。下面以CREATE TABLE为例进行讲解。\n举例：定义数据表tb1，然后定义字段id、字段a、字段b和字段c，其中字段c为计算列，用于计算a+b的值。首先创建测试表tb1，语句如下：\nCREATE TABLE tb1(\nid INT,\na INT,\nb INT,\nc INT GENERATED ALWAYS AS (a + b) VIRTUAL\n);\n插入演示数据，语句如下：\nINSERT INTO tb1(a,b) VALUES (100,200);\n查询数据表tb1中的数据，结果如下：\nmysql&gt; SELECT * FROM tb1;\n+------+------+------+------+\n| id   | a    | b    | c    |\n+------+------+------+------+\n| NULL |  100 |  200 |  300 |\n+------+------+------+------+\n1 row in set (0.00 sec)\n更新数据中的数据，语句如下：\nmysql&gt; UPDATE tb1 SET a = 500;\nQuery OK, 0 rows affected (0.00 sec)\nRows matched: 1  Changed: 0  Warnings: 0\n5. DDL语言数据定义语言：create、drop、alter\n5.1创建数据库\n方式1：创建数据库\n\nCREATE DATABASE 数据库名; \n\n方式2：创建数据库并指定字符集\n\nCREATE DATABASE 数据库名 CHARACTER SET 字符集;\n\n方式3：判断数据库是否已经存在，不存在则创建数据库（推荐）\n\nCREATE DATABASE IF NOT EXISTS 数据库名; \n如果MySQL中已经存在相关的数据库，则忽略创建语句，不再创建数据库。\n\n\n\n\n\n\n\n\n\n注意：DATABASE 不能改名。一些可视化工具可以改名，它是建新库，把所有表复制到新库，再删旧库完成的。\n使用数据库\n查看当前所有的数据库\n\nSHOW DATABASES; #有一个S，代表多个数据库\n\n查看当前正在使用的数据库\n\nSELECT DATABASE();  #使用的一个 mysql 中的全局函数\n\n查看指定库下所有的表\n\nSHOW TABLES FROM 数据库名;\n\n查看数据库的创建信息\n\nSHOW CREATE DATABASE 数据库名;\n或者：\nSHOW CREATE DATABASE 数据库名\\G\n\n使用/切换数据库\n\nUSE 数据库名;\n\n\n\n\n\n\n\n\n\n注意：要操作表格和数据之前必须先说明是对哪个数据库进行操作，否则就要对所有对象加上“数据库名.”。\n修改数据库\n更改数据库字符集\n\nALTER DATABASE 数据库名 CHARACTER SET 字符集;  #比如：gbk、utf8等\n删除数据库\n方式1：删除指定的数据库\n\nDROP DATABASE 数据库名;\n\n方式2：删除指定的数据库（推荐）\n\nDROP DATABASE IF EXISTS 数据库名;\n5.2 创建表 createcreate table 表名(\n  字段名 字段类型 not null,#非空\n  字段名 字段类型 primary key,#主键\n  字段名 字段类型 unique,#唯一\n  字段名 字段类型 default 值,#默认\n  constraint 约束名 foreign key(字段名) references 主表（被引用列）\n)\nCREATE TABLE IF NOT EXISTS stuinfo(\n\tstuId INT,\n\tstuName VARCHAR(20),\n\tgender CHAR,\n\tbornDate DATETIME\n);\nDESC studentinfo;\n\n\n\n\n\n支持类型\n可以起约束名\n\n\n\n\n列级约束\n除了外键\n不可以\n\n\n表级约束\n除了非空和默认\n可以，但对主键无效\n\n\n\n\n列级约束可以在一个字段上追加多个，中间用空格隔开，没有顺序要求\n标识符命名规则\n数据库名、表名不得超过30个字符，变量名限制为29个\n必须只能包含 A–Z, a–z, 0–9, _共63个字符\n数据库名、表名、字段名等对象名中间不要包含空格\n同一个MySQL软件中，数据库不能同名；同一个库中，表不能重名；同一个表中，字段不能重名\n必须保证你的字段没有和保留字、数据库系统或常用方法冲突。如果坚持使用，请在SQL语句中使用`（着重号）引起来\n保持字段名和类型的一致性：在命名字段并为其指定数据类型的时候一定要保证一致性，假如数据类型在一个表里是整数，那在另一个表里可就别变成字符型了\n\nMySQL中的数据类型\n\n\n\n类型\n类型举例\n\n\n\n\n整数类型\nTINYINT、SMALLINT、MEDIUMINT、INT(或INTEGER)、BIGINT\n\n\n浮点类型\nFLOAT、DOUBLE\n\n\n定点数类型\nDECIMAL\n\n\n位类型\nBIT\n\n\n日期时间类型\nYEAR、TIME、DATE、DATETIME、TIMESTAMP\n\n\n文本字符串类型\nCHAR、VARCHAR、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT\n\n\n枚举类型\nENUM\n\n\n集合类型\nSET\n\n\n二进制字符串类型\nBINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB\n\n\nJSON类型\nJSON对象、JSON数组\n\n\n空间数据类型\n单值：GEOMETRY、POINT、LINESTRING、POLYGON；集合：MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、GEOMETRYCOLLECTION\n\n\n\n\n其中，常用的几类类型介绍如下：\n\n\n\n\n数据类型\n描述\n\n\n\n\nINT\n从-2^31到2^31-1的整型数据。存储大小为 4个字节\n\n\nCHAR(size)\n定长字符数据。若未指定，默认为1个字符，最大长度255\n\n\nVARCHAR(size)\n可变长字符数据，根据字符串实际长度保存，必须指定长度\n\n\nFLOAT(M,D)\n单精度，占用4个字节，M=整数位+小数位，D=小数位。 D&lt;=M&lt;=255,0&lt;=D&lt;=30，默认M+D&lt;=6\n\n\nDOUBLE(M,D)\n双精度，占用8个字节，D&lt;=M&lt;=255,0&lt;=D&lt;=30，默认M+D&lt;=15\n\n\nDECIMAL(M,D)\n高精度小数，占用M+2个字节，D&lt;=M&lt;=65，0&lt;=D&lt;=30，最大取值范围与DOUBLE相同。\n\n\nDATE\n日期型数据，格式’YYYY-MM-DD’\n\n\nBLOB\n二进制形式的长文本数据，最大可达4G\n\n\nTEXT\n长文本数据，最大可达4G\n\n\n\n\n常见约束\nNOT NULL：非空，该字段的值必填\nUNIQUE：唯一，该字段的值不可重复\nDEFAULT：默认，该字段的值不用手动插入有默认值\nCHECK：检查，mysql不支持\nPRIMARY KEY：主键，该字段的值不可重复并且非空 unique+not null\nFOREIGN KEY：外键，该字段的值引用了另外的表的字段\n主键和唯一\n1、区别：\n①、一个表至多有一个主键，但可以有多个唯一\n②、主键不允许为空，唯一可以为空\n2、相同点\n都具有唯一性，都支持组合键，但不推荐\n外键：\n\n用于限制两个表的关系，从表的字段值引用了主表的某字段值\n外键列和主表的被引用列要求类型一致，意义一样，名称无要求\n主表的被引用列要求是一个key（一般就是主键）\n插入数据，先插入主表，删除数据，先删除从表\n\n可以通过以下两种方式来删除主表的记录\n#方式一：级联删除\nALTER TABLE stuinfo ADD CONSTRAINT fk_stu_major FOREIGN KEY(majorid) REFERENCES major(id) ON DELETE CASCADE;\n\n#方式二：级联置空\nALTER TABLE stuinfo ADD CONSTRAINT fk_stu_major FOREIGN KEY(majorid) REFERENCES major(id) ON DELETE SET NULL;\n自增长列特点：\n\n不用手动插入值，可以自动提供序列值，默认从1开始，步长为1\n\n如果要更改起始值：手动插入值\n如果要更改步长：更改系统变量\nset auto_increment_increment=值;\n\n一个表至多有一个自增长列\n\n自增长列只能支持数值型\n\n自增长列必须为一个key\n\n\n创建表时设置自增长列\ncreate table 表(\n\t字段名 字段类型 约束 auto_increment\n)\n修改表时设置自增长列\nalter table 表 modify column 字段名 字段类型 约束 auto_increment\n删除自增长列\nalter table 表 modify column 字段名 字段类型 约束 \n5.3 修改表 alter语法：ALTER TABLE 表名 ADD|MODIFY|DROP|CHANGE COLUMN 字段名 【字段类型】;\n\n#修改字段名\nALTER TABLE studentinfo CHANGE  COLUMN sex gender CHAR;\n\n#修改表名\nALTER TABLE stuinfo RENAME [TO]  studentinfo;\n\n#修改字段类型和列级约束\nALTER TABLE studentinfo MODIFY COLUMN borndate DATE ;\n\n#添加字段\nALTER TABLE studentinfo ADD COLUMN email VARCHAR(20) first;\n\n#删除字段\nALTER TABLE studentinfo DROP COLUMN email;\n\n#添加列\nalter table 表名 add column 列名 类型 【first|after 字段名】;\n\n#修改列名\nalter table 表名 change column 旧列名 新列名 类型;\n\n#删除列\nalter table 表名 drop column 列名;\n\n修改表时添加或删除约束\n1、非空\n添加非空\nalter table 表名 modify column 字段名 字段类型 not null;\n删除非空\nalter table 表名 modify column 字段名 字段类型 ;\n\n2、默认\n添加默认\nalter table 表名 modify column 字段名 字段类型 default 值;\n删除默认\nalter table 表名 modify column 字段名 字段类型 ;\n3、主键\n添加主键\nalter table 表名 add【 constraint 约束名】 primary key(字段名);\n删除主键\nalter table 表名 drop primary key;\n\n4、唯一\n添加唯一\nalter table 表名 add【 constraint 约束名】 unique(字段名);\n删除唯一\nalter table 表名 drop index 索引名;\n5、外键\n添加外键\nalter table 表名 add【 constraint 约束名】 foreign key(字段名) references 主表（被引用列）;\n删除外键\nalter table 表名 drop foreign key 约束名;\n\n5.4 删除表DROP TABLE [IF EXISTS] studentinfo;\n复制表\n\n复制表的结构create table 表名 like 旧表;\n复制表的结构+数据create table 表名select 查询列表 from 旧表【where 筛选】;\n\n6. TCL语言事务控制语言：commit、rollback\n6.1 数据库事务含义：通过一组逻辑操作单元（一组DML——sql语句），将数据从一种状态切换到另外一种状态\n特点（ACID）：\n\n原子性：要么都执行，要么都回滚\n一致性：保证数据的状态操作前和操作后保持一致\n隔离性：多个事务同时操作相同数据库的同一个数据时，一个事务的执行不受另外一个事务的干扰\n持久性：一个事务一旦提交，则数据将持久化到本地，除非其他事务对其进行修改\n\n相关步骤：\n开启事务\n编写事务的一组逻辑操作单元（多条sql语句）\n提交事务或回滚事务\n\n事务的分类：隐式事务，没有明显的开启和结束事务的标志，比如 insert、update、delete语句本身就是一个事务\n显式事务，具有明显的开启和结束事务的标志\n1、开启事务\n取消自动提交事务的功能\n\n2、编写事务的一组逻辑操作单元（多条sql语句）\ninsert\nupdate\ndelete\n\n3、提交事务或回滚事务\n使用到的关键字set autocommit=0;\nstart transaction;\ncommit;\nrollback;\n\nsavepoint  断点\ncommit to 断点\nrollback to 断点\n事务的隔离级别:事务并发问题如何发生？\n\n\n\n\n\n\n\n\n\n当多个事务同时操作同一个数据库的相同数据时\n事务的并发问题有哪些？\n\n\n\n\n\n\n\n\n\n脏读：一个事务读取到了另外一个事务未提交的数据不可重复读：同一个事务中，多次读取到的数据不一致幻读：一个事务读取数据时，另外一个事务进行更新，导致第一个事务读取到了没有更新的数据\n如何避免事务的并发问题？\n\n\n\n\n\n\n\n\n\n通过设置事务的隔离级别1、READ UNCOMMITTED2、READ COMMITTED 可以避免脏读3、REPEATABLE READ 可以避免脏读、不可重复读和一部分幻读4、SERIALIZABLE可以避免脏读、不可重复读和幻读\n设置隔离级别：\nset session|global  transaction isolation level 隔离级别名;\n查看隔离级别：\nselect @@tx_isolation;\n7. 视图含义：理解成一张虚拟的表\n视图和表的区别：\n\n\n\n\n\n使用方式\n占用物理空间\n\n\n\n\n视图\n完全相同\n占用物理空间\n\n\n表\n完全相同\n占用\n\n\n\n\n\n\n\n\n视图的好处：\n\nsql语句提高重用性，效率高\n和表实现了分离，提高了安全性\n\n常见的数据库对象\n\n\n\n\n对象\n描述\n\n\n\n\n表(TABLE)\n表是存储数据的逻辑单元，以行和列的形式存在，列就是字段，行就是记录\n\n\n数据字典\n就是系统表，存放数据库相关信息的表。系统表的数据通常由数据库系统维护，程序员通常不应该修改，只可查看\n\n\n约束(CONSTRAINT)\n执行数据校验的规则，用于保证数据完整性的规则\n\n\n视图(VIEW)\n一个或者多个数据表里的数据的逻辑显示，视图并不存储数据\n\n\n索引(INDEX)\n用于提高查询性能，相当于书的目录\n\n\n存储过程(PROCEDURE)\n用于完成一次完整的业务处理，没有返回值，但可通过传出参数将多个值传给调用环境\n\n\n存储函数(FUNCTION)\n用于完成一次特定的计算，具有一个返回值\n\n\n触发器(TRIGGER)\n相当于一个事件监听器，当数据库发生特定事件后，触发器被触发，完成相应的处理\n\n\n\n\n 7.1 视图的创建、删除、查看CREATE VIEW  视图名\nAS\n查询语句;\n\n#视图的删除\nDROP VIEW test_v1,test_v2,test_v3;\n\n#视图查看\nSHOW TABLES;\n\n###视图结构的查看\t\nDESC test_v7;\nSHOW CREATE VIEW test_v7;\n7.2 视图的增删改查1、查看视图的数据 ★\nSELECT * FROM my_v4;\nSELECT * FROM my_v1 WHERE last_name='Partners';\n\n2、插入视图的数据\nINSERT INTO my_v4(last_name,department_id) VALUES('虚竹',90);\n\n3、修改视图的数据\nUPDATE my_v4 SET last_name ='梦姑' WHERE last_name='虚竹';\n\n4、删除视图的数据\nDELETE FROM my_v4;\n​    \n7.3 某些视图不能更新​    包含以下关键字的sql语句：\n\n分组函数、distinct、group  by、having、union或者union all\n常量视图\nSelect中包含子查询\njoin\nfrom一个不能更新的视图\nwhere子句的子查询引用了from子句中的表\n\n\n7.4 视图逻辑的更新#方式一：\nCREATE OR REPLACE VIEW test_v7\nAS\nSELECT last_name FROM employees\nWHERE employee_id&gt;100;\n\n#方式二:\nALTER VIEW test_v7\nAS\nSELECT employee_id FROM employees;\n\nSELECT * FROM test_v7;\n视图优点1. 操作简单\n将经常使用的查询操作定义为视图，可以使开发人员不需要关心视图对应的数据表的结构、表与表之间的关联关系，也不需要关心数据表之间的业务逻辑和查询条件，而只需要简单地操作视图即可，极大简化了开发人员对数据库的操作。\n2. 减少数据冗余\n视图跟实际数据表不一样，它存储的是查询语句。所以，在使用的时候，我们要通过定义视图的查询语句来获取结果集。而视图本身不存储数据，不占用数据存储的资源，减少了数据冗余。\n3. 数据安全\nMySQL将用户对数据的访问限制在某些数据的结果集上，而这些数据的结果集可以使用视图来实现。用户不必直接查询或操作数据表。这也可以理解为视图具有隔离性。视图相当于在用户和实际的数据表之间加了一层虚拟表。\n同时，MySQL可以根据权限将用户对数据的访问限制在某些视图上，用户不需要查询数据表，可以直接通过视图获取数据表中的信息。这在一定程度上保障了数据表中数据的安全性。\n4. 适应灵活多变的需求当业务系统的需求发生变化后，如果需要改动数据表的结构，则工作量相对较大，可以使用视图来减少改动的工作量。这种方式在实际工作中使用得比较多。\n5. 能够分解复杂的查询逻辑数据库中如果存在复杂的查询逻辑，则可以将问题进行分解，创建多个视图获取数据，再将创建的多个视图结合起来，完成复杂的查询逻辑。\n视图不足如果我们在实际数据表的基础上创建了视图，那么，如果实际数据表的结构变更了，我们就需要及时对相关的视图进行相应的维护。特别是嵌套的视图（就是在视图的基础上创建视图），维护会变得比较复杂，可读性不好，容易变成系统的潜在隐患。因为创建视图的 SQL 查询可能会对字段重命名，也可能包含复杂的逻辑，这些都会增加维护的成本。\n实际项目中，如果视图过多，会导致数据库维护成本的问题。\n所以，在创建视图的时候，你要结合实际项目需求，综合考虑视图的优点和不足，这样才能正确使用视图，使系统整体达到最优。\n8 存储过程含义：一组经过预先编译的sql语句的集合好处：\n\n提高了sql语句的重用性，减少了开发程序员的压力\n提高了效率\n减少了传输次数\n\n分类：\n1、无返回无参2、仅仅带in类型，无返回有参3、仅仅带out类型，有返回无参4、既带in又带out，有返回有参5、带inout，有返回有参注意：in、out、inout都可以在一个存储过程中带多个\n和视图、函数的对比：\n它和视图有着同样的优点，清晰、安全，还可以减少网络传输量。不过它和视图不同，视图是虚拟表，通常不对底层数据表直接操作，而存储过程是程序化的 SQL，可以直接操作底层数据表，相比于面向集合的操作方式，能够实现一些更复杂的数据处理。\n一旦存储过程被创建出来，使用它就像使用函数一样简单，我们直接通过调用存储过程名即可。相较于函数，存储过程是没有返回值的。\n8.1 创建存储过程语法：\ncreate procedure 存储过程名(in|out|inout 参数名  参数类型,...)\nbegin\n\t存储过程体\nend\n类似于方法：\n修饰符 返回类型 方法名(参数类型 参数名,...){\n\n\t方法体;\n}\n说明：\n1、参数前面的符号的意思\n\nIN：当前参数为输入参数，也就是表示入参；\n存储过程只是读取这个参数的值。如果没有定义参数种类，默认就是 IN，表示输入参数。\n\nOUT：当前参数为输出参数，也就是表示出参；\n执行完成之后，调用这个存储过程的客户端或者应用程序就可以读取这个参数返回的值了。\n\nINOUT：当前参数既可以为输入参数，也可以为输出参数。\n\n\n2、形参类型可以是 MySQL数据库中的任意类型。\n3、characteristics 表示创建存储过程时指定的对存储过程的约束条件，其取值信息如下：\nLANGUAGE SQL\n| [NOT] DETERMINISTIC\n| { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }\n| SQL SECURITY { DEFINER | INVOKER }\n| COMMENT 'string'\n\nLANGUAGE SQL：说明存储过程执行体是由SQL语句组成的，当前系统支持的语言为SQL。\n[NOT] DETERMINISTIC：指明存储过程执行的结果是否确定。DETERMINISTIC表示结果是确定的。每次执行存储过程时，相同的输入会得到相同的输出。NOT DETERMINISTIC表示结果是不确定的，相同的输入可能得到不同的输出。如果没有指定任意一个值，默认为NOT DETERMINISTIC。\n{ CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }：指明子程序使用SQL语句的限制。\nCONTAINS SQL表示当前存储过程的子程序包含SQL语句，但是并不包含读写数据的SQL语句；\nNO SQL表示当前存储过程的子程序中不包含任何SQL语句；\nREADS SQL DATA表示当前存储过程的子程序中包含读数据的SQL语句；\nMODIFIES SQL DATA表示当前存储过程的子程序中包含写数据的SQL语句。\n默认情况下，系统会指定为CONTAINS SQL。\n\n\nSQL SECURITY { DEFINER | INVOKER }：执行当前存储过程的权限，即指明哪些用户能够执行当前存储过程。\nDEFINER表示只有当前存储过程的创建者或者定义者才能执行当前存储过程；\nINVOKER表示拥有当前存储过程的访问权限的用户能够执行当前存储过程。\n如果没有设置相关的值，则MySQL默认指定值为DEFINER。\n\n\nCOMMENT 'string'：注释信息，可以用来描述存储过程。\n\n4、存储过程体中可以有多条 SQL 语句，如果仅仅一条SQL 语句，则可以省略 BEGIN 和 END\n编写存储过程并不是一件简单的事情，可能存储过程中需要复杂的 SQL 语句。\n1. BEGIN…END：BEGIN…END 中间包含了多个语句，每个语句都以（;）号为结束符。\n2. DECLARE：DECLARE 用来声明变量，使用的位置在于 BEGIN…END 语句中间，而且需要在其他语句使用之前进行变量的声明。\n3. SET：赋值语句，用于对变量进行赋值。\n4. SELECT… INTO：把从数据表中查询的结果存放到变量中，也就是为变量赋值。\n5、需要设置新的结束标记\nDELIMITER 新的结束标记\n因为MySQL默认的语句结束符号为分号‘;’。为了避免与存储过程中SQL语句结束符相冲突，需要使用DELIMITER改变存储过程的结束符。\n比如：“DELIMITER //”语句的作用是将MySQL的结束符设置为//，并以“END //”结束存储过程。存储过程定义完毕之后再使用“DELIMITER ;”恢复默认结束符。DELIMITER也可以指定其他符号作为结束符。\n当使用DELIMITER命令时，应该避免使用反斜杠（‘\\’）字符，因为反斜线是MySQL的转义字符。\n示例：\nDELIMITER $\n\nCREATE PROCEDURE 存储过程名(IN|OUT|INOUT 参数名  参数类型,...)\n[characteristics ...]\nBEGIN\n\tsql语句1;\n\tsql语句2;\n\nEND $\n8.2 调用存储过程call 存储过程名(实参列表)\n学过的函数：LENGTH、SUBSTR、CONCAT等语法：\nCREATE FUNCTION 函数名(参数名 参数类型,...) RETURNS 返回类型\nBEGIN\n\t函数体\n\nEND\n格式：\n1、调用in模式的参数：\nCALL sp1('值');\n2、调用out模式的参数：\nSET @name;\nCALL sp1(@name);\nSELECT @name;\n3、调用inout模式的参数：\nSET @name=值;\nCALL sp1(@name);\nSELECT @name;\n 调用函数：SELECT 函数名（实参列表）\n函数和存储过程的区别\n\n\n\n\n\n关键字\n调用语法\n返回值\n应用场景\n\n\n\n\n函数\nFUNCTION\nSELECT\n只能是一个\n一般用于查询结果为一个值并返回时，当有返回值而且仅仅一个\n\n\n存储过程\nPROCEDURE\nCALL\n可以有0个或多个\n一般用于更新\n\n\n\n\n8.3 查看存储过程show create procedure 存储过程名;\n8.4 删除drop procedure 存储过程名;\n关于存储过程使用的争议尽管存储过程有诸多优点，但是对于存储过程的使用，一直都存在着很多争议，比如有些公司对于大型项目要求使用存储过程，而有些公司在手册中明确禁止使用存储过程，为什么这些公司对存储过程的使用需求差别这么大呢？\n6.1 优点1、存储过程可以一次编译多次使用。存储过程只在创建时进行编译，之后的使用都不需要重新编译，这就提升了 SQL 的执行效率。\n2、可以减少开发工作量。将代码封装成模块，实际上是编程的核心思想之一，这样可以把复杂的问题拆解成不同的模块，然后模块之间可以重复使用，在减少开发工作量的同时，还能保证代码的结构清晰。\n3、存储过程的安全性强。我们在设定存储过程的时候可以设置对用户的使用权限，这样就和视图一样具有较强的安全性。\n4、可以减少网络传输量。因为代码封装到存储过程中，每次使用只需要调用存储过程即可，这样就减少了网络传输量。\n5、良好的封装性。在进行相对复杂的数据库操作时，原本需要使用一条一条的 SQL 语句，可能要连接多次数据库才能完成的操作，现在变成了一次存储过程，只需要连接一次即可。\n6.2 缺点基于上面这些优点，不少大公司都要求大型项目使用存储过程，比如微软、IBM 等公司。但是国内的阿里并不推荐开发人员使用存储过程，这是为什么呢？\n\n\n\n\n\n\n\n\n\n \n第13章_约束讲师：尚硅谷-宋红康（江湖人称：康师傅）\n官网：http://www.atguigu.com\n\n1. 约束(constraint)概述1.1 为什么需要约束数据完整性（Data Integrity）是指数据的精确性（Accuracy）和可靠性（Reliability）。它是防止数据库中存在不符合语义规定的数据和防止因错误信息的输入输出造成无效操作或错误信息而提出的。\n为了保证数据的完整性，SQL规范以约束的方式对表数据进行额外的条件限制。从以下四个方面考虑：\n\n实体完整性（Entity Integrity）：例如，同一个表中，不能存在两条完全相同无法区分的记录\n域完整性（Domain Integrity）：例如：年龄范围0-120，性别范围“男/女”\n引用完整性（Referential Integrity）：例如：员工所在部门，在部门表中要能找到这个部门\n用户自定义完整性（User-defined Integrity）：例如：用户名唯一、密码不能为空等，本部门经理的工资不得高于本部门职工的平均工资的5倍。\n\n1.2 什么是约束约束是表级的强制规定。\n可以在创建表时规定约束（通过 CREATE TABLE 语句），或者在表创建之后通过 ALTER TABLE 语句规定约束。\n1.3 约束的分类\n根据约束数据列的限制，约束可分为：\n单列约束：每个约束只约束一列\n多列约束：每个约束可约束多列数据\n\n\n根据约束的作用范围，约束可分为：\n列级约束：只能作用在一个列上，跟在列的定义后面\n表级约束：可以作用在多个列上，不与列一起，而是单独定义\n\n\n\n\t\t\t位置\t\t\t支持的约束类型\t\t\t\t\t是否可以起约束名\n列级约束：\t列的后面\t\t语法都支持，但外键没有效果\t\t不可以\n表级约束：\t所有列的下面\t   默认和非空不支持，其他支持\t   可以（主键没有效果）\n\n根据约束起的作用，约束可分为：\nNOT NULL 非空约束，规定某个字段不能为空\nUNIQUE  唯一约束，规定某个字段在整个表中是唯一的\nPRIMARY KEY  主键(非空且唯一)约束\nFOREIGN KEY  外键约束\nCHECK  检查约束\nDEFAULT  默认值约束\n\n\n\n\n注意： MySQL不支持check约束，但可以使用check约束，而没有任何效果\n\n查看某个表已有的约束\n\n#information_schema数据库名（系统库）\n#table_constraints表名称（专门存储各个表的约束）\nSELECT * FROM information_schema.table_constraints \nWHERE table_name = '表名称';\n2. 非空约束2.1 作用限定某个字段/某列的值不允许为空\n\n2.2 关键字NOT NULL\n2.3 特点\n默认，所有的类型的值都可以是NULL，包括INT、FLOAT等数据类型\n非空约束只能出现在表对象的列上，只能某个列单独限定非空，不能组合非空\n\n一个表可以有很多列都分别限定了非空\n\n空字符串’’不等于NULL，0也不等于NULL\n\n\n2.4 添加非空约束（1）建表时\nCREATE TABLE 表名称(\n\t字段名  数据类型,\n    字段名  数据类型 NOT NULL,  \n    字段名  数据类型 NOT NULL\n);\n举例：\nCREATE TABLE emp(\nid INT(10) NOT NULL,\nNAME VARCHAR(20) NOT NULL,\nsex CHAR NULL\n);\nCREATE TABLE student(\n\tsid int,\n    sname varchar(20) not null,\n    tel char(11) ,\n    cardid char(18) not null\n);\ninsert into student values(1,'张三','13710011002','110222198912032545'); #成功\n\ninsert into student values(2,'李四','13710011002',null);#身份证号为空\nERROR 1048 (23000): Column 'cardid' cannot be null\n\ninsert into student values(2,'李四',null,'110222198912032546');#成功，tel允许为空\n\ninsert into student values(3,null,null,'110222198912032547');#失败\nERROR 1048 (23000): Column 'sname' cannot be null\n（2）建表后\nalter table 表名称 modify 字段名 数据类型 not null;\n举例：\nALTER TABLE emp\nMODIFY sex VARCHAR(30) NOT NULL;\nalter table student modify sname varchar(20) not null;\n2.5 删除非空约束alter table 表名称 modify 字段名 数据类型 NULL;#去掉not null，相当于修改某个非注解字段，该字段允许为空\n\n或 \n\nalter table 表名称 modify 字段名 数据类型;#去掉not null，相当于修改某个非注解字段，该字段允许为空\n举例：\nALTER TABLE emp\nMODIFY sex VARCHAR(30) NULL;\nALTER TABLE emp\nMODIFY NAME VARCHAR(15) DEFAULT 'abc' NULL;\n3. 唯一性约束3.1 作用用来限制某个字段/某列的值不能重复。\n \n3.2 关键字UNIQUE\n3.3 特点\n同一个表可以有多个唯一约束。\n唯一约束可以是某一个列的值唯一，也可以多个列组合的值唯一。\n唯一性约束允许列值为空。\n在创建唯一约束的时候，如果不给唯一约束命名，就默认和列名相同。\nMySQL会给唯一约束的列上默认创建一个唯一索引。\n\n3.4 添加唯一约束（1）建表时\ncreate table 表名称(\n\t字段名  数据类型,\n    字段名  数据类型  unique,  \n    字段名  数据类型  unique key,\n    字段名  数据类型\n);\ncreate table 表名称(\n\t字段名  数据类型,\n    字段名  数据类型,  \n    字段名  数据类型,\n    [constraint 约束名] unique key(字段名)\n);\n举例：\ncreate table student(\n\tsid int,\n    sname varchar(20),\n    tel char(11) unique,\n    cardid char(18) unique key\n);\nCREATE TABLE t_course(\n\tcid INT UNIQUE,\n\tcname VARCHAR(100) UNIQUE,\n\tdescription VARCHAR(200)\n);\n\nCREATE TABLE USER(\n id INT NOT NULL,\n NAME VARCHAR(25),\n PASSWORD VARCHAR(16),\n -- 使用表级约束语法\n CONSTRAINT uk_name_pwd UNIQUE(NAME,PASSWORD)\n);\n\n\n\n\n\n\n\n\n\n表示用户名和密码组合不能重复\ninsert into student values(1,'张三','13710011002','101223199012015623');\ninsert into student values(2,'李四','13710011003','101223199012015624');\nmysql&gt; select * from student;\n+-----+-------+-------------+--------------------+\n| sid | sname | tel         | cardid             |\n+-----+-------+-------------+--------------------+\n|   1 | 张三  | 13710011002 | 101223199012015623 |\n|   2 | 李四  | 13710011003 | 101223199012015624 |\n+-----+-------+-------------+--------------------+\n2 rows in set (0.00 sec)\ninsert into student values(3,'王五','13710011004','101223199012015624'); #身份证号重复\nERROR 1062 (23000): Duplicate entry '101223199012015624' for key 'cardid'\n\ninsert into student values(3,'王五','13710011003','101223199012015625'); \nERROR 1062 (23000): Duplicate entry '13710011003' for key 'tel'\n（2）建表后指定唯一键约束\n#字段列表中如果是一个字段，表示该列的值唯一。如果是两个或更多个字段，那么复合唯一，即多个字段的组合是唯一的\n#方式1：\nalter table 表名称 add unique key(字段列表); \n#方式2：\nalter table 表名称 modify 字段名 字段类型 unique;\n举例：\nALTER TABLE USER \nADD UNIQUE(NAME,PASSWORD);\nALTER TABLE USER \nADD CONSTRAINT uk_name_pwd UNIQUE(NAME,PASSWORD);\nALTER TABLE USER \nMODIFY NAME VARCHAR(20) UNIQUE;\n举例：\ncreate table student(\n\tsid int primary key,\n    sname varchar(20),\n    tel char(11) ,\n    cardid char(18) \n);\nalter table student add unique key(tel);\nalter table student add unique key(cardid);\n3.5 关于复合唯一约束create table 表名称(\n\t字段名  数据类型,\n    字段名  数据类型,  \n    字段名  数据类型,\n    unique key(字段列表) #字段列表中写的是多个字段名，多个字段名用逗号分隔，表示那么是复合唯一，即多个字段的组合是唯一的\n);\n#学生表\ncreate table student(\n\tsid int,\t#学号\n    sname varchar(20),\t\t\t#姓名\n    tel char(11) unique key,  #电话\n    cardid char(18) unique key #身份证号\n);\n\n#课程表\ncreate table course(\n\tcid int,  #课程编号\n    cname varchar(20)     #课程名称\n);\n\n#选课表\ncreate table student_course(\n    id int,\n\tsid int,\n    cid int,\n    score int,\n    unique key(sid,cid)  #复合唯一\n);\ninsert into student values(1,'张三','13710011002','101223199012015623');#成功\ninsert into student values(2,'李四','13710011003','101223199012015624');#成功\ninsert into course values(1001,'Java'),(1002,'MySQL');#成功\nmysql&gt; select * from student;\n+-----+-------+-------------+--------------------+\n| sid | sname | tel         | cardid             |\n+-----+-------+-------------+--------------------+\n|   1 | 张三  | 13710011002 | 101223199012015623 |\n|   2 | 李四  | 13710011003 | 101223199012015624 |\n+-----+-------+-------------+--------------------+\n2 rows in set (0.00 sec)\n\nmysql&gt; select * from course;\n+------+-------+\n| cid  | cname |\n+------+-------+\n| 1001 | Java  |\n| 1002 | MySQL |\n+------+-------+\n2 rows in set (0.00 sec)\ninsert into student_course values\n(1, 1, 1001, 89),\n(2, 1, 1002, 90),\n(3, 2, 1001, 88),\n(4, 2, 1002, 56);#成功\nmysql&gt; select * from student_course;\n+----+------+------+-------+\n| id | sid  | cid  | score |\n+----+------+------+-------+\n|  1 |    1 | 1001 |    89 |\n|  2 |    1 | 1002 |    90 |\n|  3 |    2 | 1001 |    88 |\n|  4 |    2 | 1002 |    56 |\n+----+------+------+-------+\n4 rows in set (0.00 sec)\ninsert into student_course values (5, 1, 1001, 88);#失败\n\n#ERROR 1062 (23000): Duplicate entry '1-1001' for key 'sid'   违反sid-cid的复合唯一\n3.5 删除唯一约束\n添加唯一性约束的列上也会自动创建唯一索引。\n删除唯一约束只能通过删除唯一索引的方式删除。\n删除时需要指定唯一索引名，唯一索引名就和唯一约束名一样。\n如果创建唯一约束时未指定名称，如果是单列，就默认和列名相同；如果是组合列，那么默认和()中排在第一个的列名相同。也可以自定义唯一性约束名。\n\nSELECT * FROM information_schema.table_constraints WHERE table_name = '表名'; #查看都有哪些约束\nALTER TABLE USER \nDROP INDEX uk_name_pwd;\n\n\n\n\n\n\n\n\n\n注意：可以通过 show index from 表名称;查看表的索引\n4. PRIMARY KEY 约束4.1 作用用来唯一标识表中的一行记录。\n4.2 关键字primary key\n4.3 特点\n主键约束相当于唯一约束+非空约束的组合，主键约束列不允许重复，也不允许出现空值。\n\n\n\n一个表最多只能有一个主键约束，建立主键约束可以在列级别创建，也可以在表级别上创建。\n\n\n主键约束对应着表中的一列或者多列（复合主键）\n如果是多列组合的复合主键约束，那么这些列都不允许为空值，并且组合的值不允许重复。\nMySQL的主键名总是PRIMARY，就算自己命名了主键约束名也没用。\n\n当创建主键约束时，系统默认会在所在的列或列组合上建立对应的主键索引（能够根据主键查询的，就根据主键查询，效率更高）。如果删除主键约束了，主键约束对应的索引就自动删除了。\n\n\n\n需要注意的一点是，不要修改主键字段的值。因为主键是数据记录的唯一标识，如果修改了主键的值，就有可能会破坏数据的完整性。\n\n4.4 添加主键约束（1）建表时指定主键约束\ncreate table 表名称(\n\t字段名  数据类型  primary key, #列级模式\n    字段名  数据类型,  \n    字段名  数据类型  \n);\ncreate table 表名称(\n\t字段名  数据类型,\n    字段名  数据类型,  \n    字段名  数据类型,\n    [constraint 约束名] primary key(字段名) #表级模式\n);\n举例：\ncreate table temp(\n\tid int primary key,\n    name varchar(20)\n);\nmysql&gt; desc temp;\n+-------+-------------+------+-----+---------+-------+\n| Field | Type        | Null | Key | Default | Extra |\n+-------+-------------+------+-----+---------+-------+\n| id    | int(11)     | NO   | PRI | NULL    |       |\n| name  | varchar(20) | YES  |     | NULL    |       |\n+-------+-------------+------+-----+---------+-------+\n2 rows in set (0.00 sec)\ninsert into temp values(1,'张三');#成功\ninsert into temp values(2,'李四');#成功\nmysql&gt; select * from temp;\n+----+------+\n| id | name |\n+----+------+\n|  1 | 张三 |\n|  2 | 李四 |\n+----+------+\n2 rows in set (0.00 sec)\ninsert into temp values(1,'张三');#失败\nERROR 1062 (23000): Duplicate（重复） entry（键入，输入） '1' for key 'PRIMARY'\n\n\ninsert into temp values(1,'王五');#失败\nERROR 1062 (23000): Duplicate entry '1' for key 'PRIMARY'\n\ninsert into temp values(3,'张三');#成功\nmysql&gt; select * from temp;\n+----+------+\n| id | name |\n+----+------+\n|  1 | 张三 |\n|  2 | 李四 |\n|  3 | 张三 |\n+----+------+\n3 rows in set (0.00 sec)\ninsert into temp values(4,null);#成功\n\n\ninsert into temp values(null,'李琦');#失败\nERROR 1048 (23000): Column 'id' cannot be null\nmysql&gt; select * from temp;\n+----+------+\n| id | name |\n+----+------+\n|  1 | 张三 |\n|  2 | 李四 |\n|  3 | 张三 |\n|  4 | NULL |\n+----+------+\n4 rows in set (0.00 sec)\n#演示一个表建立两个主键约束\ncreate table temp(\n\tid int primary key,\n    name varchar(20) primary key\n);\nERROR 1068 (42000): Multiple（多重的） primary key defined（定义）\n再举例：\n\n列级约束\n\nCREATE TABLE emp4(\nid INT PRIMARY KEY AUTO_INCREMENT ,\nNAME VARCHAR(20)\n);\n\n表级约束\n\nCREATE TABLE emp5(\nid INT NOT NULL AUTO_INCREMENT,\nNAME VARCHAR(20),\npwd VARCHAR(15),\nCONSTRAINT emp5_id_pk PRIMARY KEY(id)\n);\n（2）建表后增加主键约束\nALTER TABLE 表名称 ADD PRIMARY KEY(字段列表); #字段列表可以是一个字段，也可以是多个字段，如果是多个字段的话，是复合主键\nALTER TABLE student ADD PRIMARY KEY (sid);\nALTER TABLE emp5 ADD PRIMARY KEY(NAME,pwd);\n4.5 关于复合主键create table 表名称(\n\t字段名  数据类型,\n    字段名  数据类型,  \n    字段名  数据类型,\n    primary key(字段名1,字段名2)  #表示字段1和字段2的组合是唯一的，也可以有更多个字段\n);\n#学生表\ncreate table student(\n\tsid int primary key,  #学号\n    sname varchar(20)     #学生姓名\n);\n\n#课程表\ncreate table course(\n\tcid int primary key,  #课程编号\n    cname varchar(20)     #课程名称\n);\n\n#选课表\ncreate table student_course(\n\tsid int,\n    cid int,\n    score int,\n    primary key(sid,cid)  #复合主键\n);\ninsert into student values(1,'张三'),(2,'李四');\ninsert into course values(1001,'Java'),(1002,'MySQL');\nmysql&gt; select * from student;\n+-----+-------+\n| sid | sname |\n+-----+-------+\n|   1 | 张三  |\n|   2 | 李四  |\n+-----+-------+\n2 rows in set (0.00 sec)\n\nmysql&gt; select * from course;\n+------+-------+\n| cid  | cname |\n+------+-------+\n| 1001 | Java  |\n| 1002 | MySQL |\n+------+-------+\n2 rows in set (0.00 sec)\ninsert into student_course values(1, 1001, 89),(1,1002,90),(2,1001,88),(2,1002,56);\nmysql&gt; select * from student_course;\n+-----+------+-------+\n| sid | cid  | score |\n+-----+------+-------+\n|   1 | 1001 |    89 |\n|   1 | 1002 |    90 |\n|   2 | 1001 |    88 |\n|   2 | 1002 |    56 |\n+-----+------+-------+\n4 rows in set (0.00 sec)\ninsert into student_course values(1, 1001, 100);\nERROR 1062 (23000): Duplicate entry '1-1001' for key 'PRIMARY'\nmysql&gt; desc student_course;\n+-------+---------+------+-----+---------+-------+\n| Field | Type    | Null | Key | Default | Extra |\n+-------+---------+------+-----+---------+-------+\n| sid   | int(11) | NO   | PRI | NULL    |       |\n| cid   | int(11) | NO   | PRI | NULL    |       |\n| score | int(11) | YES  |     | NULL    |       |\n+-------+---------+------+-----+---------+-------+\n3 rows in set (0.00 sec)\n\n再举例\n\nCREATE TABLE emp6(\nid INT NOT NULL,\nNAME VARCHAR(20),\npwd VARCHAR(15),\nCONSTRAINT emp7_pk PRIMARY KEY(NAME,pwd)\n);\n4.6 删除主键约束alter table 表名称 drop primary key;\n举例：\nALTER TABLE student DROP PRIMARY KEY;\nALTER TABLE emp5 DROP PRIMARY KEY;\n\n\n\n\n\n\n\n\n\n说明：删除主键约束，不需要指定主键名，因为一个表只有一个主键，删除主键约束后，非空还存在。\n5. 自增列：AUTO_INCREMENT5.1 作用某个字段的值自增\n5.2 关键字auto_increment\n5.3 特点和要求（1）一个表最多只能有一个自增长列\n（2）当需要产生唯一标识符或顺序值时，可设置自增长\n（3）自增长列约束的列必须是键列（主键列，唯一键列）\n（4）自增约束的列的数据类型必须是整数类型\n（5）如果自增列指定了 0 和 null，会在当前最大值的基础上自增；如果自增列手动指定了具体值，直接赋值为具体值。\n错误演示：\ncreate table employee(\n\teid int auto_increment,\n    ename varchar(20)\n);\n# ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key   \ncreate table employee(\n\teid int primary key,\n    ename varchar(20) unique key auto_increment\n);\n# ERROR 1063 (42000): Incorrect column specifier for column 'ename'  因为ename不是整数类型\n5.4 如何指定自增约束（1）建表时\ncreate table 表名称(\n\t字段名  数据类型  primary key auto_increment,\n    字段名  数据类型  unique key not null,  \n    字段名  数据类型  unique key,\n    字段名  数据类型  not null default 默认值, \n);\ncreate table 表名称(\n\t字段名  数据类型 default 默认值 ,\n    字段名  数据类型 unique key auto_increment,  \n    字段名  数据类型 not null default 默认值,,\n    primary key(字段名)\n);\ncreate table employee(\n\teid int primary key auto_increment,\n    ename varchar(20)\n);\nmysql&gt; desc employee;\n+-------+-------------+------+-----+---------+----------------+\n| Field | Type        | Null | Key | Default | Extra          |\n+-------+-------------+------+-----+---------+----------------+\n| eid   | int(11)     | NO   | PRI | NULL    | auto_increment |\n| ename | varchar(20) | YES  |     | NULL    |                |\n+-------+-------------+------+-----+---------+----------------+\n2 rows in set (0.00 sec)\n（2）建表后\nalter table 表名称 modify 字段名 数据类型 auto_increment;\n例如：\ncreate table employee(\n\teid int primary key ,\n    ename varchar(20)\n);\nalter table employee modify eid int auto_increment;\nmysql&gt; desc employee;\n+-------+-------------+------+-----+---------+----------------+\n| Field | Type        | Null | Key | Default | Extra          |\n+-------+-------------+------+-----+---------+----------------+\n| eid   | int(11)     | NO   | PRI | NULL    | auto_increment |\n| ename | varchar(20) | YES  |     | NULL    |                |\n+-------+-------------+------+-----+---------+----------------+\n2 rows in set (0.00 sec)\n5.5 如何删除自增约束#alter table 表名称 modify 字段名 数据类型 auto_increment;#给这个字段增加自增约束\n\nalter table 表名称 modify 字段名 数据类型; #去掉auto_increment相当于删除\nalter table employee modify eid int;\nmysql&gt; desc employee;\n+-------+-------------+------+-----+---------+-------+\n| Field | Type        | Null | Key | Default | Extra |\n+-------+-------------+------+-----+---------+-------+\n| eid   | int(11)     | NO   | PRI | NULL    |       |\n| ename | varchar(20) | YES  |     | NULL    |       |\n+-------+-------------+------+-----+---------+-------+\n2 rows in set (0.00 sec)\n5.6 MySQL 8.0新特性—自增变量的持久化在MySQL 8.0之前，自增主键AUTO_INCREMENT的值如果大于max(primary key)+1，在MySQL重启后，会重置AUTO_INCREMENT=max(primary key)+1，这种现象在某些情况下会导致业务主键冲突或者其他难以发现的问题。下面通过案例来对比不同的版本中自增变量是否持久化。在MySQL 5.7版本中，测试步骤如下：创建的数据表中包含自增主键的id字段，语句如下：\nCREATE TABLE test1(\nid INT PRIMARY KEY AUTO_INCREMENT\n);\n插入4个空值，执行如下：\nINSERT INTO test1\nVALUES(0),(0),(0),(0);\n查询数据表test1中的数据，结果如下：\nmysql&gt; SELECT * FROM test1;\n+----+\n| id |\n+----+\n|  1 |\n|  2 |\n|  3 |\n|  4 |\n+----+\n4 rows in set (0.00 sec)\n删除id为4的记录，语句如下：\nDELETE FROM test1 WHERE id = 4;\n再次插入一个空值，语句如下：\nINSERT INTO test1 VALUES(0);\n查询此时数据表test1中的数据，结果如下：\nmysql&gt; SELECT * FROM test1;\n+----+\n| id |\n+----+\n|  1 |\n|  2 |\n|  3 |\n|  5 |\n+----+\n4 rows in set (0.00 sec)\n从结果可以看出，虽然删除了id为4的记录，但是再次插入空值时，并没有重用被删除的4，而是分配了5。删除id为5的记录，结果如下：\nDELETE FROM test1 where id=5;\n重启数据库，重新插入一个空值。\nINSERT INTO test1 values(0);\n再次查询数据表test1中的数据，结果如下：\nmysql&gt; SELECT * FROM test1;\n+----+\n| id |\n+----+\n|  1 |\n|  2 |\n|  3 |\n|  4 |\n+----+\n4 rows in set (0.00 sec)\n从结果可以看出，新插入的0值分配的是4，按照重启前的操作逻辑，此处应该分配6。出现上述结果的主要原因是自增主键没有持久化。在MySQL 5.7系统中，对于自增主键的分配规则，是由InnoDB数据字典内部一个计数器来决定的，而该计数器只在内存中维护，并不会持久化到磁盘中。当数据库重启时，该计数器会被初始化。\n在MySQL 8.0版本中，上述测试步骤最后一步的结果如下：\nmysql&gt; SELECT * FROM test1;\n+----+\n| id |\n+----+\n|  1 |\n|  2 |\n|  3 |\n|  6 |\n+----+\n4 rows in set (0.00 sec)\n从结果可以看出，自增变量已经持久化了。\nMySQL 8.0将自增主键的计数器持久化到重做日志中。每次计数器发生改变，都会将其写入重做日志中。如果数据库重启，InnoDB会根据重做日志中的信息来初始化计数器的内存值。\n6. FOREIGN KEY 约束6.1 作用限定某个表的某个字段的引用完整性。\n比如：员工表的员工所在部门的选择，必须在部门表能找到对应的部分。\n\n6.2 关键字FOREIGN KEY\n6.3 主表和从表/父表和子表主表（父表）：被引用的表，被参考的表\n从表（子表）：引用别人的表，参考别人的表\n例如：员工表的员工所在部门这个字段的值要参考部门表：部门表是主表，员工表是从表。\n例如：学生表、课程表、选课表：选课表的学生和课程要分别参考学生表和课程表，学生表和课程表是主表，选课表是从表。\n6.4 特点（1）从表的外键列，必须引用/参考主表的主键或唯一约束的列\n​          为什么？因为被依赖/被参考的值必须是唯一的\n（2）在创建外键约束时，如果不给外键约束命名，默认名不是列名，而是自动产生一个外键名（例如 student_ibfk_1;），也可以指定外键约束名。\n（3）创建(CREATE)表时就指定外键约束的话，先创建主表，再创建从表\n（4）删表时，先删从表（或先删除外键约束），再删除主表\n（5）当主表的记录被从表参照时，主表的记录将不允许删除，如果要删除数据，需要先删除从表中依赖该记录的数据，然后才可以删除主表的数据\n（6）在“从表”中指定外键约束，并且一个表可以建立多个外键约束\n（7）从表的外键列与主表被参照的列名字可以不相同，但是数据类型必须一样，逻辑意义一致。如果类型不一样，创建子表时，就会出现错误“ERROR 1005 (HY000): Can’t create table’database.tablename’(errno: 150)”。\n​          例如：都是表示部门编号，都是int类型。\n（8）当创建外键约束时，系统默认会在所在的列上建立对应的普通索引。但是索引名是外键的约束名。（根据外键查询效率很高）\n（9）删除外键约束后，必须手动删除对应的索引\n6.5 添加外键约束（1）建表时\ncreate table 主表名称(\n\t字段1  数据类型  primary key,\n    字段2  数据类型\n);\n\ncreate table 从表名称(\n\t字段1  数据类型  primary key,\n    字段2  数据类型,\n    [CONSTRAINT &lt;外键约束名称&gt;] FOREIGN KEY（从表的某个字段) references 主表名(被参考字段)\n);\n#(从表的某个字段)的数据类型必须与主表名(被参考字段)的数据类型一致，逻辑意义也一样\n#(从表的某个字段)的字段名可以与主表名(被参考字段)的字段名一样，也可以不一样\n\n-- FOREIGN KEY: 在表级指定子表中的列\n-- REFERENCES: 标示在父表中的列\ncreate table dept( #主表\n\tdid int primary key,\t\t#部门编号\n    dname varchar(50)\t\t\t#部门名称\n);\n\ncreate table emp(#从表\n\teid int primary key,  #员工编号\n    ename varchar(5),     #员工姓名\n    deptid int,\t\t\t\t#员工所在的部门\n    foreign key (deptid) references dept(did)   #在从表中指定外键约束\n    #emp表的deptid和和dept表的did的数据类型一致，意义都是表示部门的编号\n);\n\n说明：\n（1）主表dept必须先创建成功，然后才能创建emp表，指定外键成功。\n（2）删除表时，先删除从表emp，再删除主表dept\n（2）建表后\n一般情况下，表与表的关联都是提前设计好了的，因此，会在创建表的时候就把外键约束定义好。不过，如果需要修改表的设计（比如添加新的字段，增加新的关联关系），但没有预先定义外键约束，那么，就要用修改表的方式来补充定义。\n格式：\nALTER TABLE 从表名 ADD [CONSTRAINT 约束名] FOREIGN KEY (从表的字段) REFERENCES 主表名(被引用字段) [on update xx][on delete xx];\n举例：\nALTER TABLE emp1\nADD [CONSTRAINT emp_dept_id_fk] FOREIGN KEY(dept_id) REFERENCES dept(dept_id);\n举例：\ncreate table dept(\n\tdid int primary key,\t\t#部门编号\n    dname varchar(50)\t\t\t#部门名称\n);\n\ncreate table emp(\n\teid int primary key,  #员工编号\n    ename varchar(5),     #员工姓名\n    deptid int\t\t\t\t#员工所在的部门\n);\n#这两个表创建时，没有指定外键的话，那么创建顺序是随意\nalter table emp add foreign key (deptid) references dept(did);\n6.6 演示问题（1）失败：不是键列\ncreate table dept(\n\tdid int ,\t\t#部门编号\n    dname varchar(50)\t\t\t#部门名称\n);\n\ncreate table emp(\n\teid int primary key,  #员工编号\n    ename varchar(5),     #员工姓名\n    deptid int,\t\t\t\t#员工所在的部门\n    foreign key (deptid) references dept(did)\n);\n#ERROR 1215 (HY000): Cannot add foreign key constraint  原因是dept的did不是键列\n（2）失败：数据类型不一致\ncreate table dept(\n\tdid int primary key,\t\t#部门编号\n    dname varchar(50)\t\t\t#部门名称\n);\n\ncreate table emp(\n\teid int primary key,  #员工编号\n    ename varchar(5),     #员工姓名\n    deptid char,\t\t\t\t#员工所在的部门\n    foreign key (deptid) references dept(did)\n);\n#ERROR 1215 (HY000): Cannot add foreign key constraint  原因是从表的deptid字段和主表的did字段的数据类型不一致，并且要它俩的逻辑意义一致\n（3）成功，两个表字段名一样\ncreate table dept(\n\tdid int primary key,\t\t#部门编号\n    dname varchar(50)\t\t\t#部门名称\n);\n\ncreate table emp(\n\teid int primary key,  #员工编号\n    ename varchar(5),     #员工姓名\n    did int,\t\t\t\t#员工所在的部门\n    foreign key (did) references dept(did)  \n    #emp表的deptid和和dept表的did的数据类型一致，意义都是表示部门的编号\n    #是否重名没问题，因为两个did在不同的表中\n);\n（4）添加、删除、修改问题\ncreate table dept(\n\tdid int primary key,\t\t#部门编号\n    dname varchar(50)\t\t\t#部门名称\n);\n\ncreate table emp(\n\teid int primary key,  #员工编号\n    ename varchar(5),     #员工姓名\n    deptid int,\t\t\t\t#员工所在的部门\n    foreign key (deptid) references dept(did)  \n    #emp表的deptid和和dept表的did的数据类型一致，意义都是表示部门的编号\n);\ninsert into dept values(1001,'教学部');\ninsert into dept values(1003, '财务部');\n\ninsert into emp values(1,'张三',1001); #添加从表记录成功，在添加这条记录时，要求部门表有1001部门\n\ninsert into emp values(2,'李四',1005);#添加从表记录失败\nERROR 1452 (23000): Cannot add（添加） or update（修改） a child row: a foreign key constraint fails (`atguigudb`.`emp`, CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`deptid`) REFERENCES `dept` (`did`)) 从表emp添加记录失败，因为主表dept没有1005部门\nmysql&gt; select * from dept;\n+------+--------+\n| did  | dname  |\n+------+--------+\n| 1001 | 教学部  |\n| 1003 | 财务部  |\n+------+--------+\n2 rows in set (0.00 sec)\n\nmysql&gt; select * from emp;\n+-----+-------+--------+\n| eid | ename | deptid |\n+-----+-------+--------+\n|   1 | 张三   |   1001 |\n+-----+-------+--------+\n1 row in set (0.00 sec)\nupdate emp set deptid = 1002 where eid = 1;#修改从表失败 \nERROR 1452 (23000): Cannot add（添加） or update（修改） a child row（子表的记录）: a foreign key constraint fails（外键约束失败） (`atguigudb`.`emp`, CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`deptid`) REFERENCES `dept` (`did`))  #部门表did字段现在没有1002的值，所以员工表中不能修改员工所在部门deptid为1002\n\nupdate dept set did = 1002 where did = 1001;#修改主表失败\nERROR 1451 (23000): Cannot delete（删除） or update（修改） a parent row（父表的记录）: a foreign key constraint fails (`atguigudb`.`emp`, CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`deptid`) REFERENCES `dept` (`did`)) #部门表did的1001字段已经被emp引用了，所以部门表的1001字段就不能修改了。\n\nupdate dept set did = 1002 where did = 1003;#修改主表成功  因为部门表的1003部门没有被emp表引用，所以可以修改\ndelete from dept where did=1001; #删除主表失败\nERROR 1451 (23000): Cannot delete（删除） or update（修改） a parent row（父表记录）: a foreign key constraint fails (`atguigudb`.`emp`, CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`deptid`) REFERENCES `dept` (`did`))  #因为部门表did的1001字段已经被emp引用了，所以部门表的1001字段对应的记录就不能被删除\n总结：约束关系是针对双方的\n\n添加了外键约束后，主表的修改和删除数据受约束\n\n添加了外键约束后，从表的添加和修改数据受约束\n\n在从表上建立外键，要求主表必须存在\n删除主表时，要求从表从表先删除，或将从表中外键引用该主表的关系先删除\n\n6.7 约束等级\nCascade方式：在父表上update/delete记录时，同步update/delete掉子表的匹配记录 \n\nSet null方式：在父表上update/delete记录时，将子表上匹配记录的列设为null，但是要注意子表的外键列不能为not null  \n\nNo action方式：如果子表中有匹配的记录，则不允许对父表对应候选键进行update/delete操作  \n\nRestrict方式：同no action， 都是立即检查外键约束\n\nSet default方式（在可视化工具SQLyog中可能显示空白）：父表有变更时，子表将外键列设置成一个默认的值，但Innodb不能识别\n\n\n如果没有指定等级，就相当于Restrict方式。\n对于外键约束，最好是采用: ON UPDATE CASCADE ON DELETE RESTRICT 的方式。\n（1）演示1：on update cascade on delete set null\ncreate table dept(\n\tdid int primary key,\t\t#部门编号\n    dname varchar(50)\t\t\t#部门名称\n);\n\ncreate table emp(\n\teid int primary key,  #员工编号\n    ename varchar(5),     #员工姓名\n    deptid int,\t\t\t\t#员工所在的部门\n    foreign key (deptid) references dept(did)  on update cascade on delete set null\n    #把修改操作设置为级联修改等级，把删除操作设置为set null等级\n);\ninsert into dept values(1001,'教学部');\ninsert into dept values(1002, '财务部');\ninsert into dept values(1003, '咨询部');\n\n\ninsert into emp values(1,'张三',1001); #在添加这条记录时，要求部门表有1001部门\ninsert into emp values(2,'李四',1001);\ninsert into emp values(3,'王五',1002);\n\nmysql&gt; select * from dept;\n\nmysql&gt; select * from emp;\n\n#修改主表成功，从表也跟着修改，修改了主表被引用的字段1002为1004，从表的引用字段就跟着修改为1004了\nmysql&gt; update dept set did = 1004 where did = 1002;\nQuery OK, 1 row affected (0.00 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql&gt; select * from dept;\n+------+--------+\n| did  | dname  |\n+------+--------+\n| 1001 | 教学部 |\n| 1003 | 咨询部 |\n| 1004 | 财务部 | #原来是1002，修改为1004\n+------+--------+\n3 rows in set (0.00 sec)\n\nmysql&gt; select * from emp;\n+-----+-------+--------+\n| eid | ename | deptid |\n+-----+-------+--------+\n|   1 | 张三  |   1001 |\n|   2 | 李四  |   1001 |\n|   3 | 王五  |   1004 | #原来是1002，跟着修改为1004\n+-----+-------+--------+\n3 rows in set (0.00 sec)\n#删除主表的记录成功，从表对应的字段的值被修改为null\nmysql&gt; delete from dept where did = 1001;\nQuery OK, 1 row affected (0.01 sec)\n\nmysql&gt; select * from dept;\n+------+--------+\n| did  | dname  | #记录1001部门被删除了\n+------+--------+\n| 1003 | 咨询部  |\n| 1004 | 财务部  |\n+------+--------+\n2 rows in set (0.00 sec)\n\nmysql&gt; select * from emp;\n+-----+-------+--------+\n| eid | ename | deptid |\n+-----+-------+--------+\n|   1 | 张三  |   NULL | #原来引用1001部门的员工，deptid字段变为null\n|   2 | 李四  |   NULL |\n|   3 | 王五  |   1004 |\n+-----+-------+--------+\n3 rows in set (0.00 sec)\n（2）演示2：on update set null on delete cascade\ncreate table dept(\n\tdid int primary key,\t\t#部门编号\n    dname varchar(50)\t\t\t#部门名称\n);\n\ncreate table emp(\n\teid int primary key,  #员工编号\n    ename varchar(5),     #员工姓名\n    deptid int,\t\t\t\t#员工所在的部门\n    foreign key (deptid) references dept(did)  on update set null on delete cascade\n    #把修改操作设置为set null等级，把删除操作设置为级联删除等级\n);\ninsert into dept values(1001,'教学部');\ninsert into dept values(1002, '财务部');\ninsert into dept values(1003, '咨询部');\n\ninsert into emp values(1,'张三',1001); #在添加这条记录时，要求部门表有1001部门\ninsert into emp values(2,'李四',1001);\ninsert into emp values(3,'王五',1002);\nmysql&gt; select * from dept;\n+------+--------+\n| did  | dname  |\n+------+--------+\n| 1001 | 教学部 |\n| 1002 | 财务部 |\n| 1003 | 咨询部 |\n+------+--------+\n3 rows in set (0.00 sec)\n\nmysql&gt; select * from emp;\n+-----+-------+--------+\n| eid | ename | deptid |\n+-----+-------+--------+\n|   1 | 张三  |   1001 |\n|   2 | 李四  |   1001 |\n|   3 | 王五  |   1002 |\n+-----+-------+--------+\n3 rows in set (0.00 sec)\n#修改主表，从表对应的字段设置为null\nmysql&gt; update dept set did = 1004 where did = 1002;\nQuery OK, 1 row affected (0.00 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql&gt; select * from dept;\n+------+--------+\n| did  | dname  |\n+------+--------+\n| 1001 | 教学部 |\n| 1003 | 咨询部 |\n| 1004 | 财务部 | #原来did是1002\n+------+--------+\n3 rows in set (0.00 sec)\n\nmysql&gt; select * from emp;\n+-----+-------+--------+\n| eid | ename | deptid |\n+-----+-------+--------+\n|   1 | 张三  |   1001 |\n|   2 | 李四  |   1001 |\n|   3 | 王五  |   NULL | #原来deptid是1002，因为部门表1002被修改了，1002没有对应的了，就设置为null\n+-----+-------+--------+\n3 rows in set (0.00 sec)\n#删除主表的记录成功，主表的1001行被删除了，从表相应的记录也被删除了\nmysql&gt; delete from dept where did=1001;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql&gt; select * from dept;\n+------+--------+\n| did  | dname  | #部门表中1001部门被删除\n+------+--------+\n| 1003 | 咨询部 |\n| 1004 | 财务部 |\n+------+--------+\n2 rows in set (0.00 sec)\n\nmysql&gt; select * from emp;\n+-----+-------+--------+\n| eid | ename | deptid |#原来1001部门的员工也被删除了\n+-----+-------+--------+\n|   3 | 王五  |   NULL |\n+-----+-------+--------+\n1 row in set (0.00 sec)\n\n（3）演示：on update cascade on delete cascade\ncreate table dept(\n\tdid int primary key,\t\t#部门编号\n    dname varchar(50)\t\t\t#部门名称\n);\n\ncreate table emp(\n\teid int primary key,  #员工编号\n    ename varchar(5),     #员工姓名\n    deptid int,\t\t\t\t#员工所在的部门\n    foreign key (deptid) references dept(did)  on update cascade on delete cascade\n    #把修改操作设置为级联修改等级，把删除操作也设置为级联删除等级\n);\ninsert into dept values(1001,'教学部');\ninsert into dept values(1002, '财务部');\ninsert into dept values(1003, '咨询部');\n\ninsert into emp values(1,'张三',1001); #在添加这条记录时，要求部门表有1001部门\ninsert into emp values(2,'李四',1001);\ninsert into emp values(3,'王五',1002);\nmysql&gt; select * from dept;\n+------+--------+\n| did  | dname  |\n+------+--------+\n| 1001 | 教学部 |\n| 1002 | 财务部 |\n| 1003 | 咨询部 |\n+------+--------+\n3 rows in set (0.00 sec)\n\nmysql&gt; select * from emp;\n+-----+-------+--------+\n| eid | ename | deptid |\n+-----+-------+--------+\n|   1 | 张三  |   1001 |\n|   2 | 李四  |   1001 |\n|   3 | 王五  |   1002 |\n+-----+-------+--------+\n3 rows in set (0.00 sec)\n#修改主表，从表对应的字段自动修改\nmysql&gt; update dept set did = 1004 where did = 1002;\nQuery OK, 1 row affected (0.00 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql&gt; select * from dept;\n+------+--------+\n| did  | dname  |\n+------+--------+\n| 1001 | 教学部 |\n| 1003 | 咨询部 |\n| 1004 | 财务部 | #部门1002修改为1004\n+------+--------+\n3 rows in set (0.00 sec)\n\nmysql&gt; select * from emp;\n+-----+-------+--------+\n| eid | ename | deptid |\n+-----+-------+--------+\n|   1 | 张三  |   1001 |\n|   2 | 李四  |   1001 |\n|   3 | 王五  |   1004 | #级联修改\n+-----+-------+--------+\n3 rows in set (0.00 sec)\n#删除主表的记录成功，主表的1001行被删除了，从表相应的记录也被删除了\nmysql&gt; delete from dept where did=1001;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql&gt; select * from dept;\n+------+--------+\n| did  | dname  | #1001部门被删除了\n+------+--------+\n| 1003 | 咨询部 |\n| 1004 | 财务部 | \n+------+--------+\n2 rows in set (0.00 sec)\n\nmysql&gt; select * from emp;\n+-----+-------+--------+\n| eid | ename | deptid |  #1001部门的员工也被删除了\n+-----+-------+--------+\n|   3 | 王五  |   1004 |\n+-----+-------+--------+\n1 row in set (0.00 sec)\n6.8 删除外键约束流程如下：\n(1)第一步先查看约束名和删除外键约束\nSELECT * FROM information_schema.table_constraints WHERE table_name = '表名称';#查看某个表的约束名\n\nALTER TABLE 从表名 DROP FOREIGN KEY 外键约束名;\n\n（2）第二步查看索引名和删除索引。（注意，只能手动删除）\nSHOW INDEX FROM 表名称; #查看某个表的索引名\n\nALTER TABLE 从表名 DROP INDEX 索引名;\n\n举例：\nmysql&gt; SELECT * FROM information_schema.table_constraints WHERE table_name = 'emp';\n\nmysql&gt; alter table emp drop foreign key emp_ibfk_1;\nQuery OK, 0 rows affected (0.02 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql&gt; show index from emp;\n\nmysql&gt; alter table emp drop index deptid;\nQuery OK, 0 rows affected (0.01 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql&gt;  show index from emp;\n6.9 开发场景问题1：如果两个表之间有关系（一对一、一对多），比如：员工表和部门表（一对多），它们之间是否一定要建外键约束？\n答：不是的\n问题2：建和不建外键约束有什么区别？\n答：建外键约束，你的操作（创建表、删除表、添加、修改、删除）会受到限制，从语法层面受到限制。例如：在员工表中不可能添加一个员工信息，它的部门的值在部门表中找不到。\n不建外键约束，你的操作（创建表、删除表、添加、修改、删除）不受限制，要保证数据的引用完整性，只能依靠程序员的自觉，或者是在Java程序中进行限定。例如：在员工表中，可以添加一个员工的信息，它的部门指定为一个完全不存在的部门。\n问题3：那么建和不建外键约束和查询有没有关系？\n答：没有\n\n\n\n\n\n\n\n\n\n在 MySQL 里，外键约束是有成本的，需要消耗系统资源。对于大并发的 SQL 操作，有可能会不适合。比如大型网站的中央数据库，可能会因为外键约束的系统开销而变得非常慢。所以， MySQL 允许你不使用系统自带的外键约束，在应用层面完成检查数据一致性的逻辑。也就是说，即使你不用外键约束，也要想办法通过应用层面的附加逻辑，来实现外键约束的功能，确保数据的一致性。\n6.10 阿里开发规范【强制】不得使用外键与级联，一切外键概念必须在应用层解决。 \n说明：（概念解释）学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群；级联更新是强阻塞，存在数据库更新风暴的风险；外键影响数据库的插入速度。\n7. CHECK 约束7.1 作用检查某个字段的值是否符号xx要求，一般指的是值的范围\n2、关键字CHECK\n3、说明：MySQL 5.7 不支持MySQL5.7 可以使用check约束，但check约束对数据验证没有任何作用。添加数据时，没有任何错误或警告\n但是MySQL 8.0中可以使用check约束了。\ncreate table employee(\n\teid int primary key,\n    ename varchar(5),\n    gender char check ('男' or '女')\n);\ninsert into employee values(1,'张三','妖');\nmysql&gt; select * from employee;\n+-----+-------+--------+\n| eid | ename | gender |\n+-----+-------+--------+\n|   1 | 张三   | 妖     |\n+-----+-------+--------+\n1 row in set (0.00 sec)\n\n再举例\n\nCREATE TABLE temp(\nid INT AUTO_INCREMENT,\nNAME VARCHAR(20),\nage INT CHECK(age &gt; 20),\nPRIMARY KEY(id)\n);\n\n再举例\n\nage tinyint check(age &gt;20) 或 sex char(2) check(sex in(‘男’,’女’))\n\n再举例\n\nCHECK(height&gt;=0 AND height&lt;3)\n8. DEFAULT约束8.1 作用给某个字段/某列指定默认值，一旦设置默认值，在插入数据时，如果此字段没有显式赋值，则赋值为默认值。\n8.2 关键字DEFAULT\n8.3 如何给字段加默认值（1）建表时\ncreate table 表名称(\n\t字段名  数据类型  primary key,\n    字段名  数据类型  unique key not null,  \n    字段名  数据类型  unique key,\n    字段名  数据类型  not null default 默认值, \n);\ncreate table 表名称(\n\t字段名  数据类型 default 默认值 ,\n    字段名  数据类型 not null default 默认值,  \n    字段名  数据类型 not null default 默认值,\n    primary key(字段名),\n    unique key(字段名)\n);\n\n说明：默认值约束一般不在唯一键和主键列上加\ncreate table employee(\n\teid int primary key,\n    ename varchar(20) not null,\n    gender char default '男',\n    tel char(11) not null default '' #默认是空字符串\n);\nmysql&gt; desc employee;\n+--------+-------------+------+-----+---------+-------+\n| Field  | Type        | Null | Key | Default | Extra |\n+--------+-------------+------+-----+---------+-------+\n| eid    | int(11)     | NO   | PRI | NULL    |       |\n| ename  | varchar(20) | NO   |     | NULL    |       |\n| gender | char(1)     | YES  |     | 男      |       |\n| tel    | char(11)    | NO   |     |         |       |\n+--------+-------------+------+-----+---------+-------+\n4 rows in set (0.00 sec)\ninsert into employee values(1,'汪飞','男','13700102535'); #成功\nmysql&gt; select * from employee;\n+-----+-------+--------+-------------+\n| eid | ename | gender | tel         |\n+-----+-------+--------+-------------+\n|   1 | 汪飞  | 男     | 13700102535 |\n+-----+-------+--------+-------------+\n1 row in set (0.00 sec)\ninsert into employee(eid,ename) values(2,'天琪'); #成功\nmysql&gt; select * from employee;\n+-----+-------+--------+-------------+\n| eid | ename | gender | tel         |\n+-----+-------+--------+-------------+\n|   1 | 汪飞  | 男     | 13700102535 |\n|   2 | 天琪  | 男     |             |\n+-----+-------+--------+-------------+\n2 rows in set (0.00 sec)\ninsert into employee(eid,ename) values(3,'二虎');\n#ERROR 1062 (23000): Duplicate entry '' for key 'tel'  \n#如果tel有唯一性约束的话会报错，如果tel没有唯一性约束，可以添加成功\n再举例：\nCREATE TABLE myemp(\nid INT AUTO_INCREMENT PRIMARY KEY,\nNAME VARCHAR(15),\nsalary DOUBLE(10,2) DEFAULT 2000\n);\n（2）建表后\nalter table 表名称 modify 字段名 数据类型 default 默认值;\n\n#如果这个字段原来有非空约束，你还保留非空约束，那么在加默认值约束时，还得保留非空约束，否则非空约束就被删除了\n#同理，在给某个字段加非空约束也一样，如果这个字段原来有默认值约束，你想保留，也要在modify语句中保留默认值约束，否则就删除了\nalter table 表名称 modify 字段名 数据类型 default 默认值 not null;\ncreate table employee(\n\teid int primary key,\n    ename varchar(20),\n    gender char,\n    tel char(11) not null\n);\nmysql&gt; desc employee;\n+--------+-------------+------+-----+---------+-------+\n| Field  | Type        | Null | Key | Default | Extra |\n+--------+-------------+------+-----+---------+-------+\n| eid    | int(11)     | NO   | PRI | NULL    |       |\n| ename  | varchar(20) | YES  |     | NULL    |       |\n| gender | char(1)     | YES  |     | NULL    |       |\n| tel    | char(11)    | NO   |     | NULL    |       |\n+--------+-------------+------+-----+---------+-------+\n4 rows in set (0.00 sec)\nalter table employee modify gender char default '男';  #给gender字段增加默认值约束\nalter table employee modify tel char(11) default ''; #给tel字段增加默认值约束\nmysql&gt; desc employee;\n+--------+-------------+------+-----+---------+-------+\n| Field  | Type        | Null | Key | Default | Extra |\n+--------+-------------+------+-----+---------+-------+\n| eid    | int(11)     | NO   | PRI | NULL    |       |\n| ename  | varchar(20) | YES  |     | NULL    |       |\n| gender | char(1)     | YES  |     | 男      |       |\n| tel    | char(11)    | YES  |     |         |       |\n+--------+-------------+------+-----+---------+-------+\n4 rows in set (0.00 sec)\nalter table employee modify tel char(11) default ''  not null;#给tel字段增加默认值约束，并保留非空约束\nmysql&gt; desc employee;\n+--------+-------------+------+-----+---------+-------+\n| Field  | Type        | Null | Key | Default | Extra |\n+--------+-------------+------+-----+---------+-------+\n| eid    | int(11)     | NO   | PRI | NULL    |       |\n| ename  | varchar(20) | YES  |     | NULL    |       |\n| gender | char(1)     | YES  |     | 男      |       |\n| tel    | char(11)    | NO   |     |         |       |\n+--------+-------------+------+-----+---------+-------+\n4 rows in set (0.00 sec)\n8.4 如何删除默认值约束alter table 表名称 modify 字段名 数据类型 ;#删除默认值约束，也不保留非空约束\n\nalter table 表名称 modify 字段名 数据类型  not null; #删除默认值约束，保留非空约束\nalter table employee modify gender char; #删除gender字段默认值约束，如果有非空约束，也一并删除\nalter table employee modify tel char(11)  not null;#删除tel字段默认值约束，保留非空约束\nmysql&gt; desc employee;\n+--------+-------------+------+-----+---------+-------+\n| Field  | Type        | Null | Key | Default | Extra |\n+--------+-------------+------+-----+---------+-------+\n| eid    | int(11)     | NO   | PRI | NULL    |       |\n| ename  | varchar(20) | YES  |     | NULL    |       |\n| gender | char(1)     | YES  |     | NULL    |       |\n| tel    | char(11)    | NO   |     | NULL    |       |\n+--------+-------------+------+-----+---------+-------+\n4 rows in set (0.00 sec)\n9. 面试面试1、为什么建表时，加 not null default ‘’ 或 default 0\n答：不想让表中出现null值。\n面试2、为什么不想要 null 的值\n答:（1）不好比较。null是一种特殊值，比较时只能用专门的is null 和 is not null来比较。碰到运算符，通常返回null。\n​     （2）效率不高。影响提高索引效果。因此，我们往往在建表时 not null default ‘’ 或 default 0\n面试3、带AUTO_INCREMENT约束的字段值是从1开始的吗？在MySQL中，默认AUTO_INCREMENT的初始值是1，每新增一条记录，字段值自动加1。设置自增属性（AUTO_INCREMENT）的时候，还可以指定第一条插入记录的自增字段的值，这样新插入的记录的自增字段值从初始值开始递增，如在表中插入第一条记录，同时指定id值为5，则以后插入的记录的id值就会从6开始往上增加。添加主键约束时，往往需要设置字段自动增加属性。\n面试4、并不是每个表都可以任意选择存储引擎？外键约束（FOREIGN KEY）不能跨引擎使用。\nMySQL支持多种存储引擎，每一个表都可以指定一个不同的存储引擎，需要注意的是：外键约束是用来保证数据的参照完整性的，如果表之间需要关联外键，却指定了不同的存储引擎，那么这些表之间是不能创建外键约束的。所以说，存储引擎的选择也不完全是随意的。\n第16章_变量、流程控制与游标在MySQL数据库的存储过程和函数中，可以使用变量来存储查询或计算的中间结果数据，或者输出最终的结果数据。\n在 MySQL 数据库中，变量分为系统变量以及用户自定义变量。\n1.1 系统变量1.1.1 系统变量分类变量由系统定义，不是用户定义，属于服务器层面。启动MySQL服务，生成MySQL服务实例期间，MySQL将为MySQL服务器内存中的系统变量赋值，这些系统变量定义了当前MySQL服务实例的属性、特征。这些系统变量的值要么是编译MySQL时参数的默认值，要么是配置文件（例如my.ini等）中的参数值。大家可以通过网址 https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html 查看MySQL文档的系统变量。\n系统变量分为全局系统变量（需要添加global 关键字）以及会话系统变量（需要添加 session 关键字），有时也把全局系统变量简称为全局变量，有时也把会话系统变量称为local变量。如果不写，默认会话级别。静态变量（在 MySQL 服务实例运行期间它们的值不能使用 set 动态修改）属于特殊的全局系统变量。\n每一个MySQL客户机成功连接MySQL服务器后，都会产生与之对应的会话。会话期间，MySQL服务实例会在MySQL服务器内存中生成与该会话对应的会话系统变量，这些会话系统变量的初始值是全局系统变量值的复制。如下图：\n\n\n全局系统变量针对于所有会话（连接）有效，但不能跨重启\n会话系统变量仅针对于当前会话（连接）有效。会话期间，当前会话对某个会话系统变量值的修改，不会影响其他会话同一个会话系统变量的值。\n会话1对某个全局系统变量值的修改会导致会话2中同一个全局系统变量值的修改。\n\n在MySQL中有些系统变量只能是全局的，例如 max_connections 用于限制服务器的最大连接数；有些系统变量作用域既可以是全局又可以是会话，例如 character_set_client 用于设置客户端的字符集；有些系统变量的作用域只能是当前会话，例如 pseudo_thread_id 用于标记当前会话的 MySQL 连接 ID。\n1.1.2 查看系统变量\n查看所有或部分系统变量\n\n#查看所有全局变量\nSHOW GLOBAL VARIABLES;\n\n#查看所有会话变量\nSHOW SESSION VARIABLES;\n或\nSHOW VARIABLES;\n#查看满足条件的部分系统变量。\nSHOW GLOBAL VARIABLES LIKE '%标识符%';\n\n#查看满足条件的部分会话变量\nSHOW SESSION VARIABLES LIKE '%标识符%';\n举例：\nSHOW GLOBAL VARIABLES LIKE 'admin_%';\n\n查看指定系统变量\n\n作为 MySQL 编码规范，MySQL 中的系统变量以两个“@”开头，其中“@@global”仅用于标记全局系统变量，“@@session”仅用于标记会话系统变量。“@@”首先标记会话系统变量，如果会话系统变量不存在，则标记全局系统变量。\n#查看指定的系统变量的值\nSELECT @@global.变量名;\n\n#查看指定的会话变量的值\nSELECT @@session.变量名;\n#或者\nSELECT @@变量名;\n\n修改系统变量的值\n\n有些时候，数据库管理员需要修改系统变量的默认值，以便修改当前会话或者MySQL服务实例的属性、特征。具体方法：\n方式1：修改MySQL配置文件，继而修改MySQL系统变量的值（该方法需要重启MySQL服务）\n方式2：在MySQL服务运行期间，使用“set”命令重新设置系统变量的值\n#为某个系统变量赋值\n#方式1：\nSET @@global.变量名=变量值;\n#方式2：\nSET GLOBAL 变量名=变量值;\n\n\n#为某个会话变量赋值\n#方式1：\nSET @@session.变量名=变量值;\n#方式2：\nSET SESSION 变量名=变量值;\n举例：\nSELECT @@global.autocommit;\nSET GLOBAL autocommit=0;\nSELECT @@session.tx_isolation;\nSET @@session.tx_isolation='read-uncommitted';\nSET GLOBAL max_connections = 1000;\nSELECT @@global.max_connections;\n1.2 用户变量1.2.1 用户变量分类用户变量是用户自己定义的，作为 MySQL 编码规范，MySQL 中的用户变量以一个“@”开头。根据作用范围不同，又分为会话用户变量和局部变量。\n\n会话用户变量：作用域和会话变量一样，只对当前连接会话有效。\n\n局部变量：只在 BEGIN 和 END 语句块中有效。局部变量只能在存储过程和函数中使用。\n\n\n1.2.2 会话用户变量\n变量的定义\n\n#方式1：“=”或“:=”\nSET @用户变量 = 值;\nSET @用户变量 := 值;\n\n#方式2：“:=” 或 INTO关键字\nSELECT @用户变量 := 表达式 [FROM 等子句];\nSELECT 表达式 INTO @用户变量  [FROM 等子句];\n\n\n查看用户变量的值 （查看、比较、运算等）\n\nSELECT @用户变量\n\n举例\n\nSET @a = 1;\n\nSELECT @a;\nSELECT @num := COUNT(*) FROM employees;\n\nSELECT @num;\nSELECT AVG(salary) INTO @avgsalary FROM employees;\n\nSELECT @avgsalary;\nSELECT @big;  #查看某个未声明的变量时，将得到NULL值\n1.2.3 局部变量定义：可以使用DECLARE语句定义一个局部变量\n作用域：仅仅在定义它的 BEGIN … END 中有效\n位置：只能放在 BEGIN … END 中，而且只能放在第一句\nBEGIN\n\t#声明局部变量\n\tDECLARE 变量名1 变量数据类型 [DEFAULT 变量默认值];\n\tDECLARE 变量名2,变量名3,... 变量数据类型 [DEFAULT 变量默认值];\n\n\t#为局部变量赋值\n\tSET 变量名1 = 值;\n\tSELECT 值 INTO 变量名2 [FROM 子句];\n\n\t#查看局部变量的值\n\tSELECT 变量1,变量2,变量3;\nEND\n\n1.定义变量\nDECLARE 变量名 类型 [default 值];  # 如果没有DEFAULT子句，初始值为NULL\n举例：\nDECLARE　myparam　INT　DEFAULT 100;\n2.变量赋值\n方式1：一般用于赋简单的值\nSET 变量名=值;\nSET 变量名:=值;\n方式2：一般用于赋表中的字段值\nSELECT 字段名或表达式 INTO 变量名 FROM 表;\n3.使用变量（查看、比较、运算等）\nSELECT 局部变量名;\n举例1：声明局部变量，并分别赋值为employees表中employee_id为102的last_name和salary\nDELIMITER //\n\nCREATE PROCEDURE set_value()\nBEGIN\n\tDECLARE emp_name VARCHAR(25);\n\tDECLARE sal DOUBLE(10,2);\n\t\n\tSELECT last_name,salary INTO emp_name,sal\n\tFROM employees \n\tWHERE employee_id = 102;\n\t\n\tSELECT emp_name,sal;\nEND //\n\nDELIMITER ;\n举例2：声明两个变量，求和并打印 （分别使用会话用户变量、局部变量的方式实现）\n#方式1：使用用户变量\nSET @m=1;\nSET @n=1;\nSET @sum=@m+@n;\n\nSELECT @sum;\n#方式2：使用局部变量\nDELIMITER //\n\nCREATE PROCEDURE add_value()\nBEGIN\n\t#局部变量\n\tDECLARE m INT DEFAULT 1;\n\tDECLARE n INT DEFAULT 3;\n\tDECLARE SUM INT;\n\t\n\tSET SUM = m+n;\n\tSELECT SUM;\nEND //\n\nDELIMITER ;\n举例3：创建存储过程“different_salary”查询某员工和他领导的薪资差距，并用IN参数emp_id接收员工id，用OUT参数dif_salary输出薪资差距结果。\n#声明\nDELIMITER //\n\nCREATE PROCEDURE different_salary(IN emp_id INT,OUT dif_salary DOUBLE)\nBEGIN\n\t#声明局部变量\n\tDECLARE emp_sal,mgr_sal DOUBLE DEFAULT 0.0;\n\tDECLARE mgr_id INT;\n\t\n\tSELECT salary INTO emp_sal FROM employees WHERE employee_id = emp_id;\n\tSELECT manager_id INTO mgr_id FROM employees WHERE employee_id = emp_id;\n\tSELECT salary INTO mgr_sal FROM employees WHERE employee_id = mgr_id;\n\tSET dif_salary = mgr_sal - emp_sal;\n\nEND //\n\nDELIMITER ;\n\n#调用\nSET @emp_id = 102;\nCALL different_salary(@emp_id,@diff_sal);\n\n\n#查看\nSELECT @diff_sal;\n1.2.4 对比会话用户变量与局部变量\t\t\t  作用域\t\t\t\t\t定义位置\t\t\t\t  语法\n会话用户变量\t  当前会话\t\t\t\t   会话的任何地方\t\t\t\t加@符号，不用指定类型\n局部变量\t   定义它的BEGIN END中 \t\tBEGIN END的第一句话\t\t  一般不用加@,需要指定类型\n2. 定义条件与处理程序定义条件是事先定义程序执行过程中可能遇到的问题，处理程序定义了在遇到问题时应当采取的处理方式，并且保证存储过程或函数在遇到警告或错误时能继续执行。这样可以增强存储程序处理问题的能力，避免程序异常停止运行。\n说明：定义条件和处理程序在存储过程、存储函数中都是支持的。\n2.1 案例分析案例分析：创建一个名称为“UpdateDataNoCondition”的存储过程。代码如下：\nDELIMITER //\n\nCREATE PROCEDURE UpdateDataNoCondition()\n\tBEGIN\n\t\tSET @x = 1;\n\t\tUPDATE employees SET email = NULL WHERE last_name = 'Abel';\n\t\tSET @x = 2;\n\t\tUPDATE employees SET email = 'aabbel' WHERE last_name = 'Abel';\n\t\tSET @x = 3;\n\tEND //\n\nDELIMITER ;\n调用存储过程：\nmysql&gt; CALL UpdateDataNoCondition();\nERROR 1048 (23000): Column 'email' cannot be null\n\nmysql&gt; SELECT @x;\n+------+\n| @x   |\n+------+\n|   1  |\n+------+\n1 row in set (0.00 sec)\n\n可以看到，此时@x变量的值为1。结合创建存储过程的SQL语句代码可以得出：在存储过程中未定义条件和处理程序，且当存储过程中执行的SQL语句报错时，MySQL数据库会抛出错误，并退出当前SQL逻辑，不再向下继续执行。\n2.2 定义条件定义条件就是给MySQL中的错误码命名，这有助于存储的程序代码更清晰。它将一个错误名字和指定的错误条件关联起来。这个名字可以随后被用在定义处理程序的DECLARE HANDLER语句中。\n定义条件使用DECLARE语句，语法格式如下：\nDECLARE 错误名称 CONDITION FOR 错误码（或错误条件）\n错误码的说明：\n\nMySQL_error_code和sqlstate_value都可以表示MySQL的错误。\nMySQL_error_code是数值类型错误代码。\nsqlstate_value是长度为5的字符串类型错误代码。\n\n\n例如，在ERROR 1418 (HY000)中，1418是MySQL_error_code，’HY000’是sqlstate_value。\n例如，在ERROR 1142（42000）中，1142是MySQL_error_code，’42000’是sqlstate_value。\n\n举例1：定义“Field_Not_Be_NULL”错误名与MySQL中违反非空约束的错误类型是“ERROR 1048 (23000)”对应。\n#使用MySQL_error_code\nDECLARE Field_Not_Be_NULL CONDITION FOR 1048;\n\n#使用sqlstate_value\nDECLARE Field_Not_Be_NULL CONDITION FOR SQLSTATE '23000';\n举例2：定义”ERROR 1148(42000)”错误，名称为command_not_allowed。\n#使用MySQL_error_code\nDECLARE command_not_allowed CONDITION FOR 1148;\n\n#使用sqlstate_value\nDECLARE command_not_allowed CONDITION FOR SQLSTATE '42000';\n2.3 定义处理程序可以为SQL执行过程中发生的某种类型的错误定义特殊的处理程序。定义处理程序时，使用DECLARE语句的语法如下：\nDECLARE 处理方式 HANDLER FOR 错误类型 处理语句\n\n处理方式：处理方式有3个取值：CONTINUE、EXIT、UNDO。\nCONTINUE：表示遇到错误不处理，继续执行。\nEXIT：表示遇到错误马上退出。\nUNDO：表示遇到错误后撤回之前的操作。MySQL中暂时不支持这样的操作。\n\n\n错误类型（即条件）可以有如下取值：\nSQLSTATE '字符串错误码'：表示长度为5的sqlstate_value类型的错误代码；\nMySQL_error_code：匹配数值类型错误代码；\n错误名称：表示DECLARE … CONDITION定义的错误条件名称。\nSQLWARNING：匹配所有以01开头的SQLSTATE错误代码；\nNOT FOUND：匹配所有以02开头的SQLSTATE错误代码；\nSQLEXCEPTION：匹配所有没有被SQLWARNING或NOT FOUND捕获的SQLSTATE错误代码；\n\n\n处理语句：如果出现上述条件之一，则采用对应的处理方式，并执行指定的处理语句。语句可以是像“SET 变量 = 值”这样的简单语句，也可以是使用BEGIN ... END编写的复合语句。\n\n定义处理程序的几种方式，代码如下：\n#方法1：捕获sqlstate_value\nDECLARE CONTINUE HANDLER FOR SQLSTATE '42S02' SET @info = 'NO_SUCH_TABLE';\n\n#方法2：捕获mysql_error_value\nDECLARE CONTINUE HANDLER FOR 1146 SET @info = 'NO_SUCH_TABLE';\n\n#方法3：先定义条件，再调用\nDECLARE no_such_table CONDITION FOR 1146;\nDECLARE CONTINUE HANDLER FOR NO_SUCH_TABLE SET @info = 'NO_SUCH_TABLE';\n\n#方法4：使用SQLWARNING\nDECLARE EXIT HANDLER FOR SQLWARNING SET @info = 'ERROR';\n\n#方法5：使用NOT FOUND\nDECLARE EXIT HANDLER FOR NOT FOUND SET @info = 'NO_SUCH_TABLE';\n\n#方法6：使用SQLEXCEPTION\nDECLARE EXIT HANDLER FOR SQLEXCEPTION SET @info = 'ERROR';\n2.4 案例解决在存储过程中，定义处理程序，捕获sqlstate_value值，当遇到MySQL_error_code值为1048时，执行CONTINUE操作，并且将@proc_value的值设置为-1。\nDELIMITER //\n\nCREATE PROCEDURE UpdateDataNoCondition()\n\tBEGIN\n\t\t#定义处理程序\n\t\tDECLARE CONTINUE HANDLER FOR 1048 SET @proc_value = -1;\n\t\t\n\t\tSET @x = 1;\n\t\tUPDATE employees SET email = NULL WHERE last_name = 'Abel';\n\t\tSET @x = 2;\n\t\tUPDATE employees SET email = 'aabbel' WHERE last_name = 'Abel';\n\t\tSET @x = 3;\n\tEND //\n\nDELIMITER ;\n调用过程：\nmysql&gt; CALL UpdateDataWithCondition();\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&gt; SELECT @x,@proc_value;\n+------+-------------+\n| @x   | @proc_value |\n+------+-------------+\n|    3 |       \t -1  |\n+------+-------------+\n1 row in set (0.00 sec)\n\n举例：\n创建一个名称为“InsertDataWithCondition”的存储过程，代码如下。\n在存储过程中，定义处理程序，捕获sqlstate_value值，当遇到sqlstate_value值为23000时，执行EXIT操作，并且将@proc_value的值设置为-1。\n#准备工作\nCREATE TABLE departments\nAS\nSELECT * FROM atguigudb.`departments`;\n\nALTER TABLE departments\nADD CONSTRAINT uk_dept_name UNIQUE(department_id);\nDELIMITER //\n\nCREATE PROCEDURE InsertDataWithCondition()\n\tBEGIN\n\t\tDECLARE duplicate_entry CONDITION FOR SQLSTATE '23000' ;\n\t\tDECLARE EXIT HANDLER FOR duplicate_entry SET @proc_value = -1;\n\t\t\n\t\tSET @x = 1;\n\t\tINSERT INTO departments(department_name) VALUES('测试');\n\t\tSET @x = 2;\n\t\tINSERT INTO departments(department_name) VALUES('测试');\n\t\tSET @x = 3;\n\tEND //\n\nDELIMITER ;\n调用存储过程：\nmysql&gt; CALL InsertDataWithCondition();\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&gt; SELECT @x,@proc_value;\n+------+-------------+\n| @x   | @proc_value |\n+------+-------------+\n|    2 |       \t -1  |\n+------+-------------+\n1 row in set (0.00 sec)\n\n3. 流程控制解决复杂问题不可能通过一个 SQL 语句完成，我们需要执行多个 SQL 操作。流程控制语句的作用就是控制存储过程中 SQL 语句的执行顺序，是我们完成复杂操作必不可少的一部分。只要是执行的程序，流程就分为三大类：\n\n顺序结构：程序从上往下依次执行\n分支结构：程序按条件进行选择执行，从两条或多条路径中选择一条执行\n循环结构：程序满足一定条件下，重复执行一组语句\n\n针对于MySQL 的流程控制语句主要有 3 类。注意：只能用于存储程序。\n\n条件判断语句：IF 语句和 CASE 语句\n循环语句：LOOP、WHILE 和 REPEAT 语句\n跳转语句：ITERATE 和 LEAVE 语句\n\n3.1 分支结构之 IF\nIF 语句的语法结构是：\n\nIF 表达式1 THEN 操作1\n[ELSEIF 表达式2 THEN 操作2]……\n[ELSE 操作N]\nEND IF\n根据表达式的结果为TRUE或FALSE执行相应的语句。这里“[]”中的内容是可选的。\n\n特点：① 不同的表达式对应不同的操作 ② 使用在begin end中\n\n举例1：\nIF val IS NULL \n\tTHEN SELECT 'val is null';\nELSE SELECT 'val is not null';\n\nEND IF;\n\n举例2：声明存储过程“update_salary_by_eid1”，定义IN参数emp_id，输入员工编号。判断该员工薪资如果低于8000元并且入职时间超过5年，就涨薪500元；否则就不变。\nDELIMITER //\n\nCREATE PROCEDURE update_salary_by_eid1(IN emp_id INT)\nBEGIN\n\tDECLARE emp_salary DOUBLE;\n\tDECLARE hire_year DOUBLE;\n\n\tSELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id;\n\n\tSELECT DATEDIFF(CURDATE(),hire_date)/365 INTO hire_year\n\tFROM employees WHERE employee_id = emp_id;\n\n\tIF emp_salary &lt; 8000 AND hire_year &gt; 5\n\tTHEN UPDATE employees SET salary = salary + 500 WHERE employee_id = emp_id;\n\tEND IF;\nEND //\n\n\nDELIMITER ;\n\n举例3：声明存储过程“update_salary_by_eid2”，定义IN参数emp_id，输入员工编号。判断该员工薪资如果低于9000元并且入职时间超过5年，就涨薪500元；否则就涨薪100元。\nDELIMITER //\n\nCREATE PROCEDURE update_salary_by_eid2(IN emp_id INT)\nBEGIN\n\tDECLARE emp_salary DOUBLE;\n\tDECLARE hire_year DOUBLE;\n\n\tSELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id;\n\n\tSELECT DATEDIFF(CURDATE(),hire_date)/365 INTO hire_year\n\tFROM employees WHERE employee_id = emp_id;\n\n\tIF emp_salary &lt; 8000 AND hire_year &gt; 5\n\t\tTHEN UPDATE employees SET salary = salary + 500 WHERE employee_id = emp_id;\n\tELSE \n\t\tUPDATE employees SET salary = salary + 100 WHERE employee_id = emp_id;\n\tEND IF;\nEND //\n\n\nDELIMITER ;\n\n举例4：声明存储过程“update_salary_by_eid3”，定义IN参数emp_id，输入员工编号。判断该员工薪资如果低于9000元，就更新薪资为9000元；薪资如果大于等于9000元且低于10000的，但是奖金比例为NULL的，就更新奖金比例为0.01；其他的涨薪100元。\nDELIMITER //\n\nCREATE PROCEDURE update_salary_by_eid3(IN emp_id INT)\nBEGIN\n\tDECLARE emp_salary DOUBLE;\n\tDECLARE bonus DECIMAL(3,2);\n\n\tSELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id;\n\tSELECT commission_pct INTO bonus FROM employees WHERE employee_id = emp_id;\n\n\tIF emp_salary &lt; 9000\n\t\tTHEN UPDATE employees SET salary = 9000 WHERE employee_id = emp_id;\n\tELSEIF emp_salary &lt; 10000 AND bonus IS NULL\n\t\tTHEN UPDATE employees SET commission_pct = 0.01 WHERE employee_id = emp_id;\n\tELSE\n\t\tUPDATE employees SET salary = salary + 100 WHERE employee_id = emp_id;\n\tEND IF;\nEND //\n\nDELIMITER ;\n\n\n3.2 分支结构之 CASECASE 语句的语法结构1：\n#情况一：类似于switch\nCASE 表达式\nWHEN 值1 THEN 结果1或语句1(如果是语句，需要加分号) \nWHEN 值2 THEN 结果2或语句2(如果是语句，需要加分号)\n...\nELSE 结果n或语句n(如果是语句，需要加分号)\nEND [case]（如果是放在begin end中需要加上case，如果放在select后面不需要）\nCASE 语句的语法结构2：\n#情况二：类似于多重if\nCASE \nWHEN 条件1 THEN 结果1或语句1(如果是语句，需要加分号) \nWHEN 条件2 THEN 结果2或语句2(如果是语句，需要加分号)\n...\nELSE 结果n或语句n(如果是语句，需要加分号)\nEND [case]（如果是放在begin end中需要加上case，如果放在select后面不需要）\n\n举例1：\n\n使用CASE流程控制语句的第1种格式，判断val值等于1、等于2，或者两者都不等。\nCASE val\n　　　WHEN 1 THEN SELECT 'val is 1';\n　　　WHEN 2 THEN SELECT 'val is 2';\n　　　ELSE SELECT 'val is not 1 or 2';\nEND CASE;\n\n举例2：\n\n使用CASE流程控制语句的第2种格式，判断val是否为空、小于0、大于0或者等于0。\nCASE\n\tWHEN val IS NULL THEN SELECT 'val is null';\n\tWHEN val &lt; 0 THEN SELECT 'val is less than 0';\n\tWHEN val &gt; 0 THEN SELECT 'val is greater than 0';\n\tELSE SELECT 'val is 0';\nEND CASE;\n\n举例3：声明存储过程“update_salary_by_eid4”，定义IN参数emp_id，输入员工编号。判断该员工薪资如果低于9000元，就更新薪资为9000元；薪资大于等于9000元且低于10000的，但是奖金比例为NULL的，就更新奖金比例为0.01；其他的涨薪100元。\n\nDELIMITER //\n\nCREATE PROCEDURE update_salary_by_eid4(IN emp_id INT)\nBEGIN\n\tDECLARE emp_sal DOUBLE;\n\tDECLARE bonus DECIMAL(3,2);\n\n\tSELECT salary INTO emp_sal FROM employees WHERE employee_id = emp_id;\n\tSELECT commission_pct INTO bonus FROM employees WHERE employee_id = emp_id;\n\n\tCASE\n\tWHEN emp_sal&lt;9000\n\t\tTHEN UPDATE employees SET salary=9000 WHERE employee_id = emp_id;\n\tWHEN emp_sal&lt;10000 AND bonus IS NULL\n\t\tTHEN UPDATE employees SET commission_pct=0.01 WHERE employee_id = emp_id;\n\tELSE\n\t\tUPDATE employees SET salary=salary+100 WHERE employee_id = emp_id;\n\tEND CASE;\nEND //\n\nDELIMITER ;\n\n举例4：声明存储过程update_salary_by_eid5，定义IN参数emp_id，输入员工编号。判断该员工的入职年限，如果是0年，薪资涨50；如果是1年，薪资涨100；如果是2年，薪资涨200；如果是3年，薪资涨300；如果是4年，薪资涨400；其他的涨薪500。\n\nDELIMITER //\n\nCREATE PROCEDURE update_salary_by_eid5(IN emp_id INT)\nBEGIN\n\tDECLARE emp_sal DOUBLE;\n\tDECLARE hire_year DOUBLE;\n\n\tSELECT salary INTO emp_sal FROM employees WHERE employee_id = emp_id;\n\t\n\tSELECT ROUND(DATEDIFF(CURDATE(),hire_date)/365) INTO hire_year FROM employees WHERE employee_id = emp_id;\n\n\tCASE hire_year\n\t\tWHEN 0 THEN UPDATE employees SET salary=salary+50 WHERE employee_id = emp_id;\n\t\tWHEN 1 THEN UPDATE employees SET salary=salary+100 WHERE employee_id = emp_id;\n\t\tWHEN 2 THEN UPDATE employees SET salary=salary+200 WHERE employee_id = emp_id;\n\t\tWHEN 3 THEN UPDATE employees SET salary=salary+300 WHERE employee_id = emp_id;\n\t\tWHEN 4 THEN UPDATE employees SET salary=salary+400 WHERE employee_id = emp_id;\n\t\tELSE UPDATE employees SET salary=salary+500 WHERE employee_id = emp_id;\n\tEND CASE;\nEND //\n\nDELIMITER ;\n3.3 循环结构之LOOPLOOP循环语句用来重复执行某些语句。LOOP内的语句一直重复执行直到循环被退出（使用LEAVE子句），跳出循环过程。\nLOOP语句的基本格式如下：\n[loop_label:] LOOP\n\t循环执行的语句\nEND LOOP [loop_label]\n其中，loop_label表示LOOP语句的标注名称，该参数可以省略。\n举例1：\n使用LOOP语句进行循环操作，id值小于10时将重复执行循环过程。\nDECLARE id INT DEFAULT 0;\nadd_loop:LOOP\n\tSET id = id +1;\n\tIF id &gt;= 10 THEN LEAVE add_loop;\n\tEND IF;\n\nEND LOOP add_loop;\n举例2：当市场环境变好时，公司为了奖励大家，决定给大家涨工资。声明存储过程“update_salary_loop()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家涨薪，薪资涨为原来的1.1倍。直到全公司的平均薪资达到12000结束。并统计循环次数。\nDELIMITER //\n\nCREATE PROCEDURE update_salary_loop(OUT num INT)\nBEGIN\n\tDECLARE avg_salary DOUBLE;\n\tDECLARE loop_count INT DEFAULT 0;\n\t\n\tSELECT AVG(salary) INTO avg_salary FROM employees;\n\t\n\tlabel_loop:LOOP\n\t\tIF avg_salary &gt;= 12000 THEN LEAVE label_loop;\n\t\tEND IF;\n\t\t\n\t\tUPDATE employees SET salary = salary * 1.1;\n\t\tSET loop_count = loop_count + 1;\n\t\tSELECT AVG(salary) INTO avg_salary FROM employees;\n\tEND LOOP label_loop;\n\t\n\tSET num = loop_count;\n\nEND //\n\nDELIMITER ;\n3.4 循环结构之WHILEWHILE语句创建一个带条件判断的循环过程。WHILE在执行语句执行时，先对指定的表达式进行判断，如果为真，就执行循环内的语句，否则退出循环。WHILE语句的基本格式如下：\n[while_label:] WHILE 循环条件  DO\n\t循环体\nEND WHILE [while_label];\nwhile_label为WHILE语句的标注名称；如果循环条件结果为真，WHILE语句内的语句或语句群被执行，直至循环条件为假，退出循环。\n举例1：\nWHILE语句示例，i值小于10时，将重复执行循环过程，代码如下：\nDELIMITER //\n\nCREATE PROCEDURE test_while()\nBEGIN\t\n\tDECLARE i INT DEFAULT 0;\n\t\n\tWHILE i &lt; 10 DO\n\t\tSET i = i + 1;\n\tEND WHILE;\n\t\n\tSELECT i;\nEND //\n\nDELIMITER ;\n#调用\nCALL test_while();\n举例2：市场环境不好时，公司为了渡过难关，决定暂时降低大家的薪资。声明存储过程“update_salary_while()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家降薪，薪资降为原来的90%。直到全公司的平均薪资达到5000结束。并统计循环次数。\nDELIMITER //\n\nCREATE PROCEDURE update_salary_while(OUT num INT)\nBEGIN\n\tDECLARE avg_sal DOUBLE ;\n\tDECLARE while_count INT DEFAULT 0;\n\t\n\tSELECT AVG(salary) INTO avg_sal FROM employees;\n\t\n\tWHILE avg_sal &gt; 5000 DO\n\t\tUPDATE employees SET salary = salary * 0.9;\n\t\t\n\t\tSET while_count = while_count + 1;\n\t\t\n\t\tSELECT AVG(salary) INTO avg_sal FROM employees;\n\tEND WHILE;\n\t\n\tSET num = while_count;\n\nEND //\n\nDELIMITER ;\n3.5 循环结构之REPEATREPEAT语句创建一个带条件判断的循环过程。与WHILE循环不同的是，REPEAT 循环首先会执行一次循环，然后在 UNTIL 中进行表达式的判断，如果满足条件就退出，即 END REPEAT；如果条件不满足，则会就继续执行循环，直到满足退出条件为止。\nREPEAT语句的基本格式如下：\n[repeat_label:] REPEAT\n　　　　循环体的语句\nUNTIL 结束循环的条件表达式\nEND REPEAT [repeat_label]\nrepeat_label为REPEAT语句的标注名称，该参数可以省略；REPEAT语句内的语句或语句群被重复，直至expr_condition为真。\n举例1：\nDELIMITER //\n\nCREATE PROCEDURE test_repeat()\nBEGIN\t\n\tDECLARE i INT DEFAULT 0;\n\t\n\tREPEAT \n\t\tSET i = i + 1;\n\tUNTIL i &gt;= 10\n\tEND REPEAT;\n\t\n\tSELECT i;\nEND //\n\nDELIMITER ;\n举例2：当市场环境变好时，公司为了奖励大家，决定给大家涨工资。声明存储过程“update_salary_repeat()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家涨薪，薪资涨为原来的1.15倍。直到全公司的平均薪资达到13000结束。并统计循环次数。\nDELIMITER //\n\nCREATE PROCEDURE update_salary_repeat(OUT num INT)\nBEGIN\n\tDECLARE avg_sal DOUBLE ;\n\tDECLARE repeat_count INT DEFAULT 0;\n\t\n\tSELECT AVG(salary) INTO avg_sal FROM employees;\n\t\n\tREPEAT\n\t\tUPDATE employees SET salary = salary * 1.15;\n\t\t\n\t\tSET repeat_count = repeat_count + 1;\n\t\t\n\t\tSELECT AVG(salary) INTO avg_sal FROM employees;\n\tUNTIL avg_sal &gt;= 13000\n\tEND REPEAT;\n\t\n\tSET num = repeat_count;\n\t\t\nEND //\n\nDELIMITER ;\n对比三种循环结构：\n1、这三种循环都可以省略名称，但如果循环中添加了循环控制语句（LEAVE或ITERATE）则必须添加名称。2、LOOP：一般用于实现简单的”死”循环WHILE：先判断后执行REPEAT：先执行后判断，无条件至少执行一次\n3.6 跳转语句之LEAVE语句LEAVE语句：可以用在循环语句内，或者以 BEGIN 和 END 包裹起来的程序体内，表示跳出循环或者跳出程序体的操作。如果你有面向过程的编程语言的使用经验，你可以把 LEAVE 理解为 break。\n基本格式如下：\nLEAVE 标记名\n其中，label参数表示循环的标志。LEAVE和BEGIN … END或循环一起被使用。\n举例1：创建存储过程 “leave_begin()”，声明INT类型的IN参数num。给BEGIN…END加标记名，并在BEGIN…END中使用IF语句判断num参数的值。\n\n如果num&lt;=0，则使用LEAVE语句退出BEGIN…END；\n如果num=1，则查询“employees”表的平均薪资；\n如果num=2，则查询“employees”表的最低薪资；\n如果num&gt;2，则查询“employees”表的最高薪资。\n\nIF语句结束后查询“employees”表的总人数。\nDELIMITER //\n\nCREATE PROCEDURE leave_begin(IN num INT)\n\n\tbegin_label: BEGIN\n\t\tIF num&lt;=0 \n\t\t\tTHEN LEAVE begin_label;\n\t\tELSEIF num=1 \n\t\t\tTHEN SELECT AVG(salary) FROM employees;\n\t\tELSEIF num=2 \n\t\t\tTHEN SELECT MIN(salary) FROM employees;\n\t\tELSE \n\t\t\tSELECT MAX(salary) FROM employees;\n\t\tEND IF;\n\t\t\n\t\tSELECT COUNT(*) FROM employees;\n\tEND //\n\n\nDELIMITER ;\n举例2：\n当市场环境不好时，公司为了渡过难关，决定暂时降低大家的薪资。声明存储过程“leave_while()”，声明OUT参数num，输出循环次数，存储过程中使用WHILE循环给大家降低薪资为原来薪资的90%，直到全公司的平均薪资小于等于10000，并统计循环次数。\nDELIMITER //\nCREATE PROCEDURE leave_while(OUT num INT)\n\nBEGIN \n\t#\n\tDECLARE avg_sal DOUBLE;#记录平均工资\n\tDECLARE while_count INT DEFAULT 0; #记录循环次数\n\t\n\tSELECT AVG(salary) INTO avg_sal FROM employees; #① 初始化条件\n\t\n\twhile_label:WHILE TRUE DO  #② 循环条件\n\t\t\n\t\t#③ 循环体\n\t\tIF avg_sal &lt;= 10000 THEN\n\t\t\tLEAVE while_label;\n\t\tEND IF;\n\t\t\n\t\tUPDATE employees SET salary  = salary * 0.9;\n\t\tSET while_count = while_count + 1;\n\t\t\n\t\t#④ 迭代条件\n\t\tSELECT AVG(salary) INTO avg_sal FROM employees;\n\t\n\tEND WHILE;\n\t\n\t#赋值\n\tSET num = while_count;\n\nEND //\n\nDELIMITER ;\n3.7 跳转语句之ITERATE语句ITERATE语句：只能用在循环语句（LOOP、REPEAT和WHILE语句）内，表示重新开始循环，将执行顺序转到语句段开头处。如果你有面向过程的编程语言的使用经验，你可以把 ITERATE 理解为 continue，意思为“再次循环”。\n语句基本格式如下：\nITERATE label\nlabel参数表示循环的标志。ITERATE语句必须跟在循环标志前面。\n举例：  定义局部变量num，初始值为0。循环结构中执行num + 1操作。\n\n如果num &lt; 10，则继续执行循环；\n如果num &gt; 15，则退出循环结构；\n\nDELIMITER //\n\nCREATE PROCEDURE test_iterate()\n\nBEGIN\n\tDECLARE num INT DEFAULT 0;\n\t\n\tmy_loop:LOOP\n\t\tSET num = num + 1;\n\t\n\t\tIF num &lt; 10 \n\t\t\tTHEN ITERATE my_loop;\n\t\tELSEIF num &gt; 15 \n\t\t\tTHEN LEAVE my_loop;\n\t\tEND IF;\n\t\n\t\tSELECT '尚硅谷：让天下没有难学的技术';\n\t\n\tEND LOOP my_loop;\n\nEND //\n\nDELIMITER ;\n4. 游标4.1 什么是游标（或光标）虽然我们也可以通过筛选条件 WHERE 和 HAVING，或者是限定返回记录的关键字 LIMIT 返回一条记录，但是，却无法在结果集中像指针一样，向前定位一条记录、向后定位一条记录，或者是随意定位到某一条记录，并对记录的数据进行处理。\n这个时候，就可以用到游标。游标，提供了一种灵活的操作方式，让我们能够对结果集中的每一条记录进行定位，并对指向的记录中的数据进行操作的数据结构。游标让 SQL 这种面向集合的语言有了面向过程开发的能力。\n在 SQL 中，游标是一种临时的数据库对象，可以指向存储在数据库表中的数据行指针。这里游标充当了指针的作用，我们可以通过操作游标来对数据行进行操作。\nMySQL中游标可以在存储过程和函数中使用。\n比如，我们查询了 employees 数据表中工资高于15000的员工都有哪些：\nSELECT employee_id,last_name,salary FROM employees\nWHERE salary &gt; 15000;\n \n这里我们就可以通过游标来操作数据行，如图所示此时游标所在的行是“108”的记录，我们也可以在结果集上滚动游标，指向结果集中的任意一行。\n4.2 使用游标步骤游标必须在声明处理程序之前被声明，并且变量和条件还必须在声明游标或处理程序之前被声明。\n如果我们想要使用游标，一般需要经历四个步骤。不同的 DBMS 中，使用游标的语法可能略有不同。\n第一步，声明游标\n在MySQL中，使用DECLARE关键字来声明游标，其语法的基本形式如下：\nDECLARE cursor_name CURSOR FOR select_statement; \n这个语法适用于 MySQL，SQL Server，DB2 和 MariaDB。如果是用 Oracle 或者 PostgreSQL，需要写成：\nDECLARE cursor_name CURSOR IS select_statement;\n要使用 SELECT 语句来获取数据结果集，而此时还没有开始遍历数据，这里 select_statement 代表的是 SELECT 语句，返回一个用于创建游标的结果集。\n比如：\nDECLARE cur_emp CURSOR FOR \nSELECT employee_id,salary FROM employees;\nDECLARE cursor_fruit CURSOR FOR \nSELECT f_name, f_price FROM fruits ;\n第二步，打开游标\n打开游标的语法如下：\nOPEN cursor_name\n当我们定义好游标之后，如果想要使用游标，必须先打开游标。打开游标的时候 SELECT 语句的查询结果集就会送到游标工作区，为后面游标的逐条读取结果集中的记录做准备。\nOPEN　cur_emp ;\n第三步，使用游标（从游标中取得数据）\n语法如下：\nFETCH cursor_name INTO var_name [, var_name] ...\n这句的作用是使用 cursor_name 这个游标来读取当前行，并且将数据保存到 var_name 这个变量中，游标指针指到下一行。如果游标读取的数据行有多个列名，则在 INTO 关键字后面赋值给多个变量名即可。\n注意：var_name必须在声明游标之前就定义好。\nFETCH　cur_emp INTO emp_id, emp_sal ;\n注意：游标的查询结果集中的字段数，必须跟 INTO 后面的变量数一致，否则，在存储过程执行的时候，MySQL 会提示错误。\n第四步，关闭游标\nCLOSE cursor_name\n有 OPEN 就会有 CLOSE，也就是打开和关闭游标。当我们使用完游标后需要关闭掉该游标。因为游标会占用系统资源，如果不及时关闭，游标会一直保持到存储过程结束，影响系统运行的效率。而关闭游标的操作，会释放游标占用的系统资源。\n关闭游标之后，我们就不能再检索查询结果中的数据行，如果需要检索只能再次打开游标。\nCLOSE　cur_emp;\n4.3 举例创建存储过程“get_count_by_limit_total_salary()”，声明IN参数 limit_total_salary，DOUBLE类型；声明OUT参数total_count，INT类型。函数的功能可以实现累加薪资最高的几个员工的薪资值，直到薪资总和达到limit_total_salary参数的值，返回累加的人数给total_count。\nDELIMITER //\n\nCREATE PROCEDURE get_count_by_limit_total_salary(IN limit_total_salary DOUBLE,OUT total_count INT)\n\nBEGIN\n\tDECLARE sum_salary DOUBLE DEFAULT 0;  #记录累加的总工资\n\tDECLARE cursor_salary DOUBLE DEFAULT 0; #记录某一个工资值\n\tDECLARE emp_count INT DEFAULT 0; #记录循环个数\n\t#定义游标\n\tDECLARE emp_cursor CURSOR FOR SELECT salary FROM employees ORDER BY salary DESC;\n\t#打开游标\n\tOPEN emp_cursor;\n\t\n\tREPEAT\n\t\t#使用游标（从游标中获取数据）\n\t\tFETCH emp_cursor INTO cursor_salary;\n\t\t\n\t\tSET sum_salary = sum_salary + cursor_salary;\n\t\tSET emp_count = emp_count + 1;\n\t\t\n\t\tUNTIL sum_salary &gt;= limit_total_salary\n\tEND REPEAT;\n\t\n\tSET total_count = emp_count;\n\t#关闭游标\n\tCLOSE emp_cursor;\n\t\nEND //\n\nDELIMITER ;\n4.5 小结游标是 MySQL 的一个重要的功能，为逐条读取结果集中的数据，提供了完美的解决方案。跟在应用层面实现相同的功能相比，游标可以在存储程序中使用，效率高，程序也更加简洁。\n但同时也会带来一些性能问题，比如在使用游标的过程中，会对数据行进行加锁，这样在业务并发量大的时候，不仅会影响业务之间的效率，还会消耗系统资源，造成内存不足，这是因为游标是在内存中进行的处理。\n建议：养成用完之后就关闭的习惯，这样才能提高系统的整体效率。\n补充：MySQL 8.0的新特性—全局变量的持久化在MySQL数据库中，全局变量可以通过SET GLOBAL语句来设置。例如，设置服务器语句超时的限制，可以通过设置系统变量max_execution_time来实现：\nSET GLOBAL MAX_EXECUTION_TIME=2000;\n使用SET GLOBAL语句设置的变量值只会临时生效。数据库重启后，服务器又会从MySQL配置文件中读取变量的默认值。MySQL 8.0版本新增了SET PERSIST命令。例如，设置服务器的最大连接数为1000：\nSET PERSIST global max_connections = 1000;\nMySQL会将该命令的配置保存到数据目录下的mysqld-auto.cnf文件中，下次启动时会读取该文件，用其中的配置来覆盖默认的配置文件。\n举例：\n查看全局变量max_connections的值，结果如下：\nmysql&gt; show variables like '%max_connections%';\n+------------------------+-------+\n| Variable_name          | Value |\n+------------------------+-------+\n| max_connections        | 151   |\n| mysqlx_max_connections | 100   |\n+------------------------+-------+\n2 rows in set, 1 warning (0.00 sec)\n设置全局变量max_connections的值：\nmysql&gt; set persist max_connections=1000;\nQuery OK, 0 rows affected (0.00 sec)\n重启MySQL服务器，再次查询max_connections的值：\nmysql&gt; show variables like '%max_connections%';\n+------------------------+-------+\n| Variable_name          | Value |\n+------------------------+-------+\n| max_connections        | 1000  |\n| mysqlx_max_connections | 100   |\n+------------------------+-------+\n2 rows in set, 1 warning (0.00 sec)\n第17章_触发器讲师：尚硅谷-宋红康（江湖人称：康师傅）\n官网：http://www.atguigu.com\n\n在实际开发中，我们经常会遇到这样的情况：有 2 个或者多个相互关联的表，如商品信息和库存信息分别存放在 2 个不同的数据表中，我们在添加一条新商品记录的时候，为了保证数据的完整性，必须同时在库存表中添加一条库存记录。\n这样一来，我们就必须把这两个关联的操作步骤写到程序里面，而且要用事务包裹起来，确保这两个操作成为一个原子操作，要么全部执行，要么全部不执行。要是遇到特殊情况，可能还需要对数据进行手动维护，这样就很容易忘记其中的一步，导致数据缺失。\n这个时候，咱们可以使用触发器。你可以创建一个触发器，让商品信息数据的插入操作自动触发库存数据的插入操作。这样一来，就不用担心因为忘记添加库存数据而导致的数据缺失了。\n1. 触发器概述MySQL从5.0.2版本开始支持触发器。MySQL的触发器和存储过程一样，都是嵌入到MySQL服务器的一段程序。\n触发器是由事件来触发某个操作，这些事件包括INSERT、UPDATE、DELETE事件。所谓事件就是指用户的动作或者触发某项行为。如果定义了触发程序，当数据库执行这些语句时候，就相当于事件发生了，就会自动激发触发器执行相应的操作。\n当对数据表中的数据执行插入、更新和删除操作，需要自动执行一些数据库逻辑时，可以使用触发器来实现。\n2. 触发器的创建2.1 创建触发器语法创建触发器的语法结构是：\nCREATE TRIGGER 触发器名称 \n{BEFORE|AFTER} {INSERT|UPDATE|DELETE} ON 表名 \nFOR EACH ROW \n触发器执行的语句块;\n说明：\n\n表名：表示触发器监控的对象。\nBEFORE|AFTER：表示触发的时间。BEFORE 表示在事件之前触发；AFTER 表示在事件之后触发。\nINSERT|UPDATE|DELETE：表示触发的事件。\nINSERT 表示插入记录时触发；\nUPDATE 表示更新记录时触发；\nDELETE 表示删除记录时触发。\n\n\n\n\n触发器执行的语句块：可以是单条SQL语句，也可以是由BEGIN…END结构组成的复合语句块。\n\n2.2 代码举例举例1：\n1、创建数据表：\nCREATE TABLE test_trigger (\nid INT PRIMARY KEY AUTO_INCREMENT,\nt_note VARCHAR(30)\n);\n\n\nCREATE TABLE test_trigger_log (\nid INT PRIMARY KEY AUTO_INCREMENT,\nt_log VARCHAR(30)\n);\n2、创建触发器：创建名称为before_insert的触发器，向test_trigger数据表插入数据之前，向test_trigger_log数据表中插入before_insert的日志信息。\nDELIMITER //\n\nCREATE TRIGGER before_insert\nBEFORE INSERT ON test_trigger \nFOR EACH ROW\nBEGIN\n\tINSERT INTO test_trigger_log (t_log)\n\tVALUES('before_insert');\n\nEND //\n\nDELIMITER ;\n3、向test_trigger数据表中插入数据\nINSERT INTO test_trigger (t_note) VALUES ('测试 BEFORE INSERT 触发器');\n4、查看test_trigger_log数据表中的数据\nmysql&gt; SELECT * FROM test_trigger_log;\n+----+---------------+\n| id | t_log         |\n+----+---------------+\n|  1 | before_insert |\n+----+---------------+\n1 row in set (0.00 sec)\n举例2：\n1、创建名称为after_insert的触发器，向test_trigger数据表插入数据之后，向test_trigger_log数据表中插入after_insert的日志信息。\nDELIMITER //\n\nCREATE TRIGGER after_insert\nAFTER INSERT ON test_trigger\nFOR EACH ROW\nBEGIN\n\tINSERT INTO test_trigger_log (t_log)\n\tVALUES('after_insert');\nEND //\n\nDELIMITER ;\n2、向test_trigger数据表中插入数据。\nINSERT INTO test_trigger (t_note) VALUES ('测试 AFTER INSERT 触发器');\n3、查看test_trigger_log数据表中的数据\nmysql&gt; SELECT * FROM test_trigger_log;\n+----+---------------+\n| id | t_log         |\n+----+---------------+\n|  1 | before_insert |\n|  2 | before_insert |\n|  3 | after_insert  |\n+----+---------------+\n3 rows in set (0.00 sec)\n举例3：定义触发器“salary_check_trigger”，基于员工表“employees”的INSERT事件，在INSERT之前检查将要添加的新员工薪资是否大于他领导的薪资，如果大于领导薪资，则报sqlstate_value为’HY000’的错误，从而使得添加失败。\nDELIMITER //\n\nCREATE TRIGGER salary_check_trigger\nBEFORE INSERT ON employees FOR EACH ROW\nBEGIN\n\tDECLARE mgrsalary DOUBLE;\n\tSELECT salary INTO mgrsalary FROM employees WHERE employee_id = NEW.manager_id;\n\n\tIF NEW.salary &gt; mgrsalary THEN\n\t\tSIGNAL SQLSTATE 'HY000' SET MESSAGE_TEXT = '薪资高于领导薪资错误';\n\tEND IF;\nEND //\n\nDELIMITER ;\n上面触发器声明过程中的NEW关键字代表INSERT添加语句的新记录。\n3. 查看、删除触发器3.1 查看触发器查看触发器是查看数据库中已经存在的触发器的定义、状态和语法信息等。\n方式1：查看当前数据库的所有触发器的定义\nSHOW TRIGGERS\\G\n方式2：查看当前数据库中某个触发器的定义\nSHOW CREATE TRIGGER 触发器名\n方式3：从系统库information_schema的TRIGGERS表中查询“salary_check_trigger”触发器的信息。\nSELECT * FROM information_schema.TRIGGERS;\n3.2 删除触发器触发器也是数据库对象，删除触发器也用DROP语句，语法格式如下：\nDROP TRIGGER  IF EXISTS 触发器名称;\n4. 触发器的优缺点4.1 优点1、触发器可以确保数据的完整性。\n假设我们用进货单头表（demo.importhead）来保存进货单的总体信息，包括进货单编号、供货商编号、仓库编号、总计进货数量、总计进货金额和验收日期。\n \n用进货单明细表（demo.importdetails）来保存进货商品的明细，包括进货单编号、商品编号、进货数量、进货价格和进货金额。\n \n每当我们录入、删除和修改一条进货单明细数据的时候，进货单明细表里的数据就会发生变动。这个时候，在进货单头表中的总计数量和总计金额就必须重新计算，否则，进货单头表中的总计数量和总计金额就不等于进货单明细表中数量合计和金额合计了，这就是数据不一致。\n为了解决这个问题，我们就可以使用触发器，规定每当进货单明细表有数据插入、修改和删除的操作时，自动触发 2 步操作：\n1）重新计算进货单明细表中的数量合计和金额合计；\n2）用第一步中计算出来的值更新进货单头表中的合计数量与合计金额。\n这样一来，进货单头表中的合计数量与合计金额的值，就始终与进货单明细表中计算出来的合计数量与合计金额的值相同，数据就是一致的，不会互相矛盾。\n2、触发器可以帮助我们记录操作日志。\n利用触发器，可以具体记录什么时间发生了什么。比如，记录修改会员储值金额的触发器，就是一个很好的例子。这对我们还原操作执行时的具体场景，更好地定位问题原因很有帮助。\n3、触发器还可以用在操作数据前，对数据进行合法性检查。\n比如，超市进货的时候，需要库管录入进货价格。但是，人为操作很容易犯错误，比如说在录入数量的时候，把条形码扫进去了；录入金额的时候，看串了行，录入的价格远超售价，导致账面上的巨亏……这些都可以通过触发器，在实际插入或者更新操作之前，对相应的数据进行检查，及时提示错误，防止错误数据进入系统。\n4.2 缺点1、触发器最大的一个问题就是可读性差。\n因为触发器存储在数据库中，并且由事件驱动，这就意味着触发器有可能不受应用层的控制。这对系统维护是非常有挑战的。\n比如，创建触发器用于修改会员储值操作。如果触发器中的操作出了问题，会导致会员储值金额更新失败。我用下面的代码演示一下：\nmysql&gt; update demo.membermaster set memberdeposit=20 where memberid = 2;\nERROR 1054 (42S22): Unknown column 'aa' in 'field list'\n结果显示，系统提示错误，字段“aa”不存在。\n这是因为，触发器中的数据插入操作多了一个字段，系统提示错误。可是，如果你不了解这个触发器，很可能会认为是更新语句本身的问题，或者是会员信息表的结构出了问题。说不定你还会给会员信息表添加一个叫“aa”的字段，试图解决这个问题，结果只能是白费力。\n2、相关数据的变更，可能会导致触发器出错。\n特别是数据表结构的变更，都可能会导致触发器出错，进而影响数据操作的正常运行。这些都会由于触发器本身的隐蔽性，影响到应用中错误原因排查的效率。\n4.3 注意点注意，如果在子表中定义了外键约束，并且外键指定了ON UPDATE/DELETE CASCADE/SET NULL子句，此时修改父表被引用的键值或删除父表被引用的记录行时，也会引起子表的修改和删除操作，此时基于子表的UPDATE和DELETE语句定义的触发器并不会被激活。\n例如：基于子表员工表（t_employee）的DELETE语句定义了触发器t1，而子表的部门编号（did）字段定义了外键约束引用了父表部门表（t_department）的主键列部门编号（did），并且该外键加了“ON DELETE SET NULL”子句，那么如果此时删除父表部门表（t_department）在子表员工表（t_employee）有匹配记录的部门记录时，会引起子表员工表（t_employee）匹配记录的部门编号（did）修改为NULL，但是此时不会激活触发器t1。只有直接对子表员工表（t_employee）执行DELETE语句时才会激活触发器t1。\nMySQL8新特性MySQL从5.7版本直接跳跃发布了8.0版本，可见这是一个令人兴奋的里程碑版本。MySQL 8版本在功能上做了显著的改进与增强，开发者对MySQL的源代码进行了重构，最突出的一点是多MySQL Optimizer优化器进行了改进。不仅在速度上得到了改善，还为用户带来了更好的性能和更棒的体验。\n1. 新特性概述1.1 MySQL8.0 新增特性1. 更简便的NoSQL支持NoSQL泛指非关系型数据库和数据存储。随着互联网平台的规模飞速发展，传统的关系型数据库已经越来越不能满足需求。从5.6版本开始，MySQL就开始支持简单的NoSQL存储功能。MySQL 8对这一功能做了优化，以更灵活的方式实现NoSQL功能，不再依赖模式（schema）。\n2. 更好的索引在查询中，正确地使用索引可以提高查询的效率。MySQL 8中新增了隐藏索引和降序索引。隐藏索引可以用来测试去掉索引对查询性能的影响。在查询中混合存在多列索引时，使用降序索引可以提高查询的性能。\n3.更完善的JSON支持MySQL从5.7开始支持原生JSON数据的存储，MySQL 8对这一功能做了优化，增加了聚合函数JSON_ARRAYAGG()和JSON_OBJECTAGG()，将参数聚合为JSON数组或对象，新增了行内操作符 -&gt;&gt;，是列路径运算符 -&gt;的增强，对JSON排序做了提升，并优化了JSON的更新操作。\n4.安全和账户管理MySQL 8中新增了caching_sha2_password 授权插件、角色、密码历史记录和FIPS模式支持，这些特性提高了数据库的安全性和性能，使数据库管理员能够更灵活地进行账户管理工作。\n5.InnoDB的变化InnoDB是MySQL默认的存储引擎，是事务型数据库的首选引擎，支持事务安全表（ACID），支持行锁定和外键。在MySQL 8 版本中，InnoDB在自增、索引、加密、死锁、共享锁等方面做了大量的改进和优化，并且支持原子数据定义语言（DDL），提高了数据安全性，对事务提供更好的支持。\n6.数据字典在之前的MySQL版本中，字典数据都存储在元数据文件和非事务表中。从MySQL 8开始新增了事务数据字典，在这个字典里存储着数据库对象信息，这些数据字典存储在内部事务表中。\n7. 原子数据定义语句MySQL 8开始支持原子数据定义语句（Automic DDL），即原子DDL。目前，只有InnoDB存储引擎支持原子DDL。原子数据定义语句（DDL）将与DDL操作相关的数据字典更新、存储引擎操作、二进制日志写入结合到一个单独的原子事务中，这使得即使服务器崩溃，事务也会提交或回滚。使用支持原子操作的存储引擎所创建的表，在执行DROP TABLE、CREATE TABLE、ALTER TABLE、 RENAME TABLE、TRUNCATE TABLE、CREATE TABLESPACE、DROP TABLESPACE等操作时，都支持原子操作，即事务要么完全操作成功，要么失败后回滚，不再进行部分提交。对于从MySQL 5.7复制到MySQL 8 版本中的语句，可以添加IF EXISTS或IF NOT EXISTS语句来避免发生错误。\n8.资源管理MySQL 8开始支持创建和管理资源组，允许将服务器内运行的线程分配给特定的分组，以便线程根据组内可用资源执行。组属性能够控制组内资源，启用或限制组内资源消耗。数据库管理员能够根据不同的工作负载适当地更改这些属性。目前，CPU时间是可控资源，由“虚拟CPU”这个概念来表示，此术语包含CPU的核心数，超线程，硬件线程等等。服务器在启动时确定可用的虚拟CPU数量。拥有对应权限的数据库管理员可以将这些CPU与资源组关联，并为资源组分配线程。资源组组件为MySQL中的资源组管理提供了SQL接口。资源组的属性用于定义资源组。MySQL中存在两个默认组，系统组和用户组，默认的组不能被删除，其属性也不能被更改。对于用户自定义的组，资源组创建时可初始化所有的属性，除去名字和类型，其他属性都可在创建之后进行更改。在一些平台下，或进行了某些MySQL的配置时，资源管理的功能将受到限制，甚至不可用。例如，如果安装了线程池插件，或者使用的是macOS系统，资源管理将处于不可用状态。在FreeBSD和Solaris系统中，资源线程优先级将失效。在Linux系统中，只有配置了CAP_SYS_NICE属性，资源管理优先级才能发挥作用。\n9.字符集支持MySQL 8中默认的字符集由latin1更改为utf8mb4，并首次增加了日语所特定使用的集合，utf8mb4_ja_0900_as_cs。\n10.优化器增强MySQL优化器开始支持隐藏索引和降序索引。隐藏索引不会被优化器使用，验证索引的必要性时不需要删除索引，先将索引隐藏，如果优化器性能无影响就可以真正地删除索引。降序索引允许优化器对多个列进行排序，并且允许排序顺序不一致。\n11.公用表表达式公用表表达式（Common Table Expressions）简称为CTE，MySQL现在支持递归和非递归两种形式的CTE。CTE通过在SELECT语句或其他特定语句前使用WITH语句对临时结果集进行命名。\n基础语法如下：\nWITH cte_name (col_name1,col_name2 ...) AS (Subquery)\nSELECT * FROM cte_name;\nSubquery代表子查询，子查询前使用WITH语句将结果集命名为cte_name，在后续的查询中即可使用cte_name进行查询。\n12.窗口函数MySQL 8开始支持窗口函数。在之前的版本中已存在的大部分聚合函数在MySQL 8中也可以作为窗口函数来使用。\n\n13.正则表达式支持MySQL在8.0.4以后的版本中采用支持Unicode的国际化组件库实现正则表达式操作，这种方式不仅能提供完全的Unicode支持，而且是多字节安全编码。MySQL增加了REGEXP_LIKE()、EGEXP_INSTR()、REGEXP_REPLACE()和 REGEXP_SUBSTR()等函数来提升性能。另外，regexp_stack_limit和regexp_time_limit 系统变量能够通过匹配引擎来控制资源消耗。\n14.内部临时表TempTable存储引擎取代MEMORY存储引擎成为内部临时表的默认存储引擎。TempTable存储引擎为VARCHAR和VARBINARY列提供高效存储。internal_tmp_mem_storage_engine会话变量定义了内部临时表的存储引擎，可选的值有两个，TempTable和MEMORY，其中TempTable为默认的存储引擎。temptable_max_ram系统配置项定义了TempTable存储引擎可使用的最大内存数量。\n15.日志记录在MySQL 8中错误日志子系统由一系列MySQL组件构成。这些组件的构成由系统变量log_error_services来配置，能够实现日志事件的过滤和写入。\n16.备份锁新的备份锁允许在线备份期间执行数据操作语句，同时阻止可能造成快照不一致的操作。新备份锁由 LOCK INSTANCE FOR BACKUP 和 UNLOCK INSTANCE 语法提供支持，执行这些操作需要备份管理员特权。\n17.增强的MySQL复制MySQL 8复制支持对JSON文档进行部分更新的二进制日志记录，该记录使用紧凑的二进制格式，从而节省记录完整JSON文档的空间。当使用基于语句的日志记录时，这种紧凑的日志记录会自动完成，并且可以通过将新的binlog_row_value_options系统变量值设置为PARTIAL_JSON来启用。\n1.2 MySQL8.0移除的旧特性在MySQL 5.7版本上开发的应用程序如果使用了MySQL8.0 移除的特性，语句可能会失败，或者产生不同的执行结果。为了避免这些问题，对于使用了移除特性的应用，应当尽力修正避免使用这些特性，并尽可能使用替代方法。\n1. 查询缓存查询缓存已被移除，删除的项有：（1）语句：FLUSH QUERY CACHE和RESET QUERY CACHE。（2）系统变量：query_cache_limit、query_cache_min_res_unit、query_cache_size、query_cache_type、query_cache_wlock_invalidate。（3）状态变量：Qcache_free_blocks、Qcache_free_memory、Qcache_hits、Qcache_inserts、Qcache_lowmem_prunes、Qcache_not_cached、Qcache_queries_in_cache、Qcache_total_blocks。（4）线程状态：checking privileges on cached query、checking query cache for query、invalidating query cache entries、sending cached result to client、storing result in query cache、waiting for query cache lock。\n2.加密相关删除的加密相关的内容有：ENCODE()、DECODE()、ENCRYPT()、DES_ENCRYPT()和DES_DECRYPT()函数，配置项des-key-file，系统变量have_crypt，FLUSH语句的DES_KEY_FILE选项，HAVE_CRYPT CMake选项。对于移除的ENCRYPT()函数，考虑使用SHA2()替代，对于其他移除的函数，使用AES_ENCRYPT()和AES_DECRYPT()替代。\n3.空间函数相关在MySQL 5.7版本中，多个空间函数已被标记为过时。这些过时函数在MySQL 8中都已被移除，只保留了对应的ST_和MBR函数。\n4.\\N和NULL在SQL语句中，解析器不再将\\N视为NULL，所以在SQL语句中应使用NULL代替\\N。这项变化不会影响使用LOAD DATA INFILE或者SELECT…INTO OUTFILE操作文件的导入和导出。在这类操作中，NULL仍等同于\\N。\n5. mysql_install_db在MySQL分布中，已移除了mysql_install_db程序，数据字典初始化需要调用带着—initialize或者—initialize-insecure选项的mysqld来代替实现。另外，—bootstrap和INSTALL_SCRIPTDIR CMake也已被删除。\n6.通用分区处理程序通用分区处理程序已从MySQL服务中被移除。为了实现给定表分区，表所使用的存储引擎需要自有的分区处理程序。提供本地分区支持的MySQL存储引擎有两个，即InnoDB和NDB，而在MySQL 8中只支持InnoDB。\n7.系统和状态变量信息在INFORMATION_SCHEMA数据库中，对系统和状态变量信息不再进行维护。GLOBAL_VARIABLES、SESSION_VARIABLES、GLOBAL_STATUS、SESSION_STATUS表都已被删除。另外，系统变量show_compatibility_56也已被删除。被删除的状态变量有Slave_heartbeat_period、Slave_last_heartbeat,Slave_received_heartbeats、Slave_retried_transactions、Slave_running。以上被删除的内容都可使用性能模式中对应的内容进行替代。\n8.mysql_plugin工具mysql_plugin工具用来配置MySQL服务器插件，现已被删除，可使用—plugin-load或—plugin-load-add选项在服务器启动时加载插件或者在运行时使用INSTALL PLUGIN语句加载插件来替代该工具。\n2. 新特性1：窗口函数2.1 使用窗口函数前后对比假设我现在有这样一个数据表，它显示了某购物网站在每个城市每个区的销售额：\nCREATE TABLE sales(\nid INT PRIMARY KEY AUTO_INCREMENT,\ncity VARCHAR(15),\ncounty VARCHAR(15),\nsales_value DECIMAL\n\n);\n\nINSERT INTO sales(city,county,sales_value)\nVALUES\n('北京','海淀',10.00),\n('北京','朝阳',20.00),\n('上海','黄埔',30.00),\n('上海','长宁',10.00);\n查询：\nmysql&gt; SELECT * FROM sales;\n+----+------+--------+-------------+\n| id | city | county | sales_value |\n+----+------+--------+-------------+\n|  1 | 北京 | 海淀    |          10 |\n|  2 | 北京 | 朝阳    |          20 |\n|  3 | 上海 | 黄埔    |          30 |\n|  4 | 上海 | 长宁    |          10 |\n+----+------+--------+-------------+\n4 rows in set (0.00 sec)\n需求：现在计算这个网站在每个城市的销售总额、在全国的销售总额、每个区的销售额占所在城市销售额中的比率，以及占总销售额中的比率。\n如果用分组和聚合函数，就需要分好几步来计算。\n第一步，计算总销售金额，并存入临时表 a：\nCREATE TEMPORARY TABLE a       -- 创建临时表\nSELECT SUM(sales_value) AS sales_value -- 计算总计金额\nFROM sales;\n查看一下临时表 a ：\nmysql&gt; SELECT * FROM a;\n+-------------+\n| sales_value |\n+-------------+\n|          70 |\n+-------------+\n1 row in set (0.00 sec)\n第二步，计算每个城市的销售总额并存入临时表 b：\nCREATE TEMPORARY TABLE b    -- 创建临时表\nSELECT city,SUM(sales_value) AS sales_value  -- 计算城市销售合计\nFROM sales\nGROUP BY city;\n查看临时表 b ：\nmysql&gt; SELECT * FROM b;\n+------+-------------+\n| city | sales_value |\n+------+-------------+\n| 北京 |          30 |\n| 上海 |          40 |\n+------+-------------+\n2 rows in set (0.00 sec)\n第三步，计算各区的销售占所在城市的总计金额的比例，和占全部销售总计金额的比例。我们可以通过下面的连接查询获得需要的结果：\nmysql&gt; SELECT s.city AS 城市,s.county AS 区,s.sales_value AS 区销售额,\n    -&gt; b.sales_value AS 市销售额,s.sales_value/b.sales_value AS 市比率,\n    -&gt; a.sales_value AS 总销售额,s.sales_value/a.sales_value AS 总比率\n    -&gt; FROM sales s\n    -&gt; JOIN b ON (s.city=b.city) -- 连接市统计结果临时表\n    -&gt; JOIN a                   -- 连接总计金额临时表\n    -&gt; ORDER BY s.city,s.county;\n+------+------+----------+----------+--------+----------+--------+\n| 城市  | 区   | 区销售额  | 市销售额   | 市比率  | 总销售额  | 总比率  |\n+------+------+----------+----------+--------+----------+--------+\n| 上海  | 长宁 |       10 |       40 | 0.2500 |       70 | 0.1429 |\n| 上海  | 黄埔 |       30 |       40 | 0.7500 |       70 | 0.4286 |\n| 北京  | 朝阳 |       20 |       30 | 0.6667 |       70 | 0.2857 |\n| 北京  | 海淀 |       10 |       30 | 0.3333 |       70 | 0.1429 |\n+------+------+----------+----------+--------+----------+--------+\n4 rows in set (0.00 sec)\n结果显示：市销售金额、市销售占比、总销售金额、总销售占比都计算出来了。\n同样的查询，如果用窗口函数，就简单多了。我们可以用下面的代码来实现：\nmysql&gt; SELECT city AS 城市,county AS 区,sales_value AS 区销售额,\n    -&gt; SUM(sales_value) OVER(PARTITION BY city) AS 市销售额,  -- 计算市销售额\n    -&gt; sales_value/SUM(sales_value) OVER(PARTITION BY city) AS 市比率,\n    -&gt; SUM(sales_value) OVER() AS 总销售额,   -- 计算总销售额\n    -&gt; sales_value/SUM(sales_value) OVER() AS 总比率\n    -&gt; FROM sales\n    -&gt; ORDER BY city,county;\n+------+------+----------+----------+--------+----------+--------+\n| 城市  | 区   | 区销售额  | 市销售额   | 市比率  | 总销售额  | 总比率  |\n+------+------+----------+----------+--------+----------+--------+\n| 上海  | 长宁 |       10 |       40  | 0.2500 |       70 | 0.1429 |\n| 上海  | 黄埔 |       30 |       40  | 0.7500 |       70 | 0.4286 |\n| 北京  | 朝阳 |       20 |       30  | 0.6667 |       70 | 0.2857 |\n| 北京  | 海淀 |       10 |       30  | 0.3333 |       70 | 0.1429 |\n+------+------+----------+-----------+--------+----------+--------+\n4 rows in set (0.00 sec)\n结果显示，我们得到了与上面那种查询同样的结果。\n使用窗口函数，只用了一步就完成了查询。而且，由于没有用到临时表，执行的效率也更高了。很显然，在这种需要用到分组统计的结果对每一条记录进行计算的场景下，使用窗口函数更好。\n2.2 窗口函数分类MySQL从8.0版本开始支持窗口函数。窗口函数的作用类似于在查询中对数据进行分组，不同的是，分组操作会把分组的结果聚合成一条记录，而窗口函数是将结果置于每一条数据记录中。\n窗口函数可以分为静态窗口函数和动态窗口函数。\n\n静态窗口函数的窗口大小是固定的，不会因为记录的不同而不同；\n动态窗口函数的窗口大小会随着记录的不同而变化。\n\nMySQL官方网站窗口函数的网址为https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html#function_row-number。\n窗口函数总体上可以分为序号函数、分布函数、前后函数、首尾函数和其他函数，如下表：\n\n2.3 语法结构窗口函数的语法结构是：\n函数 OVER（[PARTITION BY 字段名 ORDER BY 字段名 ASC|DESC]）\n或者是：\n函数 OVER 窗口名 … WINDOW 窗口名 AS （[PARTITION BY 字段名 ORDER BY 字段名 ASC|DESC]）\n\nOVER 关键字指定函数窗口的范围。\n如果省略后面括号中的内容，则窗口会包含满足WHERE条件的所有记录，窗口函数会基于所有满足WHERE条件的记录进行计算。\n如果OVER关键字后面的括号不为空，则可以使用如下语法设置窗口。\n\n\n窗口名：为窗口设置一个别名，用来标识窗口。\nPARTITION BY子句：指定窗口函数按照哪些字段进行分组。分组后，窗口函数可以在每个分组中分别执行。\nORDER BY子句：指定窗口函数按照哪些字段进行排序。执行排序操作使窗口函数按照排序后的数据记录的顺序进行编号。\nFRAME子句：为分区中的某个子集定义规则，可以用来作为滑动窗口使用。\n\n2.4 分类讲解创建表：\nCREATE TABLE goods(\nid INT PRIMARY KEY AUTO_INCREMENT,\ncategory_id INT,\ncategory VARCHAR(15),\nNAME VARCHAR(30),\nprice DECIMAL(10,2),\nstock INT,\nupper_time DATETIME\n\n);\n添加数据：\nINSERT INTO goods(category_id,category,NAME,price,stock,upper_time)\nVALUES\n(1, '女装/女士精品', 'T恤', 39.90, 1000, '2020-11-10 00:00:00'),\n(1, '女装/女士精品', '连衣裙', 79.90, 2500, '2020-11-10 00:00:00'),\n(1, '女装/女士精品', '卫衣', 89.90, 1500, '2020-11-10 00:00:00'),\n(1, '女装/女士精品', '牛仔裤', 89.90, 3500, '2020-11-10 00:00:00'),\n(1, '女装/女士精品', '百褶裙', 29.90, 500, '2020-11-10 00:00:00'),\n(1, '女装/女士精品', '呢绒外套', 399.90, 1200, '2020-11-10 00:00:00'),\n(2, '户外运动', '自行车', 399.90, 1000, '2020-11-10 00:00:00'),\n(2, '户外运动', '山地自行车', 1399.90, 2500, '2020-11-10 00:00:00'),\n(2, '户外运动', '登山杖', 59.90, 1500, '2020-11-10 00:00:00'),\n(2, '户外运动', '骑行装备', 399.90, 3500, '2020-11-10 00:00:00'),\n(2, '户外运动', '运动外套', 799.90, 500, '2020-11-10 00:00:00'),\n(2, '户外运动', '滑板', 499.90, 1200, '2020-11-10 00:00:00');\n下面针对goods表中的数据来验证每个窗口函数的功能。\n1. 序号函数1．ROW_NUMBER()函数\nROW_NUMBER()函数能够对数据中的序号进行顺序显示。\n举例：查询 goods 数据表中每个商品分类下价格降序排列的各个商品信息。\nmysql&gt; SELECT ROW_NUMBER() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num,\n    -&gt; id, category_id, category, NAME, price, stock\n    -&gt; FROM goods;\n+---------+----+-------------+---------------+------------+---------+-------+\n| row_num | id | category_id | category      | NAME       | price   | stock |\n+---------+----+-------------+---------------+------------+---------+-------+\n|       1 |  6 |           1 | 女装/女士精品   | 呢绒外套    |  399.90 |  1200 |\n|       2 |  3 |           1 | 女装/女士精品   | 卫衣        |   89.90 |  1500 |\n|       3 |  4 |           1 | 女装/女士精品   | 牛仔裤      |   89.90 |  3500 |\n|       4 |  2 |           1 | 女装/女士精品   | 连衣裙      |   79.90 |  2500 |\n|       5 |  1 |           1 | 女装/女士精品   | T恤        |   39.90 |  1000 |\n|       6 |  5 |           1 | 女装/女士精品   | 百褶裙      |   29.90 |   500 |\n|       1 |  8 |           2 | 户外运动       | 山地自行车   | 1399.90 |  2500 |\n|       2 | 11 |           2 | 户外运动       | 运动外套     |  799.90 |   500 |\n|       3 | 12 |           2 | 户外运动       | 滑板        |  499.90 |  1200 |\n|       4 |  7 |           2 | 户外运动       | 自行车      |  399.90 |  1000 |\n|       5 | 10 |           2 | 户外运动       | 骑行装备    |  399.90 |  3500 |\n|       6 |  9 |           2 | 户外运动       | 登山杖      |   59.90 |  1500 |\n+---------+----+-------------+---------------+------------+---------+-------+\n12 rows in set (0.00 sec)\n举例：查询 goods 数据表中每个商品分类下价格最高的3种商品信息。\nmysql&gt; SELECT *\n    -&gt; FROM (\n    -&gt;  SELECT ROW_NUMBER() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num,\n    -&gt;  id, category_id, category, NAME, price, stock\n    -&gt;  FROM goods) t\n    -&gt; WHERE row_num &lt;= 3;\n+---------+----+-------------+---------------+------------+---------+-------+\n| row_num | id | category_id | category      | NAME       | price   | stock |\n+---------+----+-------------+---------------+------------+---------+-------+\n|       1 |  6 |           1 | 女装/女士精品   | 呢绒外套    |  399.90  |  1200 |\n|       2 |  3 |           1 | 女装/女士精品   | 卫衣        |   89.90 |  1500 |\n|       3 |  4 |           1 | 女装/女士精品   | 牛仔裤      |   89.90  |  3500 |\n|       1 |  8 |           2 | 户外运动       | 山地自行车   | 1399.90  |  2500 |\n|       2 | 11 |           2 | 户外运动       | 运动外套     |  799.90  |   500 |\n|       3 | 12 |           2 | 户外运动       | 滑板        |  499.90  |  1200 |\n+---------+----+-------------+---------------+------------+----------+-------+\n6 rows in set (0.00 sec)\n在名称为“女装/女士精品”的商品类别中，有两款商品的价格为89.90元，分别是卫衣和牛仔裤。两款商品的序号都应该为2，而不是一个为2，另一个为3。此时，可以使用RANK()函数和DENSE_RANK()函数解决。\n2．RANK()函数\n使用RANK()函数能够对序号进行并列排序，并且会跳过重复的序号，比如序号为1、1、3。\n举例：使用RANK()函数获取 goods 数据表中各类别的价格从高到低排序的各商品信息。\nmysql&gt; SELECT RANK() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num,\n    -&gt; id, category_id, category, NAME, price, stock\n    -&gt; FROM goods;\n+---------+----+-------------+---------------+------------+---------+-------+\n| row_num | id | category_id | category      | NAME       | price   | stock |\n+---------+----+-------------+---------------+------------+---------+-------+\n|       1 |  6 |           1 | 女装/女士精品   | 呢绒外套    |  399.90 |  1200 |\n|       2 |  3 |           1 | 女装/女士精品   | 卫衣        |   89.90 |  1500 |\n|       2 |  4 |           1 | 女装/女士精品   | 牛仔裤      |   89.90 |  3500 |\n|       4 |  2 |           1 | 女装/女士精品   | 连衣裙      |   79.90 |  2500 |\n|       5 |  1 |           1 | 女装/女士精品   | T恤         |   39.90 |  1000 |\n|       6 |  5 |           1 | 女装/女士精品   | 百褶裙      |   29.90 |   500 |\n|       1 |  8 |           2 | 户外运动       | 山地自行车   | 1399.90 |  2500 |\n|       2 | 11 |           2 | 户外运动       | 运动外套     |  799.90 |   500 |\n|       3 | 12 |           2 | 户外运动       | 滑板        |  499.90 |  1200 |\n|       4 |  7 |           2 | 户外运动       | 自行车      |  399.90 |  1000 |\n|       4 | 10 |           2 | 户外运动       | 骑行装备    |  399.90 |  3500 |\n|       6 |  9 |           2 | 户外运动       | 登山杖      |   59.90 |  1500 |\n+---------+----+-------------+---------------+------------+---------+-------+\n12 rows in set (0.00 sec)\n举例：使用RANK()函数获取 goods 数据表中类别为“女装/女士精品”的价格最高的4款商品信息。\nmysql&gt; SELECT *\n    -&gt; FROM(\n    -&gt;  SELECT RANK() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num,\n    -&gt;  id, category_id, category, NAME, price, stock\n    -&gt;  FROM goods) t\n    -&gt; WHERE category_id = 1 AND row_num &lt;= 4;\n+---------+----+-------------+---------------+----------+--------+-------+\n| row_num | id | category_id | category      | NAME     | price  | stock |\n+---------+----+-------------+---------------+----------+--------+-------+\n|       1 |  6 |           1 | 女装/女士精品   | 呢绒外套  | 399.90 |  1200 |\n|       2 |  3 |           1 | 女装/女士精品   | 卫衣      |  89.90 |  1500 |\n|       2 |  4 |           1 | 女装/女士精品   | 牛仔裤    |  89.90 |  3500 |\n|       4 |  2 |           1 | 女装/女士精品   | 连衣裙    |  79.90 |  2500 |\n+---------+----+-------------+---------------+----------+--------+-------+\n4 rows in set (0.00 sec)\n可以看到，使用RANK()函数得出的序号为1、2、2、4，相同价格的商品序号相同，后面的商品序号是不连续的，跳过了重复的序号。\n3．DENSE_RANK()函数\nDENSE_RANK()函数对序号进行并列排序，并且不会跳过重复的序号，比如序号为1、1、2。\n举例：使用DENSE_RANK()函数获取 goods 数据表中各类别的价格从高到低排序的各商品信息。\nmysql&gt; SELECT DENSE_RANK() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num,\n    -&gt; id, category_id, category, NAME, price, stock\n    -&gt; FROM goods;\n+---------+----+-------------+---------------+------------+---------+-------+\n| row_num | id | category_id | category      | NAME       | price   | stock |\n+---------+----+-------------+---------------+------------+---------+-------+\n|       1 |  6 |           1 | 女装/女士精品   | 呢绒外套    |  399.90 |  1200 |\n|       2 |  3 |           1 | 女装/女士精品   | 卫衣        |   89.90 |  1500 |\n|       2 |  4 |           1 | 女装/女士精品   | 牛仔裤      |   89.90 |  3500 |\n|       3 |  2 |           1 | 女装/女士精品   | 连衣裙      |   79.90 |  2500 |\n|       4 |  1 |           1 | 女装/女士精品   | T恤        |   39.90 |  1000 |\n|       5 |  5 |           1 | 女装/女士精品   | 百褶裙      |   29.90 |   500 |\n|       1 |  8 |           2 | 户外运动       | 山地自行车   | 1399.90 |  2500 |\n|       2 | 11 |           2 | 户外运动       | 运动外套     |  799.90 |   500 |\n|       3 | 12 |           2 | 户外运动       | 滑板        |  499.90 |  1200 |\n|       4 |  7 |           2 | 户外运动       | 自行车       |  399.90 |  1000 |\n|       4 | 10 |           2 | 户外运动       | 骑行装备     |  399.90 |  3500 |\n|       5 |  9 |           2 | 户外运动       | 登山杖       |   59.90 |  1500 |\n+---------+----+-------------+---------------+------------+---------+-------+\n12 rows in set (0.00 sec)\n举例：使用DENSE_RANK()函数获取 goods 数据表中类别为“女装/女士精品”的价格最高的4款商品信息。\nmysql&gt; SELECT *\n    -&gt; FROM(\n    -&gt;  SELECT DENSE_RANK() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num,\n    -&gt;  id, category_id, category, NAME, price, stock\n    -&gt;  FROM goods) t\n    -&gt; WHERE category_id = 1 AND row_num &lt;= 3;\n+---------+----+-------------+---------------+----------+--------+-------+\n| row_num | id | category_id | category      | NAME     | price  | stock |\n+---------+----+-------------+---------------+----------+--------+-------+\n|       1 |  6 |           1 | 女装/女士精品   | 呢绒外套  | 399.90 |  1200 |\n|       2 |  3 |           1 | 女装/女士精品   | 卫衣     |  89.90  |  1500 |\n|       2 |  4 |           1 | 女装/女士精品   | 牛仔裤    |  89.90 |  3500 |\n|       3 |  2 |           1 | 女装/女士精品   | 连衣裙    |  79.90 |  2500 |\n+---------+----+-------------+---------------+----------+--------+-------+\n4 rows in set (0.00 sec)\n可以看到，使用DENSE_RANK()函数得出的行号为1、2、2、3，相同价格的商品序号相同，后面的商品序号是连续的，并且没有跳过重复的序号。\n2. 分布函数1．PERCENT_RANK()函数\nPERCENT_RANK()函数是等级值百分比函数。按照如下方式进行计算。\n(rank - 1) / (rows - 1)\n其中，rank的值为使用RANK()函数产生的序号，rows的值为当前窗口的总记录数。\n举例：计算 goods 数据表中名称为“女装/女士精品”的类别下的商品的PERCENT_RANK值。\n#写法一：\nSELECT RANK() OVER (PARTITION BY category_id ORDER BY price DESC) AS r,\nPERCENT_RANK() OVER (PARTITION BY category_id ORDER BY price DESC) AS pr,\nid, category_id, category, NAME, price, stock\nFROM goods\nWHERE category_id = 1;\n\n#写法二：\nmysql&gt; SELECT RANK() OVER w AS r,\n    -&gt; PERCENT_RANK() OVER w AS pr,\n    -&gt; id, category_id, category, NAME, price, stock\n    -&gt; FROM goods\n    -&gt; WHERE category_id = 1 WINDOW w AS (PARTITION BY category_id ORDER BY price DESC);\n+---+-----+----+-------------+---------------+----------+--------+-------+\n| r | pr  | id | category_id | category      | NAME     | price  | stock |\n+---+-----+----+-------------+---------------+----------+--------+-------+\n| 1 |   0 |  6 |           1 | 女装/女士精品   | 呢绒外套  | 399.90 |  1200 |\n| 2 | 0.2 |  3 |           1 | 女装/女士精品   | 卫衣     |  89.90 |  1500 |\n| 2 | 0.2 |  4 |           1 | 女装/女士精品   | 牛仔裤   |  89.90 |  3500  |\n| 4 | 0.6 |  2 |           1 | 女装/女士精品   | 连衣裙   |  79.90 |  2500  |\n| 5 | 0.8 |  1 |           1 | 女装/女士精品   | T恤      |  39.90 |  1000 |\n| 6 |   1 |  5 |           1 | 女装/女士精品   | 百褶裙   |  29.90  |   500 |\n+---+-----+----+-------------+---------------+----------+--------+-------+\n6 rows in set (0.00 sec)\n2．CUME_DIST()函数\nCUME_DIST()函数主要用于查询小于或等于某个值的比例。\n举例：查询goods数据表中小于或等于当前价格的比例。\nmysql&gt; SELECT CUME_DIST() OVER(PARTITION BY category_id ORDER BY price ASC) AS cd,\n    -&gt; id, category, NAME, price\n    -&gt; FROM goods;\n+---------------------+----+---------------+------------+---------+\n| cd                  | id | category      | NAME       | price   |\n+---------------------+----+---------------+------------+---------+\n| 0.16666666666666666 |  5 | 女装/女士精品   | 百褶裙      |   29.90 |\n|  0.3333333333333333 |  1 | 女装/女士精品   | T恤        |   39.90 |\n|                 0.5 |  2 | 女装/女士精品   | 连衣裙      |   79.90 |\n|  0.8333333333333334 |  3 | 女装/女士精品   | 卫衣        |   89.90 |\n|  0.8333333333333334 |  4 | 女装/女士精品   | 牛仔裤      |   89.90 |\n|                   1 |  6 | 女装/女士精品   | 呢绒外套    |  399.90 |\n| 0.16666666666666666 |  9 | 户外运动       | 登山杖      |   59.90 |\n|                 0.5 |  7 | 户外运动       | 自行车      |  399.90 |\n|                 0.5 | 10 | 户外运动       | 骑行装备     |  399.90 |\n|  0.6666666666666666 | 12 | 户外运动       | 滑板        |  499.90 |\n|  0.8333333333333334 | 11 | 户外运动       | 运动外套    |  799.90 |\n|                   1 |  8 | 户外运动       | 山地自行车   | 1399.90 |\n+---------------------+----+---------------+------------+---------+\n12 rows in set (0.00 sec)\n3. 前后函数1．LAG(expr,n)函数\nLAG(expr,n)函数返回当前行的前n行的expr的值。\n举例：查询goods数据表中前一个商品价格与当前商品价格的差值。\nmysql&gt; SELECT id, category, NAME, price, pre_price, price - pre_price AS diff_price\n    -&gt; FROM (\n    -&gt;  SELECT  id, category, NAME, price,LAG(price,1) OVER w AS pre_price\n    -&gt;  FROM goods\n    -&gt;  WINDOW w AS (PARTITION BY category_id ORDER BY price)) t;\n+----+---------------+------------+---------+-----------+------------+\n| id | category      | NAME       | price   | pre_price | diff_price |\n+----+---------------+------------+---------+-----------+------------+\n|  5 | 女装/女士精品   | 百褶裙      |   29.90 |      NULL |       NULL |\n|  1 | 女装/女士精品   | T恤        |   39.90 |     29.90 |      10.00 |\n|  2 | 女装/女士精品   | 连衣裙      |   79.90 |     39.90 |      40.00 |\n|  3 | 女装/女士精品   | 卫衣       |   89.90 |     79.90 |      10.00 |\n|  4 | 女装/女士精品   | 牛仔裤      |   89.90 |     89.90 |       0.00 |\n|  6 | 女装/女士精品   | 呢绒外套    |  399.90 |     89.90 |     310.00 |\n|  9 | 户外运动       | 登山杖      |   59.90 |      NULL |       NULL |\n|  7 | 户外运动       | 自行车      |  399.90 |     59.90 |     340.00 |\n| 10 | 户外运动       | 骑行装备    |  399.90 |    399.90 |       0.00 |\n| 12 | 户外运动       | 滑板       |  499.90 |    399.90 |     100.00 |\n| 11 | 户外运动       | 运动外套    |  799.90 |    499.90 |     300.00 |\n|  8 | 户外运动       | 山地自行车  | 1399.90 |    799.90 |     600.00 |\n+----+---------------+------------+---------+-----------+------------+\n12 rows in set (0.00 sec)\n2．LEAD(expr,n)函数\nLEAD(expr,n)函数返回当前行的后n行的expr的值。\n举例：查询goods数据表中后一个商品价格与当前商品价格的差值。\nmysql&gt; SELECT id, category, NAME, behind_price, price,behind_price - price AS diff_price\n    -&gt; FROM(\n    -&gt;  SELECT id, category, NAME, price,LEAD(price, 1) OVER w AS behind_price\n    -&gt;  FROM goods WINDOW w AS (PARTITION BY category_id ORDER BY price)) t;\n+----+---------------+------------+--------------+---------+------------+\n| id | category      | NAME       | behind_price | price   | diff_price |\n+----+---------------+------------+--------------+---------+------------+\n|  5 | 女装/女士精品   | 百褶裙      |        39.90 |   29.90 |      10.00 |\n|  1 | 女装/女士精品   | T恤        |        79.90 |   39.90 |      40.00 |\n|  2 | 女装/女士精品   | 连衣裙      |        89.90 |   79.90 |      10.00 |\n|  3 | 女装/女士精品   | 卫衣        |        89.90 |   89.90 |       0.00 |\n|  4 | 女装/女士精品   | 牛仔裤      |       399.90 |   89.90 |     310.00 |\n|  6 | 女装/女士精品   | 呢绒外套     |         NULL |  399.90 |       NULL |\n|  9 | 户外运动       | 登山杖       |       399.90 |   59.90 |     340.00 |\n|  7 | 户外运动       | 自行车       |       399.90 |  399.90 |       0.00 |\n| 10 | 户外运动       | 骑行装备     |       499.90 |  399.90 |     100.00 |\n| 12 | 户外运动       | 滑板        |       799.90 |  499.90 |     300.00 |\n| 11 | 户外运动       | 运动外套     |      1399.90 |  799.90 |     600.00 |\n|  8 | 户外运动       | 山地自行车   |         NULL | 1399.90 |       NULL |\n+----+---------------+------------+--------------+---------+------------+\n12 rows in set (0.00 sec)\n4. 首尾函数1．FIRST_VALUE(expr)函数\nFIRST_VALUE(expr)函数返回第一个expr的值。\n举例：按照价格排序，查询第1个商品的价格信息。\nmysql&gt; SELECT id, category, NAME, price, stock,FIRST_VALUE(price) OVER w AS first_price\n    -&gt; FROM goods WINDOW w AS (PARTITION BY category_id ORDER BY price);\n+----+---------------+------------+---------+-------+-------------+\n| id | category      | NAME       | price   | stock | first_price |\n+----+---------------+------------+---------+-------+-------------+\n|  5 | 女装/女士精品   | 百褶裙      |   29.90 |   500 |       29.90 |\n|  1 | 女装/女士精品   | T恤        |   39.90 |  1000 |       29.90 |\n|  2 | 女装/女士精品   | 连衣裙      |   79.90 |  2500 |       29.90 |\n|  3 | 女装/女士精品   | 卫衣       |   89.90 |  1500 |       29.90 |\n|  4 | 女装/女士精品   | 牛仔裤      |   89.90 |  3500 |       29.90 |\n|  6 | 女装/女士精品   | 呢绒外套    |  399.90 |  1200 |       29.90 |\n|  9 | 户外运动       | 登山杖      |   59.90 |  1500 |       59.90 |\n|  7 | 户外运动       | 自行车      |  399.90 |  1000 |       59.90 |\n| 10 | 户外运动       | 骑行装备    |  399.90 |  3500 |       59.90 |\n| 12 | 户外运动       | 滑板       |  499.90 |  1200 |       59.90 |\n| 11 | 户外运动       | 运动外套    |  799.90 |   500 |       59.90 |\n|  8 | 户外运动       | 山地自行车  | 1399.90 |  2500 |       59.90 |\n+----+---------------+------------+---------+-------+-------------+\n12 rows in set (0.00 sec)\n2．LAST_VALUE(expr)函数\nLAST_VALUE(expr)函数返回最后一个expr的值。\n举例：按照价格排序，查询最后一个商品的价格信息。\nmysql&gt; SELECT id, category, NAME, price, stock,LAST_VALUE(price) OVER w AS last_price\n    -&gt; FROM goods WINDOW w AS (PARTITION BY category_id ORDER BY price);\n+----+---------------+------------+---------+-------+------------+\n| id | category      | NAME       | price   | stock | last_price |\n+----+---------------+------------+---------+-------+------------+\n|  5 | 女装/女士精品   | 百褶裙     |   29.90 |   500 |      29.90 |\n|  1 | 女装/女士精品   | T恤        |   39.90 |  1000 |      39.90 |\n|  2 | 女装/女士精品   | 连衣裙     |   79.90 |  2500 |      79.90 |\n|  3 | 女装/女士精品   | 卫衣       |   89.90 |  1500 |      89.90 |\n|  4 | 女装/女士精品   | 牛仔裤     |   89.90 |  3500 |      89.90 |\n|  6 | 女装/女士精品   | 呢绒外套   |  399.90 |  1200 |     399.90 |\n|  9 | 户外运动       | 登山杖     |   59.90 |  1500 |      59.90 |\n|  7 | 户外运动       | 自行车     |  399.90 |  1000 |     399.90 |\n| 10 | 户外运动       | 骑行装备   |  399.90 |  3500 |     399.90 |\n| 12 | 户外运动       | 滑板       |  499.90 |  1200 |     499.90 |\n| 11 | 户外运动       | 运动外套   |  799.90 |   500 |     799.90 |\n|  8 | 户外运动       | 山地自行车 | 1399.90 |  2500 |    1399.90 |\n+----+---------------+------------+---------+-------+------------+\n12 rows in set (0.00 sec)\n5. 其他函数1．NTH_VALUE(expr,n)函数\nNTH_VALUE(expr,n)函数返回第n个expr的值。\n举例：查询goods数据表中排名第2和第3的价格信息。\nmysql&gt; SELECT id, category, NAME, price,NTH_VALUE(price,2) OVER w AS second_price,\n    -&gt; NTH_VALUE(price,3) OVER w AS third_price\n    -&gt; FROM goods WINDOW w AS (PARTITION BY category_id ORDER BY price);\n+----+---------------+------------+---------+--------------+-------------+\n| id | category      | NAME       | price   | second_price | third_price |\n+----+---------------+------------+---------+--------------+-------------+\n|  5 | 女装/女士精品   | 百褶裙     |   29.90 |         NULL |        NULL |\n|  1 | 女装/女士精品   | T恤        |   39.90 |        39.90 |        NULL |\n|  2 | 女装/女士精品   | 连衣裙     |   79.90 |        39.90 |       79.90 |\n|  3 | 女装/女士精品   | 卫衣       |   89.90 |        39.90 |       79.90 |\n|  4 | 女装/女士精品   | 牛仔裤     |   89.90 |        39.90 |       79.90 |\n|  6 | 女装/女士精品   | 呢绒外套   |  399.90 |        39.90 |       79.90 |\n|  9 | 户外运动       | 登山杖     |   59.90 |         NULL |        NULL |\n|  7 | 户外运动       | 自行车     |  399.90 |       399.90 |      399.90 |\n| 10 | 户外运动       | 骑行装备   |  399.90 |       399.90 |      399.90 |\n| 12 | 户外运动       | 滑板       |  499.90 |       399.90 |      399.90 |\n| 11 | 户外运动       | 运动外套   |  799.90 |       399.90 |      399.90 |\n|  8 | 户外运动       | 山地自行车 | 1399.90 |       399.90 |      399.90 |\n+----+---------------+------------+---------+--------------+-------------+\n12 rows in set (0.00 sec)\n2．NTILE(n)函数\nNTILE(n)函数将分区中的有序数据分为n个桶，记录桶编号。\n举例：将goods表中的商品按照价格分为3组。\nmysql&gt; SELECT NTILE(3) OVER w AS nt,id, category, NAME, price\n    -&gt; FROM goods WINDOW w AS (PARTITION BY category_id ORDER BY price);\n+----+----+---------------+------------+---------+\n| nt | id | category      | NAME       | price   |\n+----+----+---------------+------------+---------+\n|  1 |  5 | 女装/女士精品 | 百褶裙     |   29.90 |\n|  1 |  1 | 女装/女士精品 | T恤        |   39.90 |\n|  2 |  2 | 女装/女士精品 | 连衣裙     |   79.90 |\n|  2 |  3 | 女装/女士精品 | 卫衣       |   89.90 |\n|  3 |  4 | 女装/女士精品 | 牛仔裤     |   89.90 |\n|  3 |  6 | 女装/女士精品 | 呢绒外套   |  399.90 |\n|  1 |  9 | 户外运动      | 登山杖     |   59.90 |\n|  1 |  7 | 户外运动      | 自行车     |  399.90 |\n|  2 | 10 | 户外运动      | 骑行装备   |  399.90 |\n|  2 | 12 | 户外运动      | 滑板       |  499.90 |\n|  3 | 11 | 户外运动      | 运动外套   |  799.90 |\n|  3 |  8 | 户外运动      | 山地自行车 | 1399.90 |\n+----+----+---------------+------------+---------+\n12 rows in set (0.00 sec)\n2.5 小 结窗口函数的特点是可以分组，而且可以在分组内排序。另外，窗口函数不会因为分组而减少原表中的行数，这对我们在原表数据的基础上进行统计和排序非常有用。\n3. 新特性2：公用表表达式公用表表达式（或通用表表达式）简称为CTE（Common Table Expressions）。CTE是一个命名的临时结果集，作用范围是当前语句。CTE可以理解成一个可以复用的子查询，当然跟子查询还是有点区别的，CTE可以引用其他CTE，但子查询不能引用其他子查询。所以，可以考虑代替子查询。\n依据语法结构和执行方式的不同，公用表表达式分为普通公用表表达式和递归公用表表达式 2 种。\n3.1 普通公用表表达式普通公用表表达式的语法结构是：\nWITH CTE名称 \nAS （子查询）\nSELECT|DELETE|UPDATE 语句;\n普通公用表表达式类似于子查询，不过，跟子查询不同的是，它可以被多次引用，而且可以被其他的普通公用表表达式所引用。\n举例：查询员工所在的部门的详细信息。\nmysql&gt; SELECT * FROM departments\n    -&gt; WHERE department_id IN (\n    -&gt;                  SELECT DISTINCT department_id\n    -&gt;                  FROM employees\n    -&gt;                  );\n+---------------+------------------+------------+-------------+\n| department_id | department_name  | manager_id | location_id |\n+---------------+------------------+------------+-------------+\n|            10 | Administration   |        200 |        1700 |\n|            20 | Marketing        |        201 |        1800 |\n|            30 | Purchasing       |        114 |        1700 |\n|            40 | Human Resources  |        203 |        2400 |\n|            50 | Shipping         |        121 |        1500 |\n|            60 | IT               |        103 |        1400 |\n|            70 | Public Relations |        204 |        2700 |\n|            80 | Sales            |        145 |        2500 |\n|            90 | Executive        |        100 |        1700 |\n|           100 | Finance          |        108 |        1700 |\n|           110 | Accounting       |        205 |        1700 |\n+---------------+------------------+------------+-------------+\n11 rows in set (0.00 sec)\n这个查询也可以用普通公用表表达式的方式完成：\nmysql&gt; WITH emp_dept_id\n    -&gt; AS (SELECT DISTINCT department_id FROM employees)\n    -&gt; SELECT *\n    -&gt; FROM departments d JOIN emp_dept_id e\n    -&gt; ON d.department_id = e.department_id;\n+---------------+------------------+------------+-------------+---------------+\n| department_id | department_name  | manager_id | location_id | department_id |\n+---------------+------------------+------------+-------------+---------------+\n|            90 | Executive        |        100 |        1700 |            90 |\n|            60 | IT               |        103 |        1400 |            60 |\n|           100 | Finance          |        108 |        1700 |           100 |\n|            30 | Purchasing       |        114 |        1700 |            30 |\n|            50 | Shipping         |        121 |        1500 |            50 |\n|            80 | Sales            |        145 |        2500 |            80 |\n|            10 | Administration   |        200 |        1700 |            10 |\n|            20 | Marketing        |        201 |        1800 |            20 |\n|            40 | Human Resources  |        203 |        2400 |            40 |\n|            70 | Public Relations |        204 |        2700 |            70 |\n|           110 | Accounting       |        205 |        1700 |           110 |\n+---------------+------------------+------------+-------------+---------------+\n11 rows in set (0.00 sec)\n例子说明，公用表表达式可以起到子查询的作用。以后如果遇到需要使用子查询的场景，你可以在查询之前，先定义公用表表达式，然后在查询中用它来代替子查询。而且，跟子查询相比，公用表表达式有一个优点，就是定义过公用表表达式之后的查询，可以像一个表一样多次引用公用表表达式，而子查询则不能。\n3.2 递归公用表表达式递归公用表表达式也是一种公用表表达式，只不过，除了普通公用表表达式的特点以外，它还有自己的特点，就是可以调用自己。它的语法结构是：\nWITH RECURSIVE\nCTE名称 AS （子查询）\nSELECT|DELETE|UPDATE 语句;\n递归公用表表达式由 2 部分组成，分别是种子查询和递归查询，中间通过关键字 UNION  [ALL]进行连接。这里的种子查询，意思就是获得递归的初始值。这个查询只会运行一次，以创建初始数据集，之后递归查询会一直执行，直到没有任何新的查询数据产生，递归返回。\n案例：针对于我们常用的employees表，包含employee_id，last_name和manager_id三个字段。如果a是b的管理者，那么，我们可以把b叫做a的下属，如果同时b又是c的管理者，那么c就是b的下属，是a的下下属。\n下面我们尝试用查询语句列出所有具有下下属身份的人员信息。\n如果用我们之前学过的知识来解决，会比较复杂，至少要进行 4 次查询才能搞定：\n\n第一步，先找出初代管理者，就是不以任何别人为管理者的人，把结果存入临时表；\n\n第二步，找出所有以初代管理者为管理者的人，得到一个下属集，把结果存入临时表；\n\n第三步，找出所有以下属为管理者的人，得到一个下下属集，把结果存入临时表。\n\n第四步，找出所有以下下属为管理者的人，得到一个结果集。\n\n\n如果第四步的结果集为空，则计算结束，第三步的结果集就是我们需要的下下属集了，否则就必须继续进行第四步，一直到结果集为空为止。比如上面的这个数据表，就需要到第五步，才能得到空结果集。而且，最后还要进行第六步：把第三步和第四步的结果集合并，这样才能最终获得我们需要的结果集。\n如果用递归公用表表达式，就非常简单了。我介绍下具体的思路。\n\n用递归公用表表达式中的种子查询，找出初代管理者。字段 n 表示代次，初始值为 1，表示是第一代管理者。\n\n用递归公用表表达式中的递归查询，查出以这个递归公用表表达式中的人为管理者的人，并且代次的值加 1。直到没有人以这个递归公用表表达式中的人为管理者了，递归返回。\n\n在最后的查询中，选出所有代次大于等于 3 的人，他们肯定是第三代及以上代次的下属了，也就是下下属了。这样就得到了我们需要的结果集。\n\n\n这里看似也是 3 步，实际上是一个查询的 3 个部分，只需要执行一次就可以了。而且也不需要用临时表保存中间结果，比刚刚的方法简单多了。\n代码实现：\nWITH RECURSIVE cte \nAS \n(\nSELECT employee_id,last_name,manager_id,1 AS n FROM employees WHERE employee_id = 100 -- 种子查询，找到第一代领导\nUNION ALL\nSELECT a.employee_id,a.last_name,a.manager_id,n+1 FROM employees AS a JOIN cte\nON (a.manager_id = cte.employee_id) -- 递归查询，找出以递归公用表表达式的人为领导的人\n)\nSELECT employee_id,last_name FROM cte WHERE n &gt;= 3; \n总之，递归公用表表达式对于查询一个有共同的根节点的树形结构数据，非常有用。它可以不受层级的限制，轻松查出所有节点的数据。如果用其他的查询方式，就比较复杂了。\n3.3 小 结公用表表达式的作用是可以替代子查询，而且可以被多次引用。递归公用表表达式对查询有一个共同根节点的树形结构数据非常高效，可以轻松搞定其他查询方式难以处理的查询。\n其他查询某个数据库占用空间\nSELECT \n  ROUND(SUM(data_length / 1024 / 1024), 2) 'Data Size in MB',\n  ROUND(SUM(index_length / 1024 / 1024), 2) 'Index Size in MB',\n  ROUND(\n    SUM(\n      (index_length + data_length) / 1024 / 1024\n    ),\n    2\n  ) 'All Size in MB' \nFROM\n  information_schema.TABLES \nWHERE table_schema = '库名' ;\n查询某个库中所有表占用空间\nSELECT \n  TABLE_NAME,\n  CONCAT(\n    TRUNCATE (data_length / 1024 / 1024, 2),\n    ' MB'\n  ) AS data_size,\n  CONCAT(\n    TRUNCATE (index_length / 1024 / 1024, 2),\n    ' MB'\n  ) AS index_size \nFROM\n  information_schema.TABLES \nWHERE TABLE_SCHEMA = '库名' \nGROUP BY TABLE_NAME \nORDER BY data_length DESC ;\n查询所有数据库占用空间\nSELECT \n  TABLE_SCHEMA,\n  CONCAT(\n    TRUNCATE(SUM(data_length) / 1024 / 1024, 2),\n    ' MB'\n  ) AS data_size,\n  CONCAT(\n    TRUNCATE(SUM(index_length) / 1024 / 1024, 2),\n    'MB'\n  ) AS index_size \nFROM\n  information_schema.tables \nGROUP BY TABLE_SCHEMA \nORDER BY data_length DESC ;\n","slug":"B1-MySQL基础","date":"2021-11-10T12:30:04.000Z","categories_index":"数据库","tags_index":"MySQL","author_index":"YFR718"},{"id":"31135c47a98e6bfd7ef6f55be399b5e8","title":"PyMySQL","content":"1. 连接数据库import pymysql\n\n# 创建连接对象\nconn = pymysql.connect(host='localhost', port=3306, user='root', password='mysql',database='python', charset='utf8')\n\n# 获取游标对象\ncursor = conn.cursor()\n\n# 关闭游标\ncursor.close()\n\n# 关闭连接\nconn.close()\n2. 数据库操作2.1 DQL语言selectselect选择\n# 查询 SQL 语句\nsql = \"select * from students;\"\n# 执行 SQL 语句 返回值就是 SQL 语句在执行过程中影响的行数\nrow_count = cursor.execute(sql)\n\n# 取出结果集中一行数据,　例如:(1, '张三')\nprint(cursor.fetchone())\n\n# 取出结果集中的所有数据, 例如:((1, '张三'), (2, '李四'), (3, '王五'))\nfor line in cursor.fetchall():\n    print(line)\n        \nPython查询Mysql使用 fetchone() 方法获取单条数据, 使用fetchall() 方法获取多条数据。\n\nfetchone(): 该方法获取下一个查询结果集。结果集是一个对象\n\nfetchall(): 接收全部的返回结果行.\n\nrowcount: 这是一个只读属性，并返回执行execute()方法后影响的行数。\n\n\n2.2 DML数据操作语言insert 、update、deleteinsert插入\n# SQL 插入语句\nsql = \"\"\"INSERT INTO EMPLOYEE(FIRST_NAME,\n         LAST_NAME, AGE, SEX, INCOME)\n         VALUES ('Mac', 'Mohan', 20, 'M', 2000)\"\"\"\ntry:\n   # 执行sql语句\n   cursor.execute(sql)\n   # 提交到数据库执行\n   db.commit()\nexcept:\n   # 如果发生错误则回滚\n   db.rollback()\n插入多行数据时，数据是list，其中元素是元组。\n# 一次插入多行数据\n# print(value)\nsql = 'insert into summary(id,dataname,herf,rel) values(%s,%s,%s,%s)'\nvalue = []\nfor rep in root.findall('repository'):\n    id = rep.find('id').text\n    dataname = rep.find('name').text\n    herf = rep.find('link').attrib['href']\n    rel = rep.find('link').attrib['rel']\n    value.append((id,dataname,herf,rel))\n    \ntry:\n    cursor.executemany(sql,value)\n    db.commit()\n    print(\"summary写入成功\")\nexcept Exception as e:\n    print(e)\n    db.rollback()\n    print(\"summary写入失败\")\nfinally:\n    db.close()\n\nupdate更新\n# SQL 更新语句\nsql = \"UPDATE EMPLOYEE SET AGE = AGE + 1 WHERE SEX = '%c'\" % ('M')\ntry:\n   # 执行SQL语句\n   cursor.execute(sql)\n   # 提交到数据库执行\n   db.commit()\nexcept:\n   # 发生错误时回滚\n   db.rollback()\ndelete删除\n# SQL 删除语句\nsql = \"DELETE FROM EMPLOYEE WHERE AGE &gt; %s\" % (20)\ntry:\n   # 执行SQL语句\n   cursor.execute(sql)\n   # 提交修改\n   db.commit()\nexcept:\n   # 发生错误时回滚\n   db.rollback()\n2.3 DDL数据定义语言create、drop、altercreate创建数据表\n# 使用 cursor() 方法创建一个游标对象 cursor\ncursor = db.cursor()\n \n\n# 使用预处理语句创建表\nsql = \"\"\"CREATE TABLE EMPLOYEE (\n         FIRST_NAME  CHAR(20) NOT NULL,\n         LAST_NAME  CHAR(20),\n         AGE INT,  \n         SEX CHAR(1),\n         INCOME FLOAT )\"\"\"\n \ncursor.execute(sql)\ndrop删除数据表\n# 使用 execute() 方法执行 SQL，如果表存在则删除\ncursor.execute(\"DROP TABLE IF EXISTS EMPLOYEE\")\n \nalter更改数据表\n2.4 TCL事务控制语言commit、rollback# SQL删除记录语句\nsql = \"DELETE FROM EMPLOYEE WHERE AGE &gt; %s\" % (20)\ntry:\n   # 执行SQL语句\n   cursor.execute(sql)\n   # 向数据库提交\n   db.commit()\nexcept:\n   # 发生错误时回滚\n   db.rollback()\n","slug":"P2-PyMySQL","date":"2021-11-10T02:06:01.000Z","categories_index":"Python","tags_index":"MySQL","author_index":"YFR718"},{"id":"446bbf5d6e6c49aba6d56562199f8a0a","title":"XML解析","content":"1 xml.etree.ElementTree对Python的轻量级XML支持。\nXML是一种固有的分层数据格式，最自然的表示方法是使用树。这个模块有两个类用于此目的:\n\nElementTree将整个XML文档表示为树。\n元素表示此树中的单个节点。\n\n​        与整个文档的交互(对文件的读写)通常在ElementTree级别完成。与单个XML元素及其子元素的交互是在element级别上完成的。\n​        元素是一种灵活的容器对象，用于在内存中存储分层数据结构。它可以被描述为介于列表和字典之间的混合体。每个元素都有许多与之相关的属性:\n\n‘tag’——包含元素名称的字符串。\n‘attributes’——一个存储元素属性的Python字典。\n‘text’ -一个包含元素文本内容的字符串。\n‘tail’ -一个可选字符串，在元素的结束标签之后包含文本。以及一系列存储在Python序列中的子元素。\n\n​        要创建元素实例，请使用element构造函数或SubElement工厂函数。还可以使用ElementTree类包装元素结构，并将其与XML进行转换。\n1.1 导入数据从xml文件导入\nimport xml.etree.ElementTree as ET\ntree = ET.parse('country_data.xml')\nroot = tree.getroot()\n从html导入\nimport xml.etree.ElementTree as ET\n# 要爬取的地址\nurl = \"https://www.re3data.org/api/v1/repositories\"\nres = requests.get(url)\nroot = ET.fromstring(res.text)\n1.2 数据查询1.2.1 获取四个属性print(root.tag)\nprint(root.attrib)\nprint(root.tail)\nprint(res.text)\n1.2.2 访问子节点用 len(Element) 检查子节点个数\n用 Element[0] 访问第0个子节点，Element[1] 访问第1个子节点…\n用 for child in Element 遍历所有子节点\n用 Element.remove(child) 删除某个子节点\nfor i in root:\n    print(i.tag)\n1.2.3 查询指定标签\nElementt.iter() #递归迭代xml文件中所有节点（包含子节点，以及子节点的子节点），返回一个包含所有节点的迭代器\nElement.find(tag) #查找第一个节点为tag的直接子元素，返回一个节点对象\nElement.findall(tag) #查找节点为tag的所有直接子元素’，返回一个节点列表直接子元素的意思：只会查找当前节点的子节点那一级目录\n\nfor rep in root.findall('repository'):\n    # print(rep.find('id').text)\n    # print(rep.find('name').text)\n    print(rep.find('link').attrib['href'],rep.find('link').attrib['rel'])\n处理含namespace的文件\nnamespace = \"{http://www.re3data.org/schema/2-2}\"\nfor element in root.iter(namespace + \"re3data.orgIdentifier\"):  # ID\n    print(\"1#   \", element.text)\n    ID = element.text\n1.2.4 树结构与 XML 字符串的相互转换使用 xml.etree.ElementTree 包中的 tostring() 和 fromstring() 函数：\n\n如果 tostring() 参数不指定 encoding=\"unicode\"，函数将返回 byte 序列。\n\nfrom xml.etree.ElementTree import Element, tostring\n\ntree_str = tostring(root, encoding=\"unicode\")\n\nnew_root = fromstring(tree_str)\nprint(new_root.tag, new_root[0].tag, new_root[1].tag)\n1.3 数据创建1.3.1 创建树节点创建树节点时，一定要指定节点名：\ntree_node = Element(\"node1\")\nprint(tree_node.tag)    # 输出 node1\nprint(tree_node.text)   # 输出 None\nprint(tree_node.tail)   # 输出 None\nprint(tree_node.attrib) # 输出 {}\n也可在创建时指定属性（Element.attrib）：\ntree_node = Element(\"node2\", {\"attr1\": 1, \"attr2\": 2})\nprint(tree_node.tag)    # 输出 node2\nprint(tree_node.text)   # 输出 None\nprint(tree_node.tail)   # 输出 None\nprint(tree_node.attrib) # 输出 {'attr1': 1, 'attr2': 2}\n1.3.2 设置文本（Element.text）或附加文本（Element.tail）创建节点后，可以设置 text, tail 等类成员。这些成员的初始值为 None。\ntree_node = Element(\"node1\")\ntree_node.text = \"Hello world\"\ntree_node.tail = \"Bye\"\n1.3.3 添加子节点可以用 Element.append() 成员函数添加子节点：\nroot = Element(\"root\")\nchild1 = Element(\"child1\")\nchild2 = Element(\"child2\")\n\nroot.append(child1)\nroot.append(child2)\n","slug":"P1-XML解析","date":"2021-11-10T01:23:54.000Z","categories_index":"Python","tags_index":"XML","author_index":"YFR718"},{"id":"0e809b380a45c732473fb3fa1636bd4c","title":"ASCII码","content":"ASCII码表格：\n\n\n\n\nASCII值\n控制字符\nASCII值\n字符\nASCII值\n字符\nASCII值\n字符\n\n\n\n\n0\nNUT\n32\n(space)\n64\n@\n96\n、\n\n\n1\nSOH\n33\n!\n65\nA\n97\na\n\n\n2\nSTX\n34\n“\n66\nB\n98\nb\n\n\n3\nETX\n35\n#\n67\nC\n99\nc\n\n\n4\nEOT\n36\n$\n68\nD\n100\nd\n\n\n5\nENQ\n37\n%\n69\nE\n101\ne\n\n\n6\nACK\n38\n&amp;\n70\nF\n102\nf\n\n\n7\nBEL\n39\n,\n71\nG\n103\ng\n\n\n8\nBS\n40\n(\n72\nH\n104\nh\n\n\n9\nHT\n41\n)\n73\nI\n105\ni\n\n\n10\nLF\n42\n*\n74\nJ\n106\nj\n\n\n11\nVT\n43\n+\n75\nK\n107\nk\n\n\n12\nFF\n44\n,\n76\nL\n108\nl\n\n\n13\nCR\n45\n-\n77\nM\n109\nm\n\n\n14\nSO\n46\n.\n78\nN\n110\nn\n\n\n15\nSI\n47\n/\n79\nO\n111\no\n\n\n16\nDLE\n48\n0\n80\nP\n112\np\n\n\n17\nDCI\n49\n1\n81\nQ\n113\nq\n\n\n18\nDC2\n50\n2\n82\nR\n114\nr\n\n\n19\nDC3\n51\n3\n83\nS\n115\ns\n\n\n20\nDC4\n52\n4\n84\nT\n116\nt\n\n\n21\nNAK\n53\n5\n85\nU\n117\nu\n\n\n22\nSYN\n54\n6\n86\nV\n118\nv\n\n\n23\nTB\n55\n7\n87\nW\n119\nw\n\n\n24\nCAN\n56\n8\n88\nX\n120\nx\n\n\n25\nEM\n57\n9\n89\nY\n121\ny\n\n\n26\nSUB\n58\n:\n90\nZ\n122\nz\n\n\n27\nESC\n59\n;\n91\n[\n123\n{\n\n\n28\nFS\n60\n&lt;\n92\n/\n124\n\\\n\n\n\n29\nGS\n61\n=\n93\n]\n125\n}\n\n\n30\nRS\n62\n&gt;\n94\n^\n126\n`\n\n\n31\nUS\n63\n?\n95\n_\n127\nDEL\n\n\n\n\nASCII 码大致由以下两部分组成：\n\nASCII 非打印控制字符： ASCII 表上的数字 0-31 分配给了控制字符，用于控制像打印机等一些外围设备。\nASCII 打印字符：数字 32-126 分配给了能在键盘上找到的字符，当查看或打印文档时就会出现。\n\n","slug":"Z0-计算机基础","date":"2021-11-06T11:52:59.000Z","categories_index":"计算机基础","tags_index":"","author_index":"YFR718"},{"id":"f8534e8f12d0bc66698739d3b7153c65","title":"J5.对象与类","content":"\n面向对象编程OOP类(Class)和对象(Object)是面向对象的核心概念。\n\n类是对一类事物的描述，是抽象的、概念上的定义\n对象是实际存在的该类事物的每个个体，因而也称为实例(instance)。\n属 性：对应类中的成员变量\n行 为：对应类中的成员方法\n由类构造（construct）对象的过程称为创建类的实例（instance）。\n\n创建Java自定义类\n\n定义类（考虑修饰符、类名）\n编写类的属性（考虑修饰符、属性类型、属性名、初始化值）\n编写类的方法（考虑修饰符、返回值类型、方法名、形参等）\n\n对象的创建和使用\n创建对象语法： 类名 对象名 = new 类名();使用“对象名.对象成员”的方式访问对象成员（包括属性和方法）\n属性修饰符 数据类型 属性名 = 初始化值 ; \n\n修饰符：常用的权限修饰符有：private、缺省、protected、public，其他修饰符：static、final (暂不考虑)\n数据类型：任何基本数据类型(如int、Boolean) 或 任何引用数据类型。\n属性名：属于标识符，符合命名规则和规范即可。\n\n变量的分类：成员变量与局部变量\n\n\n\n\n\n\n成员变量\n局部变量\n\n\n\n\n声明的位置\n直接声明在类中\n方法形参或内部、代码块内、构造器内等\n\n\n修饰符\nprivate、public、static、final等\n不能用权限修饰符修饰，可以用final修饰\n\n\n初始化值\n有默认初始化值\n没有默认初始化值，必须显式赋值，方可使用\n\n\n内存加载位置\n堆空间 或 静态域内\n栈空间\n\n\n\n\n方法\n方法是类或对象行为特征的抽象，用来完成某个功能操作。在某些语言中也称为函数或过程。\n将功能封装为方法的目的是，可以实现代码重用，简化代码\nJava里的方法不能独立存在，所有的方法必须定义在类里。\n\n// 方法的声明格式：\n修饰符  返回值类型  方法名（参数类型形参1, 参数类型形参2, ….）｛\n    方法体程序代码\n    return 返回值;\n}\n修饰符：public,缺省,private, protected等返回值类型：\n返回值类型：没有返回值：void，有返回值，声明出返回值的类型。与方法体中“return 返回值”搭配使用\n方法名：属于标识符，命名时遵循标识符命名规则和规范，“见名知意” \n形参列表：可以包含零个，一个或多个参数。多个参数时，中间用“,”隔开返回值：方法在执行完毕后返还给调用它的程序的数据。\nmain方法\n\n\n\nmain方法\n\n\n\n\n\npublic\n由于Java虚拟机需要调用类的main()方法，所以该方法的访问权限必须是public\n\n\nstatic\n因为Java虚拟机在执行main()方法时不必创建对象，所以该方法必须是static的\n\n\nString[] args\n该方法接收一个String类型的数组参数，该数组中保存执行Java命令时传递给所运行的类的参数\n\n\n\n\n代码块代码块(或初始化块)的作用： 对Java类或对象进行初始化代码块(或初始化块)的分类：一个类中代码块若有修饰符，则只能被static修饰，称为静态代码块(static block)，没有使用static修饰的，为非静态代码块。\n// static代码块通常用于初始化static的属性\nclass Person {\n    public static int total; \n    static {\n        total = 100;//为total赋初值\n    }\n}\n\n静态代码块：用static 修饰的代码块\n\n\n可以有输出语句。\n可以对类的属性、类的声明进行初始化操作。\n不可以对非静态的属性初始化。即：不可以调用非静态的属性和方法。\n若有多个静态的代码块，那么按照从上到下的顺序依次执行。\n静态代码块的执行要先于非静态代码块。\n静态代码块随着类的加载而加载，且只执行一次。\n\n\n非静态代码块：没有static修饰的代码块\n\n\n可以有输出语句。\n可以对类的属性、类的声明进行初始化操作。\n除了调用非静态的结构外，还可以调用静态的变量或方法。\n若有多个非静态的代码块，那么按照从上到下的顺序依次执行。\n每次创建对象的时候，都会执行一次。且先于构造器执行。\n\n⭐程序中成员变量赋值的执行顺序\n\n声明成员变量的默认初始化\n显式初始化、多个初始化块依次被执行（同级别下按先后顺序执行）\n构造器再对成员进行初始化操作\n通过”对象.属性”或”对象.方法”的方式，可多次给属性赋值\n\n重载 overload在同一个类中，允许存在一个以上的==同名方法==，只要它们的==参数个数或者参数类型==不同即可。\n==与返回值类型无关==，只看参数列表，且参数列表必须不同。(参数个数或参数类型)。调用时，根据方法参数列表的不同来区别。\n//返回两个整数的和\nint add(int x,int y){return x+y;}\n//返回三个整数的和\nint add(int x,int y,int z){return x+y+z;} \n//返回两个小数的和\ndouble add(double x,double y){return x+y;}\n可变个数的形参public static void test(int a ,String…books);\n\n可变参数：方法参数部分指定类型的参数个数是可变多个：0个，1个或多个\n可变个数形参的方法与同名的方法之间，彼此构成重载\n可变参数方法的使用与方法参数部分使用数组是一致的\n方法的参数部分有可变形参，需要放在形参声明的最后\n在一个方法的形参位置，最多只能声明一个可变个数形参\n\n方法参数的值传递机制    形参：方法声明时的参数    实参：方法调用时实际传给形参的参数值\nJava里方法的参数传递方式只有一种：值传递。 即将实际参数值的副本（复制品）传入方法内，而参数本身不受影响。\n    形参是基本数据类型：将实参基本数据类型变量的“数据值”传递给形参    形参是引用数据类型：将实参引用数据类型变量的“地址值”传递给形参\nint[] arr = new int[10];\nSystem.out.println(arr);//地址值\nchar[] arr1 = new char[10]; System.out.println(arr1); //内容\n//源码解析\npublic void println(Object x) {\n    String s = String.valueOf(x);\n    synchronized (this) {\n        print(s);\n        newLine();\n    }\n}\npublic void println(char x[]) {\n    synchronized (this) {\n        print(x);\n        newLine();\n    }\n}\n构造器 (或构造方法)构造器的特征\n\n它具有与类相同的名称\n它不声明返回值类型。（与声明为void不同）\n不能被static、final、synchronized、abstract、native修饰，不能有return语句返回值\n\n构造器的作用：创建对象；给对象进行初始化\n\n如：Order o = new Order();    Person p = new Person(“Peter”,15);\n如同我们规定每个“人”一出生就必须先洗澡，我们就可以在“人”的构造器中加入完成“洗澡”的程序代码，于是每个“人”一出生就会自动完成“洗澡”，程序就不必再在每个人刚出生时一个一个地告诉他们要“洗澡”了。\n\n// 语法格式：\n修饰符 类名 (参数列表) {\n    初始化语句;\n}\n根据参数不同，构造器可以分为如下两类：    隐式无参构造器（系统默认提供）    显式定义一个或多个构造器（无参、有参）注 意：    Java语言中，每个类都至少有一个构造器    默认构造器的修饰符与所属类的修饰符一致    一旦显式定义了构造器，则系统不再提供默认构造器    一个类可以创建多个重载的构造器    父类的构造器不可被子类继承\n构造器重载\n构造器重载使得对象的创建更加灵活，方便创建各种不同的对象。\n// 构造器重载举例：\npublic class Person{\n    public Person(String name, int age, Date d) {this(name,age);…} \n    public Person(String name, int age) {…}\n    public Person(String name, Date d) {…}\n    public Person(){…}\n}\n关键字—this    它在方法内部使用，即这个方法所属对象的引用；    它在构造器内部使用，表示该构造器正在初始化的对象。\nthis 可以调用类的属性、方法和构造器\n什么时候使用this关键字呢？    当在方法内需要用到调用该方法的对象时，就用this。具体的：我们可以用this来区分属性和局部变量。比如：this.name = name;\n⭐this可以作为一个类中构造器相互调用的特殊格式\nclass Person{\n    //定义Person类\n    private String name ;\n    private int age ;\n    public Person(){\n        //无参构造器\n        System.out.println(\"新对象实例化\");\n    }\n    public Person(String name ){\n        this(); //调用本类中的无参构造器\n        this.name = name ;\n    }\n    public Person(String name,int age){\n        this(name); // 调用有一个参数的构造器\n        this.age = age;\n    }\n    public String getInfo(){\n    return \"姓名: \"+name+\"，年龄:\"+age;\n    }\n}\n\n    可以在类的构造器中使用”this(形参列表)”的方式，调用本类中重载的其他的构造器！    明确：构造器中不能通过”this(形参列表)”的方式调用自身构造器    如果一个类中声明了n个构造器，则最多有 n - 1个构造器中使用了”this(形参列表)”    “this(形参列表)”必须声明在类的构造器的首行！    在类的一个构造器中，最多只能声明一个”this(形参列表)”\n封装与隐藏    高内聚 ：类的内部数据操作细节自己完成，不允许外部干涉；    低耦合 ：仅对外暴露少量的方法用于使用。\n隐藏对象内部的复杂性，只对外公开简单的接口。便于外界调用，从而提高系统的可扩展性、可维护性。通俗的说，把该隐藏的隐藏起来，该暴露的暴露出来。这就是封装性的设计思想。\nJava中通过将数据声明为私有的(private)，再提供公共的（public）方法:getXxx()和setXxx()实现对该属性的操作，以实现下述目的：\n    隐藏一个类中不需要对外提供的实现细节；    使用者只能通过事先定制好的方法来访问数据，可以方便地加入控制逻辑，限制对属性的不合理操作；    便于修改，增强代码的可维护性；\n\n\n\n\n修饰符\n类内部\n同一个包\n不同包的子类\n同一个工程\n\n\n\n\nprivate\n√\n\n\n\n\n\n(缺省)\n√\n√\n\n\n\n\nprotected\n√\n√\n√\n\n\n\npublic\n√\n√\n√\n√\n\n\n\n\n包(package)的管理与作用package语句作为Java源文件的第一条语句，指明该文件中定义的类所在的包。(若缺省该语句，则指定为无名包)。它的格式为：package 顶层包名.子包名 ;\n//举例：pack1\\pack2\\PackageTest.java\npackage pack1.pack2;    //指定类PackageTest属于包pack1.pack2 \n\npublic class PackageTest{\n    public void display(){\n        System.out.println(\"in  method display()\");\n    }\n}\n    包对应于文件系统的目录，package语句中，用 “.” 来指明包(目录)的层次；    包通常用小写单词标识。通常使用所在公司域名的倒置：com.atguigu.xxx\n包的作用：\n    包帮助管理大型软件系统：将功能相近的类划分到同一个包中。比如：MVC的设计模式    包可以包含类和子包，划分项目层次，便于管理    解决类命名冲突的问题    控制访问权限\n\n\n\n\n\nJDK中主要的包介绍\n\n\n\n\njava.lang\n包含一些Java语言的核心类，如String、Math、Integer、 System和Thread，提供常用功能\n\n\njava.net\n包含执行与网络相关的操作的类和接口\n\n\njava.io\n包含能提供多种输入/输出功能的类\n\n\njava.util\n包含一些实用工具类，如定义系统特性、接口的集合框架类、使用与日期日历相关的函数\n\n\njava.text\n包含了一些java格式化相关的类\n\n\njava.sql\n包含了java进行JDBC数据库编程的相关类/接口\n\n\njava.awt\n包含了构成抽象窗口工具集（abstract window toolkits）的多个类，这些类被用来构建和管理应用程序的图形用户界面(GUI)\n\n\n\n\n继承 (inheritance)多个类中存在相同属性和行为时，将这些内容抽取到单独一个类中，那么多个类无需再定义这些属性和行为，只要继承那个类即可。\n    此处的多个类称为子类(派生类)，单独的这个类称为父类(基类或超类)。可以理解为:“子类 is a 父类”\n// 类继承语法规则:\nclass Subclass extends SuperClass{ }\n作用：    继承的出现减少了代码冗余，提高了代码的复用性。    继承的出现，更有利于功能的扩展。    继承的出现让类与类之间产生了关系，提供了多态的前提。\n\n子类继承了父类，就继承了父类的方法和属性。\n在子类中，可以使用父类中定义的方法和属性，也可以创建新的数据和方法。\n在Java 中，继承的关键字用的是“extends”，即子类不是父类的子集，而是对父类的“扩展”。\n\n关于继承的规则：\n\n子类不能直接访问父类中私有的(private)的成员变量和方法。\n⭐Java只支持单继承和多层继承，不允许多重继承\n\n方法的重写 (override/overwrite)定义：在子类中可以根据需要对从父类中继承来的方法进行改造，也称为方法的重置、覆盖。在程序执行时，子类的方法将覆盖父类的方法。\n要求：\n\n子类重写的方法必须和父类被重写的方法具有相同的方法名称、参数列表\n子类重写的方法的返回值类型不能大于父类被重写的方法的返回值类型\n子类重写的方法使用的访问权限不能小于父类被重写的方法的访问权限    子类不能重写父类中声明为private权限的方法\n子类方法抛出的异常不能大于父类被重写方法的异常\n子类与父类中同名同参数的方法必须同时声明为非static的(即为重写)，或者同时声明为static的（不是重写）。因为static方法是属于类的，子类无法覆盖父类的方法。\n\n子类继承父类\n    若子类重写了父类方法，就意味着子类里定义的方法彻底覆盖了父类里的同名方法，系统将不可能把父类里的方法转移到子类中。    对于实例变量则不存在这样的现象，即使子类里定义了与父类完全相同的实例变量，这个实例变量依然不可能覆盖父类中定义的实例变量\n关键字—super在Java类中使用super来调用父类中的指定操作：\n\nsuper可用于访问父类中定义的属性\nsuper可用于调用父类中定义的成员方法\nsuper可用于在子类构造器中调用父类的构造器\n尤其当子父类出现同名成员时，可以用super表明调用的是父类中的成员\nsuper的追溯不仅限于直接父类\nsuper和this的用法相像，this代表本类对象的引用，super代表父类的内存空间的标识\n\n调用父类的构造器\n    子类中所有的构造器默认都会访问父类中空参数的构造器    当父类中没有空参数的构造器时，子类的构造器必须通过this(参数列表)或者super(参数列表)语句指定调用本类或者父类中相应的构造器。同时，只能”二选一”，且必须放在构造器的首行    如果子类构造器中既未显式调用父类或本类的构造器，且父类中又没有无参的构造器，则编译出错\n\n\n\n\nNo.\n区别点\nthis\nsuper\n\n\n\n\n1\n访问属性\n访问本类中的属性，如果本类没有此属性则从父类中继续查找\n直接访问父类中的属性\n\n\n2\n调用方法\n访问本类中的方法，如果本类没有此方法则从父类中继续查找\n直接访问父类中的方法\n\n\n3\n调用构造器\n调用本类构造器，必须放在构造器的首行\n调用父类构造器，必须放在子类构造器的首行\n\n\n\n\n子类对象的实例化过程\n多态性对象的多态性：父类的引用指向子类的对象\nJava引用变量有两个类型：\n\n编译时类型：由声明该变量时使用的类型决定。\n运行时类型：由实际赋给该变量的对象决定。\n\n\n编译时，看左边；运行时，看右边。\n若编译时类型和运行时类型不一致，就出现了对象的多态性(Polymorphism)\n\n\n\n\n\n\n\n\n\n\n\n多态作用\n    提高了代码的通用性，常称作接口重用\n\n\n前提\n    需要存在继承或者实现关系    有方法的重写\n\n\n成员方法\n    编译时：要查看引用变量所声明的类中是否有所调用的方法。    运行时：调用实际new的对象所属的类中的重写方法。\n\n\n成员变量\n不具备多态性，只看引用变量所声明的类。\n\n\n\n\n对象的多态：在Java中,子类的对象可以替代父类的对象使用    一个变量只能有一种确定的数据类型    一个引用类型变量可能指向(引用)多种不同类型的对象\n\n子类可看做是特殊的父类，所以父类类型的引用可以指向子类的对象：向上转型(upcasting)。\n\n一个引用类型变量如果声明为父类的类型，但实际引用的是子类对象，那么该变量就==不能==再访问子类中添加的属性和方法\n虚拟方法调用\n子类中定义了与父类同名同参数的方法，在多态情况下，将此时父类的方法称为虚拟方法，父类根据赋给它的不同子类对象，动态调用属于子类的该方法。这样的方法调用在编译期是无法确定的。\ninstanceof运算符x instanceof A：检验x是否为类A的对象，返回值为boolean型。\n对象类型转换\n    从子类到父类的类型转换可以自动进行    从父类到子类的类型转换必须通过造型(强制类型转换)实现    无继承关系的引用类型间的转换是非法的    在造型前可以使用instanceof操作符测试一个对象的类型\n对象类型转换 (Casting )基本数据类型的Casting：\n\n自动类型转换：小的数据类型可以自动转换成大的数据类型如long g=20; double d=12.0f\n强制类型转换：可以把大的数据类型强制转换(casting)成小的数据类型如 float f=(float)12.0; int a=(int)1200L    对Java对象的强制类型转换称为造型    从子类到父类的类型转换可以自动进行    从父类到子类的类型转换必须通过造型(强制类型转换)实现    无继承关系的引用类型间的转换是非法的    在造型前可以使用instanceof操作符测试一个对象的类型\n\n方法的重载与重写从编译和运行的角度看：重载，是指允许存在多个同名方法，而这些方法的参数不同。编译器根据方法不同的参数表，对同名方法的名称做修饰。对于编译器而言，这些同名方法就成了不同的方法。它们的调用地址在编译期就绑定了。Java的重载是可以包括父类和子类的，即子类可以重载父类的同名不同参数的方法。所以：对于重载而言，在方法调用之前，编译器就已经确定了所要调用的方法，这称为“早绑定”或“静态绑定”；而对于多态，只有等到方法调用的那一刻，解释运行器才会确定所要调用的具体方法，这称为“晚绑定”或“动态绑定”。引用一句Bruce Eckel的话：“不要犯傻，如果它不是晚绑定，它就不是多态。”\n Object类的结构与方法Object类是所有Java类的根父类    如果在类的声明中未使用extends关键字指明其父类，则默认父类为java.lang.Object类\npublic class Person { ...\n}\n//等价于\npublic class Person extends Object { ...\n}\n\n\n\n\n方法名称\n类型\n描述\n\n\n\n\npublic Object()\n构造\n构造器\n\n\npublic boolean equals(Object obj)\n普通\n对象比较\n\n\npublic int hashCode()\n普通\n取得Hash码\n\n\npublic String toString()\n普通\n对象打印时调用\n\n\n\n\n==操作符与equals方法基本类型比较值:只要两个变量的值相等，即为true。\n引用类型比较引用(是否指向同一个对象)：只有指向同一个对象时，==才返回true。\n用“==”进行比较时，符号两边的数据类型必须兼容(可自动转换的基本数据类型除外)，否则编译出错\nequals()：所有类都继承了Object，也就获得了equals()方法。还可以重写。    只能比较引用类型，其作用与“==”相同,比较是否指向同一个对象。    格式:obj1.equals(obj2)    特例：当用equals()方法进行比较时，对类File、String、Date及包装类（Wrapper Class）来说，是比较类型及内容而不考虑引用的是否是同一个对象；    原因：在这些类中重写了Object类的equals()方法。    当自定义使用equals()时，可以重写。用于比较两个对象的“内容”是否都相等\n重写equals()方法的原则    对称性：如果x.equals(y)返回是“true”，那么y.equals(x)也应该返回是 “true”。    自反性：x.equals(x)必须返回是“true”。    传递性：如果x.equals(y)返回是“true”，而且y.equals(z)返回是“true”，那么z.equals(x)也应该返回是“true”。    一致性：如果x.equals(y)返回是“true”，只要x和y内容一直不变，不管你重复x.equals(y)多少次，返回都是“true”。    任何情况下，x.equals(null)，永远返回是“false”；x.equals(和x不同类型的对象)永远返回是“false”。\n⭐面试题：==和equals的区别\n== 既可以比较基本类型也可以比较引用类型。对于基本类型就是比较值，对于引用类型就是比较内存地址\nequals的话，它是属于java.lang.Object类里面的方法，如果该方法没有被重写过默认也是==;我们可以看到String等类的equals方法是被重写过的，而且String类在日常开发中用的比较多，久而久之，形成了equals是比较值的错误观点。\n具体要看自定义类里有没有重写Object的equals方法来判断。\n通常情况下，重写equals方法，会比较类中的相应属性是否都相等。\n\ntoString() 方法toString()方法在Object类中定义，其返回值是String类型，返回类名和它的引用地址。在进行String与其它类型数据的连接操作时，自动调用toString()方法Date now=new Date();\nSystem.out.println(“now=”+now); //相当于\nSystem.out.println(“now=”+now.toString());\n可以根据需要在用户自定义类型中重写toString()方法\n//如String 类重写了toString()方法，返回字符串的值。\ns1=“hello”;\nSystem.out.println(s1);//相当于System.out.println(s1.toString());\n    基本类型数据转换为String类型时，调用了对应包装类的toString()方法    int a=10; System.out.println(“a=”+a);\n包装类的使用    针对八种基本数据类型定义相应的引用类型—包装类（封装类）\nByte、Short、Integer、Long、Float、Double、Boolean、Character\n基本数据类型包装成包装类的实例 —-装箱通过包装类的构造器实现：\nint i = 500;   Integer t = new Integer(i);\nint i = new Integer(“12”);\n通过字符串参数构造包装类对象：\nFloat f = new Float(“4.56”);\nLong l = new Long(“asdf”);  //NumberFormatException\n获得包装类对象中包装的基本类型变量 —-拆箱调用包装类的.xxxValue()方法：\nboolean b = bObj.booleanValue();\n通过包装类的parseXxx(String s)静态方法：\n// 字符串转包装类\nFloat f = Float.parseFloat(“12.1”);\n\n// 基本数据类型包装成包装类的实例 ---装箱\n// 通过包装类的构造器实现：\nint i = 500;   Integer t = new Integer(i);\n// 通过字符串参数构造包装类对象：\nFloat f = new Float(“4.56”);\nLong l = new Long(“asdf”);  //NumberFormatException\n\n//获得包装类对象中包装的基本类型变量 ---拆箱\n// 调用包装类的.xxxValue()方法：\nboolean b = bObj.booleanValue();\n\n// 字符串转换成基本数据类型\n// 通过包装类的构造器实现：\nint i = new Integer(“12”);\n// 通过包装类的parseXxx(String s)静态方法：\nFloat f = Float.parseFloat(“12.1”);\n\n// 基本数据类型转换成字符串\n// 调用字符串重载的valueOf()方法：\nString fstr = String.valueOf(2.34f);\n//更直接的方式：\nString intStr = 5 + “”\n关键字—static当我们编写一个类时，其实就是在描述其对象的属性和行为，而并没有产生实质上的对象，==只有通过new关键字才会产生出对象==，这时系统才会分配内存空间给对象，其方法才可以供外部调用。我们有时候希望无论是否产生了对象或无论产生了多少对象的情况下，==某些特定的数据在内存空间里只有一份==。\n\n类属性作为该类==各个对象之间共享的变量==。在设计类时,分析哪些属性不因对象的不同而改变，将这些属性设置为类属性。相应的方法设置为类方法。\n如果方法与调用者无关，则这样的方法通常被声明为类方法，由于不需要创建对象就可以调用类方法，从而简化了方法的调用。\n\n    使用范围：在Java类中，可用static修饰属性、方法、代码块、内部类\n被修饰后的成员具备以下特点：    随着类的加载而加载    优先于对象存在    修饰的成员，被所有对象所共享    访问权限允许时，可不创建对象，直接被类调用\n匿名对象\n不定义对象的句柄，而直接调用这个对象的方法。这样的对象叫做匿名对象。\nnew Person().shout();\n类变量(class Variable)类变量（类属性）由该类的所有实例共享\npublic static int total = 0;\n\nPerson.total = 100; // 不用创建对象就可以访问静态成员 \n//访问方式：类名.类属性，类名.类方法\n类方法(class method)没有对象的实例时，可以用类名.方法名()的形式访问由static修饰的类方法。    在static方法内部只能访问类的static修饰的属性或方法，不能访问类的非static的结构。    在static方法中不能有this，也不能有super\n理解main方法的语法​        由于Java虚拟机需要调用类的main()方法，所以该方法的访问权限必须是public，又因为Java虚拟机在执行main()方法时不必创建对象，所以该方法必须是static的，该方法接收一个String类型的数组参数，该数组中保存执行Java命令时传递给所运行的类的参数。\n​        又因为main() 方法是静态的，我们不能直接访问该类中的非静态成员，必须创建该类的一个实例对象后，才能通过这个对象去访问类中的非静态成员，这种情况，我们在之前的例子中多次碰到。\n关键字—final\nfinal标记的类不能被继承。提高安全性，提高程序的可读性。String类、System类、StringBuffer类\nfinal标记的方法不能被子类重写。Object类中的getClass()。\n    final标记的变量(成员变量或局部变量)即称为常量。名称大写，且只能被赋值一次。\n\n\n\n\n\nfinal\n在类、变量和方法时表示“最终的”\n\n\n\n\n类不能被继承\n提高安全性，提高程序的可读性。\n\n\n方法不能被子类重写\n    比如：Object类中的getClass()\n\n\nfinal标记的变量\n名称大写，且只能被赋值一次\n\n\n\n\n// final修饰类\nfinal class A{\n}\n// final修饰方法\nclass A {\n    public final void print() {\n        System . out . println(\"A\");\n    }\n}\n//final修饰变量——常量\nprivate final String INFO = \"atguigu\";  //声明常量\n抽象类与抽象方法—abstract类的设计应该保证父类和子类能够共享特征。有时将一个父类设计得非常抽象，以至于它没有具体的实例，这样的类叫做抽象类。\n用abstract关键字来修饰一个类，这个类叫做==抽象类==。用abstract来修饰一个方法，该方法叫做==抽象方法==。    抽象方法：只有方法的声明，没有方法的实现。以分号结束：比如：public abstract void talk();    含有抽象方法的类必须被声明为抽象类。    抽象类不能被实例化。抽象类是用来被继承的，抽象类的子类必须重写父类的抽象方法，并提供方法体。若没有重写全部的抽象方法，仍为抽象类。    不能用abstract修饰变量、代码块、构造器；    不能用abstract修饰私有方法、静态方法、final的方法、final的类。\n关键字—interface接口(interface)是抽象方法和常量值定义的集合。接口的特点：    用interface来定义。    接口中的所有成员变量都默认是由public static final修饰的。    接口中的所有抽象方法都默认是由public abstract修饰的。    接口中没有构造器。    接口采用多继承机制。\npublic interface Runner {\n    public static final int ID = 1; \n    public abstract void start(); \n    public abstract void run(); \n    public abstract void stop();\n}\n定义Java类的语法格式：先写extends，后写implements    class SubClass extends SuperClass implements InterfaceA{ }一个类可以实现多个接口，接口也可以继承其它接口。    实现接口的类中必须提供接口中所有方法的具体实现内容，方可实例化。否则，仍为抽象类。    接口的主要用途就是被实现类实现。（面向接口编程）    与继承关系类似，接口与实现类之间存在多态性    接口和类是并列关系，或者可以理解为一种特殊的类。从本质上讲，接口是一种特殊的抽象类，这种抽象类中只包含常量和方法的定义，(JDK7.0及之前)，而没有变量和方法的实现。\n\n一个类可以实现多个无关的接口\n与继承关系类似，接口与实现类之间存在多态性\n\n\n\n\n\n区别点\n抽象类\n接口\n\n\n\n\n定义\n包含抽象方法的类\n主要是抽象方法和全局常量的集合\n\n\n组成\n构造方法、抽象方法、普通方法、常量、变量\n常量、抽象方法、(jdk8.0:默认方法、静态方法)\n\n\n使用\n子类继承抽象类(extends)\n子类实现接口(implements)\n\n\n关系\n抽象类可以实现多个接口\n接口不能继承抽象类，但允许继承多个接口\n\n\n常见设计模式\n模板方法\n简单工厂、工厂方法、代理模式\n\n\n对象\n都通过对象的多态性产生实例化对象\n\n\n\n局限\n抽象类有单继承的局限\n接口没有此局限\n\n\n实际\n作为一个模板\n是作为一个标准或是表示一种能力\n\n\n选择\n如果抽象类和接口都可以使用的话，\n优先使用接口，因为避免单继承的局限\n\n\n\n\n在开发中，常看到一个类不是去继承一个已经实现好的类，而是要么继承抽象类，要么实现接口。\nJava 8中关于接口的改进Java 8中，你可以为接口添加静态方法和默认方法。从技术角度来说，这是完全合法的，只是它看起来违反了接口作为一个抽象定义的理念。静态方法：使用 static 关键字修饰。可以通过接口直接调用静态方法，并执行其方法体。我们经常在相互一起使用的类中使用静态方法。你可以在标准库中找到像Collection/Collections或者Path/Paths这样成对的接口和类。默认方法：默认方法使用 default 关键字修饰。可以通过实现类对象来调用。我们在已有的接口中提供新方法的同时，还保持了与旧版本代码的兼容性。比如：java 8 API中对Collection、List、Comparator等接口提供了丰富的默认方法。\n接口中的默认方法\n    若一个接口中定义了一个默认方法，而另外一个接口中也定义了一个同名同参数的方法（不管此方法是否是默认方法），在实现类同时实现了这两个接口时，会出现：接口冲突。    解决办法：实现类必须覆盖接口中同名同参数的方法，来解决冲突。\n若一个接口中定义了一个默认方法，而父类中也定义了一个同名同参数的非抽象方法，则不会出现冲突问题。因为此时遵守：类优先原则。接口中具有相同名称和参数的默认方法会被忽略。\n内部类当一个事物的内部，还有一个部分需要一个完整的结构进行描述，而这个内部的完整的结构又只为外部事物提供服务，那么整个内部的完整结构最好使用内部类。\n    Inner class一般用在定义它的类或语句块之内，在外部引用它时必须给出完整的名称。    Inner class的名字不能与包含它的外部类类名相同；\n\n成员内部类（static成员内部类和非static成员内部类）\n\n局部内部类（不谈修饰符）、匿名内部类\n\n\n成员内部类作为类的成员的角色：    和外部类不同，Inner class还可以声明为private或protected；    可以调用外部类的结构    Inner class 可以声明为static的，但此时就不能再使用外层类的非static的成员变量；成员内部类作为类的角色：    可以在内部定义属性、方法、构造器等结构    可以声明为abstract类 ，因此可以被其它的内部类继承    可以声明为final的    编译以后生成OuterClass$InnerClass.class字节码文件（也适用于局部内部类）\n注意：\n\n非static的成员内部类中的成员不能声明为static的，只有在外部类或static的成员内部类中才可声明static成员。\n外部类访问成员内部类的成员，需要“内部类.成员”或“内部类对象.成员”的方式\n成员内部类可以直接使用外部类的所有成员，包括私有的数据\n当想要在外部类的静态成员部分使用内部类时，可以考虑内部类声明为静态的\n\nclass外部类{\n    方法(){\n    \tclass局部内部类{\n    \t}\n\t}\n    {\n        class局部内部类{\n\t\t}\n    }\n}\n\n如何使用局部内部类\n\n    只能在声明它的方法或代码块中使用，而且是先声明后使用。除此之外的任何地方都不能使用该类    但是它的对象可以通过外部方法的返回值返回使用，返回值类型只能是局部内部类的父类或父接口类型\n\n局部内部类的特点\n\n    内部类仍然是一个独立的类，在编译之后内部类会被编译成独立的.class文件，但是前面冠以外部类的类名和$符号，以及数字编号。    只能在声明它的方法或代码块中使用，而且是先声明后使用。除此之外的任何地方都不能使用该类。    局部内部类可以使用外部类的成员，包括私有的。    局部内部类可以使用外部方法的局部变量，但是必须是final的。由局部内部类和局部变量的声明周期不同所致。    局部内部类和局部变量地位类似，不能使用public,protected,缺省,private    局部内部类不能使用static修饰，因此也不能包含静态成员\n匿名内部类\n    匿名内部类不能定义任何静态成员、方法和类，只能创建匿名内部类的一个实例。一个匿名内部类一定是在new的后面，用其隐含实现一个接口或实现一个类。\nnew 父类构造器（实参列表）|实现接口(){\n    //匿名内部类的类体部分\n}\n匿名内部类的特点    匿名内部类必须继承父类或实现接口    匿名内部类只能有一个对象    匿名内部类对象只能使用多态形式引用\n关键字—native使用 native 关键字说明这个方法是==原生函数==，也就是这个方法是用 ==C/C++==等非Java 语言实现的，并且被编译成了 DLL，由 java 去调用。 \n（1）为什么要用 native 方法java 使用起来非常方便，然而有些层次的任务用 java 实现起来不容易，或者我们对程序的效率很在意时，问题就来了。例如：有时 java 应用需要与 java 外面的环境交互。这是本地方法存在的主要原因，你可以想想 java 需要与一些底层系统如操作系统或某些硬件交换信息时的情况。本地方法正是这样一种交流机制：它为我们提供了一个非常简洁的接口，而且我们无需去了解 java 应用之外的繁琐的细节。（2）native 声明的方法，对于调用者，可以当做和其他 Java 方法一样使用一个native method 方法可以返回任何 java 类型，包括非基本类型，而且同样可以进行异常控制。native method 的存在并不会对其他类调用这些本地方法产生任何影响，实际上调用这些方法的其他类甚至不知道它所调用的是一个本地方法。JVM 将控制调用本地方法的所有细节。如果一个含有本地方法的类被继承，子类会继承这个本地方法并且可以用 java语言重写这个方法（如果需要的话）。 \n","slug":"J1-对象与类","date":"2021-11-05T06:23:36.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"30c2d1a91e3f3491d3662bdaaf39ffa5","title":"Leetcode：1~100","content":"1. 两数之和给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。\n你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。\n你可以按任意顺序返回答案。\n\n\n\n\n\n\n\n\n\n输入：nums = [2,7,11,15], target = 9输出：[0,1]解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。\n\n\n\n\n\n\n\n暴力解法\n双重for循环\n\n\n// Java\nclass Solution {\n    public int[] twoSum(int[] nums, int target) {\n        for(int i = 0;i&lt;nums.length;i++) { \n            for(int j = i+1;j&lt;nums.length;j++) {\n                if(nums[i] + nums[j] == target) {\n                    return new int[] {i,j};\n                }\n            }\n        }\n        return new int[] {0};\n    }\n}\n\n\n\n\n\n\n\n 哈希表\n\n\n// Java\nclass Solution {\n    public int[] twoSum(int[] nums, int target) {\n        Map&lt;Integer, Integer&gt; hashTable = new HashMap&lt;Integer, Integer&gt;();\n        for (int i = 0; i &lt; nums.length; i++) {\n            if(hashTable.containsKey(target - nums[i])) {\n                return new int[]{hashTable.get(target - nums[i]),i};\n            }\n            hashTable.put(nums[i],i);\n        }\n        return new int[] {0};\n\n    }\n}\n# Python\nclass Solution(object):\n    def twoSum(self, nums, target):\n        \"\"\"\n        :type nums: List[int]\n        :type target: int\n        :rtype: List[int]\n        \"\"\"\n        d = {}\n        for i in range(len(nums)):\n            r = target-nums[i]\n            if r in d:\n                return [d[r],i]\n            else:\n                d[nums[i]]=i\n2. 两数相加给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。\n请你将两个数相加，并以相同形式返回一个表示和的链表。\n你可以假设除了数字 0 之外，这两个数都不会以 0 开头。\n\n\n\n\n\n\n\n\n\n输入：l1 = [2,4,3], l2 = [5,6,4]输出：[7,0,8]解释：342 + 465 = 807.\n\n\n\n\n\n\n\n模拟\n模拟加法，更新进位，注意最后结果如果有进位就再加一个节点。\n\n\n/**\n * Definition for singly-linked list.\n * public class ListNode {\n *     int val;\n *     ListNode next;\n *     ListNode() {}\n *     ListNode(int val) { this.val = val; }\n *     ListNode(int val, ListNode next) { this.val = val; this.next = next; }\n * }\n */\nclass Solution {\n    public ListNode addTwoNumbers(ListNode l1, ListNode l2) {\n        ListNode ans = new ListNode();\n        ListNode a = ans;\n        int t = 0;\n        while(l1 != null || l2 != null){\n            if(l1 != null) {\n                t += l1.val;\n                l1 = l1.next;\n            }\n            if(l2 != null) {\n                t += l2.val;\n                l2 = l2.next;\n            }\n            a.next = new ListNode(t%10);\n            a = a.next;\n            t = t/10;\n        }\n        if(t != 0) a.next = new ListNode(t);\n        return ans.next;\n\n    }\n}\n3. 无重复字符的最长子串给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。\n\n\n\n\n\n\n\n\n\n输入: s = “abcabcbb”输出: 3解释: 因为无重复字符的最长子串是 “abc”，所以其长度为 3。\n\n\n\n\n\n\n\n双指针 \n\n设置左指针初值-1，右指针初值0，int数组对区间(l,r]出现过的字母计数\nfor循环对右指针指向的字母计数\n如果计数&lt;=1,更新答案\n如果计数&gt;1,左指针向右移，直到计数=1\n\n\n\n// 数组计数\nclass Solution {\n    public int lengthOfLongestSubstring(String s) {\n        int a[] = new int[1000];\n        int l = -1, r = 0;\n        int maxl = 0;\n        for(r = 0; r&lt; s.length();r++){\n            a[(int)s.charAt(r)]+=1;     // 计数\n            if(a[(int)s.charAt(r)]&lt;=1){ // 不重复更新答案\n                maxl = maxl&gt; (r-l) ? maxl: r-l; \n            }else{                      //重复左指针向右移，直到不重复\n                while(a[(int)s.charAt(r)]&gt;1){\n                    l+=1;\n                    a[(int)s.charAt(l)]-=1;\n                }\n            }\n        }\n        return maxl;\n    }\n    \n}\n// 哈希集合\nclass Solution {\n    public int lengthOfLongestSubstring(String s) {\n        Set&lt;Character&gt; occ = new HashSet&lt;Character&gt;();\n        int len = s.length();\n        int l = -1, r = 0;\n        int maxl = 0;\n        for(r = 0; r&lt; s.length();r++){\n            while(occ.contains(s.charAt(r))) {\n                occ.remove(s.charAt(++l));\n            }\n            occ.add(s.charAt(r));\n            maxl = Math.max(maxl,r-l);\n        }\n        return maxl;\n    }\n    \n}\n\n\n\n\n\n\n\n\n\nHashSet：存储集合，可以用来判断元素是否存在\nHashSet&lt;Character&gt; hs = new HashSep&lt;Character&gt;();\n//添加\nhs.add(1);\n//判断是否存在\nhs.contains()\n//删除元素\nhs.remove(1)\n\n4. 寻找两个正序数组的中位数给定两个大小分别为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。请你找出并返回这两个正序数组的 中位数 。\n\n\n\n\n\n\n\n\n\n输入：nums1 = [1,3], nums2 = [2]输出：2.00000解释：合并数组 = [1,2,3] ，中位数 2\n\n\n\n\n\n\n\nsort\n利用两数组有序的性质，将两个数组归并为一个数组。\n然后输出中位数即可。\n\n\nclass Solution {\n    public double findMedianSortedArrays(int[] nums1, int[] nums2) {\n        int l1 = nums1.length, l2 = nums2.length;\n        int[] nums = new int[l1+l2];\n        for(int i = 0;i &lt; l1; i++) {\n            nums[i] = nums1[i];\n        }\n        for(int i = 0; i &lt; l2; i++) {\n            nums[l1+i] = nums2[i];\n        }\n        Arrays.sort(nums);\n\n        if((l1 + l2) % 2 == 1) {\n            return (double)nums[(l1 + l2) / 2];\n        }else{\n            return (double)(nums[(l1 + l2) / 2]+nums[(l1 + l2) / 2 - 1]) / 2.0;\n        }\n    }\n}\n\n\n\n\n\n\n\n递归\n转换为查找两个数组第k小数，\n分别看两个数组k/2的数，小的那个左边的数一定不是第k小，递归下去。\n注意边界。\n\n\nclass Solution {\n    public double findMedianSortedArrays(int[] nums1, int[] nums2) {\n        int l1 = nums1.length, l2 = nums2.length;\n        if((l1 + l2) % 2 == 1) {\n            return findk(nums1,nums2,0,0,(l1+l2)/2+1);\n        }else{\n            return (findk(nums1,nums2,0,0,(l1+l2)/2)+findk(nums1,nums2,0,0,(l1+l2)/2+1)) / 2.0;\n        }\n    }\n    private double findk(int[] nums1,int[] nums2,int i,int j,int k){\n        //边界情况\n        if(i == nums1.length) return (double)nums2[j+k-1];\n        if(j == nums2.length) return (double)nums1[i+k-1];\n        if(k==1) return Math.min(nums1[i],nums2[j]);\n        //根据情况去递归下去\n        int x = Math.min(nums1.length - 1, i+k/2 - 1);\n        int y = Math.min(nums2.length - 1, j+k/2 - 1);\n        if(nums1[x] &lt; nums2[y]) return findk(nums1,nums2,x+1,j,k - (x - i) - 1);\n        else return findk(nums1,nums2,i,y+1,k - (y - j) - 1);\n    }\n}\n5. 最长回文子串给你一个字符串 s，找到 s 中最长的回文子串。\n\n\n\n\n\n\n\n\n\n输入：s = “babad”输出：”bab”解释：”aba” 同样是符合题意的答案。\n\n\n\n\n\n\n\n中心扩散法\n从中间向两边扩，直到不对称。依次对每一个中心进行扩展，更新最大值。\n注意单中心和双中心两种情况。\n\n\nclass Solution {\n    public String longestPalindrome(String s) {\n        int len = s.length();\n        int maxl = 0;\n        String ans = \"\";\n        for (int i = 0; i &lt; len; i++){\n            int j = 0;\n            while(i-j&gt;=0 &amp;&amp; i+j &lt; len &amp;&amp; s.charAt(i-j) == s.charAt(i+j)){\n                if(2*j+1&gt;maxl){\n                    maxl=2*j+1;\n                    ans = s.substring(i-j,i+j+1);\n                }\n                j++;\n            }\n             j = 0;\n            while(i-j&gt;=0 &amp;&amp; i+j+1 &lt; len &amp;&amp; s.charAt(i-j) == s.charAt(i+j+1)){\n                if(2*(j+1)&gt;maxl){\n                    maxl=2*(j+1);\n                    ans = s.substring(i-j,i+j+2);\n                }\n                j++;\n            }\n        }\n        return ans;\n    }\n}\n6. Z 字形变换将一个给定字符串 s 根据给定的行数 numRows ，以从上往下、从左到右进行 Z 字形排列。\n比如输入字符串为 “PAYPALISHIRING” 行数为 3 时，排列如下：\n\n\n\n\n\n\n\n\n\nP    A    H   NA P L S  I  I GY     I     R之后，你的输出需要从左往右逐行读取，产生出一个新的字符串，比如：”PAHNAPLSIIGYIR”。\n\n\n\n\n\n\n\n数学规律\n设行数为k，找一下每一行的规律：\n同一行竖直方向的字母下标起始为i,公差为2*(k-1)，\n同一行斜方向的字母下标起始为2(k-1) - i,公差为2\\(k-1)。\n注意不要越界。\n\n\nclass Solution {\npublic:\n    string convert(string s, int numRows) {\n        string ans = \"\";\n        for(int i = 0;i&lt;numRows;i++){\n            int j = 0;\n            if(numRows==1) return s;\n            while(i+j*2*(numRows-1)&lt;s.size()){\n                ans+=s[i+j*2*(numRows-1)];\n                if(i!=numRows-1 &amp;&amp; i!=0){\n                    if(i+j*2*(numRows-1)+2*(numRows-1-i)&lt;s.size()){\n                        ans+=s[i+j*2*(numRows-1)+2*(numRows-1-i)];\n                    }\n                }\n                j++;\n            }\n        }\n        return ans;\n    }\n};\n7. 整数反转给你一个 32 位的有符号整数 x ，返回将 x 中的数字部分反转后的结果。\n如果反转后整数超过 32 位的有符号整数的范围 [−231,  231 − 1] ，就返回 0。\n假设环境不允许存储 64 位整数（有符号或无符号）。\n\n\n\n\n\n\n\n\n\n输入：x = 123输出：321 \n\n\n\n\n\n\n\n初等数学\n不断取最后一位加到答案的最右边。\n注意处理越界情况：\n\n\\begin{array}{l}\nans*10 + x > pow(2,31) - 1\\\\\nans > (pow(2,31) - 1 - x)/10\\\\\nans*10 + x 注意：上面的ans为正数\n下面是ans为负数，其中x也是负数。\n\n\nclass Solution {\n    public int reverse(int x) {\n        int ans = 0;\n        while(x !=0 ) {\n            if( x &lt; 0 &amp;&amp; ans &lt; (-Math.pow(2,31) - x %10) / 10) {\n                return 0;\n            }\n            if( x &gt; 0 &amp;&amp; ans &gt; (Math.pow(2,31) - x %10 -1) / 10){\n                return 0;\n            }\n            ans = ans * 10 + x % 10;\n            x = x / 10;\n        }\n        return ans;\n    }\n}\n8. 字符串转换整数 (atoi)请你来实现一个 myAtoi(string s) 函数，使其能将字符串转换成一个 32 位有符号整数（类似 C/C++ 中的 atoi 函数）。\n函数 myAtoi(string s) 的算法如下：\n读入字符串并丢弃无用的前导空格检查下一个字符（假设还未到字符末尾）为正还是负号，读取该字符（如果有）。 确定最终结果是负数还是正数。 如果两者都不存在，则假定结果为正。读入下一个字符，直到到达下一个非数字字符或到达输入的结尾。字符串的其余部分将被忽略。将前面步骤读入的这些数字转换为整数（即，”123” -&gt; 123， “0032” -&gt; 32）。如果没有读入数字，则整数为 0 。必要时更改符号（从步骤 2 开始）。如果整数数超过 32 位有符号整数范围 [−231,  231 − 1] ，需要截断这个整数，使其保持在这个范围内。具体来说，小于 −231 的整数应该被固定为 −231 ，大于 231 − 1 的整数应该被固定为 231 − 1 。返回整数作为最终结果。注意：\n本题中的空白字符只包括空格字符 ‘ ‘ 。除前导空格或数字后的其余字符串外，请勿忽略 任何其他字符。\n示例 ：\n\n\n\n\n\n\n\n\n\n输入：s = “42”输出：42\n\n\n\n\n\n\n\n模拟\n一步步模拟，注意数据越界的情况（和7处理类似）\n\n\nclass Solution {\n    public int myAtoi(String s) {\n        int l = 0;\n        int f = 1;\n        while(l &lt; s.length() &amp;&amp; s.charAt(l) == ' ') {\n            l++;\n        }\n        if(l &lt; s.length() &amp;&amp; s.charAt(l)=='-'){\n            f = -1;\n            l++;\n        }else if( l &lt; s.length() &amp;&amp; s.charAt(l)=='+' ){\n            l++;\n        }\n        int ans = 0;\n        while(l &lt; s.length() &amp;&amp; (s.charAt(l)&gt;='0' &amp;&amp; s.charAt(l)&lt;='9')){\n            int x = (int)s.charAt(l)-(int)'0';\n            if(f==-1 &amp;&amp; f*ans&lt;(-Math.pow(2,31)+x)/10) return -(int)Math.pow(-2,31);\n            if(f==1 &amp;&amp; ans&gt;(Math.pow(2,31)-x-1)/10) return (int)(Math.pow(2,31) - 1);\n            ans = ans*10 + x;\n            l++;\n        }\n\n        return f*ans;\n    }\n}\n9. 回文数给你一个整数 x ，如果 x 是一个回文整数，返回 true ；否则，返回 false 。\n回文数是指正序（从左向右）和倒序（从右向左）读都是一样的整数。例如，121 是回文，而 123 不是。\n示例 ：\n\n\n\n\n\n\n\n\n\n输入：x = 121输出：true\n\n\n\n\n\n\n\n反转数字\n把整数倒过来比一下值就有了。\n\n\nclass Solution {\n    public boolean isPalindrome(int x) {\n        if(x &lt; 0) return false;\n        int y = 0;\n        int xx = x;\n        while(xx != 0){\n            y = y * 10 + xx % 10;\n            xx = xx / 10;\n        }\n        if(y == x){\n            return true;\n        }\n        else {\n            return false;\n        }\n\n    }\n}\n\n10. 正则表达式匹配给你一个字符串 s 和一个字符规律 p，请你来实现一个支持 ‘.’ 和 ‘*’ 的正则表达式匹配。\n‘.’ 匹配任意单个字符‘*’ 匹配零个或多个前面的那一个元素所谓匹配，是要涵盖 整个 字符串 s的，而不是部分字符串。\n示例 ：\n\n\n\n\n\n\n\n\n\n输入：s = “aa” p = “a”输出：false解释：”a” 无法匹配 “aa” 整个字符串。\n\n\n\n\n\n\n\n太难了，我摊牌了\n\n\n11. 盛最多水的容器给你 n 个非负整数 a1，a2，…，an，每个数代表坐标中的一个点 (i, ai) 。在坐标内画 n 条垂直线，垂直线 i 的两个端点分别为 (i, ai) 和 (i, 0) 。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。\n说明：你不能倾斜容器。\n \n示例 ：\n\n\n\n\n\n\n\n\n\n输入：[1,8,6,2,5,4,8,3,7]输出：49解释：图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。\n :::warning 贪心+双指针\n左右指针分别从两端开始，计算一下此时的水量。下面重点贪心：\n\n假设我想水量更多，移动指针，宽度变短，必须使短板边长才有可能，也就是短板不可能在是更优解的边界。\n那么我们尝试移动高度矮的指针，重新计算水量。\n按照步骤2循环直到两个指针碰头。\n\n:::\nclass Solution {\n    public int maxArea(int[] height) {\n        int l = 0;\n        int r = height.length - 1;\n        int ans = 0;\n        while(l&lt;r){\n            ans = Math.max(ans, Math.min(height[l], height[r])*(r-l));\n            if(height[l] &gt; height[r] ) {\n                r--;\n            }else{\n                l++;\n            }\n        }\n        return ans;\n    }\n}\n12. 整数转罗马数字罗马数字包含以下七种字符： I， V， X， L，C，D 和 M。\n字符          数值I             1V             5X             10L             50C             100D             500M             1000例如， 罗马数字 2 写做 II ，即为两个并列的 1。12 写做 XII ，即为 X + II 。 27 写做  XXVII, 即为 XX + V + II 。\n通常情况下，罗马数字中小的数字在大的数字的右边。但也存在特例，例如 4 不写做 IIII，而是 IV。数字 1 在数字 5 的左边，所表示的数等于大数 5 减小数 1 得到的数值 4 。同样地，数字 9 表示为 IX。这个特殊的规则只适用于以下六种情况：\nI 可以放在 V (5) 和 X (10) 的左边，来表示 4 和 9。X 可以放在 L (50) 和 C (100) 的左边，来表示 40 和 90。C 可以放在 D (500) 和 M (1000) 的左边，来表示 400 和 900。给你一个整数，将其转为罗马数字。\n示例 :\n\n\n\n\n\n\n\n\n\n输入: num = 3输出: “III”\n :::warning 贪心\n用两个数组列好数字以及对应的字母，\n贪心法从大到小不断减数，尾端添加对应的字母\n:::\nclass Solution {\n    public String intToRoman(int num) {\n        int[] nums = {1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1};\n        String[] s = {\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"};\n        int l = 0;\n        String ans = \"\";\n        while(l &lt; nums.length){\n            while(num&gt;=nums[l]){\n                num -= nums[l];\n                ans += s[l];\n            }\n            l++;\n        }\n        return ans;\n\n    }\n}\n13. 罗马数字转整数罗马数字包含以下七种字符: I， V， X， L，C，D 和 M。\n字符          数值I             1V             5X             10L             50C             100D             500M             1000例如， 罗马数字 2 写做 II ，即为两个并列的 1 。12 写做 XII ，即为 X + II 。 27 写做  XXVII, 即为 XX + V + II 。\n通常情况下，罗马数字中小的数字在大的数字的右边。但也存在特例，例如 4 不写做 IIII，而是 IV。数字 1 在数字 5 的左边，所表示的数等于大数 5 减小数 1 得到的数值 4 。同样地，数字 9 表示为 IX。这个特殊的规则只适用于以下六种情况：\nI 可以放在 V (5) 和 X (10) 的左边，来表示 4 和 9。X 可以放在 L (50) 和 C (100) 的左边，来表示 40 和 90。C 可以放在 D (500) 和 M (1000) 的左边，来表示 400 和 900。给定一个罗马数字，将其转换成整数。\n示例:\n\n\n\n\n\n\n\n\n\n输入: s = “III”输出: 3\n :::warning 模拟\n从左向右枚举每一个字母，加上对应的钱数。\n注意小单位在大单位左边的情况代表负，之前加了一个，应该减去两个。\n:::\nclass Solution {\n    public int romanToInt(String s) {\n        Map&lt;Character, Integer&gt; values = new HashMap&lt;Character, Integer&gt;(){\n            {\n                put('I',1);\n                put('V',5);\n                put('X',10);\n                put('L',50);\n                put('C',100);\n                put('D',500);\n                put('M',1000);\n            }\n        };\n        int ans = 0;\n        for(int i = 0;i&lt;s.length();i++){\n            ans+=values.get(s.charAt(i));\n            if(i+1&lt;s.length() &amp;&amp; values.get(s.charAt(i))&lt;values.get(s.charAt(i+1))){\n                ans-=2*values.get(s.charAt(i));\n            }\n        }\n        return ans;\n    }\n}\n14. 最长公共前缀编写一个函数来查找字符串数组中的最长公共前缀。\n如果不存在公共前缀，返回空字符串 “”。\n示例：\n\n\n\n\n\n\n\n\n\n输入：strs = [“flower”,”flow”,”flight”]输出：”fl”\n :::warning 模拟\n首先做到最短的字符串长度，\n然后从左向右看每一位是否都相等，不相等返回结果即可。\n特判长度为0的情况。\n:::\nclass Solution {\n    public String longestCommonPrefix(String[] strs) {\n        int max = strs[0].length();\n        for( int i = 0; i&lt; strs.length; i++){\n            max = Math.min(strs[i].length(),max);\n        }\n        if(max==0) return \"\";\n        String ans = \"\";\n        int j;\n        for(int i = 0;i &lt; max; i++){\n            int f = 1;\n            for( j = 1;j&lt;strs.length;j++){\n                if(strs[j].charAt(i)!=strs[j-1].charAt(i)){\n                    f = 0;\n                }\n            }\n            if(f==0){\n                break;\n            }\n            ans+=strs[0].charAt(i);\n        }\n        return ans;\n    }\n}\n15. 三数之和给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。\n注意：答案中不可以包含重复的三元组。\n示例：\n\n\n\n\n\n\n\n\n\n输入：nums = [-1,0,1,2,-1,-4]输出：[[-1,-1,2],[-1,0,1]]\n\n\n\n\n\n\n\n\n前提知识：\n有序数组求两数之和为定值，可以用双指针：和大于定值时右指针左移，和小于定值左指针右移，O(n)\n本题策略：\n暴力要O(n^3)，可以先排序，枚举第一个数，问题转化为前提知识的模型。\n最后O(n^2)可以解决。\n注意跳过重复结果。\n\n\nclass Solution {\n    public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) {\n        Arrays.sort(nums);\n        List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;List&lt;Integer&gt;&gt;();\n       \n        for(int i = 0;i &lt; nums.length; i++){\n            //确保不重复\n            if(i&gt;0 &amp;&amp; nums[i]==nums[i-1]) continue;\n            int l = i+1, r = nums.length-1,target = -nums[i];\n            while(l&lt;r){\n\n                if(nums[l]+nums[r]&gt;target) {\n                    r--;\n                }\n                else if(nums[l]+nums[r]&lt;target){\n                    l++;\n\n                }\n                else {\n                    List&lt;Integer&gt; n = new ArrayList&lt;Integer&gt;();\n                    n.add(nums[i]);\n                    n.add(nums[l]);\n                    n.add(nums[r]);\n                    ans.add(n);\n                    r--;l++;\n                    //确保不重复\n                    while(l&lt;nums.length &amp;&amp; nums[l]==nums[l-1]) l++;\n                    while(r&gt;=0 &amp;&amp; nums[r]==nums[r+1]) r--;\n                }\n            }\n        }\n        return ans;\n    }\n}\n16. 最接近的三数之和给你一个长度为 n 的整数数组 nums 和 一个目标值 target。请你从 nums 中选出三个整数，使它们的和与 target 最接近。\n返回这三个数的和。\n假定每组输入只存在恰好一个解。\n示例：\n\n\n\n\n\n\n\n\n\n输入：nums = [-1,2,1,-4], target = 1输出：2解释：与 target 最接近的和是 2 (-1 + 2 + 1 = 2) 。\n\n\n\n\n\n\n\n暴力\n三次循环解决\n\n\nclass Solution {\n    public int threeSumClosest(int[] nums, int target) {\n        Arrays.sort(nums);\n        int ans = nums[0]+nums[1]+nums[2];\n        for(int i = 0;i&lt;nums.length;i++){\n            for(int j = i+1;j&lt;nums.length;j++){\n                for(int k = j+1;k&lt;nums.length;k++){\n                    //System.out.println(\"\"+i+j+k);\n                    if(Math.abs(nums[i]+nums[j]+nums[k]-target)&lt;Math.abs(target-ans)){\n                        ans = nums[i]+nums[j]+nums[k];\n                    }\n                    if(nums[i]+nums[j]+nums[k]&gt;target){\n                        break;\n                    }\n                }\n            }\n        }\n        return ans;\n\n    }\n}\n\n\n\n\n\n\n\n排序+双指针\n与15题很像不断枚举指针：\n和大于右指针左移，和小于左指针右移。。。。。。。\n\n\n17. 电话号码的字母组合给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。答案可以按 任意顺序 返回。\n给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。\n\n示例：\n\n\n\n\n\n\n\n\n\n输入：digits = “23”输出：[“ad”,”ae”,”af”,”bd”,”be”,”bf”,”cd”,”ce”,”cf”]\n\n\n\n\n\n\n\n\n\n\n18. 四数之和给你一个由 n 个整数组成的数组 nums ，和一个目标值 target 。请你找出并返回满足下述全部条件且不重复的四元组 [nums[a], nums[b], nums[c], nums[d]] （若两个四元组元素一一对应，则认为两个四元组重复）：\n0 &lt;= a, b, c, d &lt; na、b、c 和 d 互不相同nums[a] + nums[b] + nums[c] + nums[d] == target你可以按 任意顺序 返回答案 。\n示例：\n\n\n\n\n\n\n\n\n\n输入：nums = [1,0,-1,0,-2,2], target = 0输出：[[-2,-1,1,2],[-2,0,0,2],[-1,0,0,1]]\n\n\n\n\n\n\n\n排序+双指针\n与15题很像,再加一层循环即可。\n\n\nclass Solution {\n    public List&lt;List&lt;Integer&gt;&gt; fourSum(int[] nums, int target) {\n        Arrays.sort(nums);\n        List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;List&lt;Integer&gt;&gt;();\n        for(int i = 0;i&lt;nums.length;i++){\n            if(i&gt;0 &amp;&amp; nums[i]==nums[i-1]) continue;\n            for(int j = i+1;j&lt;nums.length;j++){\n                if(j&gt;i+1 &amp;&amp; nums[j]==nums[j-1]) continue;\n                int l = j+1,r = nums.length-1,t = target-(nums[i]+nums[j]);\n                while(l&lt;r){\n                    if(nums[l]+nums[r]&gt;t) r--;\n                    else if(nums[l]+nums[r]&lt;t) l++;\n                    else{\n                        List&lt;Integer&gt; new_ans = new ArrayList&lt;Integer&gt;();\n                        new_ans.add(nums[i]);\n                        new_ans.add(nums[j]);\n                        new_ans.add(nums[l]);\n                        new_ans.add(nums[r]);\n                        ans.add(new_ans);\n                        l++;r--;\n                        while(l&lt;nums.length &amp;&amp; nums[l]==nums[l-1]) l++;\n                        while(r&gt;j &amp;&amp; nums[r]==nums[r+1]) r--;\n\n                    }\n                }\n            }\n        }\n        return ans;\n    }\n}\n19. 删除链表的倒数第 N 个结点给你一个链表，删除链表的倒数第 n 个结点，并且返回链表的头结点。\n\n示例：\n\n\n\n\n\n\n\n\n\n输入：head = [1,2,3,4,5], n = 2输出：[1,2,3,5]\n\n\n\n\n\n\n\n双指针\n右指针先走n步，然后左右指针一起向右走，直到右指针到头，然后左指针进行删除节点操作。\n特判解决删除第一个节点的情况。\n\n\n/**\n * Definition for singly-linked list.\n * public class ListNode {\n *     int val;\n *     ListNode next;\n *     ListNode() {}\n *     ListNode(int val) { this.val = val; }\n *     ListNode(int val, ListNode next) { this.val = val; this.next = next; }\n * }\n */\nclass Solution {\n    public ListNode removeNthFromEnd(ListNode head, int n) {\n        int len = 0;\n        ListNode l = head;\n        while(l!=null){\n            len++;\n            l = l.next;\n        }\n        if(n==len) return head.next;\n        ListNode l1 = head;\n        ListNode l2 = head;\n        while(n!= 0){\n            l2 = l2.next;\n            n--;\n        }\n        while(l2.next != null){\n            l1 = l1.next;\n            l2 = l2.next;\n        }\n        if(l1.next !=null) l1.next = l1.next.next;\n        else l1.next = null;\n        return head;\n\n    }\n}\nps：可以添加额外头节点，优化特判情况。\n\n\n\n\n\n\n\n走两遍\n先走一遍计算长度，再走一遍找到倒数第n个节点。\n\n\n\n\n\n\n\n\n\n栈\n先全入栈，再出栈n个即可找到删除的位置。\n\n\nclass Solution {\n    public ListNode removeNthFromEnd(ListNode head, int n) {\n        ListNode dummy = new ListNode(0, head);\n        Deque&lt;ListNode&gt; stack = new LinkedList&lt;ListNode&gt;();\n        ListNode cur = dummy;\n        while (cur != null) {\n            stack.push(cur);\n            cur = cur.next;\n        }\n        for (int i = 0; i &lt; n; ++i) {\n            stack.pop();\n        }\n        ListNode prev = stack.peek();\n        prev.next = prev.next.next;\n        ListNode ans = dummy.next;\n        return ans;\n    }\n}\n20. 有效的括号给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串 s ，判断字符串是否有效。\n有效字符串需满足：\n左括号必须用相同类型的右括号闭合。左括号必须以正确的顺序闭合。\n示例：\n\n\n\n\n\n\n\n\n\n输入：s = “()”输出：true\n\n\n\n\n\n\n\n栈\n用字符数组模拟栈：\n\n遇到左括号入栈。\n遇到右括号出栈，判断是否匹配，不匹配直接返回。\n最后遍历完如果栈非空说明不匹配。\n\n\n\nclass Solution {\n    public boolean isValid(String s) {\n        char z[] = new char[10005];\n        int l = 0;\n        for(int i = 0;i&lt;s.length();i++){\n            if(s.charAt(i)=='(' || s.charAt(i)=='[' || s.charAt(i)=='{'){\n                z[l++] = s.charAt(i);\n            }else{\n                if(l&lt;=0) return false;\n                if((s.charAt(i)==')' &amp;&amp; z[l-1]!='(') || (s.charAt(i)==']' &amp;&amp; z[l-1]!='[') || (s.charAt(i)=='}' &amp;&amp; z[l-1]!='{')){\n                    return false;\n                }else l--;\n            }\n        }\n        if(l!=0) return false;\n        return true;\n    }\n}\n21. 合并两个有序链表将两个升序链表合并为一个新的 升序 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 \n示例：\n\n输入：l1 = [1,2,4], l2 = [1,3,4]输出：[1,1,2,3,4,4]\n来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/merge-two-sorted-lists著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。\n\n\n\n\n\n\n\n模拟\n直接一个个加到新链表里，\n注意最后剩一个链表的情况，直接追加到结果里。\n\n\nclass Solution {\n    public ListNode mergeTwoLists(ListNode l1, ListNode l2) {\n        ListNode ans = new ListNode();\n        ListNode a = ans;\n        while(l1 != null &amp;&amp; l2 !=null){\n            if(l1.val&lt;l2.val){\n                a.next = l1;\n                a = l1;\n                l1 = l1.next;\n            }else{\n                a.next = l2;\n                a = l2;\n                l2 = l2.next;\n            }\n        }\n        if(l1 != null) a.next = l1;\n        if(l2 != null) a.next = l2;\n        return ans.next;\n\n    }\n}\n\n\n\n\n\n\n\n\n递归\n先判断边界情况，再判断常规情况。\n\n\nclass Solution {\n    public ListNode mergeTwoLists(ListNode l1, ListNode l2) {\n        if(l1 == null) return l2;\n        if(l2 == null) return l1;\n        if(l1.val &lt; l2.val){\n            ListNode a = l1;\n            a.next = mergeTwoLists(l1.next,l2);\n            return a;\n        }else{\n            ListNode a = l2;\n            a.next = mergeTwoLists(l2.next,l1);\n            return a;\n        }\n\n    }\n}\n22. 括号生成数字 n 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 有效的 括号组合。\n\n\n\n\n\n\n\n\n\n输入：n = 3输出：[“((()))”,”(()())”,”(())()”,”()(())”,”()()()”]\n23. 合并K个升序链表给你一个链表数组，每个链表都已经按升序排列。\n请你将所有链表合并到一个升序链表中，返回合并后的链表。\n\n\n\n\n\n\n\n\n\n输入：lists = [[1,4,5],[1,3,4],[2,6]]输出：[1,1,2,3,4,4,5,6]解释：链表数组如下：[  1-&gt;4-&gt;5,  1-&gt;3-&gt;4,  2-&gt;6]将它们合并到一个有序链表中得到。1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6\n\n\n\n\n\n\n\n归并合并\n按照归并排序的思想，两两归并，直到合为一体\n\n\nclass Solution {\n    private ListNode[] merge2(ListNode[] list){\n        int len = list.length;\n        //System.out.println(len);\n        if(len == 1) return list;\n        ListNode[] new_list = new ListNode[(len+1)/2];\n        for(int i=0;i+1&lt;len;i+=2){\n            ListNode listhead = new ListNode();\n            ListNode list_tial = listhead; \n            while(list[i]!=null &amp;&amp; list[i+1]!=null){\n                if(list[i].val&lt;list[i+1].val){\n                    list_tial.next = list[i];\n                    list_tial = list[i];\n                    list[i] = list[i].next;\n                }else{\n                    list_tial.next = list[i+1];\n                    list_tial = list[i+1];\n                    list[i+1] = list[i+1].next;  \n                }\n            }\n            if(list[i] != null) list_tial.next = list[i];\n            if(list[i+1] != null) list_tial.next = list[i+1];\n            new_list[i/2] = listhead.next;\n        }\n        if(len%2==1){\n            new_list[(len+1)/2-1] = list[len-1];\n        }\n        return new_list;\n\n    }\n    public ListNode mergeKLists(ListNode[] lists) {\n        ListNode[] ans= merge2(lists);\n        if(ans.length==0) return null;\n        //System.out.println(ans.length);\n\n        while(ans.length!=1){\n            ans = merge2(ans);\n        }\n        return ans[0];\n    }\n}\n24. 两两交换链表中的节点给你一个链表，两两交换其中相邻的节点，并返回交换后链表的头节点。你必须在不修改节点内部的值的情况下完成本题（即，只能进行节点交换）。\n \n\n\n\n\n\n\n\n\n\n输入：head = [1,2,3,4]输出：[2,1,4,3]\n\n\n\n\n\n\n\n递归\n大事化小，只交换前两个，剩下的交给递归\n\n\nclass Solution {\n    public ListNode swapPairs(ListNode head) {\n        if(head == null) return head;\n        if(head.next == null) return head;\n        ListNode l1 = head.next;\n        ListNode l2 = head.next.next;\n        l1.next = head;\n        head.next = swapPairs(l2);\n        return l1;\n    }\n}\n25. K 个一组翻转链表给你一个链表，每 k 个节点一组进行翻转，请你返回翻转后的链表。\nk 是一个正整数，它的值小于或等于链表的长度。\n如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。\n进阶：\n\n你可以设计一个只使用常数额外空间的算法来解决此问题吗？\n你不能只是单纯的改变节点内部的值，而是需要实际进行节点交换。\n\n\n\n\n\n\n\n\n\n\n\n输入：head = [1,2,3,4,5], k = 2输出：[2,1,4,3,5]\n\n\n\n\n\n\n\n递归\n大事化小，只交换前k个，剩下的交给递归\n\n\nclass Solution {\n    public ListNode reverseKGroup(ListNode head, int k) {\n        if(k==1) return head;\n        int len =0;\n        ListNode n = head;\n        while(n!=null &amp;&amp; len &lt; k){\n            n = n.next;\n            len++;\n        }\n        if(len &lt; k) return head;\n        else{\n            ListNode l1 = head,l2 = head.next,l3 = head.next.next;\n            l1.next = reverseKGroup(n,k);\n            for(int i = 1;i&lt;k;i++){\n                l2.next = l1;\n                l1 = l2;\n                l2 = l3;\n                if(l3!=null)\n                l3 =l3.next;\n            }\n            return l1; \n\n        }\n\n    }\n}\n26. 删除有序数组中的重复项给你一个有序数组 nums ，请你 原地 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。\n不要使用额外的数组空间，你必须在 原地 修改输入数组 并在使用 O(1) 额外空间的条件下完成。\n示例：\n\n\n\n\n\n\n\n\n\n输入：nums = [1,1,2]输出：2, nums = [1,2]解释：函数应该返回新的长度 2 ，并且原数组 nums 的前两个元素被修改为 1, 2 。不需要考虑数组中超出新长度后面的元素。\n\n\n\n\n\n\n\n双指针\n左指针为下一个数应该的位置，右指针为遍历的每一个数。\n当右指针的数与它左边数不同，说明不重复，将其放到左指针的位置，左指针右移。\n\n\nclass Solution {\n    public int removeDuplicates(int[] nums) {\n        int len = nums.length;\n        int l = 1;\n        for(int i = 1;i&lt;nums.length;i++){\n            if(nums[i]!=nums[i-1]){\n                nums[l++] = nums[i];\n                \n            }else{\n                len--;\n            }\n        }\n        return len;\n    }\n}\n27. 移除元素给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素，并返回移除后数组的新长度。\n不要使用额外的数组空间，你必须仅使用 O(1) 额外空间并 原地 修改输入数组。\n元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。\n示例：\n\n\n\n\n\n\n\n\n\n输入：nums = [3,2,2,3], val = 3输出：2, nums = [2,2]解释：函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。你不需要考虑数组中超出新长度后面的元素。例如，函数返回的新长度为 2 ，而 nums = [2,2,3,3] 或 nums = [2,2,0,0]，也会被视作正确答案。\n\n\n\n\n\n\n\n双指针\n与26题相似，判断条件改为与目标值不同。\n\n\nclass Solution {\n    public int removeElement(int[] nums, int val) {\n        int len = nums.length,len_now = nums.length;\n        int l = 0;\n        for(int i=0;i&lt;len;i++){\n            if(nums[i]!=val){\n                nums[l++] = nums[i];\n            }else{\n                len_now--;\n            }\n        }\n        return len_now;\n    }\n}\n28. 实现 strStr()实现 strStr() 函数。\n给你两个字符串 haystack 和 needle ，请你在 haystack 字符串中找出 needle 字符串出现的第一个位置（下标从 0 开始）。如果不存在，则返回  -1 。\n说明：\n当 needle 是空字符串时，我们应当返回什么值呢？这是一个在面试中很好的问题。\n对于本题而言，当 needle 是空字符串时我们应当返回 0 。这与 C 语言的 strstr() 以及 Java 的 indexOf() 定义相符。\n示例：\n\n\n\n\n\n\n\n\n\n输入：haystack = “hello”, needle = “ll”输出：2\n\n\n\n\n\n\n\n暴力\n狂暴就完事了\n\n\nclass Solution {\n    public int strStr(String haystack, String needle) {\n        int l2 = needle.length(),l1 = haystack.length();\n        if(l2==0) return 0;\n        if(l1&lt;l2) return -1;\n        for(int i = 0;i&lt;=l1-l2;i++){\n            int f = 1;\n            for(int j = 0;j&lt;l2;j++){\n                if(haystack.charAt(i+j)!=needle.charAt(j)){\n                    f = 0;\n                    break;\n                }\n            }\n            if(f==1){\n                return i;\n            }\n        }\n        return -1;\n\n    }\n}\n\n\n\n\n\n\n\nKMP\n标准KMP模板题\n\n\n29. 两数相除给定两个整数，被除数 dividend 和除数 divisor。将两数相除，要求不使用乘法、除法和 mod 运算符。\n返回被除数 dividend 除以除数 divisor 得到的商。\n整数除法的结果应当截去（truncate）其小数部分，例如：truncate(8.345) = 8 以及 truncate(-2.7335) = -2\n示例 1:\n\n\n\n\n\n\n\n\n\n输入: dividend = 10, divisor = 3输出: 3解释: 10/3 = truncate(3.33333..) = truncate(3) = 3\n\n\n\n\n\n\n\n倍减法\n标准KMP模板题\n\n\nclass Solution {\n    public int divide(int dividend, int divisor) {\n        int f1=1,f2=1,ans=0;\n        if(divisor==dividend) return 1;\n        if(divisor==-2147483648) return 0;\n\n        if(dividend==-Math.pow(2,31) &amp;&amp; divisor==-1){\n            return (int)(Math.pow(2,31)-1);\n        }\n        if(dividend&lt;0){\n            f1 = -1;\n        }\n        if(divisor&lt;0){\n            f2 = -1;\n        }\n        int num = 1;\n        while(Math.abs(divisor)&lt;Math.pow(2,28)){\n            divisor = divisor + divisor;\n            num = num +num;\n        }\n        while(num !=0){\n            System.out.println(\"\"+num+\" \"+divisor+\" \"+dividend);\n            while(f1*(dividend-f1*f2*divisor)&gt;=0){\n                dividend-=f1*f2*divisor;\n                ans+=num;\n            }\n            num = num&gt;&gt;1;\n            if(f2==-1) divisor = -divisor;\n            divisor = divisor&gt;&gt;1;\n            if(f2==-1) divisor = -divisor;\n\n        }\n        return ans*f1*f2;\n\n    }\n}\n30. 串联所有单词的子串给定一个字符串 s 和一些 长度相同 的单词 words 。找出 s 中恰好可以由 words 中所有单词串联形成的子串的起始位置。\n注意子串要与 words 中的单词完全匹配，中间不能有其他字符 ，但不需要考虑 words 中单词串联的顺序。\n\n\n\n\n\n\n\n\n\n输入：s = “barfoothefoobarman”, words = [“foo”,”bar”]输出：[0,9]解释：从索引 0 和 9 开始的子串分别是 “barfoo” 和 “foobar” 。输出的顺序不重要, [9,0] 也是有效答案。\n31. 下一个排列实现获取 下一个排列 的函数，算法需要将给定数字序列重新排列成字典序中下一个更大的排列（即，组合出下一个更大的整数）。\n如果不存在下一个更大的排列，则将数字重新排列成最小的排列（即升序排列）。\n必须 原地 修改，只允许使用额外常数空间。\n\n\n\n\n\n\n\n\n\n输入：nums = [1,2,3]输出：[1,3,2]\n32. 最长有效括号给你一个只包含 ‘(‘ 和 ‘)’ 的字符串，找出最长有效（格式正确且连续）括号子串的长度。\n\n\n\n\n\n\n\n\n\n输入：s = “(()”输出：2解释：最长有效括号子串是 “()”\n33. 搜索旋转排序数组整数数组 nums 按升序排列，数组中的值 互不相同 。\n在传递给函数之前，nums 在预先未知的某个下标 k（0 &lt;= k &lt; nums.length）上进行了 旋转，使数组变为 [nums[k], nums[k+1], …, nums[n-1], nums[0], nums[1], …, nums[k-1]]（下标 从 0 开始 计数）。例如， [0,1,2,4,5,6,7] 在下标 3 处经旋转后可能变为 [4,5,6,7,0,1,2] 。\n给你 旋转后 的数组 nums 和一个整数 target ，如果 nums 中存在这个目标值 target ，则返回它的下标，否则返回 -1 。\n\n\n\n\n\n\n\n\n\n输入：nums = [4,5,6,7,0,1,2], target = 0输出：4\n\n\n\n\n\n\n\n两次二分 \n先二分找到分界点，再对分界点前或者后寻找答案。\n\n\nclass Solution {\n    public int search(int[] nums, int target) {\n        int len = nums.length;\n        int l = 0,r = len-1,mid ;\n        if(nums[0]&gt;nums[len-1]){\n\n            while(l&lt;r){\n                mid = (l+r)&gt;&gt;1;\n                if(nums[mid]&gt;=nums[0]){\n                    l = mid+1;\n                }else{\n                    r = mid-1;\n                }\n            }\n        }\n        //System.out.println(\"\"+l+\" \"+r);\n        if(nums[0]&lt;=target){\n            l = 0;\n            System.out.println(\"\"+l+\" \"+r);\n            while(l&lt;r){\n                mid = (l+r)&gt;&gt;1;\n                if(nums[mid]==target){\n                return mid;\n                }\n                if(nums[mid]&gt;target){\n                    r = mid-1;\n                    \n                }else{\n                    l = mid+1;\n                }\n            }\n            System.out.println(\"\"+l+\" \"+r);\n            if(nums[l]==target){\n                return l;\n            }\n        }else{\n            r = len-1;\n            while(l&lt;r){\n                mid = (l+r)&gt;&gt;1;\n                if(nums[mid]==target){\n                return mid;\n                }\n                if(nums[mid]&gt;target){\n                    r = mid-1;\n                    \n                }else{\n                    l = mid+1;\n                }\n            }\n            if(nums[l]==target){\n                return l;\n            }\n        }\n        return -1;\n    }\n}\n34. 在排序数组中查找元素的第一个和最后一个位置给定一个按照升序排列的整数数组 nums，和一个目标值 target。找出给定目标值在数组中的开始位置和结束位置。\n如果数组中不存在目标值 target，返回 [-1, -1]。\n进阶：\n你可以设计并实现时间复杂度为 O(log n) 的算法解决此问题吗？\n\n\n\n\n\n\n\n\n\n输入：nums = [5,7,7,8,8,10], target = 8输出：[3,4]\n\n\n\n\n\n\n\n二分\n很明显，排序的用二分。直接二分找到左右边界。\n\n\nclass Solution {\n    public int[] searchRange(int[] nums, int target) {\n        if(nums.length==0) return new int[]{-1,-1};\n        int l = 0,r = nums.length-1;\n        int mid;\n        int ansl = -1,ansr = -1;\n        while(l&lt;r){\n            mid = (l+r)&gt;&gt;1;\n            if(nums[mid]&lt;target) l = mid+1;\n            else r = mid;\n        }\n        ansl = l;\n        l = 0;\n        r = nums.length-1;\n        while(l&lt;r){\n            mid = (l+r+1)&gt;&gt;1;\n            if(nums[mid]&lt;=target) l = mid;\n            else r = mid-1;\n        }\n        ansr = l;\n        if(ansl&lt;=ansr &amp;&amp; ansl&lt;nums.length &amp;&amp; ansr&gt;=0 &amp;&amp; nums[ansl]==target &amp;&amp; nums[ansr]==target){\n            return new int[]{ansl,ansr};\n        }\n        return new int[]{-1,-1};\n    }\n}\n35. 搜索插入位置给定一个排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。\n请必须使用时间复杂度为 O(log n) 的算法。\n\n\n\n\n\n\n\n\n\n输入: nums = [1,3,5,6], target = 5输出: 2\n\n\n\n\n\n\n\n二分\n很明显，排序的用二分。二分模板题。\n\n\nclass Solution {\n    public int searchInsert(int[] nums, int target) {\n        int l = 0,r = nums.length-1;\n        int mid;\n        while(l&lt;=r){\n            mid = (l+r)&gt;&gt;1;\n            if(nums[mid]==target) return mid;\n            if(nums[mid]&lt;target) l = mid+1;\n            else r = mid-1;\n        }\n        return l;\n    }\n}\n36. 有效的数独请你判断一个 9 x 9 的数独是否有效。只需要 根据以下规则 ，验证已经填入的数字是否有效即可。\n\n数字 1-9 在每一行只能出现一次。\n数字 1-9 在每一列只能出现一次。\n数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。（请参考示例图）\n\n注意：\n\n一个有效的数独（部分已被填充）不一定是可解的。\n只需要根据以上规则，验证已经填入的数字是否有效即可。\n空白格用 '.' 表示。\n\n\n\n\n\n\n\n\n\n\n\n输入：board =[[“5”,”3”,”.”,”.”,”7”,”.”,”.”,”.”,”.”],[“6”,”.”,”.”,”1”,”9”,”5”,”.”,”.”,”.”],[“.”,”9”,”8”,”.”,”.”,”.”,”.”,”6”,”.”],[“8”,”.”,”.”,”.”,”6”,”.”,”.”,”.”,”3”],[“4”,”.”,”.”,”8”,”.”,”3”,”.”,”.”,”1”],[“7”,”.”,”.”,”.”,”2”,”.”,”.”,”.”,”6”],[“.”,”6”,”.”,”.”,”.”,”.”,”2”,”8”,”.”],[“.”,”.”,”.”,”4”,”1”,”9”,”.”,”.”,”5”],[“.”,”.”,”.”,”.”,”8”,”.”,”.”,”7”,”9”]]输出：true\n\n\n\n\n\n\n\n暴力验证\n直接暴力验证\n\n\nclass Solution {\n    public boolean isValidSudoku(char[][] board) {\n        int[] a = new int[10];\n        for(int i = 0;i&lt;9;i++){\n            for(int j = 0;j&lt;=9;j++) a[j] = 0;\n            for(int j = 0;j&lt;9;j++){\n                if(board[i][j]&gt;='0'){\n                    a[board[i][j]-'0']++;\n                    if(a[board[i][j]-'0']&gt;1){\n                        System.out.println(\"1 \"+i+j);\n                        return false;\n                    }\n                }\n            }\n            for(int j = 0;j&lt;=9;j++) a[j] = 0;\n            for(int j = 0;j&lt;9;j++){\n                if(board[j][i]&gt;='0'){\n                    a[board[j][i]-'0']++;\n                    if(a[board[j][i]-'0']&gt;1){\n                        System.out.println(\"2 \"+i+j+(board[j][i]-'0'));\n                        return false;\n                    }\n                }\n            }\n        }\n        for(int i = 0;i&lt;9;i+=3){\n            for(int j = 0;j&lt;9;j+=3){\n                for(int k = 0;k&lt;=9;k++) a[k] = 0;\n                for(int m = 0;m&lt;3;m++){\n                    for(int n = 0;n&lt;3;n++){\n                        if(board[i+m][j+n]&gt;='0'){\n                            a[board[i+m][j+n]-'0']++;\n                            if(a[board[i+m][j+n]-'0']&gt;1){\n                        System.out.println(\"3 \"+i+j);\n                        return false;\n                    }\n                        }\n                    }\n                }\n            }\n        }\n        return true;\n    }\n}\n37. 解数独编写一个程序，通过填充空格来解决数独问题。\n数独的解法需 遵循如下规则：\n\n数字 1-9 在每一行只能出现一次。\n数字 1-9 在每一列只能出现一次。\n数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。（请参考示例图）\n\n数独部分空格内已填入了数字，空白格用 '.' 表示。\n\n\n\n\n\n\n\n\n\n\n输入：board = [[“5”,”3”,”.”,”.”,”7”,”.”,”.”,”.”,”.”],[“6”,”.”,”.”,”1”,”9”,”5”,”.”,”.”,”.”],[“.”,”9”,”8”,”.”,”.”,”.”,”.”,”6”,”.”],[“8”,”.”,”.”,”.”,”6”,”.”,”.”,”.”,”3”],[“4”,”.”,”.”,”8”,”.”,”3”,”.”,”.”,”1”],[“7”,”.”,”.”,”.”,”2”,”.”,”.”,”.”,”6”],[“.”,”6”,”.”,”.”,”.”,”.”,”2”,”8”,”.”],[“.”,”.”,”.”,”4”,”1”,”9”,”.”,”.”,”5”],[“.”,”.”,”.”,”.”,”8”,”.”,”.”,”7”,”9”]]输出：[[“5”,”3”,”4”,”6”,”7”,”8”,”9”,”1”,”2”],[“6”,”7”,”2”,”1”,”9”,”5”,”3”,”4”,”8”],[“1”,”9”,”8”,”3”,”4”,”2”,”5”,”6”,”7”],[“8”,”5”,”9”,”7”,”6”,”1”,”4”,”2”,”3”],[“4”,”2”,”6”,”8”,”5”,”3”,”7”,”9”,”1”],[“7”,”1”,”3”,”9”,”2”,”4”,”8”,”5”,”6”],[“9”,”6”,”1”,”5”,”3”,”7”,”2”,”8”,”4”],[“2”,”8”,”7”,”4”,”1”,”9”,”6”,”3”,”5”],[“3”,”4”,”5”,”2”,”8”,”6”,”1”,”7”,”9”]]解释：输入的数独如上图所示，唯一有效的解决方案如下所示：\n\n\n\n\n\n\n\n\n不会了哇\n哇哇哇\n\n\n38. 外观数列难度中等844\n给定一个正整数 n ，输出外观数列的第 n 项。\n「外观数列」是一个整数序列，从数字 1 开始，序列中的每一项都是对前一项的描述。\n你可以将其视作是由递归公式定义的数字字符串序列：\n\ncountAndSay(1) = \"1\"\ncountAndSay(n) 是对 countAndSay(n-1) 的描述，然后转换成另一个数字字符串。\n\n前五项如下：\n\n\n\n\n\n\n\n\n\n1\n11\n21\n1211\n111221第一项是数字 1描述前一项，这个数是 1 即 “ 一 个 1 ”，记作 “11”描述前一项，这个数是 11 即 “ 二 个 1 ” ，记作 “21”描述前一项，这个数是 21 即 “ 一 个 2 + 一 个 1 ” ，记作 “1211”描述前一项，这个数是 1211 即 “ 一 个 1 + 一 个 2 + 二 个 1 ” ，记作 “111221”\n要 描述 一个数字字符串，首先要将字符串分割为 最小 数量的组，每个组都由连续的最多 相同字符 组成。然后对于每个组，先描述字符的数量，然后描述字符，形成一个描述组。要将描述转换为数字字符串，先将每组中的字符数量用数字替换，再将所有描述组连接起来。\n例如，数字字符串 \"3322251\" 的描述如下图：\n\n\n\n\n\n\n\n\n\n\n输入：n = 1输出：”1”解释：这是一个基本样例。\n\n\n\n\n\n\n\n模拟\n还是太慢，需要以后优化\n\n\nclass Solution {\n    public String countAndSay(int n) {\n\n        StringBuilder s = new StringBuilder(\"1\");\n        for(int k = 1;k&lt;n;k++){\n            StringBuilder new_s = new StringBuilder();\n            int l = 0;\n            char c = s.charAt(0);\n            for(int i = 0;i&lt;s.length();i++){\n                if(i!=0 &amp;&amp; s.charAt(i)!=s.charAt(i-1)){\n                    String sss = \"\"+l+c;\n                    new_s.append(sss);\n                    l = 0;\n                }\n                l+=1;\n                c = s.charAt(i);\n            }\n            String sss = \"\"+l+c;\n            new_s.append(sss);\n            s = new_s;\n            //System.out.println(s);\n        }\n        return s.toString();\n\n    }\n}\n39. 组合总和难度中等1661\n给定一个无重复元素的正整数数组 candidates 和一个正整数 target ，找出 candidates 中所有可以使数字和为目标数 target 的唯一组合。\ncandidates 中的数字可以无限制重复被选取。如果至少一个所选数字数量不同，则两种组合是唯一的。 \n对于给定的输入，保证和为 target 的唯一组合数少于 150 个。\n\n\n\n\n\n\n\n\n\n输入: candidates = [2,3,6,7], target = 7输出: [[7],[2,2,3]]\n\n\n\n\n\n\n\ndfs\n标准dfs+剪枝\n\n\nclass Solution {\n    public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target){\n        List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;List&lt;Integer&gt;&gt;();\n        ArrayList&lt;Integer&gt; ls = new ArrayList&lt;Integer&gt;();\n        dfs(0,0,target,ls,ans,candidates);\n\n        return ans;\n\n    }\n    private void dfs(int k,int sums,int target,List&lt;Integer&gt; ls,List&lt;List&lt;Integer&gt;&gt; ans,int[] candidates){\n        if(sums == target) ans.add(new ArrayList&lt;Integer&gt;(ls));\n        if(sums &gt; target) return ;\n        for(int i = k; i&lt;candidates.length; i++){\n            sums+=candidates[i];\n            ls.add(candidates[i]);\n            dfs(i,sums,target,ls,ans,candidates);\n            sums -= candidates[i];\n            ls.remove(ls.size() - 1);\n        }\n    }\n}\n40. 组合总和 II给定一个数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。\ncandidates 中的每个数字在每个组合中只能使用一次。\n注意：解集不能包含重复的组合。 \n\n\n\n\n\n\n\n\n\n输入: candidates = [10,1,2,7,6,1,5], target = 8,输出:[[1,1,6],[1,2,5],[1,7],[2,6]]\n41. 缺失的第一个正数给你一个未排序的整数数组 nums ，请你找出其中没有出现的最小的正整数。\n请你实现时间复杂度为 O(n) 并且只使用常数级别额外空间的解决方案。\n\n\n\n\n\n\n\n\n\n输入：nums = [1,2,0]输出：3\n42. 接雨水给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。\n\n\n\n\n\n\n\n\n\n\n输入：height = [0,1,0,2,1,0,1,3,2,1,2,1]输出：6解释：上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 \n\n\n\n\n\n\n\n贪心 \n观察图像，发现会有最高点。\n\n从左一直到最高点，可以用贪心计算水量\n右向左到最高点也是同理\n最后看山顶是否有水\n\n\n\nclass Solution {\n    public int trap(int[] height) {\n        int heightest = 0;\n        int heightest_li = 0;\n        int heightest_ri = 0;\n        for(int i = 0;i&lt;height.length;i++){\n            if(height[i]&gt;heightest){\n                heightest = height[i];\n                heightest_li = i;\n            }\n            if(height[i]==heightest){\n                heightest_ri = i;\n            }\n        }\n        int ans = 0;\n        int sums = height[0];\n        int full = 0;\n        int l = height[0];\n        int li = 0;\n        for(int i = 1;i&lt;=heightest_li;i++){\n            sums+=height[i];\n            if(height[i]&gt;l){\n                System.out.println(\"\"+sums+\" \"+full);\n                full+=(i-li)*l;\n                l = height[i];\n                li = i;\n            }\n        }\n        l = height[height.length-1];\n        li = height.length-1;\n\n        for(int i = height.length-1;i&gt;=heightest_ri;i--){\n            sums+=height[i];\n            if(height[i]&gt;l){\n                full+=(li - i)*l;\n                l = height[i];\n                li = i;\n            }\n\n        }\n        for(int i = heightest_li;i&lt;=heightest_ri;i++){\n            sums+=height[i];\n        }\n        full+=(heightest_ri-heightest_li+1)*heightest;\n\n        return full-sums + 2*heightest;\n    }\n}\n43. 字符串相乘给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。\n\n\n\n\n\n\n\n\n\n输入: num1 = “2”, num2 = “3”输出: “6”\n44. 通配符匹配难度困难792\n给定一个字符串 (s) 和一个字符模式 (p) ，实现一个支持 '?' 和 '*' 的通配符匹配。\n‘?’ 可以匹配任何单个字符。‘*’ 可以匹配任意字符串（包括空字符串）。\n两个字符串完全匹配才算匹配成功。\n说明:\n\ns 可能为空，且只包含从 a-z 的小写字母。\np 可能为空，且只包含从 a-z 的小写字母，以及字符 ? 和 *。\n\n\n\n\n\n\n\n\n\n\n输入:s = “aa”p = “a”输出: false解释: “a” 无法匹配 “aa” 整个字符串。\n45. 跳跃游戏 II给你一个非负整数数组 nums ，你最初位于数组的第一个位置。\n数组中的每个元素代表你在该位置可以跳跃的最大长度。\n你的目标是使用最少的跳跃次数到达数组的最后一个位置。\n假设你总是可以到达数组的最后一个位置。\n\n\n\n\n\n\n\n\n\n输入: nums = [2,3,1,1,4]输出: 2解释: 跳到最后一个位置的最小跳跃数是 2。     从下标为 0 跳到下标为 1 的位置，跳 1 步，然后跳 3 步到达数组的最后一个位置。\n\n\n\n\n\n\n\n贪心\n跳之前，看一下跳哪个最合适（下一次可以跳的最远）\n贪心就好\n\n\nclass Solution {\n    public int jump(int[] nums) {\n        int l = 0;\n        int jump = nums[l];\n        int ans = 0;\n        while(l&lt;nums.length-1){\n            ans++;\n            int max_jump = 0;\n            int max_out = 0;\n            int max_i = 0;\n            if(l+jump&gt;=nums.length-1) return ans;\n            for(int j = l+1;j&lt;=l+jump;j++){\n                if(nums[j]-(l+jump-j)&gt;=max_out){\n                    max_out = nums[j]-(l+jump-j);\n                    max_jump =nums[j];\n                    max_i =j;\n                }\n            }\n            jump = max_jump;\n            l = max_i; \n            System.out.println(\"\"+max_i+\" \"+max_jump);\n        }\n        return ans;\n    }\n}\n46. 全排列给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。\n\n\n\n\n\n\n\n\n\n输入：nums = [1,2,3]输出：[[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]]\n\n\n\n\n\n\n\ndfs\n常规dfs\n\n\nclass Solution {\n    public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) {\n        List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;List&lt;Integer&gt;&gt;();\n        List&lt;Integer&gt; tmp = new ArrayList&lt;Integer&gt;();\n        int[] map = new int[nums.length];\n        dfs(ans,nums,map,0,tmp);\n        return ans;\n        \n    }\n    private void dfs(List&lt;List&lt;Integer&gt;&gt; ans,int[] nums,int[] map,int l,List&lt;Integer&gt; tmp){\n        if(l==nums.length) ans.add(new ArrayList&lt;Integer&gt;(tmp));\n        for(int i = 0;i&lt;nums.length;i++){\n            if(map[i]==0){\n                map[i] = 1;\n                tmp.add(nums[i]);\n                dfs(ans,nums,map,l+1,tmp);\n                tmp.remove(tmp.size()-1);\n                map[i]=0;\n            }\n        }\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n咕咕咕\n\n\n","slug":"l01","date":"2021-11-04T12:06:25.000Z","categories_index":"LeetCode","tags_index":"哈希","author_index":"YFR718"},{"id":"3316961ecf1671c6065b043220187c47","title":"Java基础","content":"Java概述是SUN(Stanford University Network，斯坦福大学网络公司 ) 1995年推出的一门高级编程语言。\nJava语言特性\n特点一：面向对象\n两个基本概念：类、对象\n三大特性：封装、继承、多态\n\n特点二：健壮性\n吸收了C/C++语言的优点，但去掉了其影响程序健壮性的部分（如指针、内存的申请与释放等），提供了一个相对安全的内存管理和访问机制\n\n特点三：跨平台性\n跨平台性：通过Java语言编写的应用程序在不同的系统平台上都可以运行。“Write once , Run Anywhere”\n原理：只要在需要运行 java 应用程序的操作系统上，先安装一个Java虚拟机 (JVM Java Virtual Machine) 即可。由JVM来负责Java程序在该系统中的运行。\n因为有了JVM，同一个Java 程序在三个不同的操作系统中都可以执行。这样就实现了Java 程序的跨平台性。\n\n\n核心机制—Java虚拟机\nJVM是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器。\n对于不同的平台，有不同的虚拟机。\n只有某平台提供了对应的java虚拟机，java程序才可在此平台运行\nJava虚拟机机制屏蔽了底层运行平台的差别，实现了“一次编译，到处运行”\n\n核心机制—垃圾回收​        不再使用的内存空间应回收——垃圾回收。在C/C++等语言中，由程序员负责回收无用内存。Java 语言消除了程序员回收无用内存空间的责任：它提供一种系统级线程跟踪存储空间的分配情况。并在JVM空闲时，检查并释放那些可被释放的存储空间。垃圾回收在Java程序运行过程中自动进行，程序员无法精确控制和干预。\n\n\n\n\nJava语言的环境\n功能\n包含\n\n\n\n\nJDK(Java Development Kit    Java开发工具包)\nJDK是提供给Java开发人员使用的，其中包含了java的开发工具，也包括了JRE。所以安装了JDK，就不用在单独安装JRE了。\nJRE + 开发工具集（编译工具javac.exe 打包工具jar.exe)\n\n\nJRE(Java Runtime Environment    Java运行环境)\n包括Java虚拟机(JVM Java Virtual Machine)和Java程序所需的核心类库等，如果想要运行一个开发好的Java程序，计算机中只需要安装JRE即可。\nJVM + Java SE标准类库\n\n\nJVM (Java Virtal Machine Java虚拟机）\nJVM是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器。\n\n\n\n\nHello Java​        我们来编写第一个简单的Java程序：\npublic class HelloJava {\n    public static void main(String[] args) {\n        System.out.println(\"2333\");\n    }\n}\n\npublic称为访问修饰符（accessmodifier），这些修饰符用于控制程序的其他部分对这段代码的访问级别。\nclass表明Java程序中的全部内容都包含在类中。\nHelloJava：类名。\nstatic：静态方法。\nvoid：无参数返回。\nmain：主方法，程序的入口。\nString[]：程序执行的命令行参数。\nargs：参数名。\nSystem.out.println()：输出一行语句。\n\nJava运行步骤：\n将 Java 代码编写到扩展名为 .java 的文件中。\n通过 javac 命令对该 java 文件进行编译。\n通过 java 命令对生成的 class 文件进行运行。\n\nC:\\Users\\24378\\Desktop\\Java&gt;javac f1.java\nC:\\Users\\24378\\Desktop\\Java&gt;java f1\nhello Java\n一些规范Java命名规范\n\n\n\n类型(名)\n约束\n列\n\n\n\n\n项目\n全部小写，画线-分隔\nspring-cloud\n\n\n包\n全部小写\ncom.yfr.softmax\n\n\n类\n单次首字母大写\nNewWorld\n\n\n变量/方法\n首字母小写\nuserName\n\n\n常量\n全部大写，下划线_分隔\nINT_MAX\n\n\n\n\nJava代码编写规范public class HelloJava {\n    public static void main(String[] args) {\n        System.out.println(\"2333\");\n    }\n}\n\n注意大括号的风格\n二元三元运算符两边用一个空格隔开\n逗号语句后如不换行，紧跟一个空格\n将类似操作，或一组操作放在一起不用空行隔开，而用空行隔开不同组的代码\n每个if while for等语句，都不要省略大括号{}\n\nJava注释\n\n\n\n注释类型\n\n\n\n\n\n\n单行注释\n//注释文字\n\n\n\n多行注释\n/  注释文字 /\n\n\n\n文档注释：\n/*@author  指定java程序的作者@version  指定源文件的版本/\n\n\n\n\n# 生成注释文档\nPS C:\\Users\\24378\\Desktop\\Java&gt; javadoc -d mydoc -author -version .\\f1.java\n正在加载源文件.\\f1.java...\n正在构造 Javadoc 信息...\n...\nJava注意事项\nJava源文件以“java”为扩展名。源文件的基本组成部分是类（class），如本例中的HelloWorld类。\n\nJava应用程序的执行入口是main()方法。它有固定的书写格式：\npublic static void main(String[] args)  {...}\n\n\n\nJava语言严格区分大小写。\n\nJava方法由一条条语句构成，每个语句以“;”结束。\n\n大括号都是成对出现的，缺一不可。\n\n一个源文件中最多只能有一个public类。其它类的个数不限，如果源文件包含一个public类，则文件名必须按该类名命名。\n\n\n基本数据类型整数\n\n\n\n类型\n存储需求\n取值范围\n例\n\n\n\n\nint\n4字节\n（21亿）\n10\n\n\nshort\n2字节\n（32768）\n\n\n\nlong\n8字节\n\n10L\n\n\nbyte\n1字节\n（128）\n\n\n\n\n浮点数\n\n\n\n类型\n存储需求\n符号位\n指数位\n尾数位\n例\n\n\n\n\nfloat\n4字节\n1\n8\n23\n3.14F\n\n\ndouble\n8字节\n1\n11\n52\n3.14\n\n\n\n\n更详细\nchar类型\n\n\n\n类型\n存储需求\n\n\n\n\n\nchar\n2字节\n1\n\n\n\n\nchar类型的字面量值要用单引号括起来。例如：’A’.\n在Java中，char类型描述了UTF-16编码中的一个代码单元。对char类型字符运行时，直接当做ASCII表对应的整数来对待。\nUnicode 编码:一种编码，将世界上所有的符号都纳入其中。每一个符号都给予一个独一无二的编码，使用 Unicode 没有乱码的问题。\nUnicode 的缺点：Unicode 只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储：无法区别 Unicode 和ASCII：计算机无法区分三个字节表示一个符号还是分别表示三个符号。另外，我们知道，英文字母只用一个字节表示就够了，如果unicode统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储空间来说是极大的浪费。\nUTF-8:一种变长的编码方式。它可以使用 1-6 个字节表示一个符号，根据不同的符号而变化字节长度。\nUTF-8的编码规则：1）对于单字节的UTF-8编码，该字节的最高位为0，其余7位用来对字符进行编码（等同于ASCII码）。2）对于多字节的UTF-8编码，如果编码包含 n 个字节，那么第一个字节的前 n 位为1，第一个字节的第 n+1 位为0，该字节的剩余各位用来对字符进行编码。在第一个字节之后的所有的字节，都是最高两位为”10”，其余6位用来对字符进行编码。\nboolean\n\n\n\n类型\n存储需求\n\n\n\n\n\nboolean\n1位\ntrue/flase\n\n\n\n\n变量变量的声明与使用标识符：Java 对各种变量、方法和类等要素命名时使用的字符序列称为标识符定义合法标识符规则：\n\n由26个英文字母大小写，0-9 ，_或$ 组成\n数字不可以开头。\n不可以使用关键字和保留字，但能包含关键字和保留字。\nJava中严格区分大小写，长度无限制。\n标识符不能包含空格。\n\n变量的概念：内存中的一个存储区域，该区域的数据可以在同一类型范围内不断变化。变量是程序中最基本的存储单元。包含变量类型、变量名和存储的值。变量的作用：用于在内存中保存数据\n使用变量注意：\n\nJava中每个变量必须先声明，后使用\n使用变量名来访问这块区域的数据\n变量的作用域：其定义所在的一对{ }内\n变量只有在其作用域内才有效\n同一个作用域内，不能定义重名的变量\n\n声明变量\n\n语法：&lt;数据类型&gt; &lt;变量名称&gt;;\n\n例如：int var;\n\n\n变量的赋值\n\n语法：&lt;变量名称&gt; =  &lt;值&gt;\n例如：var = 10;\n\n声明和赋值变量\n\n语法： &lt;数据类型&gt; &lt;变量名&gt; =  &lt;初始化值&gt;\n例如：int var = 10;\n\n变量的分类-按数据类型\n\n变量的分类-按声明的位置\n\n在方法体外，类体内声明的变量称为成员变量。\n在方法体内部声明的变量称为局部变量。\n\n\n常量在Java中，利用关键字final指示常量。\nfinal int A = 1000;\n数值类型之间的转换自动类型转换：容量小的类型自动转换为容量大的数据类型。数据类型按容量大小排序为：\n\n\n有多种类型的数据混合运算时，系统首先自动将所有数据转换成容量最大的那种数据类型，然后再进行计算。\nbyte,short,char之间不会相互转换，他们三者在计算时首先转换为int类型。\nboolean类型不能与其它数据类型运算。\n当把任何基本数据类型的值和字符串(String)进行连接运算时(+)，基本数据类型的值将自动转化为字符串(String)类型。\n\nString str1 = 4; //判断对错: no\nString str2= 3.5f + \"\"; //判断str2对错: yes\nSystem.out.printIn(str2); //输出:”3.5”\nSystem.out .println(3+4+\"Hello!\"); //输出:7Hello!\nSystem.out.println(\"Hello!\" +3+4); //输出: Hello!34\nSystem.out.printIn('a'+ 1+\"Hello!\"); //输 出: 98Hello!\nSystem.out.printIn(\"Hello\"+'a'+1); //输出: Helloa1\n强制类型转换：将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符：()，但可能造成精度降低或溢出,格外要注意。\n通常，字符串不能直接转换为基本类型，但通过基本类型对应的包装类则可以实现把字符串转换成基本类型。\n// 判断是否能通过编译\nshort s = 5;\ns = s - 2; //报错：int 不能用short存储\ns = (short) (s - 2); //正确写法\n\nbyte b = 3;\nb = b + 4; //报错：int 不能用byte存储\nb = (byte) (b + 4); //正确写法\n\nchar a = 'a';\nint i = 5;\nfloat d = .314F;\ndouble result = a + i + d; //正确\n\nbyte b = 5;\nshort s = 3;\nshort t = s + b; //报错：int 不能用short存储\n进制与运算符\n\n\n\n进制\n\n\n\n\n\n\n二进制 (binary)\n0,1\n满2进1.以0b或0B开头\n\n\n十进制 (decimal)\n0-9\n满10进1\n\n\n八进制 (octal)：\n0-7\n满8进1. 以数字0开头表示\n\n\n十六进制 (hex) (hex)(hex)(hex)：\n0-9，A-F\n满16进1. 以0x或0X开头表示\n\n\n\n\n进制的基本转换    十进制 二进制互转       二进制转成十进制 乘以2的幂数       十进制转成二进制 除以2取余数    二进制 八进制互转    二进制 十六进制互转    十进制八进制互转    十进制十六进制互转\n\n\n\n\n运算符\n运算\n\n\n\n\n算术运算符\n+-*/%++–\n\n\n赋值运算符\n=,+=, -=, *=, /=, %=\n\n\n比较运算符（关系运算符）\n==,!=,&lt;,&gt;,&lt;=,&gt;=,instanceof\n\n\n逻辑运算符\n&amp;,\\\n,!,&amp;&amp;,\\\n\\\n,^\n\n\n位运算符\n&lt;&lt;,&gt;&gt;,&gt;&gt;&gt;,&amp;,\\\n,^,~\n\n\n三元运算符\n(条件表达式)?表达式1：表达式2；\n\n\n\n\n\n\n\n\n\n位运算符的细节\n\n\n\n\n&lt;&lt;\n空位补0，被移除的高位丢弃，空缺位补0。\n\n\n&gt;&gt;\n被移位的二进制最高位是0，右移后，空缺位补0；最高位是1，空缺位补1。\n\n\n&gt;&gt;&gt;\n被移位二进制最高位无论是0或者是1，空缺位都用0补。\n\n\n&amp;\n二进制位进行&amp;运算，只有1&amp;1时结果是1，否则是0;\n\n\n\\\n\n二进制位进行 \\\n运算，只有0 \\\n0时结果是0，否则是1;\n\n\n^\n相同二进制位进行 ^ 运算，结果是0；1^1=0 , 0^0=0不相同二进制位 ^ 运算结果是1。1^0=1 , 0^1=1\n\n\n~\n正数取反，各二进制码按补码各位取反负数取反，各二进制码按补码各位取反\n\n\n\n\n运算优先级：\n思考1：\nshort s = 3; \ns = s+2;//s1+2运算结果是int型，需要强制转换类型   \n① s += 2;    //+= 不改变原本的数据类型\n\n思考2：\nint i = 1;\ni *= 0.1;  //*= 不改变原本的数据类型\nSystem.out.println(i);\ni++;\nSystem.out.println(i);\n\n思考3：\nint m = 2;\nint n = 3;\nn *= m++; \nSystem.out.println(\"m=\" + m); System.out.println(\"n=\" + n); 3，6\n    \n思考4：\nint n = 10;\nn += (n++) + (++n); //10 + 10 + 12\nSystem.out.println(n);\n“&amp;”和“&amp;&amp;”的区别：“|”和“||”的区别同理    单&amp;时，左边无论真假，右边都进行运算；    双&amp;时，如果左边为真，右边参与运算，如果左边为假，那么右边不参与运算。\nint x = 1,y = 1;\nif(x++==2 &amp; ++y==2){\n    x =7;\n}\nSystem.out.println(\"x=\"+x+\",y=\"+y);\n// 2，2\nint x = 1,y = 1;\nif(x++==2 &amp;&amp; ++y==2){\n    x =7;\n}\nSystem.out.println(\"x=\"+x+\",y=\"+y);\n// 2，1\nint x = 1,y = 1;\nif(x++==1 | ++y==1){\n    x =7;\n}\nSystem.out.println(\"x=\"+x+\",y=\"+y);\n// 7，2\n\nint x = 1,y = 1;\nif(x++==1 || ++y==1){\n    x =7;\n}\nSystem.out.println(\"x=\"+x+\",y=\"+y);\n// 7，1\n\nclass Test {\n    public static void main (String [] args) {\n        boolean x=true;\n        boolean y=false;\n        short z=42;\n        //if(y == true)\n        if((z++==42)&amp;&amp;(y=true))z++;\n        if((x=false) || (++z==45)) z++;\n        System.out.println(\"z=\"+z);\n    }\n}\n// z=46\nStringString s = \"123456\";\n\n\n\n\n常用方法\n功能\n示例/说明\n\n\n\n\ns.substring(x,y)\n获得字串[x,y-1]\na.substring(0,3)\n\n\ns1 + s2\n拼接字符串\ns + “789”\n\n\nString.jion(‘’,”,s1,s2,s3)\n多个字符串放在一起用分隔符\n静态方法\n\n\ns.equals(s1)\n检查两字符串是否相等\n\n\n\ns.equalsIgnoreCase(s1)\n检查两字符串是否相等，忽略大小写\n\n\n\ns.length()\n获取字符串长度\n空串：长度为0\n\n\ns.codePointCount(0，s.length();\n得到实际的长度，即码点数量\n\n\n\ns.charAt(n)\n返回位置n的代码单元\n\n\n\nint codePointAt(int index)\n返回从给定位置开始的码点。\n\n\n\nint offsetByCodePoints(int startIndex, int cpCount)\n返回从startIndex代码点开始，位移cpCount后的码点索引。\n\n\n\nint compareTo(String other)\n按照字典顺序，如果字符串位于other之前，返回一个负数；如果字符串位于other之后，返回一个正数；如果两个字符串相等，返回0。\n\n\n\nIntStream codePoints()\n将这个字符串的码点作为一个流返回。调用toArray将它们放在一个数组中。\n\n\n\nnew String(int[ ] codePoints, int offset, int count)\nnew String(int[ ] codePoints, int offset, int count)\n\n\n\nnew String(int[ ] codePoints, int offset, int count)\nnew String(int[ ] codePoints, int offset, int count)\n\n\n\nboolean equalsIgnoreCase(String other)\nboolean equalsIgnoreCase(String other)\n\n\n\nboolean startsWith(String prefix)\n\n\n\n\nboolean endsWith(String suffix)\n如果字符串以suffix开头或结尾，则返回true。\n\n\n\nint index0f(String str, int fromIndex)\n返回与字符串str或代码点cp匹配的第一个子串的开始位置。这个位置从索引0或fromIndex开始计算。如果在原始串中不存在str，返回-1。\n\n\n\nint lastIndex0f(String str, int fromIndex)\nint lastIndex0f(String str, int fromIndex)\n\n\n\nint codePointCount(int startIndex, int endIndex)\n返回startIndex和endIndex-1之间的代码点数量。没有配成对的代用字符将计入代码点。\n\n\n\nString replace(CharSequence oldString, CharSequence newString)\n返回一个新字符串。这个字符串用newString代替原始字符串中所有的oldString。可以用String或StringBuilder对象作为CharSequence参数。\n\n\n\nString toUpperCase( )String toUpperCase( )\n返回一个新字符串。这个字符串将原始字符串中的大写字母改为小写，或者将原始字符串中的所有小写字母改成了大写字母。\n\n\n\nString trim( )\n返回一个新字符串。这个字符串将删除了原始字符串头部和尾部的空格。\n\n\n\n\n\n\n\n\n\n注意事项：\n\nString没办法修改字符串，可以通过提取字串+拼接实现\n\n一定不要使用==运算符检测两个字符串是否相等！这个运算符只能够确定两个字符串是否放置在同一个位置上。完全有可能将内容相同的多个字符串的拷贝放置在不同的位置上。\nif(s.substring(0,3) == \"123\") // false\n\n\nStringBuilder需要由较短的字符串构建字符串\nStringBuilder builder = new StringBuilder();\n//当每次需要添加一部分内容时，就调用append方法。\nbuilder.append(\"1\");\nbuilder.append(\"2\");\n//在需要构建字符串时就调用toString方法\nSystem.out.println(builder.toString());\n\n\n\n\n\n\n\n\n\n这个类的前身是StringBuffer，其效率稍有些低，但允许采用多线程的方式执行添加或删除字符的操作。如果所有字符串在一个单线程中编辑（通常都是这样），则应该用StringBuilder替代它。这两个类的API是相同的。\nAPI\n\n\n\n\n方法\n功能\n\n\n\n\n\nStringBuilder()\n构造一个空的字符串构建器。\n\n\n\nint length()\n返回构建器或缓冲器中的代码单元数量。\n\n\n\nStringBuilder append(String str)\n追加一个字符串并返回this。\n\n\n\nStringBuilder append(char c)\n追加一个代码单元并返回this。\n\n\n\nStringBuilder appendCodePoint(int cp)\n追加一个代码点，并将其转换为一个或两个代码单元并返回this。\n\n\n\nvoid setCharAt(int i, char c)\n将第i个代码单元设置为c。\n\n\n\nStringBuilder insert(int offset, String str)\n在offset位置插入一个字符串并返回this。\n\n\n\nStringBuilder insert(int offset, Char c)\n在offset位置插入一个代码单元并返回this。\n\n\n\n在offset位置插入一个代码单元并返回this。\n在offset位置插入一个代码单元并返回this。\n\n\n\n在offset位置插入一个代码单元并返回this。\n在offset位置插入一个代码单元并返回this。\n\n\n\n\n标准输入输出数据读入import java.util.Scanner;\nScanner in = new Scanner(System,in);\n// 读入一行\nString name = in.nextLine();\n//读入一个单词\nString name = in.next();\n//读入一个整数\n int a = sc.nextInt();\n\n\n\n\n\n方法\n功能\n\n\n\n\nScanner (InputStream in)\n用给定的输入流创建一个Scanner对象。\n\n\nString nextLine( )\n读取输入的下一行内容。\n\n\nString next( )\n读取输入的下一个单词（以空格作为分隔符）。\n\n\nint nextInt( ) int nextInt( )\n读取并转换下一个表示整数或浮点数的字符序列。\n\n\nboolean hasNext( )\n检测输入中是否还有其他单词。\n\n\nboolean hasNextInt( )boolean hasNextDouble( )\n检测是否还有表示整数或浮点数的下一个字符序列。\n\n\nstatic Console console( )\n\n\n\n\n格式化输出System.out.print(x)\n每一个以%字符开始的格式说明符都用相应的参数替换。格式说明符尾部的转换符将指示被格式化的数值类型：\n\n\n\n\n转换符\n类型\n\n\n\n\nd\n十进制\n\n\nx\n十六进制\n\n\no\n八进制\n\n\nf\n定点浮点数\n\n\ne\n指数浮点数\n\n\ng\n通用浮点数\n\n\na\n十六进制浮点数\n\n\ns\n字符串\n\n\nc\n字符\n\n\nb\n布尔\n\n\nh\n散列码\n\n\ntx/TX\n日期时间\n\n\n%\n百分号\n\n\nn\n行分隔符\n\n\n\n\n可以使用多个标志，例如，“%, ( .2f”使用分组的分隔符并将负数括在括号内。\n\n\n\n\n标志\n目的\n\n\n\n\n+\n打印正负号\n\n\n空格\n在正整数前加空格\n\n\n0\n数字前补0\n\n\n-\n左对齐\n\n\n(\n将负数扩在括号里\n\n\n,\n添加分组分隔符\n\n\n#(对f格式)\n包含小数点\n\n\n#(对x或0格式)\n添加前缀0x\n\n\n$\n给定被格式化的参数索引。例如，%ISd. %1$x 将以十进制和十六进制格式打印第1个参数\n\n\n\n&lt;\n格式化前面说明的数值。例如，%d%&lt;x以十进制和十六进制打印同一个数值\n\n\n\n\n\n\n\n文件输入与输出文件读取要想对文件进行读取，就需要一个用File对象构造一个Scanner对象，如下所示：\nScanner in = new Scanner(Paths.get(\"myfile.txt\"),\"UTF-8\");\n如果文件名中包含反斜杠符号，就要记住在每个反斜杠之前再加一个额外的反斜杠：“c:\\mydirectory\\myfile.txt”。\n文件写入要想写入文件，就需要构造一个PrintWriter对象。在构造器中，只需要提供文件名：\nPrintWriter out = new PrintWriter(\"myfile.txt\",\"UTF-8\");\n\n\n\n\n\n\n\n\n\n\nScanner(File f)\n构造一个从给定文件读取数据的Scanner。\n\n\nScanner(String data)\n构造一个从给定字符串读取数据的Scanner。\n\n\nPrintWriter(String fileName)\nPrintWriter(String fileName)\n\n\nstatic Path get(String pathname)\n根据给定的路径名构造一个Path。\n\n\n\n\n\n\n\n块作用域 Block块（即复合语句）是指由一对大括号括起来的若干条简单的Java语句。块确定了变量的作用域。一个块可以嵌套在另一个块中。\n注意：\n\n不能在嵌套的两个块中声明同名的变量。\n块内定义的变量块外不能使用。\n\n\n流程控制语句\n\n\n\n流程控制\n\n\n\n\n\n\n顺序结构\n\n\n\n\n分支语句\nif-else、switch-case\n\n\n\n循环结构\nfor、while、do-while\n\n\n\n特殊流程控制语句\nbreak、continue、return\n\n\n\n\nswitch (i){\n    case 0:...;\n        break;\n    case 1:...;\n        break;\n    default:...;\n        break;\n}\ncase标签可以是：\n● 类型为char、byte、short或int的常量表达式。\n● 枚举常量。\n● 从Java SE 7开始，case标签还可以是字符串字面量。\n带标签的break语句有时候，在嵌套很深的循环语句中会发生一些不可预料的事情。此时可能更加希望跳到嵌套的所有循环语句之外。通过添加一些额外的条件判断实现各层循环的检测很不方便。\n这里有一个示例说明了break语句的工作状态。请注意，标签必须放在希望跳出的最外层循环之前，并且必须紧跟一个冒号。\nlabel:\n{\n    ...\n\tif (condition) break label; // exits b1ock\n\t...\n}\n\n大数值如果基本的整数和浮点数精度不能够满足需求，那么可以使用java.math包中的两个很有用的类：BigInteger和BigDecimal。这两个类可以处理包含任意长度数字序列的数值。BigInteger类实现了任意精度的整数运算，BigDecimal实现了任意精度的浮点数运算。\n//使用静态的valueOf方法可以将普通的数值转换为大数值\nBigInteger a = BigInteger.valueOf(999999999999999999l);\n\n//使用大数值类中的add和multiply方法。\nBigInteger d = a.multiply(a);\nSystem.out.println(d);\n\n\n\n\nBigInteger\n\n\n\n\n\nBigInteger add(BigInteger other)\n+\n\n\nBigInteger subtract(BigInteger other)\n-\n\n\nBigInteger multiply(BigInteger other)\n*\n\n\nBigInteger divide(BigInteger other)\n/\n\n\nBigInteger divide(BigInteger other)\n%\n\n\nint compareTo(BigInteger other)\n相等，返回0；小于，返回负数；否则，返回正数。\n\n\nstatic BigInteger valueOf(long x)\n返回值等于x的大整数。\n\n\n\n\n\n\n\n\nBigInteger\n\n\n\n\n\nBigDecimal add(BigDecimal other)\n+\n\n\nBigDecimal subtract(BigDecimal other)\n-\n\n\nBigDecimal multiply(BigDecimal other)\n*\n\n\nBigDecimal multiply(BigDecimal other)\n/\n\n\nint compareTo(BigDecimal other)\n比较\n\n\nstatic BigDecimal valueOf(long x)\n\n\n\nstatic BigDecimal valueOf(long x, int scale)\n\n\n\n\n\nJava数组数组(Array)，是多个相同类型数据按一定顺序排列的集合，并使用一个名字命名，并通过编号的方式对这些数据进行统一管理。\n数组的常见概念\n\n数组名\n下标(或索引)\n元素\n数组的长度\n\n一维数组的创建与使用// 一维数组的声明方式：\ntype var[] 或 type[]  var;\n//Java语言中声明数组时不能指定其长度(数组中元素的数)， 例如： \nint a[5];  //非法\n//动态初始化：数组声明且为数组元素分配空间与赋值的操作分开进行\nint[] arr = new int[3];\nString names[]; names = new String[3];\n//静态初始化：在定义数组的同时就为数组元素分配空间并赋值。\nint arr[] = new int[]{ 3, 9, 8};\nint[] arr = {3,9,8};\n// 属性length\na.length 指明数组a的长度(元素个数)\n数组的默认初始化与内存分析数组是引用类型，它的元素相当于类的成员变量，因此数组一经分配空间，其中的每个元素也被按照成员变量同样的方式被隐式初始化。\n\n对于基本数据类型而言，默认初始化值各有不同\n对于引用数据类型而言，默认初始化值为null(注意与0不同！\n\n\n\n\n\n\n\n\n\n\n数组元素类型\n元素默认初始值\n\n\n\n\nbyte\n0\n\n\nshort\n0\n\n\nint\n0\n\n\nlong\n0L\n\n\nfloat\n0.0F\n\n\ndouble\n0.0\n\n\nchar\n0 或写为:’\\u0000’(表现为空)\n\n\nboolean\nfalse\n\n\n引用类型\nnull\n\n\n\n\nfor each循环依次处理数组中的每个元素（其他类型的元素集合亦可）而不必为指定下标值而分心。\nfor (v : collection) statement;\n\nint[] ints = new int[100];\nfor (int anInt : ints) {\n    ;\n}\n数组的拷贝将一个数组变量拷贝给另一个数组变量。这时，两个变量将引用同一个数组：\nint[] ints = new int[100];\nint[] b = int;\n将一个数组的所有值拷贝到一个新的数组中去，就要使用Arrays类的copyOf方法：\nint b = Arrays.copyof(a,a.length)\n多维数组// 格式1（动态初始化）：\nint[][] arr = new int[3][2];\n// 格式2（动态初始化）：\nint[][] arr = new int[3][];\n// 每个一维数组都是默认初始化值null (注意：区别于格式1）可以对这个三个一维数组分别进行初始化\narr[0] = new int[3];    \narr[1] = new int[1];   \narr[2] = new int[2];\n注：int[][]arr = new int[][3];  //非法\n// 格式3（静态初始化）：\nint[][] arr = new int[][]{{3,8,2},{2,7},{9,0,1,6}};\n// 注意特殊写法情况：\nint[] x,y[]; x是一维数组，y是二维数组。\n\n// 声明：int[] x,y[]; 在给x,y变量赋值以后，以下选项允许通过编译的是：\nx[0] = y;  no\ny[0] = x;   yes\ny[0][0] = x;   no\nx[0][0] = y;  no\ny[0][0] = x[0];  yes\nx = y;   no\n// 提示：\n一维数组：int[] x  或者int x[]   \n二维数组：int[][] y 或者 int[] y[]  或者 int y[][]\n操作数组的工具类Arraysjava.util.Arrays类即为操作数组的工具类，包含了用来操作数组（比如排序和搜索）的各种方法。\n\n\n\n\n工具类Arrays\n\n\n\n\n\nboolean equals(int[] a,int[] b)\n判断两个数组是否相等。\n\n\nString toString(int[] a)\n输出数组信息。\n\n\nvoid fill(int[] a,int val)\n将指定值填充到数组之中。\n\n\nvoid sort(int[] a)\n对数组进行排序。\n\n\nint binarySearch(int[] a,int key)\n对排序后的数组进行二分法检索指定的值。\n\n\nstatic type copyOf(type[] a, int length)\n\n\n\nstatic type copyOfRange(type[] a, int start, int end)\n\n\n\nstatic void sort(type[] a)\n优化的快速排序算法\n\n\nstatic int binarySearch(type[] a, type v)\n\n\n\nstatic int binarySearch(type[] a, int start, int end, type v)\n二分搜索算法查找值v\n\n\nstatic void fill(type[] a, type v)\n将数组的所有数据元素值设置为v。\n\n\nstatic boolean equals(type[] a, type[] b)\n如果两个数组大小相同，并且下标相同的元素都对应相等，返回true。\n\n\n\n\n\n\n\n\n\n\n\nJava关键字    定义：被Java语言赋予了特殊含义，用做专门用途的字符串（单词）    特点：关键字中所有字母都为小写\n\n\n\n\n用于定义数据类型的关键字\n\n\n\n\n\n\n\n\nclass\ninterface\nenum\nbyte\nshort\n\n\nint\nlong\nfloat\ndouble\nchar\n\n\nboolean\nvoid\n\n\n\n\n\n用于定义流程控制的关键字\n\n\n\n\n\n\nif\nelse\nswitch\ncase\ndefault\n\n\nwhile\ndo\nfor\nbreak\ncontinue\n\n\nreturn\n\n\n\n\n\n\n用于定义访问权限修饰符的关键字\n\n\n\n\n\n\nprivate\nprotect\npublic\n\n\n\n\n用于定义类，函数，变量修饰符的关键字\n\n\n\n\n\n\nabstract\nfinal\nstatic\nsynchronized\n\n\n\n用于定义类与类之间关系的关键字\n\n\n\n\n\n\nextends\nimplements\n\n\n\n\n\n用于定义建立实例及引用实例，判断实例的关键字\n\n\n\n\n\n\nnew\nthis\nsuper\ninstanceof\n\n\n\n用于异常处理的关键字\n\n\n\n\n\n\ntry\ncatch\nfinally\nthrow\nthrows\n\n\n用于包的关键字\n\n\n\n\n\n\npackage\nimport\n\n\n\n\n\n其他修饰符关键字\n\n\n\n\n\n\nnative\nstrictfp\ntransient\nvolatile\nassert\n\n\n* 用于定义数据类型值的字面值\n\n\n\n\n\n\ntrue\nfalse\nnull\n\n\n\n\n\nJava保留字：现有Java版本尚未使用，但以后版本可能会作为关键字使用。自己命名标识符时要避免使用这些保留字goto 、const\nJava的内存管理与垃圾回收JVM的内存结构\n\n\n\n\n\n\n\n\n\n\n\n栈（Stack）\n虚拟机栈用于存储局部变量等。局部变量表存放了编译期可知长度的各种基本数据类型（boolean、byte、 char 、 short 、 int 、 float 、 long 、 double）、对象引用（reference类型，它不等同于对象本身，是对象在堆内存的首地址）。 方法执行完，自动释放。\n\n\n堆（Heap）\n此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。这一点在Java虚拟机规范中的描述是：所有的对象实例以及数组都要在堆上分配。\n\n\n方法区（Method Area）\n用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。\n\n\n\n\n\n\n\n\n\n\n\n","slug":"J0-Java基础","date":"2021-11-04T06:41:20.000Z","categories_index":"JAVA","tags_index":"java","author_index":"YFR718"},{"id":"07213b6ca5497f88ac351468e0325797","title":"Hexo 个人博客搭建","content":"Hexo 个人博客搭建安装步骤\ngithub创建yfr718.github.io项目\n\n安装git、设置用户名和邮箱、ssh连接github\ngit config --global user.name &quot;你的GitHub用户名&quot;\ngit config --global user.email &quot;你的GitHub注册邮箱&quot;\nssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;\n#打开GitHub_Settings_keys 页面，新建new SSH Key\n#Title为标题，任意填即可，将刚刚复制的id_rsa.pub内容粘贴进去，最后点击Add SSH key。在Git Bash中检测GitHub公钥设置是否成功，输入 \nssh git@github.com\n\n\n\n安装Node.js\n# 安装后，检测Node.js是否安装成功，在命令行中输入 \nnode -v\n\n安装Hexo\n# 在电脑常里创建一个文件夹，可以命名为Blog，Hexo框架与以后你自己发布的网页都在这个文件夹中。\n# 在相应文件夹安装Hexo\nnpm install -g hexo-cli \n# 初始化博客\nhexo init blog\n# 查看博客网页\nhexo g #生成网页\nhexo s #运行网页\n# 完成后，打开浏览器输入地址：\nlocalhost:4000\n\n\nHexo 常用命令博客管理npm install hexo -g # 安装Hexo\nnpm update hexo -g # 升级\nhexo init # 初始化博客\nhexo clean # 清除缓存文件 db.json 和已生成的静态文件 public\nhexo g # 自动生成网站静态文件，并部署到设定的仓库\nhexo s # 启动本地服务器，用于预览主题。默认地址： http:&#x2F;&#x2F;localhost:4000&#x2F;\nhexo d # 自动生成网站静态文件，并部署到设定的仓库\nhexo clean &amp;&amp; hexo g &amp;&amp; hexo d # 本地更新后重新部署到github上\n文章管理# 新建文章\nhexo new 文章文件名\n一些markdown格式\n\n\n\n\n\n\n\n\n123456\n\n\n\n\n\n\n提示\nNormal Tips Container\n\n\n\n\n\n\n\n\n注意\nWarning!!!\n\n\n\n\n\n\n\n\naaa\nWarning!!!\n\n\n\n\n\n\n\n\n特别注意\nDanger!!!\n\n\nClick to see more\n\n隐藏内容\n\n\n\n引用块\n\n","slug":"0.Hexo个人博客搭建","date":"2021-11-02T09:47:00.000Z","categories_index":"前端","tags_index":"hexo","author_index":"YFR718"},{"id":"9b69dffe0262764ac691bc95bc415c22","title":"Python基础","content":"\n\n\n\n\n\n\n\n\n天池Python训练营\nPython基础1.变量、运算符与数据类型1.1 注释# 表示注释，作用于整行。\n‘’’ ‘’’ 或者 “”” “”” 表示区间注释，在三引号之间的所有内容被注释\n# 这是一个注释\nprint(\"Hello world\")\n'''\n这是多行注释，用三个单引号\n'''\n\"\"\"\n这是多行注释，用三个双引号\n\"\"\"\n1.2 运算符\n\n\n\n运算符\n\n\n\n\n\n\n算术运算符\n+, -, , /, //, %, *\n\n\n\n比较运算符\n&gt;, &gt;=, &lt;, &lt;=, !=\n\n\n\n逻辑运算符\nand, or, not\n\n\n\n位运算符\n~, &amp;, \\\n, ^, &lt;&lt;, &gt;&gt;\n\n\n\n三元运算符\nx if x&lt;y else y\n\n\n\n其他运算符\nis, not is, in, not in\n\n\n\n\n注意\n\nis, is not 对比的是两个变量的内存地址：假如比较的两个变量，指向的都是地址不可变的类型（str等），那么is，is not 和 ==，！= 是完全等价的。\n==, != 对比的是两个变量的值：假如对比的两个变量，指向的是地址可变的类型（list，dict，tuple等），则两者是有区别的。\n\n运算符的优先级\n\n一元运算符优于二元运算符。如正负号。\n先算术运算，后移位运算，最后位运算。例如 1 &lt;&lt; 3 + 2 &amp; 7等价于 (1 &lt;&lt; (3 + 2)) &amp; 7\n逻辑运算最后结合\n\n位运算原码、反码和补码\n二进制有三种不同的表示形式：原码、反码和补码，计算机内部使用补码来表示。\n原码：就是其二进制表示（注意，有一位符号位）。\n反码：正数的反码就是原码，负数的反码是符号位不变，其余位取反（对应正数按位取反）。\n补码：正数的补码就是原码，负数的补码是反码+1。\n符号位：最高位为符号位，0表示正数，1表示负数。在位运算中符号位也参与运算。利用位运算实现快速计算\n\n通过 &lt;&lt; ， &gt;&gt; 快速计算2的倍数问题。\n\n通过 ^ 快速交换两个整数。\na ^= b\nb ^= a\na ^= b\n\n通过 a &amp; (-a) 快速获取a 的最后为 1 位置的整数。\n\n\n利用位运算实现整数集合\n\n一个数的二进制表示可以看作是一个集合（0 表示不在集合中，1 表示在集合中）。比如集合 {1, 3, 4, 8} ，可以表示成 01 00 01 10 10 而对应的位运算也就可以看作是对集合进行的操作。\n\n元素与集合的操作：\na | (1&lt;&lt;i) -> 把 i 插入到集合中\na &amp; ~(1&lt;&lt;i) -> 把 i 从集合中删除\na &amp; (1&lt;&lt;i) -> 判断 i 是否属于该集合（零不属于，非零属于）\n\n集合之间的操作：\na 补 -> ~a\na 交 b -> a &amp; b\na 并 b -> a | b\na 差 b -> a &amp; (~b)\n\n\n1.3 变量和赋值\n在使用变量之前，需要对其先赋值。\n变量名可以包括字母、数字、下划线、但变量名不能以数字开头。\nPython 变量名是大小写敏感的，foo != Foo。\n\nfirst = 2\nsecond = 3\nthird = first + second\nprint(third) # 5\n1.4 数据类型与转换Python3 中有六个标准的数据类型：\n\nNumber（数字）：int、float、bool、complex（复数）。\nString（字符串）\nList（列表）\nTuple（元组）\nSet（集合）\nDictionary（字典）\n\nPython3 的六个标准数据类型中：\n\n不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）；\n可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。\n\n简单数据类型\n\n整型\n浮点型\n布尔型\n\n容器数据类型\n\n列表\n元组\n字典\n集合\n字符串\n\n1.5 print() 函数print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False)\n\n将对象以字符串表示的方式格式化输出到流文件对象file里。其中所有非关键字参数都按str() 方式进行转换为字符串输出；\n关键字参数sep 是实现分隔符，比如多个参数输出时想要输出中间的分隔字符；\n关键字参数end 是输出结束时的字符，默认是换行符\\n ；\n关键字参数file 是定义流输出的文件，可以是标准的系统输出sys.stdout ，也可以重定义为别的文件；\n关键字参数flush 是立即把内容输出到流文件，不作缓存。\n\n【例子】， item 值与’another string’ 两个值之间用sep 设置的参数&amp; 分割。由于end 参数没有设置，因此默认是输出解释后换行，即end 参数的默认值为\\n 。\nshoplist = ['apple', 'mango', 'carrot', 'banana']\nprint(\"This is printed with 'sep='&amp;''.\")\nfor item in shoplist:\nprint(item, 'another string', sep='&amp;')\n# This is printed with 'sep='&amp;''.\n# apple&amp;another string\n# mango&amp;another string\n# carrot&amp;another string\n# banana&amp;another string\n2 逻辑语句\n\n\n\n语句类型\n\n\n\n\n\n\n判断\nif…elif…else\n\n\n\nwhile循环\nwhile…else…\n\n\n\nfor循环\nfor 迭代变量 in 可迭代对象…else…\n\n\n\n其他\nbreak、continue、pass\n\n\n\n\n# 当while 循环正常执行完的情况下，执行else 输出，如果while 循环中执行了跳出循环的语句，比如 break ，将不执行else 代码块的内容。\ncount = 0\nwhile count &lt; 5:\n    print(\"%d is less than 5\" % count)\n    count = count + 1\nelse:\n\tprint(\"%d is not less than 5\" % count)\nfor i in 'ILoveLSGO':\n\tprint(i, end=' ') # 不换行输出\n# I L o v e L S G O\n\nfor i in range(len(member)):\n\tprint(member[i])\n\t\ndic = &#123;'a': 1, 'b': 2, 'c': 3, 'd': 4&#125;\nfor key, value in dic.items():\n\tprint(key, value, sep=':', end=' ')\n# a:1 b:2 c:3 d:4\n\ndic = &#123;'a': 1, 'b': 2, 'c': 3, 'd': 4&#125;\nfor key in dic.keys():\n\tprint(key, end=' ')\n# a b c d\n\nfor num in range(10, 20): # 迭代 10 到 20 之间的数字\n    for i in range(2, num): # 根据因子迭代\n        if num % i == 0: # 确定第一个因子\n            j = num / i # 计算第二个因子\n            print('%d 等于 %d * %d' % (num, i, j))\n            break # 跳出当前循环\n    else: # 循环的 else 部分\n    \tprint(num, '是一个质数')\n# 10 等于 2 * 5\n# 11 是一个质数\n# 12 等于 2 * 6\n# 13 是一个质数\n# 14 等于 2 * 7\n# 15 等于 3 * 5\n# 16 等于 2 * 8\n# 17 是一个质数\n# 18 等于 2 * 9\n# 19 是一个质数\n\nbreak 语句可以跳出当前所在层的循环。\ncontinue 终止本轮循环并开始下一轮循环。\npass 语句的意思是“不做任何事”，如果你在需要有语句的地方不写任何语句，那么解释器会提示出错，而 pass 语句就是用来解决这些问题的。\n\n2.1assert 关键词assert 这个关键词我们称之为“断言”，当这个关键词后边的条件为 False 时，程序自动崩溃并抛出AssertionError 的异常。\nmy_list = ['lsgogroup']\nmy_list.pop(0)\nassert len(my_list) > 0\n2.2 range() 函数range([start,] stop[, step=1])\n\nfor i in range(2, 9): # 不包含9\n\tprint(i)\n\n这个BIF（Built-in functions）有三个参数，其中用中括号括起来的两个表示这两个参数是可选的。\nstep=1 表示第三个参数的默认值是1。\nrange 这个BIF的作用是生成一个从start 参数的值开始到stop 参数的值结束的数字序列，该序列包含start 的值但不包含stop 的值。\n\n2.3 enumerate()函数enumerate(sequence, [start=0])\n\nsequence — 一个序列、迭代器或其他支持迭代对象。\nstart — 下标起始位置。\n返回 enumerate(枚举) 对象\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\nlst = list(enumerate(seasons))\nprint(lst)\n# [(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]\n\nlst = list(enumerate(seasons, start=1)) # 下标从 1 开始\nprint(lst)\n# [(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]\nenumerate() 与 for 循环的结合使用\nfor i, a in enumerate(A)\n\tdo something with a\n用 enumerate(A) 不仅返回了 A 中的元素，还顺便给该元素一个索引值 (默认从 0 开始)。此外，用enumerate(A, j) 还可以确定索引起始值为 j 。\n2.4 推导式列表推导式[ expr for value in collection [if condition] ]\n\nx = [-4, -2, 0, 2, 4]\ny = [a * 2 for a in x]\nprint(y)\n# [-8, -4, 0, 4, 8]\n\nx = [i ** 2 for i in range(1, 10)]\nprint(x)\n# [1, 4, 9, 16, 25, 36, 49, 64, 81]\n\nx = [(i, i ** 2) for i in range(6)]\nprint(x)\n# [(0, 0), (1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n\nx = [i for i in range(100) if (i % 2) != 0 and (i % 3) == 0]\nprint(x)\n# [3, 9, 15, 21, 27, 33, 39, 45, 51, 57, 63, 69, 75, 81, 87, 93, 99]\n\na = [(i, j) for i in range(0, 3) for j in range(0, 3)]\nprint(a)\n# [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n\nx = [[i, j] for i in range(0, 3) for j in range(0, 3)]\nprint(x)\n# [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]]\nx[0][0] = 10\nprint(x)\n# [[10, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]]\n\na = [(i, j) for i in range(0, 3) if i &lt; 1 for j in range(0, 3) if j > 1]\nprint(a)\n# [(0, 2)]\n元组推导式( expr for value in collection [if condition] )\n\na = (x for x in range(10))\nprint(a)\n# &lt;generator object &lt;genexpr> at 0x0000025BE511CC48>\nprint(tuple(a))\n# (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n字典推导式&#123; key_expr: value_expr for value in collection [if condition] &#125;\n\nb = &#123;i: i % 2 == 0 for i in range(10) if i % 3 == 0&#125;\nprint(b)\n# &#123;0: True, 3: False, 6: True, 9: False&#125;\n集合推导式&#123; expr for value in collection [if condition] &#125;\n\nc = &#123;i for i in [1, 2, 3, 4, 5, 5, 6, 4, 3, 2, 1]&#125;\nprint(c)\n# &#123;1, 2, 3, 4, 5, 6&#125;\n3 异常处理异常就是运行期检测到的错误。计算机语言针对可能出现的错误定义了异常类型，某种错误引发对应的异常时，异常处理程序将被启动，从而恢复程序的正常运行。\n3.1 Python 标准异常总结\nBaseException：所有异常的 基类\nException：常规异常的 基类\nStandardError：所有的内建标准异常的基类\nArithmeticError：所有数值计算异常的基类\nFloatingPointError：浮点计算异常\nOverflowError：数值运算超出最大限制\nZeroDivisionError：除数为零\nAssertionError：断言语句（assert）失败\nAttributeError：尝试访问未知的对象属性\nEOFError：没有内建输入，到达EOF标记\nEnvironmentError：操作系统异常的基类\nIOError：输入/输出操作失败\nOSError：操作系统产生的异常（例如打开一个不存在的文件）\nWindowsError：系统调用失败\nImportError：导入模块失败的时候\nKeyboardInterrupt：用户中断执行\nLookupError：无效数据查询的基类\nIndexError：索引超出序列的范围\nKeyError：字典中查找一个不存在的关键字\nMemoryError：内存溢出（可通过删除对象释放内存）\nNameError：尝试访问一个不存在的变量\nUnboundLocalError：访问未初始化的本地变量\nReferenceError：弱引用试图访问已经垃圾回收了的对象\nRuntimeError：一般的运行时异常\nNotImplementedError：尚未实现的方法\nSyntaxError：语法错误导致的异常\nIndentationError：缩进错误导致的异常\nTabError：Tab和空格混用\nSystemError：一般的解释器系统异常\nTypeError：不同类型间的无效操作\nValueError：传入无效的参数\nUnicodeError：Unicode相关的异常\nUnicodeDecodeError：Unicode解码时的异常\nUnicodeEncodeError：Unicode编码错误导致的异常\nUnicodeTranslateError：Unicode转换错误导致的异常\n\n异常体系内部有层次关系，Python异常体系中的部分关系如下所示：\n\n3.2 Python标准警告总结\nWarning：警告的基类\nDeprecationWarning：关于被弃用的特征的警告\nFutureWarning：关于构造将来语义会有改变的警告\nUserWarning：用户代码生成的警告\nPendingDeprecationWarning：关于特性将会被废弃的警告\nRuntimeWarning：可疑的运行时行为(runtime behavior)的警告\nSyntaxWarning：可疑语法的警告\nImportWarning：用于在导入模块过程中触发的警告\nUnicodeWarning：与Unicode相关的警告\nBytesWarning：与字节或字节码相关的警告\nResourceWarning：与资源使用相关的警告\n\n3.3 异常处理语句try - excepttry:\n\t检测范围\nexcept Exception[as reason]:\n\t出现异常后的处理代码\ntry 语句按照如下方式工作：\n\n首先，执行try 子句（在关键字try 和关键字except 之间的语句）\n如果没有异常发生，忽略except 子句， try 子句执行后结束。\n如果在执行try 子句的过程中发生了异常，那么try 子句余下的部分将被忽略。如果异常的类型和except 之后的名称相符，那么对应的except 子句将被执行。最后执行try 语句之后的代码。\n如果一个异常没有与任何的except 匹配，那么这个异常将会传递给上层的try 中。\n\ntry:\n    int(\"abc\")\n    s = 1 + '1'\n    f = open('test.txt')\n    print(f.read())\n    f.close()\nexcept OSError as error:\n\tprint('打开文件出错\\n原因是：' + str(error))\nexcept TypeError as error:\n\tprint('类型出错\\n原因是：' + str(error))\nexcept ValueError as error:\n\tprint('数值出错\\n原因是：' + str(error))\n# 数值出错\n# 原因是：invalid literal for int() with base 10: 'abc'\n\ndict1 = &#123;'a': 1, 'b': 2, 'v': 22&#125;\ntry:\n\tx = dict1['y']\nexcept KeyError:\n\tprint('键错误')\nexcept LookupError:\n\tprint('查询错误')\nelse:\n\tprint(x)\n# 键错误\ntry-except-else 语句尝试查询不在dict 中的键值对，从而引发了异常。这一异常准确地说应属于KeyError ，但由于KeyError 是LookupError 的子类，且将LookupError 置于KeyError 之前，因此程序优先执行该except 代码块。所以，使用多个except 代码块时，必须坚持对其规范排序，要从最具针对性的异常到最通用的异常。\ntry:\n    s = 1 + '1'\n    int(\"abc\")\n    f = open('test.txt')\n    print(f.read())\n    f.close()\nexcept (OSError, TypeError, ValueError) as error:\n\tprint('出错了！\\n原因是：' + str(error))\n# 出错了！\n# 原因是：unsupported operand type(s) for +: 'int' and 'str'\n一个 except 子句可以同时处理多个异常，这些异常将被放在一个括号里成为一个元组。\ntry - except - finallytry:\n\t检测范围\nexcept Exception[as reason]:\n\t出现异常后的处理代码\nfinally:\n\t无论如何都会被执行的代码\n不管try 子句里面有没有发生异常， finally 子句都会执行。如果一个异常在try 子句里被抛出，而又没有任何的except 把它截住，那么这个异常会在finally 子句执行后被抛出。\ntry - except - else如果在try 子句执行时没有发生异常，Python将执行else 语句后的语句。\ntry:\n\t检测范围\nexcept(Exception1[, Exception2[,...ExceptionN]]]):\n\t发生以上多个异常中的一个，执行这块代码\nelse:\n\t如果没有异常执行这块代码\nraise语句Python 使用raise 语句抛出一个指定的异常。\ntry:\n\traise NameError('HiThere')\nexcept NameError:\n\tprint('An exception flew by!')\n# An exception flew by!\n4 列表 list4.1 列表的定义列表是有序集合，没有固定大小，能够保存任意数量任意类型的 Python 对象，语法为 [元素1, 元素2, …, 元素n] 。\n\n关键点是「中括号 []」和「逗号 ,」\n中括号 把所有元素绑在一起\n逗号 将每个元素一一分开\n\n列表的创建\n\n直接定义\n利用range() 创建列表\n利用推导式创建列表\n\nx = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n\nx = list(range(10))\n\nx = [i for i in range(100) if (i % 2) != 0 and (i % 3) == 0]\n\nx = [[0 for col in range(3)] for row in range(4)]\n由于list的元素可以是任何对象，因此列表中所保存的是对象的指针。即使保存一个简单的[1,2,3] ，也有3个指针和3个整数对象。x = [a] * 4 操作中，只是创建4个指向list的引用，所以一旦a 改变， x 中4个a 也会随之改变。\n4.2 列表的操作添加元素\nlist.append(obj) 在列表末尾添加新的对象，只接受一个参数，参数可以是任何数据类型，被追加的元素在 list中保持着原结构类型。\nlist.extend(seq) 在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）\nlist.insert(index, obj) 在编号 index 位置前插入 obj 。\n\nx = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\nx.append('Thursday')\nx.extend(['Thursday', 'Sunday'])\nx.insert(2, 'Sunday')\n删除元素\nlist.remove(obj) 移除列表中某个值的第一个匹配项\nlist.pop([index=-1]) 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值\ndel var1[, var2 ……] 删除单个或多个对象。\n\nx = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\nx.remove('Monday')\ny = x.pop()\ndel x[0:2]\n获取列表中的元素\n通过元素的索引值，从列表获取单个元素，注意，列表索引值是从0开始的。\n通过将索引指定为-1，可让Python返回最后一个列表元素，索引 -2 返回倒数第二个列表元素，以此类推。\n切片的通用写法是 start : stop : step\n\nx = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\nprint(x[3:]) # ['Thursday', 'Friday']\nprint(x[-3:]) # ['Wednesday', 'Thursday', 'Friday']\n列表的常用操作符\n等号操作符： ==\n连接操作符 +\n重复操作符 *\n成员关系操作符 in 、not in\n\n「等号 ==」，只有成员、成员位置都相同时才返回True。和元组拼接一样， 列表拼接也有两种方式，用「加号 +」和「乘号 *」，前者首尾拼接，后者复制拼接。\nlist1 = [123, 456]\nlist2 = [456, 123]\nlist3 = [123, 456]\nprint(list1 == list2) # False\nprint(list1 == list3) # True\n\nlist4 = list1 + list2 # extend()\nprint(list4) # [123, 456, 456, 123]\n\nlist5 = list3 * 3\nprint(list5) # [123, 456, 123, 456, 123, 456]\n\nlist3 *= 3\nprint(list3) # [123, 456, 123, 456, 123, 456]\n\nprint(123 in list3) # True\nprint(456 not in list3) # False\n前面三种方法（ append , extend , insert ）可对列表增加元素，它们没有返回值，是直接修改了原数据对象。将两个list相加，需要创建新的 list 对象，从而需要消耗额外的内存，特别是当 list 较大时，尽量不要使用 “+” 来添加list。\n列表的其它方法\n\n\n\nlist方法\n\n\n\n\n\nlist.count(obj)\n统计某个元素在列表中出现的次数\n\n\nlist.index(x[, start[, end]])\n从列表中找出某个值第一个匹配项的索引位置\n\n\nlist.reverse()\n反向列表中元素\n\n\nlist.sort(key=None, reverse=False)\n对原列表进行排序。\n\n\n\n\n\n\n\n# 获取列表的第二个元素\ndef takeSecond(elem):\nreturn elem[1]\nx = [(2, 2), (3, 4), (4, 1), (1, 3)]\nx.sort(key=takeSecond)\nprint(x)\n# [(4, 1), (2, 2), (1, 3), (3, 4)]\nx.sort(key=lambda a: a[0])\nprint(x)\n# [(1, 3), (2, 2), (3, 4), (4, 1)]\n5 元组「元组」定义语法为： (元素1, 元素2, …, 元素n)\n\n小括号把所有元素绑在一起\n逗号将每个元素一一分开\n\n5.1 创建和访问一个元组\nPython 的元组与列表类似，不同之处在于tuple被创建后就不能对其进行修改，类似字符串。\n元组使用小括号，列表使用方括号。\n\nt1 = (1, 10.31, 'python')\nt2 = 1, 10.31, 'python'\nprint(t1, type(t1))\n# (1, 10.31, 'python') &lt;class 'tuple'>\nprint(t2, type(t2))\n# (1, 10.31, 'python') &lt;class 'tuple'>\n\ntuple1 = (1, 2, 3, 4, 5, 6, 7, 8)\nprint(tuple1[1]) # 2\nprint(tuple1[5:]) # (6, 7, 8)\nprint(tuple1[:5]) # (1, 2, 3, 4, 5)\ntuple2 = tuple1[:]\nprint(tuple2) # (1, 2, 3, 4, 5, 6, 7, 8)\n\n创建元组可以用小括号 ()，也可以什么都不用，为了可读性，建议还是用 ()。\n元组中只包含一个元素时，需要在元素后面添加逗号，否则括号会被当作运算符使用：\n\n5.2 更新和删除一个元组元组有不可更改 (immutable) 的性质，因此不能直接给元组的元素赋值，但是只要元组中的元素可更改 (mutable)，那么我们可以直接更改其元素，注意这跟赋值其元素不同。\nweek = ('Monday', 'Tuesday', 'Thursday', 'Friday')\nweek = week[:2] + ('Wednesday',) + week[2:]\nprint(week) # ('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday')\n\nt1 = (1, 2, 3, [4, 5, 6])\nprint(t1) # (1, 2, 3, [4, 5, 6])\nt1[3][0] = 9\nprint(t1) # (1, 2, 3, [9, 5, 6])\n\n5.3 元组相关的操作符\n比较操作符\n逻辑操作符\n连接操作符 +\n重复操作符 *\n成员关系操作符 in 、not in\n\nt1 = (2, 3, 4, 5)\nt2 = ('老马的程序人生', '小马的程序人生')\nt3 = t1 + t2\nprint(t3)\n# (2, 3, 4, 5, '老马的程序人生', '小马的程序人生')\nt4 = t2 * 2\nprint(t4)\n# ('老马的程序人生', '小马的程序人生', '老马的程序人生', '小马的程序人生')\n5.4 内置方法\n.count() 是记录在元组  中该元素出现几次\n.index() 是找到该元素在元组的索引\n\nt = (1, 10.31, 'python')\nprint(t.count('python')) # 1\nprint(t.index(10.31)) # 1\n5.5 解压元组t = (1, 10.31, 'python')\n(a, b, c) = t\nprint(a, b, c)\n# 1 10.31 python\n\nt = (1, 10.31, ('OK', 'python'))\n(a, b, (c, d)) = t\nprint(a, b, c, d)\n# 1 10.31 OK python\n\n# 如果你只想要元组其中几个元素，用通配符「*」，英文叫 wildcard，在计算机语言中代表一个或多个元素。下例就是把多个元素丢给了 rest 变量。\nt = 1, 2, 3, 4, 5\na, b, *rest, c = t\nprint(a, b, c) # 1 2 5\nprint(rest) # [3, 4]\n\n# 如果你根本不在乎 rest 变量，那么就用通配符「*」加上下划线「_」。\na, b, *_ = t\nprint(a, b) # 1 2\n6 字符串6.1 字符串的定义\nPython 中字符串被定义为引号之间的字符集合。\nPython 支持使用成对的 单引号 或 双引号\n\nt1 = 'i love Python!'\nprint(t1, type(t1))\n# i love Python! &lt;class 'str'>\nt2 = \"I love Python!\"\nprint(t2, type(t2))\n# I love Python! &lt;class 'str'>\nprint(5 + 8) # 13\nprint('5' + '8') # 58\nPython 的常用转义字符\n\n\n\n\n转移字符\n描述\n\n\n\n\n\\\\\\\\\n反斜杠\n\n\n\\\\’\n单引号\n\n\n\\\\”\n双引号\n\n\n\\\\n\n换行\n\n\n\\\\t\nTAB\n\n\n\\\\r\n回车\n\n\n\n\n原始字符串只需要在字符串前边加一个英文字母 r 即可。\n6.2 字符串的切片与拼接\n类似于元组具有不可修改性\n从 0 开始 (和 C 一样)\n切片通常写成 start:end 这种形式，包括「start 索引」对应的元素，不包括「end 索引」对应的元素。\n索引值可正可负，正索引从 0 开始，从左往右；负索引从 -1 开始，从右往左。使用负数索引时，会从最后一个元素开始计数。最后一个元素的位置编号是 -1。\n\nstr1 = 'I Love LsgoGroup'\nprint(str1[:6]) # I Love\nprint(str1[5]) # e\nprint(str1[:6] + \" 插入的字符串 \" + str1[6:])\n# I Love 插入的字符串 LsgoGroup\ns = 'Python'\nprint(s) # Python\nprint(s[2:4]) # th\nprint(s[-5:-2]) # yth\nprint(s[2]) # t\nprint(s[-1]) # n\n6.3 字符串的常用内置方法\ncapitalize() 将字符串的第一个字符转换为大写。\n\nstr2 = 'xiaoxie'\nprint(str2.capitalize()) # Xiaoxie\n\nlower() 转换字符串中所有大写字符为小写。\nupper() 转换字符串中的小写字母为大写。\nswapcase() 将字符串中大写转换为小写，小写转换为大写。\n\nstr2 = \"DAXIExiaoxie\"\nprint(str2.lower()) # daxiexiaoxie\nprint(str2.upper()) # DAXIEXIAOXIE\nprint(str2.swapcase()) # daxieXIAOXIE\n\ncount(str, beg= 0,end=len(string)) 返回str 在 string 里面出现的次数，如果beg 或者end 指定则返回指定范围内str 出现的次数。\n\nstr2 = \"DAXIExiaoxie\"\nprint(str2.count('xi')) # 2\n\nendswith(suffix, beg=0, end=len(string)) 检查字符串是否以指定子字符串 suffix 结束，如果是，返回True,否则返回 False。如果 beg 和 end 指定值，则在指定范围内检查。\nstartswith(substr, beg=0,end=len(string)) 检查字符串是否以指定子字符串 substr 开头，如果是，返回True，否则返回 False。如果 beg 和 end 指定值，则在指定范围内检查。\n\nstr2 = \"DAXIExiaoxie\"\nprint(str2.endswith('ie')) # True\nprint(str2.endswith('xi')) # False\nprint(str2.startswith('Da')) # False\nprint(str2.startswith('DA')) # True\n\nfind(str, beg=0, end=len(string)) 检测 str 是否包含在字符串中，如果指定范围 beg 和 end ，则检查是否包含在指定范围内，如果包含，返回开始的索引值，否则返回 -1。\nrfind(str, beg=0,end=len(string)) 类似于 find() 函数，不过是从右边开始查找。\n\nstr2 = \"DAXIExiaoxie\"\nprint(str2.find('xi')) # 5\nprint(str2.find('ix')) # -1\nprint(str2.rfind('xi')) # 9\n\nisnumeric() 如果字符串中只包含数字字符，则返回 True，否则返回 False。\n\nstr3 = '12345'\nprint(str3.isnumeric()) # True\nstr3 += 'a'\nprint(str3.isnumeric()) # False\n\nljust(width[, fillchar]) 返回一个原字符串左对齐，并使用fillchar （默认空格）填充至长度width 的新字符串。\nrjust(width[, fillchar]) 返回一个原字符串右对齐，并使用fillchar （默认空格）填充至长度width 的新字符串。\n\nstr4 = '1101'\nprint(str4.ljust(8, '0')) # 11010000\nprint(str4.rjust(8, '0')) # 00001101\n\nlstrip([chars]) 截掉字符串左边的空格或指定字符。\nrstrip([chars]) 删除字符串末尾的空格或指定字符。\nstrip([chars]) 在字符串上执行lstrip() 和rstrip() 。\n\nstr5 = ' I Love LsgoGroup '\nprint(str5.lstrip()) # 'I Love LsgoGroup '\nprint(str5.lstrip().strip('I')) # ' Love LsgoGroup '\nprint(str5.rstrip()) # ' I Love LsgoGroup'\nprint(str5.strip()) # 'I Love LsgoGroup'\nprint(str5.strip().strip('p')) # 'I Love LsgoGrou'\n\npartition(sub) 找到子字符串sub，把字符串分为一个三元组(pre_sub,sub,fol_sub) ，如果字符串中不包含sub则返回(‘原字符串’,’’,’’) 。\nrpartition(sub) 类似于partition() 方法，不过是从右边开始查找。\n\nstr5 = ' I Love LsgoGroup '\nprint(str5.strip().partition('o')) # ('I L', 'o', 've LsgoGroup')\nprint(str5.strip().partition('m')) # ('I Love LsgoGroup', '', '')\nprint(str5.strip().rpartition('o')) # ('I Love LsgoGr', 'o', 'up')\n\nreplace(old, new [, max]) 把 将字符串中的old 替换成new ，如果max 指定，则替换不超过max 次。\n\nstr5 = ' I Love LsgoGroup '\nprint(str5.strip().replace('I', 'We')) # We Love LsgoGroup\n\nsplit(str=””, num) 不带参数默认是以空格为分隔符切片字符串，如果num 参数有设置，则仅分隔num 个子字符串，返回切片后的子字符串拼接的列表。\n\nstr5 = ' I Love LsgoGroup '\nprint(str5.strip().split()) # ['I', 'Love', 'LsgoGroup']\nprint(str5.strip().split('o')) # ['I L', 've Lsg', 'Gr', 'up']\n\nu = \"www.baidu.com.cn\"\n# 分割两次\nprint(u.split(\".\", 2)) # ['www', 'baidu', 'com.cn']\n\nsplitlines([keepends]) 按照行(‘\\r’, ‘\\r\\n’, \\n’)分隔，返回一个包含各行作为元素的列表，如果参数keepends 为False，不包含换行符，如果为 True，则保留换行符。\n\nstr6 = 'I \\n Love \\n LsgoGroup'\nprint(str6.splitlines()) # ['I ', ' Love ', ' LsgoGroup']\nprint(str6.splitlines(True)) # ['I \\n', ' Love \\n', ' LsgoGroup']\n\nmaketrans(intab, outtab) 创建字符映射的转换表，第一个参数是字符串，表示需要转换的字符，第二个参数也是字符串表示转换的目标。\ntranslate(table, deletechars=””) 根据参数table 给出的表，转换字符串的字符，要过滤掉的字符放到deletechars 参数中。\n\nstr = 'this is string example....wow!!!'\nintab = 'aeiou'\nouttab = '12345'\ntrantab = str.maketrans(intab, outtab)\nprint(trantab) # &#123;97: 49, 111: 52, 117: 53, 101: 50, 105: 51&#125;\nprint(str.translate(trantab)) # th3s 3s str3ng 2x1mpl2....w4w!!!\n6.4 字符串格式化format 格式化函数\nstr = \"&#123;0&#125; Love &#123;1&#125;\".format('I', 'Lsgogroup') # 位置参数\nprint(str) # I Love Lsgogroup\nstr = \"&#123;a&#125; Love &#123;b&#125;\".format(a='I', b='Lsgogroup') # 关键字参数\nprint(str) # I Love Lsgogroup\nstr = \"&#123;0&#125; Love &#123;b&#125;\".format('I', b='Lsgogroup') # 位置参数要在关键字参数之前\nprint(str) # I Love Lsgogroup\nstr = '&#123;0:.2f&#125;&#123;1&#125;'.format(27.658, 'GB') # 保留小数点后两位\nprint(str) # 27.66GB\nPython 字符串格式化符号\n\n\n\n\n符号\n\n\n\n\n\n%c\n格式化字符及其ASCII码\n\n\n%s\n格式化字符串，用str()方法处理对象\n\n\n%r\n格式化字符串，用rper()方法处理对象\n\n\n%d\n格式化整数\n\n\n%o\n格式化无符号八进制数\n\n\n%x\n格式化无符号十六进制数\n\n\n%X\n格式化无符号十六进制数（大写）\n\n\n%f\n格式化浮点数字，可指定小数点后的精度\n\n\n%e\n用科学计数法格式化浮点数\n\n\n%E\n作用同%e，用科学计数法格式化浮点数\n\n\n%g\n根据值的大小决定使用%f或%e\n\n\n%G\n作用同%g，根据值的大小决定使用%f或%E\n\n\n\n\nprint('%c' % 97) # a\nprint('%c %c %c' % (97, 98, 99)) # a b c\nprint('%d + %d = %d' % (4, 5, 9)) # 4 + 5 = 9\nprint(\"我叫 %s 今年 %d 岁!\" % ('小明', 10)) # 我叫 小明 今年 10 岁!\nprint('%o' % 10) # 12\nprint('%x' % 10) # a\nprint('%X' % 10) # A\nprint('%f' % 27.658) # 27.658000\nprint('%e' % 27.658) # 2.765800e+01\nprint('%E' % 27.658) # 2.765800E+01\nprint('%g' % 27.658) # 27.658\ntext = \"I am %d years old.\" % 22\nprint(\"I said: %s.\" % text) # I said: I am 22 years old..\nprint(\"I said: %r.\" % text) # I said: 'I am 22 years old.'\n格式化操作符辅助指令\n\n\n\n\n符号\n功能\n\n\n\n\nm.n\nm 是显示的最小总宽度,n 是小数点后的位数(如果可用的话)\n\n\n-\n用做左对齐\n\n\n+\n在正数前面显示加号( + )\n\n\n#\n在八进制数前面显示零(‘0’)，在十六进制前面显示’0x’或者’0X’(取决于用的是’x’还是’X’)\n\n\n0\n显示的数字前面填充’0’而不是默认的空格\n\n\n\n\nprint('%5.1f' % 27.658) # ' 27.7'\nprint('%.2e' % 27.658) # 2.77e+01\nprint('%10d' % 10) # ' 10'\nprint('%-10d' % 10) # '10 '\nprint('%+d' % 10) # +10\nprint('%#o' % 10) # 0o12\nprint('%#x' % 108) # 0x6c\nprint('%010d' % 5) # 0000000005\n7 字典7.1 可变类型与不可变类型\n序列是以连续的整数为索引，与此不同的是，字典以”关键字”为索引，关键字可以是任意==不可变类型==，通常用字符串或数值。\n字典是 Python 唯一的一个 映射类型，字符串、元组、列表属于序列类型。\n\n那么如何快速判断一个数据类型 X 是不是可变类型的呢？两种方法：\n\n麻烦方法：用 id(X) 函数，对 X 进行某种操作，比较操作前后的 id ，如果不一样，则 X 不可变，如果一样，则X 可变。\n便捷方法：用 hash(X) ，只要不报错，证明 X 可被哈希，即不可变，反过来不可被哈希，即可变。\n\ni &#x3D; 1\nprint(id(i)) # 140732167000896\ni &#x3D; i + 2\nprint(id(i)) # 140732167000960\nl &#x3D; [1, 2]\nprint(id(l)) # 4300825160\nl.append(&#39;Python&#39;)\nprint(id(l)) # 4300825160\n\n整数 i 在加 1 之后的 id 和之前不一样，因此加完之后的这个 i (虽然名字没变)，但不是加之前的那个 i 了，因此整数是不可变类型。\n列表 l 在附加 ‘Python’ 之后的 id 和之前一样，因此列表是可变类型。\n\nprint(hash('Name')) # -9215951442099718823\nprint(hash((1, 2, 'Python'))) # 823362308207799471\nprint(hash([1, 2, 'Python']))\n# TypeError: unhashable type: 'list'\nprint(hash(&#123;1, 2, 3&#125;))\n# TypeError: unhashable type: 'set'\n\n数值、字符和元组 都能被哈希，因此它们是不可变类型。\n列表、集合、字典不能被哈希，因此它是可变类型。\n\n7.2 字典的定义字典 是无序的 键:值（ key:value ）对集合，键必须是互不相同的（在同一个字典之内）。\n\ndict 内部存放的顺序和 key 放入的顺序是没有关系的。\ndict 查找和插入的速度极快，不会随着 key 的增加而增加，但是需要占用大量的内存。\n\n字典 定义语法为 {元素1, 元素2, …, 元素n}\n\n其中每一个元素是一个「键值对」— 键:值 ( key:value )\n关键点是「大括号 {}」,「逗号 ,」和「冒号 :」\n大括号 — 把所有元素绑在一起\n逗号 — 将每个键值对分开\n冒号 — 将键和值分开\n\n7.3 创建和访问字典通过字符串或数值作为key 来创建字典。\ndic = &#123;'李宁': '一切皆有可能', '耐克': 'Just do it', '阿迪达斯': 'Impossible is nothing'&#125;\nprint('耐克的口号是:', dic['耐克'])\n# 耐克的口号是: Just do it\n注意：如果我们取的键在字典中不存在，会直接报错KeyError 。\n通过元组作为key 来创建字典，但一般不这样使用。\ndic = &#123;(1, 2, 3): \"Tom\", \"Age\": 12, 3: [3, 5, 7]&#125;\nprint(dic) # &#123;(1, 2, 3): 'Tom', 'Age': 12, 3: [3, 5, 7]&#125;\nprint(type(dic)) # &lt;class 'dict'>\n通过构造函数dict 来创建字典。\ndict() -&gt; 创建一个空的字典。\ndic = dict()\ndic['a'] = 1\ndic['b'] = 2\ndic['c'] = 3\nprint(dic)\n# &#123;'a': 1, 'b': 2, 'c': 3&#125;\n7.4 字典的内置方法dict.fromkeys(seq[, value]) 用于创建一个新字典，以序列 seq 中元素做字典的键， value 为字典所有键对应的初始值。\nseq = ('name', 'age', 'sex')\ndic1 = dict.fromkeys(seq)\nprint(\"新的字典为 : %s\" % str(dic1))\n# 新的字典为 : &#123;'name': None, 'age': None, 'sex': None&#125;\ndic2 = dict.fromkeys(seq, 10)\nprint(\"新的字典为 : %s\" % str(dic2))\n# 新的字典为 : &#123;'name': 10, 'age': 10, 'sex': 10&#125;\ndic3 = dict.fromkeys(seq, ('小马', '8', '男'))\nprint(\"新的字典为 : %s\" % str(dic3))\n# 新的字典为 : &#123;'name': ('小马', '8', '男'), 'age': ('小马', '8', '男'), 'sex': ('小马', '8', '男')&#125;\ndict.keys() 返回一个可迭代对象，可以使用 list() 来转换为列表，列表为字典中的所有键。\ndic = &#123;'Name': 'lsgogroup', 'Age': 7&#125;\nprint(dic.keys()) # dict_keys(['Name', 'Age'])\nlst = list(dic.keys()) # 转换为列表\nprint(lst) # ['Name', 'Age']\ndict.values() 返回一个迭代器，可以使用 list() 来转换为列表，列表为字典中的所有值。\ndic = &#123;'Sex': 'female', 'Age': 7, 'Name': 'Zara'&#125;\nprint(\"字典所有值为 : \", list(dic.values()))\n# 字典所有值为 : [7, 'female', 'Zara']\ndict.items() 以列表返回可遍历的 (键, 值) 元组数组。\ndic = &#123;'Name': 'Lsgogroup', 'Age': 7&#125;\nprint(\"Value : %s\" % dic.items())\n# Value : dict_items([('Name', 'Lsgogroup'), ('Age', 7)])\nprint(tuple(dic.items()))\n# (('Name', 'Lsgogroup'), ('Age', 7))\ndict.get(key, default=None) 返回指定键的值，如果值不在字典中返回默认值。\ndic = &#123;'Name': 'Lsgogroup', 'Age': 27&#125;\nprint(\"Age 值为 : %s\" % dic.get('Age')) # Age 值为 : 27\nprint(\"Sex 值为 : %s\" % dic.get('Sex', \"NA\")) # Sex 值为 : NA\ndict.setdefault(key, default=None) 和get() 方法 类似, 如果键不存在于字典中，将会添加键并将值设为默认值。\ndic = &#123;'Name': 'Lsgogroup', 'Age': 7&#125;\nprint(\"Age 键的值为 : %s\" % dic.setdefault('Age', None)) # Age 键的值为 : 7\nprint(\"Sex 键的值为 : %s\" % dic.setdefault('Sex', None)) # Sex 键的值为 : None\nprint(\"新字典为：\", dic)\n# 新字典为： &#123;'Age': 7, 'Name': 'Lsgogroup', 'Sex': None&#125;\nkey in dict  in操作符用于判断键是否存在于字典中，如果键在字典 dict 里返回true ，否则返回false 。而not in 操作符刚好相反，如果键在字典 dict 里返回false ，否则返回true 。\ndic = &#123;'Name': 'Lsgogroup', 'Age': 7&#125;\n# in 检测键 Age 是否存在\nif 'Age' in dic:\n\tprint(\"键 Age 存在\")\nelse:\n\tprint(\"键 Age 不存在\")\n# 检测键 Sex 是否存在 \nif 'Sex' in dic:\n\tprint(\"键 Sex 存在\")\nelse:\n\tprint(\"键 Sex 不存在\")\n# not in 检测键 Age 是否存在\nif 'Age' not in dic:\n\tprint(\"键 Age 不存在\")\nelse:\n\tprint(\"键 Age 存在\")\n# 键 Age 存在\n# 键 Sex 不存在\n# 键 Age 存在\ndict.pop(key[,default]) 删除字典给定键 key 所对应的值，返回值为被删除的值。key 值必须给出。若key不存在，则返回 default 值。\ndel dict[key] 删除字典给定键 key 所对应的值。\ndic1 = &#123;1: \"a\", 2: [1, 2]&#125;\nprint(dic1.pop(1), dic1) # a &#123;2: [1, 2]&#125;\n# 设置默认值，必须添加，否则报错\nprint(dic1.pop(3, \"nokey\"), dic1) # nokey &#123;2: [1, 2]&#125;\ndel dic1[2]\nprint(dic1) # &#123;&#125; \ndict.popitem() 随机返回并删除字典中的一对键和值，如果字典已经为空，却调用了此方法，就报出KeyError异常。\ndic1 = &#123;1: \"a\", 2: [1, 2]&#125;\nprint(dic1.popitem()) # (1, 'a')\nprint(dic1) # &#123;2: [1, 2]&#125;\ndict.clear() 用于删除字典内所有元素。\ndic = &#123;'Name': 'Zara', 'Age': 7&#125;\nprint(\"字典长度 : %d\" % len(dic)) # 字典长度 : 2\ndict.clear()\nprint(\"字典删除后长度 : %d\" % len(dic)) # 字典删除后长度 : 0\ndict.copy() 返回一个字典的浅复制。\ndic1 = &#123;'Name': 'Lsgogroup', 'Age': 7, 'Class': 'First'&#125;\ndic2 = dic1.copy()\nprint(\"新复制的字典为 : \", dic2)\n# 新复制的字典为 : &#123;'Age': 7, 'Name': 'Lsgogroup', 'Class': 'First'&#125;\ndict.update(dict2) 把字典参数 dict2 的 key:value 对 更新到字典 dict 里。\ndic = &#123;'Name': 'Lsgogroup', 'Age': 7&#125;\ndic2 = &#123;'Sex': 'female', 'Age': 8&#125;\ndic.update(dic2)\nprint(\"更新字典 dict : \", dic)\n# 更新字典 dict : &#123;'Sex': 'female', 'Age': 8, 'Name': 'Lsgogroup'&#125;\n8 集合python 中set 与dict 类似，也是一组key 的集合，但不存储value 。由于key 不能重复，所以，在set 中，没有重复的key 。\n注意， key 为不可变类型，即可哈希的值。\n8.1 集合的创建\n先创建对象再加入元素。\n在创建空集合的时候只能使用s = set() ，因为s = {} 创建的是空字典。\n\nbasket = set()\nbasket.add('apple')\nbasket.add('banana')\nprint(basket) # &#123;'banana', 'apple'&#125;\n\n直接把一堆元素用花括号括起来{元素1, 元素2, …, 元素n} 。\n重复元素在set 中会被自动被过滤。\n\nbasket = &#123;'apple', 'orange', 'apple', 'pear', 'orange', 'banana'&#125;\nprint(basket) # &#123;'banana', 'apple', 'pear', 'orange'&#125;\n\n使用set(value) 工厂函数，把列表或元组转换成集合。\n\na = set('abracadabra')\nprint(a)\n# &#123;'r', 'b', 'd', 'c', 'a'&#125;\nb = set((\"Google\", \"Lsgogroup\", \"Taobao\", \"Taobao\"))\nprint(b)\n# &#123;'Taobao', 'Lsgogroup', 'Google'&#125;\nc = set([\"Google\", \"Lsgogroup\", \"Taobao\", \"Google\"])\nprint(c)\n# &#123;'Taobao', 'Lsgogroup', 'Google'&#125;\n由于 set 存储的是无序集合，所以我们不可以为集合创建索引或执行切片(slice)操作，也没有键(keys)可用来获取集合中元素的值，但是可以判断一个元素是否在集合中。\n8.2 访问集合中的值\n可以使用len() 內建函数得到集合的大小。\n\nthisset = set(['Google', 'Baidu', 'Taobao'])\nprint(len(thisset)) # 3\n\n可以使用for 把集合中的数据一个个读取出来。\n\nthisset = set(['Google', 'Baidu', 'Taobao'])\nfor item in thisset:\nprint(item)\n# Baidu\n# Google\n# Taobao\n\n可以通过in 或not in 判断一个元素是否在集合中已经存在\n\nthisset = set(['Google', 'Baidu', 'Taobao'])\nprint('Taobao' in thisset) # True\nprint('Facebook' not in thisset) # True\n8.3 集合的内置方法\nset.add(elmnt) 用于给集合添加元素，如果添加的元素在集合中已存在，则不执行任何操作。\n\nfruits = &#123;\"apple\", \"banana\", \"cherry\"&#125;\nfruits.add(\"orange\")\nprint(fruits)\n# &#123;'orange', 'cherry', 'banana', 'apple'&#125;\nfruits.add(\"apple\")\nprint(fruits)\n# &#123;'orange', 'cherry', 'banana', 'apple'&#125;\n\nset.update(set) 用于修改当前集合，可以添加新的元素或集合到当前集合中，如果添加的元素在集合中已存在，则该元素只会出现一次，重复的会忽略。\n\nx = &#123;\"apple\", \"banana\", \"cherry\"&#125;\ny = &#123;\"google\", \"baidu\", \"apple\"&#125;\nx.update(y)\nprint(x)\n# &#123;'cherry', 'banana', 'apple', 'google', 'baidu'&#125;\ny.update([\"lsgo\", \"dreamtech\"])\nprint(y)\n# &#123;'lsgo', 'baidu', 'dreamtech', 'apple', 'google'&#125;\n\nset.remove(item) 用于移除集合中的指定元素。如果元素不存在，则会发生错误。\n\nfruits = &#123;\"apple\", \"banana\", \"cherry\"&#125;\nfruits.remove(\"banana\")\nprint(fruits) # &#123;'apple', 'cherry'&#125;\n\nset.discard(value) 用于移除指定的集合元素。remove() 方法在移除一个不存在的元素时会发生错误，而discard() 方法不会。\n\nfruits = &#123;\"apple\", \"banana\", \"cherry\"&#125;\nfruits.discard(\"banana\")\nprint(fruits) # &#123;'apple', 'cherry'&#125;\n\nset.pop() 用于随机移除一个元素。\n\nfruits = &#123;\"apple\", \"banana\", \"cherry\"&#125;\nx = fruits.pop()\nprint(fruits) # &#123;'cherry', 'apple'&#125;\nprint(x) # banana\n由于 set 是无序和无重复元素的集合，所以两个或多个 set 可以做数学意义上的集合操作。\n\nset.intersection(set1, set2 …) 返回两个集合的交集。\nset1 &amp; set2 返回两个集合的交集。\nset.intersection_update(set1, set2 …) 交集，在原始的集合上移除不重叠的元素。\n\na = set('abracadabra')\nb = set('alacazam')\nprint(a) # &#123;'r', 'a', 'c', 'b', 'd'&#125;\nprint(b) # &#123;'c', 'a', 'l', 'm', 'z'&#125;\nc = a.intersection(b)\nprint(c) # &#123;'a', 'c'&#125;\nprint(a &amp; b) # &#123;'c', 'a'&#125;\nprint(a) # &#123;'a', 'r', 'c', 'b', 'd'&#125;\na.intersection_update(b)\nprint(a) # &#123;'a', 'c'&#125;\n\nset.union(set1, set2…) 返回两个集合的并集。\nset1 | set2 返回两个集合的并集。\n\na = set('abracadabra')\nb = set('alacazam')\nprint(a) # &#123;'r', 'a', 'c', 'b', 'd'&#125;\nprint(b) # &#123;'c', 'a', 'l', 'm', 'z'&#125;\nprint(a | b) # &#123;'l', 'd', 'm', 'b', 'a', 'r', 'z', 'c'&#125;\nc = a.union(b)\nprint(c) # &#123;'c', 'a', 'd', 'm', 'r', 'b', 'z', 'l'&#125;\n\nset.difference(set) 返回集合的差集。\nset1 - set2 返回集合的差集。\nset.difference_update(set) 集合的差集，直接在原来的集合中移除元素，没有返回值。\n\na = set('abracadabra')\nb = set('alacazam')\nprint(a) # &#123;'r', 'a', 'c', 'b', 'd'&#125;\nprint(b) # &#123;'c', 'a', 'l', 'm', 'z'&#125;\nc = a.difference(b)\nprint(c) # &#123;'b', 'd', 'r'&#125;\nprint(a - b) # &#123;'d', 'b', 'r'&#125;\nprint(a) # &#123;'r', 'd', 'c', 'a', 'b'&#125;\na.difference_update(b)\nprint(a) # &#123;'d', 'r', 'b'&#125;\n\nset.symmetric_difference(set) 返回集合的异或。\nset1 ^ set2 返回集合的异或。\nset.symmetric_difference_update(set) 移除当前集合中在另外一个指定集合相同的元素，并将另外一个指定集合中不同的元素插入到当前集合中。\n\na = set('abracadabra')\nb = set('alacazam')\nprint(a) # &#123;'r', 'a', 'c', 'b', 'd'&#125;\nprint(b) # &#123;'c', 'a', 'l', 'm', 'z'&#125;\nc = a.symmetric_difference(b)\nprint(c) # &#123;'m', 'r', 'l', 'b', 'z', 'd'&#125;\nprint(a ^ b) # &#123;'m', 'r', 'l', 'b', 'z', 'd'&#125;\nprint(a) # &#123;'r', 'd', 'c', 'a', 'b'&#125;\na.symmetric_difference_update(b)\nprint(a) # &#123;'r', 'b', 'm', 'l', 'z', 'd'&#125;\n\nset.issubset(set) 判断集合是不是被其他集合包含，如果是则返回 True，否则返回 False。\nset1 &lt;= set2 判断集合是不是被其他集合包含，如果是则返回 True，否则返回 False。\n\nx = &#123;\"a\", \"b\", \"c\"&#125;\ny = &#123;\"f\", \"e\", \"d\", \"c\", \"b\", \"a\"&#125;\nz = x.issubset(y)\nprint(z) # True\nprint(x &lt;= y) # True\nx = &#123;\"a\", \"b\", \"c\"&#125;\ny = &#123;\"f\", \"e\", \"d\", \"c\", \"b\"&#125;\nz = x.issubset(y)\nprint(z) # False\nprint(x &lt;= y) # False\n\nset.issuperset(set) 用于判断集合是不是包含其他集合，如果是则返回 True，否则返回 False。\nset1 &gt;= set2 判断集合是不是包含其他集合，如果是则返回 True，否则返回 False。\n\nx = &#123;\"f\", \"e\", \"d\", \"c\", \"b\", \"a\"&#125;\ny = &#123;\"a\", \"b\", \"c\"&#125;\nz = x.issuperset(y)\nprint(z) # True\nprint(x >= y) # True\nx = &#123;\"f\", \"e\", \"d\", \"c\", \"b\"&#125;\ny = &#123;\"a\", \"b\", \"c\"&#125;\nz = x.issuperset(y)\nprint(z) # False\nprint(x >= y) # False\n\nset.isdisjoint(set) 用于判断两个集合是不是不相交，如果是返回 True，否则返回 False。\n\nx = &#123;\"f\", \"e\", \"d\", \"c\", \"b\"&#125;\ny = &#123;\"a\", \"b\", \"c\"&#125;\nz = x.isdisjoint(y)\nprint(z) # False\nx = &#123;\"f\", \"e\", \"d\", \"m\", \"g\"&#125;\ny = &#123;\"a\", \"b\", \"c\"&#125;\nz = x.isdisjoint(y)\nprint(z) # True\n8.4 集合的转换se = set(range(4))\nli = list(se)\ntu = tuple(se)\nprint(se, type(se)) # &#123;0, 1, 2, 3&#125; &lt;class 'set'>\nprint(li, type(li)) # [0, 1, 2, 3] &lt;class 'list'>\nprint(tu, type(tu)) # (0, 1, 2, 3) &lt;class 'tuple'>\n8.5 不可变集合Python 提供了不能改变元素的集合的实现版本，即不能增加或删除元素，类型名叫frozenset 。需要注意的是frozenset 仍然可以进行集合操作，只是不能用带有update 的方法。\n\nfrozenset([iterable]) 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。\n\na = frozenset(range(10)) # 生成一个新的不可变集合\nprint(a)\n# frozenset(&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;)\nb = frozenset('lsgogroup')\nprint(b)\n# frozenset(&#123;'g', 's', 'p', 'r', 'u', 'o', 'l'&#125;)\n9 序列针对序列的内置函数\nlist(sub) 把一个可迭代对象转换为列表。\n\na = list()\nprint(a) # []\nb = 'I Love LsgoGroup'\nb = list(b)\nprint(b)\n# ['I', ' ', 'L', 'o', 'v', 'e', ' ', 'L', 's', 'g', 'o', 'G', 'r', 'o', 'u', 'p']\nc = (1, 1, 2, 3, 5, 8)\nc = list(c)\nprint(c) # [1, 1, 2, 3, 5, 8]\n\ntuple(sub) 把一个可迭代对象转换为元组。\n\na = tuple()\nprint(a) # ()\nb = 'I Love LsgoGroup'\nb = tuple(b)\nprint(b)\n# ('I', ' ', 'L', 'o', 'v', 'e', ' ', 'L', 's', 'g', 'o', 'G', 'r', 'o', 'u', 'p')\nc = [1, 1, 2, 3, 5, 8]\nc = tuple(c)\nprint(c) # (1, 1, 2, 3, 5, 8)\n\nstr(obj) 把obj对象转换为字符串\n\na = 123\na = str(a)\nprint(a) # 123\n\nlen(s) 返回对象（字符、列表、元组等）长度或元素个数。a. s — 对象。\n\na = list()\nprint(len(a)) # 0\nb = ('I', ' ', 'L', 'o', 'v', 'e', ' ', 'L', 's', 'g', 'o', 'G', 'r', 'o', 'u', 'p')\nprint(len(b)) # 16\nc = 'I Love LsgoGroup'\nprint(len(c)) # 16\n\nmax(sub) 返回序列或者参数集合中的最大值\n\nprint(max(1, 2, 3, 4, 5)) # 5\nprint(max([-8, 99, 3, 7, 83])) # 99\nprint(max('IloveLsgoGroup')) # v\n\nmin(sub) 返回序列或参数集合中的最小值\n\nprint(min(1, 2, 3, 4, 5)) # 1\nprint(min([-8, 99, 3, 7, 83])) # -8\nprint(min('IloveLsgoGroup')) # G\n\nsum(iterable[, start=0]) 返回序列iterable 与可选参数start 的总和。\n\nprint(sum([1, 3, 5, 7, 9])) # 25\nprint(sum([1, 3, 5, 7, 9], 10)) # 35\nprint(sum((1, 3, 5, 7, 9))) # 25\nprint(sum((1, 3, 5, 7, 9), 20)) # 45\n\nsorted(iterable, key=None, reverse=False) 对所有可迭代的对象进行排序操作。a. iterable — 可迭代对象。  b. key — 主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代  对象中的一个元素来进行排序。  c. reverse — 排序规则， reverse = True 降序 ， reverse = False 升序（默认）。\nd. 返回重新排序的列表。\n\n\nx = [-8, 99, 3, 7, 83]\nprint(sorted(x)) # [-8, 3, 7, 83, 99]\nprint(sorted(x, reverse=True)) # [99, 83, 7, 3, -8]\nt = (&#123;\"age\": 20, \"name\": \"a\"&#125;, &#123;\"age\": 25, \"name\": \"b\"&#125;, &#123;\"age\": 10, \"name\": \"c\"&#125;)\nx = sorted(t, key=lambda a: a[\"age\"])\nprint(x)\n# [&#123;'age': 10, 'name': 'c'&#125;, &#123;'age': 20, 'name': 'a'&#125;, &#123;'age': 25, 'name': 'b'&#125;]\n\nreversed(seq) 函数返回一个反转的迭代器。a. seq — 要转换的序列，可以是 tuple, string, list 或 range。\n\ns = 'lsgogroup'\nx = reversed(s)\nprint(type(x)) # &lt;class 'reversed'>\nprint(x) # &lt;reversed object at 0x000002507E8EC2C8>\nprint(list(x))\n# ['p', 'u', 'o', 'r', 'g', 'o', 'g', 's', 'l']\nt = ('l', 's', 'g', 'o', 'g', 'r', 'o', 'u', 'p')\nprint(list(reversed(t)))\n# ['p', 'u', 'o', 'r', 'g', 'o', 'g', 's', 'l']\nr = range(5, 9)\nprint(list(reversed(r)))\n# [8, 7, 6, 5]\nx = [-8, 99, 3, 7, 83]\nprint(list(reversed(x)))\n# [83, 7, 3, 99, -8]\n\nenumerate(sequence, [start=0])\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\na = list(enumerate(seasons))\nprint(a)\n# [(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]\nb = list(enumerate(seasons, 1))\nprint(b)\n# [(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]\nfor i, element in a:\nprint('&#123;0&#125;,&#123;1&#125;'.format(i, element))\n\nzip(iter1 [,iter2 […]])a. 用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象，这样做的好处是节约了不少的内存。b. 我们可以使用 list() 转换来输出列表。c. 如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。\n\na = [1, 2, 3]\nb = [4, 5, 6]\nc = [4, 5, 6, 7, 8]\nzipped = zip(a, b)\nprint(zipped) # &lt;zip object at 0x000000C5D89EDD88>\nprint(list(zipped)) # [(1, 4), (2, 5), (3, 6)]\nzipped = zip(a, c)\nprint(list(zipped)) # [(1, 4), (2, 5), (3, 6)]\na1, a2 = zip(*zip(a, b))\nprint(list(a1)) # [1, 2, 3]\nprint(list(a2)) # [4, 5, 6]\n10 函数与Lambda表达式10.1 函数函数的定义\n函数以def 关键词开头，后接函数名和圆括号()。\n函数执行的代码以冒号起始，并且缩进。\nreturn [表达式] 结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None 。\n\ndef functionname(parameters):\n\"函数_文档字符串\"\nfunction_suite\nreturn [expression]\n函数参数Python 的函数具有非常灵活多样的参数形态，既可以实现简单的调用，又可以传入非常复杂的参数。从简到繁的参数形态如下：\n\n位置参数 (positional argument)\ndef functionname(arg1):\n    \"函数_文档字符串\"\n    function_suite\nreturn [expression]\n\n默认参数 (default argument)\ndef functionname(arg1, arg2=v):\n    \"函数_文档字符串\"\n    function_suite\nreturn [expression]\n# 默认参数一定要放在位置参数 后面，不然程序会报错。\n\n可变参数 (variable argument)\ndef functionname(arg1, arg2=v, *args):\n    for var in args:\n\t\tprint(var)\nreturn [expression]\n\n*args - 可变参数，可以是从零个到任意个，自动组装成元组。\n加了星号（*）的变量名会存放所有未命名的变量参数。\n\n\n关键字参数 (keyword argument)\ndef functionname(arg1, arg2=v, *args, **kw):\n    \"函数_文档字符串\"\n    function_suite\nreturn [expression]\n\n**kw - 关键字参数，可以是从零个到任意个，自动组装成字典。\n\ndef printinfo(arg1, *args, **kwargs):\nprint(arg1)\nprint(args)\nprint(kwargs)\nprintinfo(70, 60, 50)\n# 70\n# (60, 50)\n# &#123;&#125;\nprintinfo(70, 60, 50, a=1, b=2)\n# 70\n# (60, 50)\n# &#123;'a': 1, 'b': 2&#125;\n「可变参数」和「关键字参数」的同异总结如下：\n\n可变参数允许传入零个到任意个参数，它们在函数调用时自动组装为一个元组 (tuple)。\n关键字参数允许传入零个到任意个参数，它们在函数内部自动组装为一个字典 (dict)。\n\n\n命名关键字参数 (name keyword argument)\ndef functionname(arg1, arg2=v, *args, *, nkw, **kw):\n    \"函数_文档字符串\"\n    function_suite\nreturn [expression]\n\n, nkw - 命名关键字参数，用户想要输入的关键字参数，定义方式是在nkw 前面加个分隔符 。\n如果要限制关键字参数的名字，就可以用「命名关键字参数」\n使用命名关键字参数时，要特别注意不能缺少参数名。\n\ndef printinfo(arg1, *, nkw, **kwargs):\n    print(arg1)\n    print(nkw)\n    print(kwargs)\nprintinfo(70, nkw=10, a=1, b=2)\n# 70\n# 10\n# &#123;'a': 1, 'b': 2&#125;\nprintinfo(70, 10, a=1, b=2)\n# TypeError: printinfo() takes 1 positional argument but 2 were given\n\n没有写参数名nwk ，因此 10 被当成「位置参数」，而原函数只有 1 个位置函数，现在调用了 2 个，因此程序会报错。\n\n\n参数组合\n在 Python 中定义函数，可以用位置参数、默认参数、可变参数、命名关键字参数和关键字参数，这 5 种参数中的 4 个都可以一起使用，但是注意，参数定义的顺序必须是：\n\n位置参数、默认参数、可变参数和关键字参数。\n位置参数、默认参数、命名关键字参数和关键字参数。要注意定义可变参数和关键字参数的语法：\n*args 是可变参数， args 接收的是一个 tuple\n*kw 是关键字参数， kw 接收的是一个 dict命名关键字参数是为了限制调用者可以传入的参数名，同时可以提供默认值。定义命名关键字参数不要忘了写分隔符，否则定义的是位置参数。\n\n警告：虽然可以组合多达 5 种参数，但不要同时使用太多的组合，否则函数很难懂。\n\n\n变量作用域\nPython 中，程序的变量并不是在哪个位置都可以访问的，访问权限决定于这个变量是在哪里赋值的。\n定义在函数内部的变量拥有局部作用域，该变量称为局部变量。\n定义在函数外部的变量拥有全局作用域，该变量称为全局变量。\n局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。\n当内部作用域想修改外部作用域的变量时，就要用到global 和nonlocal 关键字了。\n\nnum = 1\ndef fun1():\n    global num # 需要使用 global 关键字声明\n    print(num) # 1\n    num = 123\n    print(num) # 123\nfun1()\nprint(num) # 123\n10.2 Lambda 表达式匿名函数的定义lambda argument_list: expression\n\nlambda - 定义匿名函数的关键词。\nargument_list - 函数参数，它们可以是位置参数、默认参数、关键字参数，和正规函数里的参数类型一样。\n: - 冒号，在函数参数和表达式中间要加个冒号。\nexpression - 只是一个表达式，输入函数参数，输出一些值。\n\n注意：\n\nexpression 中没有 return 语句，因为 lambda 不需要它来返回，表达式本身结果就是返回值。\n匿名函数拥有自己的命名空间，且不能访问自己参数列表之外或全局命名空间里的参数。\n\ndef sqr(x):\nreturn x ** 2\nprint(sqr)\n# &lt;function sqr at 0x000000BABD3A4400>\n\ny = [sqr(x) for x in range(10)]\nprint(y)\n# [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\nlbd_sqr = lambda x: x ** 2\nprint(lbd_sqr)\n# &lt;function &lt;lambda> at 0x000000BABB6AC1E0>\n\ny = [lbd_sqr(x) for x in range(10)]\nprint(y)\n# [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\nsumary = lambda arg1, arg2: arg1 + arg2\nprint(sumary(10, 20)) # 30\nfunc = lambda *args: sum(args)\nprint(func(1, 2, 3, 4, 5)) # 15\n匿名函数的应用函数式编程 是指代码中每一块都是不可变的，都由纯函数的形式组成。这里的纯函数，是指函数本身相互独立、互不影响，对于相同的输入，总会有相同的输出，没有任何副作用。\n匿名函数 常常应用于函数式编程的高阶函数 (high-order function)中，主要有两种形式：\n\n参数是函数 (filter, map)\n返回值是函数 (closure)如，在 filter 和map 函数中的应用：\nfilter(function, iterable) 过滤序列，过滤掉不符合条件的元素，返回一个迭代器对象，如果要转换为列表，可以使用 list() 来转换。\n\nodd = lambda x: x % 2 == 1\ntemplist = filter(odd, [1, 2, 3, 4, 5, 6, 7, 8, 9])\nprint(list(templist)) # [1, 3, 5, 7, 9]\n\nmap(function, *iterables) 根据提供的函数对指定序列做映射。\n\nm1 = map(lambda x: x ** 2, [1, 2, 3, 4, 5])\nprint(list(m1))\n# [1, 4, 9, 16, 25]\nm2 = map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])\nprint(list(m2))\n# [3, 7, 11, 15, 19]\n11 类与对象11.1 对象 = 属性 + 方法对象是类的实例。换句话说，类主要定义对象的结构，然后我们以类为模板创建对象。类不但包含方法定义，而且还包含所有实例共享的数据。\n封装：信息隐蔽技术我们可以使用关键字 class 定义 Python 类，关键字后面紧跟类的名称、分号和类的实现。\nclass Turtle: # Python中的类名约定以大写字母开头\n\"\"\"关于类的一个简单例子\"\"\"\n    # 属性\n    color = 'green'\n    mouth = '大嘴'\n    # 方法\n    def climb(self):\n    \tprint('我正在很努力的向前爬...')\n    \t\n    \t\ntt = Turtle()\ntt.climb()\n# 我正在很努力的向前爬...\n继承：子类自动共享父类之间数据和方法的机制\nclass MyList(list):\n\tpass\nlst = MyList([1, 5, 2, 7, 8])\nlst.append(9)\nlst.sort()\nprint(lst)\n# [1, 2, 5, 7, 8, 9]\n多态：不同对象对同一方法响应不同的行动\nclass Animal:\n\tdef run(self):\n\t\traise AttributeError('子类必须实现这个方法')\nclass People(Animal):\n\tdef run(self):\n\t\tprint('人正在走')\nclass Pig(Animal):\n\tdef run(self):\n\t\tprint('pig is walking')\n11.2 self 是什么？Python 的 self 相当于 C++ 的 this 指针。\nclass Test:\n    def prt(self):\n        print(self)\n        print(self.__class__)\nt &#x3D; Test()\nt.prt()\n# &lt;__main__.Test object at 0x000000BC5A351208&gt;\n# &lt;class &#39;__main__.Test&#39;&gt;\n类的方法与普通的函数只有一个特别的区别 —— 它们必须有一个额外的第一个参数名称（对应于该实例，即该对象本身），按照惯例它的名称是 self 。在调用方法时，我们无需明确提供与参数 self 相对应的参数。\nclass Ball:\n    def setName(self, name):\n    \tself.name = name\n    def kick(self):\n    \tprint(\"我叫%s,该死的，谁踢我...\" % self.name)\n11.3 Python 的魔法方法类有一个名为__init__(self[, param1, param2…]) 的魔法方法，该方法在类实例化时会自动调用。\nclass Ball:\n    def __init__(self, name):\n    \tself.name = name\n11.4 公有和私有在 Python 中定义私有变量只需要在变量名或函数名前加上“__”两个下划线，那么这个函数或变量就会为私有的了。\nclass JustCounter:\n    __secretCount &#x3D; 0 # 私有变量\n    publicCount &#x3D; 0 # 公开变量\n    \n    def __foo(self): # 私有方法\n\t\tprint(&#39;这是私有方法&#39;)\n11.5 继承Python 同样支持类的继承，派生类的定义如下所示：\nclass DerivedClassName(BaseClassName):\n    &lt;statement-1>\n    .\n    .\n    .\n    &lt;statement-N>\nBaseClassName （示例中的基类名）必须与派生类定义在一个作用域内。除了类，还可以用表达式，基类定义在另一个模块中时这一点非常有用：\nclass DerivedClassName(modname.BaseClassName):\n    &lt;statement-1>\n    .\n    .\n    .\n    &lt;statement-N>\n\n如果子类中定义与父类同名的方法或属性，则会自动覆盖父类对应的方法或属性。\n\nclass Fish:\n    def __init__(self):\n        self.x = r.randint(0, 10)\n        self.y = r.randint(0, 10)\n    def move(self):\n    \tself.x -= 1\n    \tprint(\"我的位置\", self.x, self.y)\nclass Shark(Fish): # 鲨鱼\n    def __init__(self):\n        Fish.__init__(self)\n        self.hungry = True\n\n使用super函数super().init()\n\nclass Shark(Fish): # 鲨鱼\n    def __init__(self):\n        super().__init__()\n        self.hungry = True\nPython 虽然支持多继承的形式，但我们一般不使用多继承，因为容易引起混乱。\n需要注意圆括号中父类的顺序，若是父类中有相同的方法名，而在子类使用时未指定，Python 从左至右搜索，即方法在子类中未找到时，从左到右查找父类中是否包含方法。\n11.6 类、类对象和实例对象类对象：创建一个类，其实也是一个对象也在内存开辟了一块空间，称为类对象，类对象只有一个。\n实例对象：就是通过实例化类创建的对象，称为实例对象，实例对象可以有多个。\n# 类对象\nclass A(object):\n\tpass\n# 实例化对象 a、b、c都属于实例对象。\na = A()\nb = A()\nc = A()\n类属性：类里面方法外面定义的变量称为类属性。类属性所属于类对象并且多个实例对象之间共享同一个类属性，说白了就是类属性所有的通过该类实例化的对象都能共享。\nclass A():\n    a = xx #类属性\n    def __init__(self):\n    \tA.a = xx #使用类属性可以通过 （类名.类属性）调用。\n实例属性：实例属性和具体的某个实例对象有关系，并且一个实例对象和另外一个实例对象是不共享属性的，说白了实例属性只能在自己的对象里面使用，其他的对象不能直接使用，因为self 是谁调用，它的值就属于该对象。\nclass 类名():\n    __init__(self)：\n    \tself.name = xx #实例属性\n类属性和实例属性区别\n\n类属性：类外面，可以通过实例对象.类属性和类名.类属性进行调用。类里面，通过self.类属性和类名.类属性进行调用。\n实例属性 ：类外面，可以通过实例对象.实例属性调用。类里面，通过self.实例属性调用。\n实例属性就相当于局部变量。出了这个类或者这个类的实例对象，就没有作用了。\n类属性就相当于类里面的全局变量，可以和这个类的所有实例对象共享。\n\n11.8 什么是绑定？Python 严格要求方法需要有实例才能被调用，这种限制其实就是 Python 所谓的绑定概念。Python 对象的数据属性通常存储在名为. dict 的字典中，我们可以直接访问dict ，或利用 Python 的内置函数vars() 获取. dict 。\n11.9 一些相关的内置函数（BIF）\nissubclass(class, classinfo) 方法用于判断参数 class 是否是类型参数 classinfo 的子类。\n一个类被认为是其自身的子类。\nclassinfo 可以是类对象的元组，只要class是其中任何一个候选类的子类，则返回True 。\n\nclass A:\n\tpass\nclass B(A):\n\tpass\nprint(issubclass(B, A)) # True\nprint(issubclass(B, B)) # True\nprint(issubclass(A, B)) # False\nprint(issubclass(B, object)) # True\n\nisinstance(object, classinfo) 方法用于判断一个对象是否是一个已知的类型，类似type() 。\ntype() 不会认为子类是一种父类类型，不考虑继承关系。\nisinstance() 会认为子类是一种父类类型，考虑继承关系。\n如果第一个参数不是对象，则永远返回False 。\n如果第二个参数不是类或者由类对象组成的元组，会抛出一个TypeError 异常。\n\na = 2\nprint(isinstance(a, int)) # True\nprint(isinstance(a, str)) # False\nprint(isinstance(a, (str, int, list))) # True\nclass A:\npass\nclass B(A):\npass\nprint(isinstance(A(), A)) # True\nprint(type(A()) == A) # True\nprint(isinstance(B(), A)) # True\nprint(type(B()) == A) # False\n\nhasattr(object, name) 用于判断对象是否包含对应的属性。\n\nclass Coordinate:\n    x = 10\n    y = -5\n    z = 0\npoint1 = Coordinate()\nprint(hasattr(point1, 'x')) # True\nprint(hasattr(point1, 'y')) # True\nprint(hasattr(point1, 'z')) # True\nprint(hasattr(point1, 'no')) # False\n\ngetattr(object, name[, default]) 用于返回一个对象属性值。\n\nclass A(object):\n\tbar = 1\na = A()\nprint(getattr(a, 'bar')) # 1\nprint(getattr(a, 'bar2', 3)) # 3\nprint(getattr(a, 'bar2'))\n# AttributeError: 'A' object has no attribute 'bar2'\n\nsetattr(object, name, value) 对应函数 getattr() ，用于设置属性值，该属性不一定是存在的。\n\nclass A(object):\nbar = 1\na = A()\nprint(getattr(a, 'bar')) # 1\nsetattr(a, 'bar', 5)\nprint(a.bar) # 5\nsetattr(a, \"age\", 28)\nprint(a.age) # 28\n\ndelattr(object, name) 用于删除属性。\n\nclass property([fget[, fset[, fdel[, doc]]]]) 用于在新式类中返回属性值。a. fget — 获取属性值的函数b. fset — 设置属性值的函数c. fdel — 删除属性值函数d. doc — 属性描述信息\n\n\nclass C(object):\n    def __init__(self):\n    \tself.__x = None\n    def getx(self):\n    \treturn self.__x\n    def setx(self, value):\n    \tself.__x = value\n    def delx(self):\n    \tdel self.__x\n    x = property(getx, setx, delx, \"I'm the 'x' property.\")\ncc = C()\ncc.x = 2\nprint(cc.x) # 2\n12 魔法方法魔法方法总是被双下划线包围，例如init 。魔法方法是面向对象的 Python 的一切，如果你不知道魔法方法，说明你还没能意识到面向对象的 Python 的强大。魔法方法的“魔力”体现在它们总能够在适当的时候被自动调用。魔法方法的第一个参数应为cls （类方法） 或者self （实例方法）。\n\ncls ：代表一个类的名称\nself ：代表一个实例对象的名称\n\n12.1 基本的魔法方法init(self[, …]):构造器，当一个实例被创建的时候调用的初始化方法\nclass Rectangle:\n    def __init__(self, x, y):\n    \tself.x = x\n    \tself.y = y\n__new__(cls[, …])\n\nnew 是在一个对象实例化的时候所调用的第一个方法，在调用init 初始化前，先调用new 。\nnew 至少要有一个参数cls ，代表要实例化的类，此参数在实例化时由 Python 解释器自动提供，后面的参数直接传递给init 。\nnew 对当前类进行了实例化，并将实例返回，传给init 的self 。但是，执行了new ，并不一定会进入init ，只有new 返回了，当前类cls 的实例，当前类的init 才会进入。\n若new 没有正确返回当前类cls 的实例，那init 是不会被调用的，即使是父类的实例也不行，将没有init 被调用。\n可利用new 实现单例模式。\nnew 方法主要是当你继承一些不可变的 class 时（比如int, str, tuple ）， 提供给你一个自定义这些类的实例化过程的途径。\n\n__del__(self)\n析构器，当一个对象将要被系统回收之时调用的方法。        Python 采用自动引用计数（ARC）方式来回收对象所占用的空间，当程序中有一个变量引用该 Python 对象时，Python会自动保证该对象引用计数为 1；当程序中有两个变量引用该 Python 对象时，Python 会自动保证该对象引用计数为 2，依此类推，如果一个对象的引用计数变成了 0，则说明程序中不再有变量引用该对象，表明程序不再需要该对象，因此Python 就会回收该对象。        大部分时候，Python 的 ARC 都能准确、高效地回收系统中的每个对象。但如果系统中出现循环引用的情况，比如对象a 持有一个实例变量引用对象 b，而对象 b 又持有一个实例变量引用对象 a，此时两个对象的引用计数都是 1，而实际上程序已经不再有变量引用它们，系统应该回收它们，此时 Python 的垃圾回收器就可能没那么快，要等专门的循环垃圾回收器（Cyclic Garbage Collector）来检测并回收这种引用循环。\n__str__(self) :\n\n当你打印一个对象的时候，触发str\n当你使用%s 格式化的时候，触发str\nstr 强转数据类型的时候，触发str\n\n__repr__(self):\n\nrepr 是str 的备胎\n有str 的时候执行str ,没有实现str 的时候，执行repr\nrepr(obj) 内置函数对应的结果是repr 的返回值\n当你使用%r 格式化的时候 触发repr\n\nstr(self) 的返回结果可读性强。也就是说， str 的意义是得到便于人们阅读的信息，就像下面的 ‘2019-10-11’ 一样。repr(self) 的返回结果应更准确。怎么说， repr 存在的目的在于调试，便于开发者使用。\n12.2 算术运算符类型工厂函数，指的是不通过类而是通过函数来创建对象。\n\n__add__(self, other) 定义加法的行为： +\n__sub__(self, other) 定义减法的行为： -\n__mul__(self, other) 定义乘法的行为： *\n__truediv__(self, other) 定义真除法的行为： /\n__floordiv__(self, other) 定义整数除法的行为： //\n__mod__(self, other) 定义取模算法的行为： %\n__divmod__(self, other) 定义当被 divmod() 调用时的行为\ndivmod(a, b) 把除数和余数运算结果结合起来，返回一个包含商和余数的元组(a // b, a % b) 。\n__mul__(self, other) 定义乘法的行为： *\n__truediv__(self, other) 定义真除法的行为： /\n__floordiv__(self, other) 定义整数除法的行为： //\n__mod__(self, other) 定义取模算法的行为： %\n__divmod__(self, other) 定义当被 divmod() 调用时的行为\ndivmod(a, b) 把除数和余数运算结果结合起来，返回一个包含商和余数的元组(a // b, a % b) 。\n\n12.3 反算术运算符反运算魔方方法，与算术运算符保持一一对应，不同之处就是反运算的魔法方法多了一个“r”。当文件左操作不支持相应的操作时被调用。\n\n__radd__(self, other) 定义加法的行为： +\n__rsub__(self, other) 定义减法的行为： -\n__rmul__(self, other) 定义乘法的行为： *\n__rtruediv__(self, other) 定义真除法的行为： /\n__rfloordiv__(self, other) 定义整数除法的行为： //\n__rmod__(self, other) 定义取模算法的行为： %\n__rdivmod__(self, other) 定义当被 divmod() 调用时的行为\n__rpow__(self, other[, module]) 定义当被 power() 调用或 ** 运算时的行为\n__rlshift__(self, other) 定义按位左移位的行为： &lt;&lt;\n__rrshift__(self, other) 定义按位右移位的行为： &gt;&gt;\n__rand__(self, other) 定义按位与操作的行为： &amp;\n__rxor__(self, other) 定义按位异或操作的行为： ^\n__ror__(self, other) 定义按位或操作的行为： |\n\na + b这里加数是a ，被加数是b ，因此是a 主动，反运算就是如果a 对象的__add() 方法没有实现或者不支持相应的操作，那么 Python 就会调用b 的\\radd__() 方法。\nclass Nint(int):\n    def __radd__(self, other):\n    \treturn int.__sub__(other, self) # 注意 self 在后面\na &#x3D; Nint(5)\nb &#x3D; Nint(3)\nprint(a + b) # 8\nprint(1 + b) # -2\n12.4 增量赋值运算符\n__iadd__(self, other) 定义赋值加法的行为： +=\n__isub__(self, other) 定义赋值减法的行为： -=\n__imul__(self, other) 定义赋值乘法的行为： *=\n__itruediv__(self, other) 定义赋值真除法的行为： /=\n__ifloordiv__(self, other) 定义赋值整数除法的行为： //=\n__imod__(self, other) 定义赋值取模算法的行为： %=\n__ipow__(self, other[, modulo]) 定义赋值幂运算的行为： **=\n__ilshift__(self, other) 定义赋值按位左移位的行为： &lt;&lt;=\n__irshift__(self, other) 定义赋值按位右移位的行为： &gt;&gt;=\n__iand__(self, other) 定义赋值按位与操作的行为： &amp;=\n__ixor__(self, other) 定义赋值按位异或操作的行为： ^=\n__ior__(self, other) 定义赋值按位或操作的行为： |=\n\n12.5 一元运算符\n__neg__(self) 定义正号的行为： +x\n__pos__(self) 定义负号的行为： -x\n__abs__(self) 定义当被abs() 调用时的行为\n__invert__(self) 定义按位求反的行为： ~x\n\n12.6 属性访问__getattr(self, name) : 定义当用户试图获取一个不存在的属性时的行为。__getattribute__(self, name) ：定义当该类的属性被访问时的行为（先调用该方法，查看是否存在该属性，若不存在，接着去调用\\getattr__ ）。__setattr__(self, name, value) ：定义当一个属性被设置时的行为。__delattr__(self, name) ：定义当一个属性被删除时的行为。\n12.7 描述符描述符就是将某种特殊类型的类的实例指派给另一个类的属性。\n\n__get__(self, instance, owner) 用于访问属性，它返回属性的值。\n\n__set__(self, instance, value) 将在属性分配操作中调用，不返回任何内容。\n\n__del__(self, instance) 控制删除操作，不返回任何内容。\n\n\n12.8 定制序列协议（Protocols）与其它编程语言中的接口很相似，它规定你哪些方法必须要定义。然而，在 Python 中的协议就显得不那么正式。事实上，在 Python 中，协议更像是一种指南。容器类型的协议\n\n如果说你希望定制的容器是不可变的话，你只需要定义__len() 和\\getitem__() 方法。\n如果你希望定制的容器是可变的话，除了__len() 和\\getitem() 方法，你还需要定义\\setitem()和\\delitem__() 两个方法。\n\n__len__(self) 定义当被len() 调用时的行为（返回容器中元素的个数）。\n\n__getitem__(self, key) 定义获取容器中元素的行为，相当于self[key] 。\n__setitem__(self, key, value) 定义设置容器中指定元素的行为，相当于self[key] = value 。\n__delitem__(self, key) 定义删除容器中指定元素的行为，相当于del self[key] 。\n\n12.9 迭代器\n迭代是 Python 最强大的功能之一，是访问集合元素的一种方式。\n迭代器是一个可以记住遍历的位置的对象。\n迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。\n迭代器只能往前不会后退。\n字符串，列表或元组对象都可用于创建迭代器：\n\nstring = 'lsgogroup'\nfor c in string:\n\tprint(c)\n\n迭代器有两个基本的方法： iter() 和 next() 。\niter(object) 函数用来生成迭代器。\nnext(iterator[, default]) 返回迭代器的下一个项目。\niterator — 可迭代对象\ndefault — 可选，用于设置在没有下一个元素时返回该默认值，如果不设置，又没有下一个元素则会触发StopIteration 异常。\n\nlinks = &#123;'B': '百度', 'A': '阿里', 'T': '腾讯'&#125;\nit = iter(links)\nprint(next(it)) # B\nprint(next(it)) # A\nprint(next(it)) # T\nprint(next(it)) # StopIteration\n\nit = iter(links)\nwhile True:\n    try:\n    \teach = next(it)\n    except StopIteration:\n    \tbreak\n    print(each)\n# B\n# A\n# T\n把一个类作为一个迭代器使用需要在类中实现两个魔法方法 iter() 与 next() 。\n\n__iter(self) 定义当迭代容器中的元素的行为，返回一个特殊的迭代器对象， 这个迭代器对象实现了next__() 方法并通过 StopIteration 异常标识迭代的完成。\n__next__() 返回下一个迭代器对象。\nStopIteration 异常用于标识迭代的完成，防止出现无限循环的情况，在 next() 方法中我们可以设置在完成指定循环次数后触发 StopIteration 异常来结束迭代。\n\n12.10 生成器\n在 Python 中，使用了 yield 的函数被称为生成器（generator）。\n跟普通函数不同的是，生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。\n在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行。\n调用一个生成器函数，返回的是一个迭代器对象。\n\n13 模块​        在前面我们脚本是用 Python 解释器来编程，如果你从 Python 解释器退出再进入，那么你定义的所有的方法和变量就都消失了。​        为此 Python 提供了一个办法，把这些定义存放在文件中，为一些脚本或者交互式的解释器实例使用，这个文件被称为模块（Module）。​        模块是一个包含所有你定义的函数和变量的文件，其后缀名是.py 。模块可以被别的程序引入，以使用该模块中的函数等功能。这也是使用 Python 标准库的方法。\n13.1 什么是模块\n容器 -&gt; 数据的封装\n函数 -&gt; 语句的封装\n类 -&gt; 方法和属性的封装\n模块 -&gt; 程序文件\n\n13.2 命名空间命名空间因为对象的不同，也有所区别，可以分为如下几种：\n\n内置命名空间（Built-in Namespaces）：Python 运行起来，它们就存在了。内置函数的命名空间都属于内置命名空间，所以，我们可以在任何程序中直接运行它们，比如id() ,不需要做什么操作，拿过来就直接使用了。\n全局命名空间（Module：Global Namespaces）：每个模块创建它自己所拥有的全局命名空间，不同模块的全局命名空间彼此独立，不同模块中相同名称的命名空间，也会因为模块的不同而不相互干扰。\n本地命名空间（Function &amp; Class：Local Namespaces）：模块中有函数或者类，每个函数或者类所定义的命名空间就是本地命名空间。如果函数返回了结果或者抛出异常，则本地命名空间也结束了。\n\n程序在查询上述三种命名空间的时候，就按照从里到外的顺序，即：Local Namespaces —&gt; Global Namesspaces —&gt; Built-inNamesspaces。\nimport hello\nhello.hi() # Hi everyone, I love lsgogroup!\nhi() # NameError: name 'hi' is not defined\n13.3 导入模块\n第一种：import 模块名\n第二种：from 模块名 import 函数名\n第三种：import 模块名 as 新名字\n\n13.4 if __name == ‘\\main__’​        对于很多编程语言来说，程序都必须要有一个入口，而 Python 则不同，它属于脚本语言，不像编译型语言那样先将程序编译成二进制再运行，而是动态的逐行解释运行。也就是从脚本第一行开始运行，没有统一的入口。假设我们有一个 const.py 文件，内容如下：\n13.5 搜索路径当解释器遇到 import 语句，如果模块在当前的搜索路径就会被导入。\nimport sys\nprint(sys.path)\n# ['C:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib',\n'C:\\\\ProgramData\\\\Anaconda3', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages',...]\n​        我们使用 import 语句的时候，Python 解释器是怎样找到对应的文件的呢？​        这就涉及到 Python 的搜索路径，搜索路径是由一系列目录名组成的，Python 解释器就依次从这些目录中去寻找所引入的模块。​        这看起来很像环境变量，事实上，也可以通过定义环境变量的方式来确定搜索路径。搜索路径是在 Python 编译或安装的时候确定的，安装新的库应该也会修改。搜索路径被存储在 sys 模块中的 path 变量中。\n13.6 包（package）包是一种管理 Python 模块命名空间的形式，采用”点模块名称”。创建包分为三个步骤：\n\n创建一个文件夹，用于存放相关的模块，文件夹的名字即包的名字。\n在文件夹中创建一个 init.py 的模块文件，内容可以为空。\n将相关的模块放入文件夹中。\n\n不妨假设你想设计一套统一处理声音文件和数据的模块（或者称之为一个”包”）。\n现存很多种不同的音频文件格式（基本上都是通过后缀名区分的，例如：.wav，.aiff，.au），所以你需要有一组不断增加的模块，用来在不同的格式之间转换。并且针对这些音频数据，还有很多不同的操作（比如混音，添加回声，增加均衡器功能，创建人造立体声效果），所以你还需要一组怎么也写不完的模块来处理这些操作。\n14 datetime模块14.1 datetime类class datetime(date):\n    def __init__(self, year, month, day, hour, minute, second, microsecond, tzinfo)\n    \tpass\n    def now(cls, tz=None):\n    \tpass\n    def timestamp(self):\n    \tpass\n    def fromtimestamp(cls, t, tz=None):\n    \tpass\n    def date(self):\n    \tpass\n    def time(self):\n    \tpass\n    def year(self):\n    \tpass\n    def month(self):\n    \tpass\n    def day(self):\n    \tpass\n    def hour(self):\n    \tpass\n    def minute(self):\n    \tpass\n    def second(self):\n    \tpass\n    def isoweekday(self):\n    \tpass\n    def strftime(self, fmt):\n    \tpass\n    def combine(cls, date, time, tzinfo=True):\n    \tpass\n\ndatetime.now(tz=None) 获取当前的日期时间，输出顺序为：年、月、日、时、分、秒、微秒。\ndatetime.timestamp() 获取以 1970年1月1日为起点记录的秒数。\ndatetime.fromtimestamp(tz=None) 使用 unixtimestamp 创建一个 datetime。\n\nimport datetime\ndt = datetime.datetime(year=2020, month=6, day=25, hour=11, minute=23, second=59)\nprint(dt) # 2020-06-25 11:23:59\nprint(dt.timestamp()) # 1593055439.0\ndt = datetime.datetime.fromtimestamp(1593055439.0)\nprint(dt) # 2020-06-25 11:23:59\nprint(type(dt)) # &lt;class 'datetime.datetime'>\ndt = datetime.datetime.now()\nprint(dt) # 2020-06-25 11:11:03.877853\nprint(type(dt)) # &lt;class 'datetime.datetime'>\n\ndatetime.strftime(fmt) 格式化 datetime 对象。\n\n%a 本地简化星期名称（如星期一，返回 Mon）%A 本地完整星期名称（如星期一，返回 Monday）%b 本地简化的月份名称（如一月，返回 Jan）%B 本地完整的月份名称（如一月，返回 January）%c 本地相应的日期表示和时间表示%d 月内中的一天（0-31）%H 24小时制小时数（0-23）%I 12小时制小时数（01-12）%j 年内的一天（001-366）%m 月份（01-12）%M 分钟数（00-59）%p 本地A.M.或P.M.的等价符%S 秒（00-59）%U 一年中的星期数（00-53）星期天为星期的开始%w 星期（0-6），星期天为星期的开始%W 一年中的星期数（00-53）星期一为星期的开始%x 本地相应的日期表示%X 本地相应的时间表示%y 两位数的年份表示（00-99）%Y 四位数的年份表示（0000-9999）%Z 当前时区的名称（如果是本地时间，返回空字符串）%% %号本身\nimport datetime\ndt = datetime.datetime(year=2020, month=6, day=25, hour=11, minute=51, second=49)\ns = dt.strftime(\"'%Y/%m/%d %H:%M:%S\")\nprint(s) # '2020/06/25 11:51:49\ns = dt.strftime('%d %B, %Y, %A')\nprint(s) # 25 June, 2020, Thursday\n\ndatetime.date() Return the date part.\ndatetime.time() Return the time part, with tzinfo None.\ndatetime.year 年\ndatetime.month 月\ndatetime.day 日\ndatetime.hour 小时\ndatetime.minute 分钟\ndatetime.second 秒\ndatetime.isoweekday 星期几\n\n在处理含有字符串日期的数据集或表格时，我们需要一种自动解析字符串的方法，无论它是什么格式的，都可以将其转化为 datetime 对象。这时，就要使用到 dateutil 中的 parser 模块。\n\nparser.parse(timestr, parserinfo=None, **kwargs)\n\nfrom dateutil import parser\ns = '2020-06-25'\ndt = parser.parse(s)\nprint(dt) # 2020-06-25 00:00:00\nprint(type(dt)) # &lt;class 'datetime.datetime'>\ns = 'March 31, 2010, 10:51pm'\ndt = parser.parse(s)\nprint(dt) # 2010-03-31 22:51:00\nprint(type(dt)) # &lt;class 'datetime.datetime'>\n14.2 date类class date:\n    def __init__(self, year, month, day):\n    \tpass\n    def today(cls):\n    \tpass\n\ndate.today() 获取当前日期信息。\n\nimport datetime\nd = datetime.date(2020, 6, 25)\nprint(d) # 2020-06-25\nprint(type(d)) # &lt;class 'datetime.date'>\nd = datetime.date.today()\nprint(d) # 2020-06-25\nprint(type(d)) # &lt;class 'datetime.date'>\n14.3 time类class time:\n    def __init__(self, hour, minute, second, microsecond, tzinfo):\n    \tpass\nimport datetime\nt = datetime.time(12, 9, 23, 12980)\nprint(t) # 12:09:23.012980\nprint(type(t)) # &lt;class 'datetime.time'>\n\n# 输入\ndate = datetime.date(2019, 10, 2)\n# 输出\n2019-10-02 00:00:00\n14.4 timedelta类timedelta 表示具体时间实例中的一段时间。你可以把它们简单想象成两个日期或时间之间的间隔。它常常被用来从 datetime 对象中添加或移除一段特定的时间。\nclass timedelta(SupportsAbs[timedelta]):\n    def __init__(self, days, seconds, microseconds, milliseconds, minutes, hours, weeks,):\n    \tpass\n    def days(self):\n    \tpass\n    def total_seconds(self):\n    \tpass\nimport datetime\n\ntd = datetime.timedelta(days=30)\nprint(td) # 30 days, 0:00:00\nprint(type(td)) # &lt;class 'datetime.timedelta'>\nprint(datetime.date.today()) # 2020-07-01\nprint(datetime.date.today() + td) # 2020-07-31\n\ndt1 = datetime.datetime(2020, 1, 31, 10, 10, 0)\ndt2 = datetime.datetime(2019, 1, 31, 10, 10, 0)\ntd = dt1 - dt2\nprint(td) # 365 days, 0:00:00\nprint(type(td)) # &lt;class 'datetime.timedelta'>\n\ntd1 = datetime.timedelta(days=30) # 30 days\ntd2 = datetime.timedelta(weeks=1) # 1 week\ntd = td1 - td2\nprint(td) # 23 days, 0:00:00\nprint(type(td)) # &lt;class 'datetime.timedelta'>\n如果将两个 datetime 对象相减，就会得到表示该时间间隔的 timedelta 对象。同样地，将两个时间间隔相减，可以得到另一个 timedelta 对象。\n15 文件与文件系统15.1 打开文件open(file, mode=’r’, buffering=None, encoding=None, errors=None, newline=None, closefd=True)Open file and return a stream. Raise OSError upon failure.a. file : 必需，文件路径（相对或者绝对路径）。b. mode : 可选，文件打开模式c. buffering : 设置缓冲d. encoding : 一般使用utf8e. errors : 报错级别f. newline : 区分换行符常见的mode 如下表所示：\n\n\n\n\n打开模式\n执行操作\n\n\n\n\n‘r’\n以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。\n\n\n‘w’\n打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑。即原有内容会被删除。如果该文件不存在，创建新文件。\n\n\n‘x’\n写模式，新建一个文件，如果该文件已存在则会报错。\n\n\n‘a’\n追加模式，打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。\n\n\n‘b’\n以二进制模式打开文件。一般用于非文本文件，如：图片。\n\n\n‘t’\n以文本模式打开（默认）。一般用于文本文件，如：txt。\n\n\n‘+’\n可读写模式（可添加到其它模式中使用）\n\n\n\n\nf = open('将进酒.txt')\nprint(f)\n# &lt;_io.TextIOWrapper name='将进酒.txt' mode='r' encoding='cp936'>\nfor each in f:\nprint(each)\n15.2 文件对象方法fileObject.close() 用于关闭一个已打开的文件。关闭后的文件不能再进行读写操作， 否则会触发ValueError错误。\nf = open(\"将进酒.txt\")\nprint('FileName:', f.name) # FileName: 将进酒.txt\nf.close()\nfileObject.read([size]) 用于从文件读取指定的字符数，如果未给定或为负则读取所有。\nf = open('将进酒.txt', 'r')\nline = f.read(20)\nprint(\"读取的字符串: %s\" % line)\n# 读取的字符串: 君不见，黄河之水天上来，奔流到海不复回。\nf.close()\nfileObject.readline() 读取整行，包括 “\\n” 字符。\nf = open('将进酒.txt', 'r')\nline = f.readline()\nprint(\"读取的字符串: %s\" % line)\n# 读取的字符串: 君不见，黄河之水天上来，奔流到海不复回。\nf.close()\nfileObject.readlines() 用于读取所有行(直到结束符 EOF)并返回列表，该列表可以由 Python 的for… in … 结构进行处理。\nf = open('将进酒.txt', 'r')\nline = f.readline()\nprint(\"读取的字符串: %s\" % line)\n# 读取的字符串: 君不见，黄河之水天上来，奔流到海不复回。\nf.close()\nfileObject.tell() 返回文件的当前位置，即文件指针当前位置。\nf = open('将进酒.txt', 'r')\nline = f.readline()\nprint(line)\n# 君不见，黄河之水天上来，奔流到海不复回。\npos = f.tell()\nprint(pos) # 42\nf.close()\nfileObject.seek(offset[, whence]) 用于移动文件读取指针到指定位置。a. offset ：开始的偏移量，也就是代表需要移动偏移的字节数，如果是负数表示从倒数第几位开始。b. whence ：可选，默认值为 0。给 offset 定义一个参数，表示要从哪个位置开始偏移；0 代表从文件开头开始算起，1 代表从当前位置开始算起，2 代表从文件末尾算起。\nf = open('将进酒.txt', 'r')\nline = f.readline()\nprint(line)\n# 君不见，黄河之水天上来，奔流到海不复回。\nline = f.readline()\nprint(line)\n# 君不见，高堂明镜悲白发，朝如青丝暮成雪。\nf.seek(0, 0)\nline = f.readline()\nprint(line)\n# 君不见，黄河之水天上来，奔流到海不复回。\nf.close()\nfileObject.write(str) 用于向文件中写入指定字符串，返回的是写入的字符长度。\nf = open('workfile.txt', 'wb+')\nprint(f.write(b'0123456789abcdef')) # 16\nprint(f.seek(5)) # 5\nprint(f.read(1)) # b'5'\nprint(f.seek(-3, 2)) # 13\nprint(f.read(1)) # b'd'\n在文件关闭前或缓冲区刷新前，字符串内容存储在缓冲区中，这时你在文件中是看不到写入的内容的。如果文件打开模式带b ，那写入文件内容时， str （参数）要用encode 方法转为bytes 形式，否则报错： TypeError: a bytes-like object is required, not ‘str’ 。\nstr = '...'\n# 文本 = Unicode字符序列\n# 相当于 string 类型\nstr = b'...'\n# 文本 = 八位序列(0到255之间的整数)\n# 字节文字总是以‘b’或‘B’作为前缀；它们产生一个字节类型的实例，而不是str类型。\n# 相当于 byte[]\nfileObject.writelines(sequence) 向文件写入一个序列字符串列表，如果需要换行则要自己加入每行的换行符\\n 。\nf = open('test.txt', 'w+')\nseq = ['小马的程序人生\\n', '老马的程序人生']\nf.writelines(seq)\nf.seek(0, 0)\nfor each in f:\nprint(each)\n# 小马的程序人生\n# 老马的程序人生\nf.close()\n15.3 简洁的 with 语句一些对象定义了标准的清理行为，无论系统是否成功的使用了它，一旦不需要它了，那么这个标准的清理行为就会执行。关键词 with 语句就可以保证诸如文件之类的对象在使用完之后一定会正确的执行它的清理方法。\ntry:\nf = open('myfile.txt', 'w')\nfor line in f:\nprint(line)\nexcept OSError as error:\nprint('出错啦!%s' % str(error))\nfinally:\nf.close()\n# 出错啦!not readable\ntry:\nwith open('myfile.txt', 'w') as f:\nfor line in f:\nprint(line)\nexcept OSError as error:\nprint('出错啦!%s' % str(error))\n# 出错啦!not readable\n16 OS我们所知道常用的操作系统就有：Windows，Mac OS，Linu，Unix等，这些操作系统底层对于文件系统的访问工作原理是不一样的，因此你可能就要针对不同的系统来考虑使用哪些文件系统模块……，这样的做法是非常不友好且麻烦的，因为这样就意味着当你的程序运行环境一改变，你就要相应的去修改大量的代码来应对。有了OS（Operation System）模块，我们不需要关心什么操作系统下使用什么模块，OS模块会帮你选择正确的模块并调用。\n\nos.getcwd() 用于返回当前工作目录。\nos.chdir(path) 用于改变当前工作目录到指定的路径。\n\nimport os\npath = 'C:\\\\'\nprint(\"当前工作目录 : %s\" % os.getcwd())\n# 当前工作目录 : C:\\Users\\Administrator\\PycharmProjects\\untitled1\nos.chdir(path)\nprint(\"目录修改成功 : %s\" % os.getcwd())\n# 目录修改成功 : C:\\\nlistdir (path=’.’) 返回path 指定的文件夹包含的文件或文件夹的名字的列表。\nimport os\ndirs = os.listdir()\nfor item in dirs:\nprint(item)\nos.mkdir(path) 创建单层目录，如果该目录已存在抛出异常。\nimport os\nif os.path.isdir(r'.\\b') is False:\n    os.mkdir(r'.\\B')\n    os.mkdir(r'.\\B\\A')\nos.mkdir(r'.\\C\\A') # FileNotFoundError\nos.makedirs(path) 用于递归创建多层目录，如果该目录已存在抛出异常。\nimport os\nos.makedirs(r'.\\E\\A')\nos.remove(path) 用于删除指定路径的文件。如果指定的路径是一个目录，将抛出 OSError 。\nimport os\nprint(\"目录为: %s\" % os.listdir(r'.\\E\\A'))\nos.remove(r'.\\E\\A\\test.txt')\nprint(\"目录为: %s\" % os.listdir(r'.\\E\\A'))\nos.rmdir(path) 用于删除单层目录。仅当这文件夹是空的才可以, 否则, 抛出 OSError 。\nimport os\nprint(\"目录为: %s\" % os.listdir(r'.\\E'))\nos.rmdir(r'.\\E\\A')\nprint(\"目录为: %s\" % os.listdir(r'.\\E'))\nos.removedirs(path) 递归删除目录，从子目录到父目录逐层尝试删除，遇到目录非空则抛出异常。\nimport os\nprint(\"目录为: %s\" % os.listdir(os.getcwd()))\nos.removedirs(r'.\\E\\A') # 先删除A 然后删除E\nprint(\"目录为: %s\" % os.listdir(os.getcwd()))\nos.rename(src, dst) 方法用于命名文件或目录，从 src 到 dst ，如果 dst 是一个存在的目录, 将抛出OSError \nimport os\nprint(\"目录为: %s\" % os.listdir(os.getcwd()))\nos.rename(\"test.txt\", \"test2.txt\")\nprint(\"重命名成功。\")\nprint(\"目录为: %s\" % os.listdir(os.getcwd()))\nos.system(command) 运行系统的shell命令（将字符串转化成命令）\nimport os\npath = os.getcwd() + '\\\\a.py'\na = os.system(r'python %s' % path)\nos.system('calc') # 打开计算器\n\nos.curdir 指代当前目录（ . ）\nos.pardir 指代上一级目录（ .. ）\nos.sep 输出操作系统特定的路径分隔符（win下为\\\\ ，Linux下为/ ）\nos.linesep 当前平台使用的行终止符（win下为\\r\\n ，Linux下为\\n ）\nos.name 指代当前使用的操作系统（包括：’mac’，’nt’）\n\nimport os\nprint(os.curdir) # .\nprint(os.pardir) # ..\nprint(os.sep) # \\\nprint(os.linesep)\nprint(os.name) # nt\n\nos.path.basename(path) 去掉目录路径，单独返回文件名\nos.path.dirname(path) 去掉文件名，单独返回目录路径\nos.path.join(path1[, path2[, …]]) 将 path1 ， path2 各部分组合成一个路径名\nos.path.split(path) 分割文件名与路径，返回(f_path,f_name) 元组。如果完全使用目录，它会将最后一个目录作为文件名分离，且不会判断文件或者目录是否存在。\nos.path.splitext(path) 分离文件名与扩展名，返回(f_path,f_name) 元组。\n\nimport os\n# 返回文件名\nprint(os.path.basename(r'C:\\test\\lsgo.txt')) # lsgo.txt\n# 返回目录路径\nprint(os.path.dirname(r'C:\\test\\lsgo.txt')) # C:\\test\n# 将目录和文件名合成一个路径\nprint(os.path.join('C:\\\\', 'test', 'lsgo.txt')) # C:\\test\\lsgo.txt\n# 分割文件名与路径\nprint(os.path.split(r'C:\\test\\lsgo.txt')) # ('C:\\\\test', 'lsgo.txt')\n# 分离文件名与扩展名\nprint(os.path.splitext(r'C:\\test\\lsgo.txt')) # ('C:\\\\test\\\\lsgo', '.txt')\n\nos.path.getsize(file) 返回指定文件大小，单位是字节。\nos.path.getatime(file) 返回指定文件最近的访问时间\nos.path.getctime(file) 返回指定文件的创建时间\nos.path.getmtime(file) 返回指定文件的最新的修改时间\n浮点型秒数，可用time模块的gmtime() 或localtime() 函数换算\n\nimport os\nimport time\nfile = r'.\\lsgo.txt'\nprint(os.path.getsize(file)) # 30\nprint(os.path.getatime(file)) # 1565593737.347196\nprint(os.path.getctime(file)) # 1565593737.347196\nprint(os.path.getmtime(file)) # 1565593797.9298275\nprint(time.gmtime(os.path.getctime(file)))\n# time.struct_time(tm_year=2019, tm_mon=8, tm_mday=12, tm_hour=7, tm_min=8, tm_sec=57, tm_wday=0,\ntm_yday=224, tm_isdst=0)\nprint(time.localtime(os.path.getctime(file)))\n# time.struct_time(tm_year=2019, tm_mon=8, tm_mday=12, tm_hour=15, tm_min=8, tm_sec=57, tm_wday=0,\ntm_yday=224, tm_isdst=0)\n\nos.path.exists(path) 判断指定路径（目录或文件）是否存在\nos.path.isabs(path) 判断指定路径是否为绝对路径\nos.path.isdir(path) 判断指定路径是否存在且是一个目录\nos.path.isfile(path) 判断指定路径是否存在且是一个文件\nos.path.islink(path) 判断指定路径是否存在且是一个符号链接\nos.path.ismount(path) 判断指定路径是否存在且是一个悬挂点\nos.path.samefile(path1,path2) 判断path1和path2两个路径是否指向同一个文件\n\nimport os\nprint(os.path.ismount('D:\\\\')) # True\nprint(os.path.ismount('D:\\\\Test')) # False\n17 序列化与反序列化Python 的 pickle 模块实现了基本的数据序列和反序列化。\n\n通过 pickle 模块的序列化操作我们能够将程序中运行的对象信息保存到文件中去，永久存储。\n通过 pickle 模块的反序列化操作，我们能够从文件中创建上一次程序保存的对象。\n\npickle模块中最常用的函数为：pickle.dump(obj, file, [,protocol]) 将obj 对象序列化存入已经打开的file 中。\n\nobj ：想要序列化的obj 对象。\nfile :文件名称。\nprotocol ：序列化使用的协议。如果该项省略，则默认为0。如果为负值或HIGHEST_PROTOCOL ，则使用最高的协议版本。\n\npickle.load(file) 将file 中的对象序列化读出。\nfile ：文件名称。\nimport pickle\ndataList = [[1, 1, 'yes'],\n            [1, 1, 'yes'],\n            [1, 0, 'no'],\n            [0, 1, 'no'],\n            [0, 1, 'no']]\ndataDic = &#123;0: [1, 2, 3, 4],\n            1: ('a', 'b'),\n            2: &#123;'c': 'yes', 'd': 'no'&#125;&#125;\n# 使用dump()将数据序列化到文件中\nfw = open(r'.\\dataFile.pkl', 'wb')\n# Pickle the list using the highest protocol available.\npickle.dump(dataList, fw, -1)\n# Pickle dictionary using protocol 0.\npickle.dump(dataDic, fw)\nfw.close()\n# 使用load()将数据从文件中序列化读出\nfr = open('dataFile.pkl', 'rb')\ndata1 = pickle.load(fr)\nprint(data1)\ndata2 = pickle.load(fr)\nprint(data2)\nfr.close()\n# [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n# &#123;0: [1, 2, 3, 4], 1: ('a', 'b'), 2: &#123;'c': 'yes', 'd': 'no'&#125;&#125;\n","slug":"P0-Python","date":"2021-11-01T08:38:34.000Z","categories_index":"Python","tags_index":"","author_index":"YFR718"}]