{"title":"Hadoop基础","uid":"d627fd9c1bed4de81fff7ae3f54e977a","slug":"B2-Hadoop","date":"2021-11-12T11:13:22.000Z","updated":"2021-12-06T05:41:07.546Z","comments":true,"path":"api/articles/B2-Hadoop.json","keywords":null,"cover":"https://img-blog.csdnimg.cn/0c9f61a46aa44eb396187322d87d7f20.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center","content":"<h1 id=\"0-大数据概念\"><a href=\"#0-大数据概念\" class=\"headerlink\" title=\"0. 大数据概念\"></a>0. 大数据概念</h1><p><strong>大数据（Big Data）</strong>：指无法在==一定时间范围内==用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的==海量、高增长率和多样化==的==信息资产==。大数据主要解决，海量数据的采集、存储和分析计算问题。</p>\n<p><strong>⭐大数据特点（4V） ：</strong></p>\n<ul>\n<li>Volume（大量）</li>\n<li>Velocity（高速）</li>\n<li>Variety（多样）</li>\n<li>Value（低价值密度）</li>\n</ul>\n<h1 id=\"1-Hadoop-概述\"><a href=\"#1-Hadoop-概述\" class=\"headerlink\" title=\"1. Hadoop 概述\"></a>1. Hadoop 概述</h1><ul>\n<li><p>Hadoop是什么<br>1）Hadoop是一个由Apache基金会所开发的分布式系统基础架构。<br>2）主要解决，海量数据的存储和海量数据的分析计算问题。<br>3）广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。</p>\n</li>\n<li><p>Hadoop发展历史</p>\n<p>1）Hadoop创始人Doug</p>\n<p>2）2001年年底Lucene成为Apache基金会的一个子项目。<br>3）对于海量数据的场景，Lucene框架面对与Google同样的困难，存储海量数据困难，检索海量速度慢。<br>4）学习和模仿Google解决这些问题的办法 ：微型版Nutch。<br>5）可以说Google是Hadoop的思想之源（Google在大数据方面的三篇论文）</p>\n<p>GFS —-&gt;HDFS、Map-Reduce —-&gt;MR、BigTable —-&gt;HBase<br>6）2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。</p>\n<p>7）2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。<br>8）2006 年 3 月份，Map-Reduce和Nutch Distributed File System （NDFS）分别被纳入到 Hadoop 项目中，Hadoop就此正式诞生，标志着大数据时代来临。</p>\n<p>9）名字来源于Doug Cutting儿子的玩具大象</p>\n</li>\n</ul>\n<h2 id=\"Hadoop-优势（4-高）\"><a href=\"#Hadoop-优势（4-高）\" class=\"headerlink\" title=\"Hadoop 优势（4 高）\"></a>Hadoop 优势（4 高）</h2><ol>\n<li>高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。</li>\n<li>高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。</li>\n<li>高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</li>\n<li>高容错性：能够自动将失败的任务重新分配。</li>\n</ol>\n<h2 id=\"Hadoop-组成（面试重点）-⭐⭐\"><a href=\"#Hadoop-组成（面试重点）-⭐⭐\" class=\"headerlink\" title=\"Hadoop 组成（面试重点） ⭐⭐\"></a>Hadoop 组成（面试重点） ⭐⭐</h2><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Hadoop1.x</th>\n<th>Hadoop2.x</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MapReduce<br>（计算+资源调度）</td>\n<td>MapReduce（计算）</td>\n</tr>\n<tr>\n<td></td>\n<td>Yarn（资源调度）</td>\n</tr>\n<tr>\n<td>HDFS（数据存储）</td>\n<td>HDFS（数据存储）</td>\n</tr>\n<tr>\n<td>Common（辅助工具）</td>\n<td>Common（辅助工具）</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>在 Hadoop1.x 时代 ， Hadoop中的MapReduce同时处理业务逻辑运算和资源的调度，耦合性较大。<br>在Hadoop2.x时代，增加了Yarn。Yarn只负责资 源 的 调 度 ， MapReduce 只负责运算。<br>在Hadoop3.x在组成上没有变化。</p>\n<h2 id=\"HDFS-架构概述\"><a href=\"#HDFS-架构概述\" class=\"headerlink\" title=\"HDFS 架构概述\"></a>HDFS 架构概述</h2><p>Hadoop Distributed File System，简称 HDFS，是一个分布式文件系统。 </p>\n<p><strong>HDFS架构组成</strong><br>1）NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。<br>2）DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。<br>3）Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份。</p>\n<h2 id=\"YARN-架构概述\"><a href=\"#YARN-架构概述\" class=\"headerlink\" title=\"YARN 架构概述\"></a>YARN 架构概述</h2><p>Yet Another Resource Negotiator 简称 YARN ，另一种资源协调者，是 Hadoop 的资源管理器。 </p>\n<p><strong>Yarn架构的组成</strong></p>\n<p>1）    ResourceManager（RM）：整个集群资源（内存、CPU等）的老大<br>2）    NodeManager（NM）：单个节点服务器资源老大<br>3）    ApplicationMaster（AM）：单个任务运行的老大<br>4）    Container：容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络</p>\n<p><img src=\"https://img-blog.csdnimg.cn/0ab47f5876d747f79f2863fb481d9cf4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<h2 id=\"MapReduce-架构概述\"><a href=\"#MapReduce-架构概述\" class=\"headerlink\" title=\"MapReduce 架构概述\"></a>MapReduce 架构概述</h2><p>MapReduce 将计算过程分为两个阶段：Map 和Reduce </p>\n<p>1）Map 阶段并行处理输入数据<br>2）Reduce 阶段对 Map 结果进行汇总</p>\n<h2 id=\"大数据技术生态体系\"><a href=\"#大数据技术生态体系\" class=\"headerlink\" title=\"大数据技术生态体系\"></a>大数据技术生态体系</h2><p><strong>HDFS、YARN、MapReduce三者关系</strong></p>\n<p><img src=\"https://img-blog.csdnimg.cn/aba19ff190574fd1b96e16f2443680a1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p><strong>大数据技术生态体系</strong></p>\n<p><img src=\"https://img-blog.csdnimg.cn/abe4a022d2f140a1bbcce6b7b4339a27.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>图中涉及的技术名词解释如下：<br>1）Sqoop：Sqoop 是一款开源的工具，主要用于在Hadoop、Hive 与传统的数据库（MySQL）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop 的HDFS 中，也可以将HDFS 的数据导进到关系型数据库中。<br>2）Flume：Flume 是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统， Flume 支持在日志系统中定制各类数据发送方，用于收集数据；<br>3）Kafka：Kafka 是一种高吞吐量的分布式发布订阅消息系统；<br>4）Spark：Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数据进行计算。<br>5）Flink：Flink 是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。<br>6）Oozie：Oozie 是一个管理Hadoop 作业（job）的工作流程调度管理系统。<br>7）Hbase：HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。<br>8）Hive：Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开发专门的MapReduce 应用，十分适合数据仓库的统计分析。<br>9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。 </p>\n<p><strong>推荐系统框架图</strong> </p>\n<p><img src=\"https://img-blog.csdnimg.cn/4ef994a3870d4d4a9738160ac460b9f8.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<h1 id=\"2-Hadoop-运行环境搭建（开发重点）\"><a href=\"#2-Hadoop-运行环境搭建（开发重点）\" class=\"headerlink\" title=\"2. Hadoop 运行环境搭建（开发重点）\"></a>2. Hadoop 运行环境搭建（开发重点）</h1><h2 id=\"基础环境准备\"><a href=\"#基础环境准备\" class=\"headerlink\" title=\"基础环境准备\"></a>基础环境准备</h2><ol>\n<li>安装VMware、xshell</li>\n<li>配置静态IP、hostname、hosts</li>\n<li>创建用户，设置权限，克隆环境</li>\n<li>安装JDK、安装Hadoop</li>\n</ol>\n<h2 id=\"Hadoop-运行模式\"><a href=\"#Hadoop-运行模式\" class=\"headerlink\" title=\"Hadoop 运行模式\"></a>Hadoop 运行模式</h2><h3 id=\"本地运行模式\"><a href=\"#本地运行模式\" class=\"headerlink\" title=\"本地运行模式\"></a>本地运行模式</h3><p>啥都不用配置，直接运行即可</p>\n<h3 id=\"完全分布式运行模式（开发重点）⭐⭐⭐\"><a href=\"#完全分布式运行模式（开发重点）⭐⭐⭐\" class=\"headerlink\" title=\"完全分布式运行模式（开发重点）⭐⭐⭐\"></a>完全分布式运行模式（开发重点）⭐⭐⭐</h3><ol>\n<li>配置台虚拟机</li>\n<li>设置免密通信、shell脚本分发安装的JDK、Hadoop环境</li>\n<li>配置集群、历史服务器、日志服务器</li>\n<li>shell控制集群启停和jps情况</li>\n</ol>\n<p><strong>集群部署规划：</strong></p>\n<ul>\n<li>NameNode和 SecondaryNameNode不要安装在同一台服务器</li>\n<li>ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在 配置在 同一台机器上。</li>\n</ul>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>hadoop102</th>\n<th>hadoop103</th>\n<th>hadoop104</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>HDFS</td>\n<td>NameNode<br>DataNode</td>\n<td>DataNode</td>\n<td>SecondaryNameNode<br>DataNode</td>\n</tr>\n<tr>\n<td>YARN</td>\n<td>NodeManager</td>\n<td>ResourceManager<br>NodeManager</td>\n<td>NodeManager</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"常用端口号说明⭐⭐\"><a href=\"#常用端口号说明⭐⭐\" class=\"headerlink\" title=\"常用端口号说明⭐⭐\"></a>常用端口号说明⭐⭐</h2><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>端口名称</th>\n<th>Hadoop2.x</th>\n<th>Hadoop3.x</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NameNode内部通信端口</td>\n<td>8020 / 9000</td>\n<td>8020 / 9000/9820</td>\n</tr>\n<tr>\n<td>NameNode HTTP UI</td>\n<td>50070</td>\n<td>9870</td>\n</tr>\n<tr>\n<td>MapReduce查看执行任务端口</td>\n<td>8088</td>\n<td>8088</td>\n</tr>\n<tr>\n<td>历史服务器通信端口</td>\n<td>19888</td>\n<td>19888</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>web</th>\n<th>服务</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>HDFS</td>\n<td>9870</td>\n<td>8020</td>\n</tr>\n<tr>\n<td>YARN</td>\n<td>8088</td>\n<td>8032</td>\n</tr>\n<tr>\n<td>history</td>\n<td>19888</td>\n<td>10020</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"常用配置文件⭐⭐\"><a href=\"#常用配置文件⭐⭐\" class=\"headerlink\" title=\"常用配置文件⭐⭐\"></a>常用配置文件⭐⭐</h2><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>配置文件</th>\n<th>2.x/3.x</th>\n<th>配置内容</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>核心配置文件</td>\n<td>core-site.xml</td>\n<td>NameNode的地址<br>指定hadoop数据的存储目录</td>\n</tr>\n<tr>\n<td>HDFS配置文件</td>\n<td>hdfs-site.xml</td>\n<td>nn web端访问地址<br>2nn web端访问地址</td>\n</tr>\n<tr>\n<td>YARN配置文件</td>\n<td>yarn-site.xml</td>\n<td>指定MR走shuffle<br>ResourceManager的地址<br>环境变量的继承</td>\n</tr>\n<tr>\n<td>MapReduce配置文件</td>\n<td>mapred-site.xml</td>\n<td>指定MapReduce程序运行在Yarn上</td>\n</tr>\n<tr>\n<td></td>\n<td>slaves/workers</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>配置 core-site.xml</strong></p>\n<pre class=\"line-numbers language-xml\" data-language=\"xml\"><code class=\"language-xml\"><span class=\"token prolog\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;</span>\n<span class=\"token prolog\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span>\n\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>configuration</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token comment\">&lt;!-- 指定NameNode的地址 --&gt;</span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>fs.defaultFS<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>hdfs://hadoop102:8020<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n    \n    <span class=\"token comment\">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>hadoop.tmp.dir<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>/opt/module/hadoop-3.1.3/data<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n\n    <span class=\"token comment\">&lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;</span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>hadoop.http.staticuser.user<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>atguigu<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>configuration</span><span class=\"token punctuation\">&gt;</span></span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>配置 hdfs-site.xml</strong></p>\n<pre class=\"line-numbers language-xml\" data-language=\"xml\"><code class=\"language-xml\"><span class=\"token prolog\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;</span>\n<span class=\"token prolog\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span>\n\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>configuration</span><span class=\"token punctuation\">&gt;</span></span>\n\t<span class=\"token comment\">&lt;!-- nn web端访问地址--&gt;</span>\n\t<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>dfs.namenode.http-address<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>hadoop102:9870<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n\t<span class=\"token comment\">&lt;!-- 2nn web端访问地址--&gt;</span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>dfs.namenode.secondary.http-address<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>hadoop104:9868<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>configuration</span><span class=\"token punctuation\">&gt;</span></span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>配置 yarn-site.xml</strong></p>\n<pre class=\"line-numbers language-xml\" data-language=\"xml\"><code class=\"language-xml\"><span class=\"token prolog\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;</span>\n<span class=\"token prolog\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>configuration</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token comment\">&lt;!-- 指定MR走shuffle --&gt;</span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>yarn.nodemanager.aux-services<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>mapreduce_shuffle<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n\n    <span class=\"token comment\">&lt;!-- 指定ResourceManager的地址--&gt;</span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>yarn.resourcemanager.hostname<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>hadoop103<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n\n    <span class=\"token comment\">&lt;!-- 环境变量的继承 --&gt;</span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>yarn.nodemanager.env-whitelist<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>configuration</span><span class=\"token punctuation\">&gt;</span></span>\n\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>配置 mapred-site.xml</strong></p>\n<pre class=\"line-numbers language-xml\" data-language=\"xml\"><code class=\"language-xml\"><span class=\"token prolog\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;</span>\n<span class=\"token prolog\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span>\n\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>configuration</span><span class=\"token punctuation\">&gt;</span></span>\n\t<span class=\"token comment\">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>mapreduce.framework.name<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n        <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>yarn<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>configuration</span><span class=\"token punctuation\">&gt;</span></span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>配置历史服务器：配置 mapred-site.xml</strong></p>\n<pre class=\"line-numbers language-xml\" data-language=\"xml\"><code class=\"language-xml\"><span class=\"token comment\">&lt;!-- 历史服务器端地址 --&gt;</span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>mapreduce.jobhistory.address<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>hadoop102:10020<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n\n<span class=\"token comment\">&lt;!-- 历史服务器web端地址 --&gt;</span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>mapreduce.jobhistory.webapp.address<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>hadoop102:19888<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>配置日志的聚集：配置 yarn-site.xml</strong></p>\n<pre class=\"line-numbers language-xml\" data-language=\"xml\"><code class=\"language-xml\"><span class=\"token comment\">&lt;!-- 开启日志聚集功能 --&gt;</span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>yarn.log-aggregation-enable<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>true<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token comment\">&lt;!-- 设置日志聚集服务器地址 --&gt;</span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>  \n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>yarn.log.server.url<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>  \n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>http://hadoop102:19888/jobhistory/logs<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token comment\">&lt;!-- 设置日志保留时间为7天 --&gt;</span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>yarn.log-aggregation.retain-seconds<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>604800<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"3-HDFS\"><a href=\"#3-HDFS\" class=\"headerlink\" title=\"3. HDFS\"></a>3. HDFS</h1><h2 id=\"HDFS概述\"><a href=\"#HDFS概述\" class=\"headerlink\" title=\"HDFS概述\"></a>HDFS概述</h2><p>HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p>\n<p>HDFS的使用场景：适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变。</p>\n<h3 id=\"HDFS优缺点\"><a href=\"#HDFS优缺点\" class=\"headerlink\" title=\"HDFS优缺点\"></a>HDFS优缺点</h3><p><strong>HDFS优点</strong></p>\n<p>1）    高容错性<br>➢    数据自动保存多个副本。它通过增加副本的形式，提高容错性。</p>\n<p>➢    某一个副本丢失以后，它可以自动恢复。</p>\n<p>2）    适合处理大数据<br>➢    数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据；<br>➢    文件规模：能够处理百万规模以上的文件数量，数量相当之大。<br>3）    可构建在廉价机器上，通过多副本机制，提高可靠性。</p>\n<p><strong>HDFS缺点</strong></p>\n<p>1）    不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。<br>2）    无法高效的对大量小文件进行存储。<br>➢    存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；<br>➢    小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。</p>\n<p>3）    不支持并发写入、文件随机修改。<br>➢    一个文件只能有一个写，不允许多个线程同时写；</p>\n<p>➢    仅支持数据append（追加），不支持文件的随机修改。</p>\n<h3 id=\"HDFS组成架构\"><a href=\"#HDFS组成架构\" class=\"headerlink\" title=\"HDFS组成架构\"></a>HDFS组成架构</h3><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>HDFS组成架构</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NameNode（nn）</td>\n<td>就是Master，它是一个主管、管理者。</td>\n</tr>\n<tr>\n<td></td>\n<td>（1）    管理HDFS的名称空间；<br>（2）    配置副本策略；<br>（3）    管理数据块（Block）映射信息；<br>（4）    处理客户端读写请求。</td>\n</tr>\n<tr>\n<td>DataNode</td>\n<td>就是Slave。NameNode下达命令，DataNode执行实际的操作。</td>\n</tr>\n<tr>\n<td></td>\n<td>（1）    存储实际的数据块；<br>（2）    执行数据块的读/写操作。</td>\n</tr>\n<tr>\n<td>Client</td>\n<td>就是客户端。</td>\n</tr>\n<tr>\n<td></td>\n<td>（1）    文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传；<br>（2）    与NameNode交互，获取文件的位置信息；<br>（3）    与DataNode交互，读取或者写入数据；<br>（4）    Client提供一些命令来管理HDFS，比如NameNode格式化；</td>\n</tr>\n<tr>\n<td>Secondary NameNode</td>\n<td>并非NameNode的热备份，当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</td>\n</tr>\n<tr>\n<td></td>\n<td>（1）    辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode ；<br>（2）    在紧急情况下，可辅助恢复NameNode。</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"HDFS-文件块大小（面试重点）-⭐⭐\"><a href=\"#HDFS-文件块大小（面试重点）-⭐⭐\" class=\"headerlink\" title=\"HDFS 文件块大小（面试重点） ⭐⭐\"></a>HDFS 文件块大小（面试重点） ⭐⭐</h3><p>HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数<br>( dfs.blocksize）来规定，默认大小在Hadoop2.x/3.x版本中是128M，1.x版本中是64M。</p>\n<p>1）集群中的block</p>\n<p>2）如果寻址时间约为10ms，即查找到目标block的时间为10ms。<br>3）寻址时间为传输时间的1%时，则为最佳状态。（专家）因此，传输时间=10ms/0.01=1000ms=1s<br>4）而目前磁盘的传输速率普遍为100MB/s。</p>\n<p>5） block大小<br>=1s*100MB/s=100MB</p>\n<p><strong>思考：为什么块的大小不能设置太小，也不能设置太大？</strong></p>\n<p>（1）    HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；<br>（2）    如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</p>\n<p>总结：HDFS块的大小设置主要取决于==磁盘传输速率==。</p>\n<h2 id=\"HDFS-的Shell-操作\"><a href=\"#HDFS-的Shell-操作\" class=\"headerlink\" title=\"HDFS 的Shell 操作\"></a>HDFS 的Shell 操作</h2><p>hadoop fs 具体命令  OR  hdfs dfs 具体命令 两个是完全相同的。 </p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">// 命令大全\n[-cat [-ignoreCrc] &lt;src&gt; ...]\n[-chgrp [-R] GROUP PATH...]\n[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]\n[-chown [-R] [OWNER][:[GROUP]] PATH...]\n[-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]\n[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]\n[-count [-q] &lt;path&gt; ...]\n[-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;]\n[-df [-h] [&lt;path&gt; ...]]\n[-du [-s] [-h] &lt;path&gt; ...]\n[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]\n[-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]\n[-help [cmd ...]]\n[-ls [-d] [-h] [-R] [&lt;path&gt; ...]]\n[-mkdir [-p] &lt;path&gt; ...]\n[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]\n[-moveToLocal &lt;src&gt; &lt;localdst&gt;]\n[-mv &lt;src&gt; ... &lt;dst&gt;]\n[-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]\n[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]\n[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]&lt;acl_spec&gt; &lt;path&gt;]]\n[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]\n[-stat [format] &lt;path&gt; ...]\n[-tail [-f] &lt;file&gt;]\n[-test -[defsz] &lt;path&gt;]\n[-text [-ignoreCrc] &lt;src&gt; ...]<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>常用命令实操</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>sbin/start-dfs.sh<br>sbin/start-yarn.sh</td>\n<td>启动Hadoop集群</td>\n</tr>\n<tr>\n<td>hadoop fs -help rm</td>\n<td>输出这个命令参数</td>\n</tr>\n<tr>\n<td>hadoop fs -mkdir /a</td>\n<td>创建/sanguo文件夹</td>\n</tr>\n<tr>\n<td>hadoop fs -moveFromLocal ./a.txt /a</td>\n<td>从本地剪切粘贴到HDFS</td>\n</tr>\n<tr>\n<td>hadoop fs -copyFromLocal a.txt /a</td>\n<td>从本地文件系统中拷贝文件到HDFS路径去</td>\n</tr>\n<tr>\n<td>hadoop fs -put ./a.txt /sangauo</td>\n<td>等同于copyFromLocal，生产环境更习惯用put</td>\n</tr>\n<tr>\n<td>hadoop fs -appendToFile a.txt /a/a.txt</td>\n<td>追加一个文件到已经存在的文件末尾</td>\n</tr>\n<tr>\n<td>hadoop fs -copyToLocal /a/a.txt ./</td>\n<td>从HDFS拷贝到本地</td>\n</tr>\n<tr>\n<td>hadoop fs -get /a/a.txt ./a.txt</td>\n<td>等同于copyToLocal，生产环境更习惯用get</td>\n</tr>\n<tr>\n<td>hadoop fs -ls /a</td>\n<td>显示目录信息</td>\n</tr>\n<tr>\n<td>hadoop fs -cat /a/a.txt</td>\n<td>显示文件内容</td>\n</tr>\n<tr>\n<td>hadoop fs -chmod 666</td>\n<td>-chgrp、-chmod、-chown：</td>\n</tr>\n<tr>\n<td>hadoop fs -chown yfr:yfr  /a/a.txt</td>\n<td>Linux文件系统中的用法一样，修改文件所属权限</td>\n</tr>\n<tr>\n<td>hadoop fs -mkdir /a</td>\n<td>创建路径</td>\n</tr>\n<tr>\n<td>hadoop fs -cp /a/a.txt /b</td>\n<td>从HDFS的一个路径拷贝到HDFS的另一个路径</td>\n</tr>\n<tr>\n<td>hadoop fs -mv /a/a.txt /b</td>\n<td>在HDFS目录中移动文件</td>\n</tr>\n<tr>\n<td>hadoop fs -tail /a/a.txt</td>\n<td>显示一个文件的末尾1kb的数据</td>\n</tr>\n<tr>\n<td>hadoop fs -rm /a/a.txt</td>\n<td>删除文件或文件夹</td>\n</tr>\n<tr>\n<td>hadoop fs -rm -r /a</td>\n<td>删除文件或文件夹</td>\n</tr>\n<tr>\n<td>hadoop fs -du -s -h /a</td>\n<td>递归删除目录及目录里面内容</td>\n</tr>\n<tr>\n<td>hadoop fs -du -h /a</td>\n<td>统计文件夹的大小信息</td>\n</tr>\n<tr>\n<td>hadoop fs -setrep 10 /a/a.txt</td>\n<td>设置HDFS中文件的副本数量</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"HDFS的API操作\"><a href=\"#HDFS的API操作\" class=\"headerlink\" title=\"HDFS的API操作\"></a>HDFS的API操作</h2><pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token comment\">//HDFS文件上传</span>\n<span class=\"token annotation punctuation\">@Test</span>\n<span class=\"token keyword\">public</span> <span class=\"token keyword\">void</span> <span class=\"token function\">testCopyFromLocalFile</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">throws</span> <span class=\"token class-name\">IOException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">InterruptedException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">URISyntaxException</span> <span class=\"token punctuation\">{</span>\n\n    <span class=\"token comment\">// 1 获取文件系统</span>\n    <span class=\"token class-name\">Configuration</span> configuration <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Configuration</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    configuration<span class=\"token punctuation\">.</span><span class=\"token function\">set</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"dfs.replication\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"2\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token class-name\">FileSystem</span> fs <span class=\"token operator\">=</span> <span class=\"token class-name\">FileSystem</span><span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token function\">URI</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"hdfs://hadoop102:8020\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> configuration<span class=\"token punctuation\">,</span> <span class=\"token string\">\"atguigu\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// 2 上传文件</span>\n    fs<span class=\"token punctuation\">.</span><span class=\"token function\">copyFromLocalFile</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token class-name\">Path</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"d:/sunwukong.txt\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Path</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"/xiyou/huaguoshan\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// 3 关闭资源</span>\n    fs<span class=\"token punctuation\">.</span><span class=\"token function\">close</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n｝<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token comment\">//HDFS文件下载</span>\n<span class=\"token annotation punctuation\">@Test</span>\n<span class=\"token keyword\">public</span> <span class=\"token keyword\">void</span> <span class=\"token function\">testCopyToLocalFile</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">throws</span> <span class=\"token class-name\">IOException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">InterruptedException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">URISyntaxException</span><span class=\"token punctuation\">{</span>\n\n    <span class=\"token comment\">// 1 获取文件系统</span>\n    <span class=\"token class-name\">Configuration</span> configuration <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Configuration</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token class-name\">FileSystem</span> fs <span class=\"token operator\">=</span> <span class=\"token class-name\">FileSystem</span><span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token function\">URI</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"hdfs://hadoop102:8020\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> configuration<span class=\"token punctuation\">,</span> <span class=\"token string\">\"atguigu\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    \n    <span class=\"token comment\">// 2 执行下载操作</span>\n    <span class=\"token comment\">// boolean delSrc 指是否将原文件删除</span>\n    <span class=\"token comment\">// Path src 指要下载的文件路径</span>\n    <span class=\"token comment\">// Path dst 指将文件下载到的路径</span>\n    <span class=\"token comment\">// boolean useRawLocalFileSystem 是否开启文件校验</span>\n    fs<span class=\"token punctuation\">.</span><span class=\"token function\">copyToLocalFile</span><span class=\"token punctuation\">(</span><span class=\"token boolean\">false</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Path</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"/xiyou/huaguoshan/sunwukong.txt\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Path</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"d:/sunwukong2.txt\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    \n    <span class=\"token comment\">// 3 关闭资源</span>\n    fs<span class=\"token punctuation\">.</span><span class=\"token function\">close</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token comment\">//HDFS更名和移动</span>\n<span class=\"token annotation punctuation\">@Test</span>\n<span class=\"token keyword\">public</span> <span class=\"token keyword\">void</span> <span class=\"token function\">testRename</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">throws</span> <span class=\"token class-name\">IOException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">InterruptedException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">URISyntaxException</span><span class=\"token punctuation\">{</span>\n\n\t<span class=\"token comment\">// 1 获取文件系统</span>\n\t<span class=\"token class-name\">Configuration</span> configuration <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Configuration</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t<span class=\"token class-name\">FileSystem</span> fs <span class=\"token operator\">=</span> <span class=\"token class-name\">FileSystem</span><span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token function\">URI</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"hdfs://hadoop102:8020\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> configuration<span class=\"token punctuation\">,</span> <span class=\"token string\">\"atguigu\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> \n\t\t\n\t<span class=\"token comment\">// 2 修改文件名称</span>\n\tfs<span class=\"token punctuation\">.</span><span class=\"token function\">rename</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token class-name\">Path</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"/xiyou/huaguoshan/sunwukong.txt\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Path</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"/xiyou/huaguoshan/meihouwang.txt\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t\n\t<span class=\"token comment\">// 3 关闭资源</span>\n\tfs<span class=\"token punctuation\">.</span><span class=\"token function\">close</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token comment\">//HDFS删除文件和目录</span>\n<span class=\"token annotation punctuation\">@Test</span>\n<span class=\"token keyword\">public</span> <span class=\"token keyword\">void</span> <span class=\"token function\">testDelete</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">throws</span> <span class=\"token class-name\">IOException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">InterruptedException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">URISyntaxException</span><span class=\"token punctuation\">{</span>\n\n\t<span class=\"token comment\">// 1 获取文件系统</span>\n\t<span class=\"token class-name\">Configuration</span> configuration <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Configuration</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t<span class=\"token class-name\">FileSystem</span> fs <span class=\"token operator\">=</span> <span class=\"token class-name\">FileSystem</span><span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token function\">URI</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"hdfs://hadoop102:8020\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> configuration<span class=\"token punctuation\">,</span> <span class=\"token string\">\"atguigu\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t\n\t<span class=\"token comment\">// 2 执行删除</span>\n\tfs<span class=\"token punctuation\">.</span><span class=\"token function\">delete</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token class-name\">Path</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"/xiyou\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t\n\t<span class=\"token comment\">// 3 关闭资源</span>\n\tfs<span class=\"token punctuation\">.</span><span class=\"token function\">close</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token comment\">//查看文件名称、权限、长度、块信息</span>\n<span class=\"token annotation punctuation\">@Test</span>\n<span class=\"token keyword\">public</span> <span class=\"token keyword\">void</span> <span class=\"token function\">testListFiles</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">throws</span> <span class=\"token class-name\">IOException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">InterruptedException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">URISyntaxException</span> <span class=\"token punctuation\">{</span>\n\n\t<span class=\"token comment\">// 1获取文件系统</span>\n\t<span class=\"token class-name\">Configuration</span> configuration <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Configuration</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t<span class=\"token class-name\">FileSystem</span> fs <span class=\"token operator\">=</span> <span class=\"token class-name\">FileSystem</span><span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token function\">URI</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"hdfs://hadoop102:8020\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> configuration<span class=\"token punctuation\">,</span> <span class=\"token string\">\"atguigu\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n\t<span class=\"token comment\">// 2 获取文件详情</span>\n\t<span class=\"token class-name\">RemoteIterator</span><span class=\"token generics\"><span class=\"token punctuation\">&lt;</span><span class=\"token class-name\">LocatedFileStatus</span><span class=\"token punctuation\">&gt;</span></span> listFiles <span class=\"token operator\">=</span> fs<span class=\"token punctuation\">.</span><span class=\"token function\">listFiles</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token class-name\">Path</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"/\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n\t<span class=\"token keyword\">while</span> <span class=\"token punctuation\">(</span>listFiles<span class=\"token punctuation\">.</span><span class=\"token function\">hasNext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n\t\t<span class=\"token class-name\">LocatedFileStatus</span> fileStatus <span class=\"token operator\">=</span> listFiles<span class=\"token punctuation\">.</span><span class=\"token function\">next</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n\t\t<span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"========\"</span> <span class=\"token operator\">+</span> fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getPath</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token string\">\"=========\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t<span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getPermission</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t<span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getOwner</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t<span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getGroup</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t<span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getLen</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t<span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getModificationTime</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t<span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getReplication</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t<span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getBlockSize</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t<span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getPath</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token function\">getName</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n\t\t<span class=\"token comment\">// 获取块信息</span>\n\t\t<span class=\"token class-name\">BlockLocation</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span> blockLocations <span class=\"token operator\">=</span> fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getBlockLocations</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t\t<span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">Arrays</span><span class=\"token punctuation\">.</span><span class=\"token function\">toString</span><span class=\"token punctuation\">(</span>blockLocations<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\t<span class=\"token punctuation\">}</span>\n\t<span class=\"token comment\">// 3 关闭资源</span>\n\tfs<span class=\"token punctuation\">.</span><span class=\"token function\">close</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token comment\">//HDFS文件和文件夹判断</span>\n<span class=\"token annotation punctuation\">@Test</span>\n<span class=\"token keyword\">public</span> <span class=\"token keyword\">void</span> <span class=\"token function\">testListStatus</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">throws</span> <span class=\"token class-name\">IOException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">InterruptedException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">URISyntaxException</span><span class=\"token punctuation\">{</span>\n\n    <span class=\"token comment\">// 1 获取文件配置信息</span>\n    <span class=\"token class-name\">Configuration</span> configuration <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Configuration</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token class-name\">FileSystem</span> fs <span class=\"token operator\">=</span> <span class=\"token class-name\">FileSystem</span><span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token function\">URI</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"hdfs://hadoop102:8020\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> configuration<span class=\"token punctuation\">,</span> <span class=\"token string\">\"atguigu\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// 2 判断是文件还是文件夹</span>\n    <span class=\"token class-name\">FileStatus</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span> listStatus <span class=\"token operator\">=</span> fs<span class=\"token punctuation\">.</span><span class=\"token function\">listStatus</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> <span class=\"token class-name\">Path</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"/\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">FileStatus</span> fileStatus <span class=\"token operator\">:</span> listStatus<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n\n        <span class=\"token comment\">// 如果是文件</span>\n        <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">isFile</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n            <span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"f:\"</span><span class=\"token operator\">+</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getPath</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token function\">getName</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n        <span class=\"token punctuation\">}</span><span class=\"token keyword\">else</span> <span class=\"token punctuation\">{</span>\n            <span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">println</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"d:\"</span><span class=\"token operator\">+</span>fileStatus<span class=\"token punctuation\">.</span><span class=\"token function\">getPath</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token function\">getName</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n        <span class=\"token punctuation\">}</span>\n    <span class=\"token punctuation\">}</span>\n\n    <span class=\"token comment\">// 3 关闭资源</span>\n    fs<span class=\"token punctuation\">.</span><span class=\"token function\">close</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"HDFS的读写流程（面试重点）⭐⭐\"><a href=\"#HDFS的读写流程（面试重点）⭐⭐\" class=\"headerlink\" title=\"HDFS的读写流程（面试重点）⭐⭐\"></a>HDFS的读写流程（面试重点）⭐⭐</h2><h3 id=\"HDFS写数据流程\"><a href=\"#HDFS写数据流程\" class=\"headerlink\" title=\"HDFS写数据流程\"></a>HDFS写数据流程</h3><p><img src=\"https://img-blog.csdnimg.cn/c93113474e434943a81881479f4165b1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>（1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。<br>（2）NameNode返回是否可以上传。<br>（3）客户端请求第一个 Block上传到哪几个DataNode服务器上。<br>（4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。<br>（5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。<br>（6）dn1、dn2、dn3逐级应答客户端。<br>（7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。<br>（8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</p>\n<h3 id=\"网络拓扑-节点距离计算\"><a href=\"#网络拓扑-节点距离计算\" class=\"headerlink\" title=\"网络拓扑-节点距离计算\"></a>网络拓扑-节点距离计算</h3><p>节点距离：两个节点到达最近的共同祖先的距离总和。 </p>\n<p>例如，假设有数据中心 d1 机架 r1 中的节点 n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。 </p>\n<p>Distance(/d1/r1/n0, /d1/r1/n0)=0（同一节点上的进程）<br>Distance(/d1/r1/n1, /d1/r1/n2)=2（同一机架上的不同节点）<br>Distance(/d1/r2/n0, /d1/r3/n2)=4（同一数据中心不同机架上的节点）<br>Distance(/d1/r2/n1, /d2/r4/n1)=6（不同数据中心的节点）</p>\n<h3 id=\"Hadoop3-1-3副本节点选择\"><a href=\"#Hadoop3-1-3副本节点选择\" class=\"headerlink\" title=\"Hadoop3.1.3副本节点选择\"></a>Hadoop3.1.3副本节点选择</h3><p>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。<br>第二个副本在另一个机架的随机一个节点<br>第三个副本在第二个副本所在机架的随机节点</p>\n<h3 id=\"HDFS的读数据流程\"><a href=\"#HDFS的读数据流程\" class=\"headerlink\" title=\"HDFS的读数据流程\"></a>HDFS的读数据流程</h3><p><img src=\"https://img-blog.csdnimg.cn/2c70058194ec4d9ba5fd11a9c2562f91.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>（1）客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。<br>（2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。<br>（3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。<br>（4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</p>\n<h2 id=\"NameNode和SecondaryNameNode\"><a href=\"#NameNode和SecondaryNameNode\" class=\"headerlink\" title=\"NameNode和SecondaryNameNode\"></a>NameNode和SecondaryNameNode</h2><p><strong>思考：NameNode中的元数据是存储在哪里的？</strong><br>        首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，==元数据需要存放在内存中==。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在==磁盘中备份元数据的FsImage==。<br>        这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件（只进行追加操作，效率很高）。==每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中==。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。<br>        但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要==定期进行FsImage和Edits的合并==，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点==SecondaryNamenode，专门用于FsImage和Edits的合并==。</p>\n<h3 id=\"NN和2NN工作机制\"><a href=\"#NN和2NN工作机制\" class=\"headerlink\" title=\"NN和2NN工作机制\"></a>NN和2NN工作机制</h3><p><img src=\"https://img-blog.csdnimg.cn/298d61e4ce994a79895396fc96ee18a6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>1）第一阶段：NameNode启动<br>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>（2）客户端对元数据进行增删改的请求。<br>（3）NameNode记录操作日志，更新滚动日志。<br>（4）NameNode在内存中对元数据进行增删改。</p>\n<p>2）第二阶段：Secondary NameNode工作<br>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。<br>（2）Secondary NameNode请求执行CheckPoint。<br>（3）NameNode滚动正在写的Edits日志。<br>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。<br>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。<br>（6）生成新的镜像文件fsimage.chkpoint。<br>（7）拷贝fsimage.chkpoint到NameNode。<br>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p>\n<p><strong>Fsimage和Edits概念</strong></p>\n<p>（1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。<br>（2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。<br>（3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字<br>（4）每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。</p>\n<p><strong>查看oiv 和oev 命令</strong></p>\n<p>查看fsimage：hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径 </p>\n<p>查看edits：hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</p>\n<h2 id=\"DataNode\"><a href=\"#DataNode\" class=\"headerlink\" title=\"DataNode\"></a>DataNode</h2><h3 id=\"DataNode-工作机制\"><a href=\"#DataNode-工作机制\" class=\"headerlink\" title=\"DataNode 工作机制\"></a>DataNode 工作机制</h3><p><img src=\"https://img-blog.csdnimg.cn/3e672ef6e14b42569128e1d1a66f390c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>（1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。<br>（2）DataNode启动后向NameNode注册，通过后，周期性（6小时）的向NameNode上报所有的块信息。</p>\n<ul>\n<li>DN向NN汇报当前解读信息的时间间隔，默认6小时；</li>\n<li>DN扫描自己节点块信息列表的时间，默认6小时</li>\n</ul>\n<p>（3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。<br>（4）集群运行中可以安全加入和退出一些机器。</p>\n<h3 id=\"DataNode节点保证数据完整性的方法：\"><a href=\"#DataNode节点保证数据完整性的方法：\" class=\"headerlink\" title=\"DataNode节点保证数据完整性的方法：\"></a><strong>DataNode节点保证数据完整性的方法：</strong></h3><p>（1）当DataNode读取Block的时候，它会计算CheckSum。<br>（2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。<br>（3）Client读取其他DataNode上的Block。<br>（4）常见的校验算法crc（32），md5（128），sha1（160）<br>（5）DataNode在其文件创建后周期验证CheckSum。</p>\n<h3 id=\"DataNode掉线时限参数设置\"><a href=\"#DataNode掉线时限参数设置\" class=\"headerlink\" title=\"DataNode掉线时限参数设置\"></a>DataNode掉线时限参数设置</h3><p>1、DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信<br>2、NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。<br>3、HDFS默认的超时时长为10分钟+30秒。<br>4、如果定义超时时间为TimeOut，则超时时长的计算公式为：<br>TimeOut = 2 <em> dfs.namenode.heartbeat.recheck-interval + 10 </em> dfs.heartbeat.interval。<br>而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。</p>\n<h1 id=\"4-MapReduce\"><a href=\"#4-MapReduce\" class=\"headerlink\" title=\"4. MapReduce\"></a>4. MapReduce</h1><h2 id=\"MapReduce概述\"><a href=\"#MapReduce概述\" class=\"headerlink\" title=\"MapReduce概述\"></a>MapReduce概述</h2><h3 id=\"MapReduce定义\"><a href=\"#MapReduce定义\" class=\"headerlink\" title=\"MapReduce定义\"></a><strong>MapReduce定义</strong></h3><p>​        MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。</p>\n<p>​        MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。</p>\n<h3 id=\"优缺点\"><a href=\"#优缺点\" class=\"headerlink\" title=\"优缺点\"></a><strong>优缺点</strong></h3><p> <strong>优点</strong><br><strong>1）MapReduce易于编程</strong><br>        它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。<br><strong>2）良好的扩展性</strong><br>        当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。<br><strong>3）高容错性</strong><br>        MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。<br><strong>4）适合PB级以上海量数据的离线处理</strong><br>        可以实现上千台服务器集群并发工作，提供数据处理能力。<br><strong>缺点</strong><br><strong>1）不擅长实时计算</strong><br>        MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。<br><strong>2）不擅长流式计算</strong><br>        流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。<br><strong>3）不擅长DAG（有向无环图）计算</strong><br>        多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。</p>\n<h3 id=\"MapReduce核心思想\"><a href=\"#MapReduce核心思想\" class=\"headerlink\" title=\"MapReduce核心思想\"></a>MapReduce核心思想</h3><p><img src=\"https://img-blog.csdnimg.cn/fb559e50aeec443c8f90d051de27884e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>（1）分布式的运算程序往往需要分成至少2个阶段。<br>（2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。<br>（3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。<br>（4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。<br>总结：分析WordCount数据流走向深入理解MapReduce核心思想。</p>\n<h3 id=\"MapReduce进程\"><a href=\"#MapReduce进程\" class=\"headerlink\" title=\"MapReduce进程\"></a>MapReduce进程</h3><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：<br>（1）MrAppMaster：负责整个程序的过程调度及状态协调。<br>（2）MapTask：负责Map阶段的整个数据处理流程。<br>（3）ReduceTask：负责Reduce阶段的整个数据处理流程。</p>\n<h3 id=\"常用数据序列化类型\"><a href=\"#常用数据序列化类型\" class=\"headerlink\" title=\"常用数据序列化类型\"></a>常用数据序列化类型</h3><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Java类型</th>\n<th>Hadoop Writable类型</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Boolean</td>\n<td>BooleanWritable</td>\n</tr>\n<tr>\n<td>Byte</td>\n<td>ByteWritable</td>\n</tr>\n<tr>\n<td>Int</td>\n<td>IntWritable</td>\n</tr>\n<tr>\n<td>Float</td>\n<td>FloatWritable</td>\n</tr>\n<tr>\n<td>Long</td>\n<td>LongWritable</td>\n</tr>\n<tr>\n<td>Double</td>\n<td>DoubleWritable</td>\n</tr>\n<tr>\n<td>String</td>\n<td>Text</td>\n</tr>\n<tr>\n<td>Map</td>\n<td>MapWritable</td>\n</tr>\n<tr>\n<td>Array</td>\n<td>ArrayWritable</td>\n</tr>\n<tr>\n<td>Null</td>\n<td>NullWritable</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"MapReduce编程规范\"><a href=\"#MapReduce编程规范\" class=\"headerlink\" title=\"MapReduce编程规范\"></a>MapReduce编程规范</h3><ol>\n<li><p>Mapper阶段<br>(1)用户自定义的Mapper要继承自己的父类<br>(2) Mapper的输入数据是KV对的形式(KV的类型可自定义)<br>(3) Mapper中的业务逻辑写在map()方法中<br>(4) Mapper的输 出数据是KV对的形式(KV的类型可自定义)<br>(5) map()方 法(MapTask进程) 对每一个 <k,v>调用- -次</k,v></p>\n</li>\n<li><p>Reducer阶 段<br>(1) 用户自定义的Reducer要继承自己的父类<br>(2) Reducer的输入数据类型对应Mapper的输出数据类型，也是KV<br>(3) Reducer的业 务逻辑写在reduce()方法中<br>(4) ReduceTask进程对每- -组相同k的<k,v>组调用- -次reduce()方法</k,v></p>\n</li>\n<li>Driver阶段<br>相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是<br>封装了MapReduce程序相关运行参数的job对象</li>\n</ol>\n<h2 id=\"Hadoop序列化\"><a href=\"#Hadoop序列化\" class=\"headerlink\" title=\"Hadoop序列化\"></a>Hadoop序列化</h2><p><strong>序列化</strong>就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 </p>\n<p><strong>反序列化</strong>就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。</p>\n<p><strong>为什么要序列化?</strong><br>        一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。<br><strong>为什么不用Java的序列化?</strong><br>        Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）。<br><strong>Hadoop序列化特点：</strong><br>（1）紧凑 ：高效使用存储空间。<br>（2）快速：读写数据的额外开销小。<br>（3）互操作：支持多语言的交互</p>\n<h3 id=\"自定义bean对象实现序列化接口（Writable）\"><a href=\"#自定义bean对象实现序列化接口（Writable）\" class=\"headerlink\" title=\"自定义bean对象实现序列化接口（Writable）\"></a>自定义bean对象实现序列化接口（Writable）</h3><p>（1）必须实现Writable接口<br>（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token keyword\">public</span> <span class=\"token class-name\">FlowBean</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n\t<span class=\"token keyword\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<p>（3）重写序列化方法</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token annotation punctuation\">@Override</span>\n<span class=\"token keyword\">public</span> <span class=\"token keyword\">void</span> <span class=\"token function\">write</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">DataOutput</span> out<span class=\"token punctuation\">)</span> <span class=\"token keyword\">throws</span> <span class=\"token class-name\">IOException</span> <span class=\"token punctuation\">{</span>\n\tout<span class=\"token punctuation\">.</span><span class=\"token function\">writeLong</span><span class=\"token punctuation\">(</span>upFlow<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\tout<span class=\"token punctuation\">.</span><span class=\"token function\">writeLong</span><span class=\"token punctuation\">(</span>downFlow<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\tout<span class=\"token punctuation\">.</span><span class=\"token function\">writeLong</span><span class=\"token punctuation\">(</span>sumFlow<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>（4）重写反序列化方法</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token annotation punctuation\">@Override</span>\n<span class=\"token keyword\">public</span> <span class=\"token keyword\">void</span> <span class=\"token function\">readFields</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">DataInput</span> in<span class=\"token punctuation\">)</span> <span class=\"token keyword\">throws</span> <span class=\"token class-name\">IOException</span> <span class=\"token punctuation\">{</span>\n\tupFlow <span class=\"token operator\">=</span> in<span class=\"token punctuation\">.</span><span class=\"token function\">readLong</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\tdownFlow <span class=\"token operator\">=</span> in<span class=\"token punctuation\">.</span><span class=\"token function\">readLong</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\tsumFlow <span class=\"token operator\">=</span> in<span class=\"token punctuation\">.</span><span class=\"token function\">readLong</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>（5）注意反序列化的顺序和序列化的顺序完全一致<br>（6）要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用。<br>（7）如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。详见后面排序案例。</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token annotation punctuation\">@Override</span>\n<span class=\"token keyword\">public</span> <span class=\"token keyword\">int</span> <span class=\"token function\">compareTo</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">FlowBean</span> o<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n\t<span class=\"token comment\">// 倒序排列，从大到小</span>\n\t<span class=\"token keyword\">return</span> <span class=\"token keyword\">this</span><span class=\"token punctuation\">.</span>sumFlow <span class=\"token operator\">&gt;</span> o<span class=\"token punctuation\">.</span><span class=\"token function\">getSumFlow</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">?</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span> <span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"MapReduce框架原理\"><a href=\"#MapReduce框架原理\" class=\"headerlink\" title=\"MapReduce框架原理\"></a>MapReduce框架原理</h2><p><img src=\"https://img-blog.csdnimg.cn/0360577c82034211a4c11623daf0ec77.png\" alt=\"\"></p>\n<h3 id=\"InputFormat数据输入\"><a href=\"#InputFormat数据输入\" class=\"headerlink\" title=\"InputFormat数据输入\"></a>InputFormat数据输入</h3><p><strong>MapTask并行度决定机制</strong></p>\n<p><strong>数据块：</strong>Block是HDFS物理上把数据分成一块一块。数据块是HDFS存储数据单位。</p>\n<p><strong>数据切片：</strong>数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是MapReduce程序计算输入数据的单位，一个切片会对应启动一个MapTask。</p>\n<p>1)一个Job的Map阶段并行度由 客户端在提交Job时的切片数决定<br>2)每一个Spli切片分配一个MapTask并行实例处理<br>3)默认情况下，切大小=BlockSize<br>4)切片时不考虑数据集整体,而是逐个针对每一个文件单独切片</p>\n<p><strong>Job提交流程源码详解</strong></p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token function\">waitForCompletion</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token function\">submit</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">// 1建立连接</span>\n<span class=\"token function\">connect</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\t\n<span class=\"token comment\">// 1）创建提交Job的代理</span>\n<span class=\"token keyword\">new</span> <span class=\"token class-name\">Cluster</span><span class=\"token punctuation\">(</span><span class=\"token function\">getConfiguration</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token comment\">// 2）判断是本地运行环境还是yarn集群运行环境</span>\n<span class=\"token function\">initialize</span><span class=\"token punctuation\">(</span>jobTrackAddr<span class=\"token punctuation\">,</span> conf<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> \n\n<span class=\"token comment\">// 2 提交job</span>\nsubmitter<span class=\"token punctuation\">.</span><span class=\"token function\">submitJobInternal</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">Job</span><span class=\"token punctuation\">.</span><span class=\"token keyword\">this</span><span class=\"token punctuation\">,</span> cluster<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// 1）创建给集群提交数据的Stag路径</span>\n<span class=\"token class-name\">Path</span> jobStagingArea <span class=\"token operator\">=</span> <span class=\"token class-name\">JobSubmissionFiles</span><span class=\"token punctuation\">.</span><span class=\"token function\">getStagingDir</span><span class=\"token punctuation\">(</span>cluster<span class=\"token punctuation\">,</span> conf<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">// 2）获取jobid ，并创建Job路径</span>\n<span class=\"token class-name\">JobID</span> jobId <span class=\"token operator\">=</span> submitClient<span class=\"token punctuation\">.</span><span class=\"token function\">getNewJobID</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">// 3）拷贝jar包到集群</span>\n<span class=\"token function\">copyAndConfigureFiles</span><span class=\"token punctuation\">(</span>job<span class=\"token punctuation\">,</span> submitJobDir<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\t\nrUploader<span class=\"token punctuation\">.</span><span class=\"token function\">uploadFiles</span><span class=\"token punctuation\">(</span>job<span class=\"token punctuation\">,</span> jobSubmitDir<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">// 4）计算切片，生成切片规划文件</span>\n<span class=\"token function\">writeSplits</span><span class=\"token punctuation\">(</span>job<span class=\"token punctuation\">,</span> submitJobDir<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\nmaps <span class=\"token operator\">=</span> <span class=\"token function\">writeNewSplits</span><span class=\"token punctuation\">(</span>job<span class=\"token punctuation\">,</span> jobSubmitDir<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\ninput<span class=\"token punctuation\">.</span><span class=\"token function\">getSplits</span><span class=\"token punctuation\">(</span>job<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">// 5）向Stag路径写XML配置文件</span>\n<span class=\"token function\">writeConf</span><span class=\"token punctuation\">(</span>conf<span class=\"token punctuation\">,</span> submitJobFile<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\nconf<span class=\"token punctuation\">.</span><span class=\"token function\">writeXml</span><span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">// 6）提交Job,返回提交状态</span>\nstatus <span class=\"token operator\">=</span> submitClient<span class=\"token punctuation\">.</span><span class=\"token function\">submitJob</span><span class=\"token punctuation\">(</span>jobId<span class=\"token punctuation\">,</span> submitJobDir<span class=\"token punctuation\">.</span><span class=\"token function\">toString</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>job<span class=\"token punctuation\">.</span><span class=\"token function\">getCredentials</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/9bfb50fd206e4ad9b1e0f767e6a3cf7f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p><strong>FileInputFormat切片源码解析</strong></p>\n<p>(1)程序先找到你数据存储的目录。<br>(2)开始遍历处理(规划切片)目录下的每一个文件<br>(3)遍历第-个文件ss.txt<br>    a)获取文件大小fs.sizeOf(ss .txt)<br>    b)计算切片大小.<br>        computeSplitSize(Math. max(minSize,Math .min(maxSize,blocksize)))=blocksize= l 28M<br>    c) 默认情况下，切片大小=blocksize<br>    d)开始切，形成第1个切片: ss.txt- - -0:128M第2个切片ss.txt- 128:256M 第3个切片ss.txt一256M:300M<br>        (每次切片时，都要判断切完剩下的部分是否大于块的1 .1倍，不大于1 .1倍就划分-块切片)<br>    e)将切片信息写到一个切片规划文件中<br>    f)整个切片的核心过程在getS$plit()方 法中完成<br>    g) InputSplit只记录 了切片的元数据信息，比如起始位置、长度以及所在的节列表等。<br>(4)提交切片规划文件到YARN上，YARN 上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数。</p>\n<p><strong>FileInputFormat切片机制</strong></p>\n<p>(1)简单地按照文件的内容长度进行切片<br>(2) 切片大小，默认等于Block大小<br>(3)切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</p>\n<p><strong>FileInputFormat切片大小的参数配置</strong></p>\n<p>(1)源码中计算切片大小的公式.<br>Math. max(ninSize, Math. min( maxSize, blockS ize));<br>mapreduce. input. fileinputformat. split. minsize=l默认值为1<br>mapreduce. input fileinputformat. split maxsize= Long MAXValue默认值Long .MAXValue<br>因此，默认情况下，切片大小=blocksize。<br>(2)切片大小设置<br>maxsize (切片最大值) :参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。<br>minsize (切片 最小值) :参数调的比blockSize大， 则可以让切片变得比blockSize还大。<br>(3)获取切片信息API<br>//获取切片的文件名称<br>String name = inputSplit. getPath() . getName() ;<br>//根据文件类型获取切片信息<br>Fi leSplit inputSplit = (FileSplit) context. getInputSplit () ;</p>\n<h3 id=\"TextInputFormat\"><a href=\"#TextInputFormat\" class=\"headerlink\" title=\"TextInputFormat\"></a>TextInputFormat</h3><p>​        FileInputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。</p>\n<p>​        TextInputFormat是默认的FileInputFormat实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text类型。</p>\n<p>以下是一个示例，比如，一个分片包含了如下4条文本记录。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Rich learning form</p>\n<p>Intelligent learning engine</p>\n<p>Learning more convenient</p>\n<p>From the real demand for more close to the enterprise</p></blockquote>\n<p>每条记录表示为以下键/值对：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>(0,Rich learning form)</p>\n<p>(20,Intelligent learning engine)</p>\n<p>(49,Learning more convenient)</p>\n<p>(74,From the real demand for more close to the enterprise)</p></blockquote>\n<h3 id=\"CombineTextInputFormat切片机制\"><a href=\"#CombineTextInputFormat切片机制\" class=\"headerlink\" title=\"CombineTextInputFormat切片机制\"></a>CombineTextInputFormat切片机制</h3><p>​        框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。<br>1）应用场景：<br>CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。<br>2）虚拟存储切片最大值设置<br>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m<br>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。<br>3）切片机制<br>生成切片过程包括：虚拟存储过程和切片过程二部分。</p>\n<p><strong>存储与切片过程</strong></p>\n<p>（1）虚拟存储过程：<br>        将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。<br>        例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。<br>（2）切片过程：<br>    （a）判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。<br>    （b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。<br>    （c）测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个        文件块，大小分别为：1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M）最终会形成3个切片，大小分        别为：（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M</p>\n<h3 id=\"MapReduce工作流程\"><a href=\"#MapReduce工作流程\" class=\"headerlink\" title=\"MapReduce工作流程\"></a>MapReduce工作流程</h3><p><img src=\"https://img-blog.csdnimg.cn/22911f1c2bd844218d98db464fc1afcd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/9636d31c8ad744eda5f337de54eb6e84.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p>上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下：<br>（1）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中<br>（2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件<br>（3）多个溢出文件会被合并成大的溢出文件<br>（4）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序<br>（5）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据<br>（6）ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）<br>（7）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）<br>注意：<br>（1）Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。<br>（2）缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb默认100M。</p>\n<h4 id=\"Shuffle机制\"><a href=\"#Shuffle机制\" class=\"headerlink\" title=\"Shuffle机制\"></a>Shuffle机制</h4><p>Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/fabe3b7c416942bf9f5096c20d739863.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<h4 id=\"Partition分区\"><a href=\"#Partition分区\" class=\"headerlink\" title=\"Partition分区\"></a>Partition分区</h4><ol>\n<li>问题引出<br>要求将统计结果按照条件输出到不同文件中(分区) .比如:将统计结果按照手机归属地不同省份输出到不同文件中(分区)</li>\n<li>默认Partitioner分区</li>\n</ol>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token keyword\">public</span> <span class=\"token keyword\">class</span> <span class=\"token class-name\">HashRartitioner</span><span class=\"token generics\"><span class=\"token punctuation\">&lt;</span><span class=\"token class-name\">K</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">V</span><span class=\"token punctuation\">&gt;</span></span> <span class=\"token keyword\">extends</span> <span class=\"token class-name\">Partitioner</span><span class=\"token generics\"><span class=\"token punctuation\">&lt;</span><span class=\"token class-name\">K</span><span class=\"token punctuation\">,</span> v<span class=\"token punctuation\">&gt;</span></span> <span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">public</span> <span class=\"token keyword\">int</span> <span class=\"token function\">getPartition</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">K</span> key<span class=\"token punctuation\">,</span> v value<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> numReduceTasks<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n    \t<span class=\"token keyword\">return</span> <span class=\"token punctuation\">(</span>key<span class=\"token punctuation\">.</span><span class=\"token function\">hashCode</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">&amp;</span> <span class=\"token class-name\">Integer</span><span class=\"token punctuation\">.</span>MAX_VALUE<span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span> numReduceTasks<span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>默认分区是根据key的hashCode对Reduce Iasks个数取模得到的。用户没法控制哪个<br>key存储到哪个分区。</p>\n<ol>\n<li>自定义Partitioner步骤<br>(1) 自定义类继承Partitioner, 重写getPartition()方法</li>\n</ol>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token keyword\">public</span> <span class=\"token keyword\">class</span> <span class=\"token class-name\">C</span> ustomPar titioner <span class=\"token keyword\">extends</span> <span class=\"token class-name\">Partitioner</span> <span class=\"token operator\">&lt;</span><span class=\"token class-name\">Text</span>，<span class=\"token class-name\">FlowBe</span> an<span class=\"token operator\">&gt;</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token annotation punctuation\">@Override</span>\n    <span class=\"token keyword\">public</span> <span class=\"token keyword\">int</span> getPartition <span class=\"token punctuation\">(</span><span class=\"token class-name\">Text</span> key<span class=\"token punctuation\">,</span> <span class=\"token class-name\">FlowBean</span> value<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> numPartitions<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">//控制分区代码逻辑</span>\n    <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n    <span class=\"token keyword\">return</span> partition<span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>​    (2)在Job驱动中，设置自定义Partitioner</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">job<span class=\"token punctuation\">.</span> setP artitionerClas <span class=\"token function\">s</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">CustomPartitioner</span><span class=\"token punctuation\">.</span><span class=\"token keyword\">class</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>​    (3)自定义Partition后， 要根据自定义P artitioner的逻辑设置相应数量的ReduceTask</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">job<span class=\"token punctuation\">.</span> <span class=\"token function\">setNumReduceTasks</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<ol>\n<li>分区总结<br>(1)如果ReduceTask的数 量&gt; getPartition的结果数,则会多产生几个空的输出文件pat-r 000xx;<br>(2)如果1&lt;ReduceTask的数 量&lt;getP artition的结果数，则有-部分分区数据无处安放，会Exception;<br>(3)如果ReduceTask的数 量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个<br>ReduceTask，最终也就只会产生一个结果文件pat-00000;<br>(4)分区号必须从零开始,逐一累加。</li>\n<li>素例分析<br>例如:假设自定义分区数为5，则<br>(1) job.setNumReduceTasks(1)， 会正常运行， 只不过会产生一个输出文件<br>(2) job.setENumReduceTasks(2), 会报错<br>(3) job.setNumReduceTasks(6), 大于5，程序会正常运行，会产生空文件</li>\n</ol>\n<h4 id=\"WritableComparable排序\"><a href=\"#WritableComparable排序\" class=\"headerlink\" title=\"WritableComparable排序\"></a>WritableComparable排序</h4><p>​        排序是MapReduc e框架中最重要的操作之一。<br>​        MapTask和ReduceTask均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。<br>​        默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。</p>\n<p>​        对于MapTask,它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘.上，而当数据处理完毕后，它会对磁盘.上所有文件进行归并排序。<br>​        对于ReduceTask，它从每个M ap Task上远程拷贝相应的数据文件，如果文件大小超过一 定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到-定阈值，则进行一次归并排序以生成一个更大文件;如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统-对内存和磁盘上的所有数据进行一次归并排序。</p>\n<p><strong>排序分类</strong></p>\n<p>(1)部分排序<br>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。<br>(2)全排序.<br>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。 但该方法在处理大型文件时效率极低，因为-台机器处理所有文件，完全丧失了 MapRe duce所提供的并行架构。<br>(3)辅助排序: (GroupingCompar ator分组)<br>在Reduce端对key进行分组。应用于:在接收的key为bean对象时，想让一个或几个字段相同(全部.字段比较不相同)的k&lt;ey进 入到同一个reduce方法时，可以采用分组排序。<br>(4)二次排序<br>在自定义排序过程中，如果c ompareTo中的判断条件为两个即为二次排序。</p>\n<p><strong>自定义排序WritableComparable原理分析</strong></p>\n<p>bean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序。</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token annotation punctuation\">@Override</span>\n<span class=\"token keyword\">public</span> <span class=\"token keyword\">int</span> <span class=\"token function\">compareTo</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">FlowBean</span> bean<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n\n\t<span class=\"token keyword\">int</span> result<span class=\"token punctuation\">;</span>\n\t\t\n\t<span class=\"token comment\">// 按照总流量大小，倒序排列</span>\n\t<span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">this</span><span class=\"token punctuation\">.</span>sumFlow <span class=\"token operator\">&gt;</span> bean<span class=\"token punctuation\">.</span><span class=\"token function\">getSumFlow</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n\t\tresult <span class=\"token operator\">=</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">;</span>\n\t<span class=\"token punctuation\">}</span><span class=\"token keyword\">else</span> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">this</span><span class=\"token punctuation\">.</span>sumFlow <span class=\"token operator\">&lt;</span> bean<span class=\"token punctuation\">.</span><span class=\"token function\">getSumFlow</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n\t\tresult <span class=\"token operator\">=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span>\n\t<span class=\"token punctuation\">}</span><span class=\"token keyword\">else</span> <span class=\"token punctuation\">{</span>\n\t\tresult <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n\t<span class=\"token punctuation\">}</span>\n\n\t<span class=\"token keyword\">return</span> result<span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h4 id=\"Combiner合并\"><a href=\"#Combiner合并\" class=\"headerlink\" title=\"Combiner合并\"></a>Combiner合并</h4><p>(1) Combiner是MR程 序中Mapper和Reducer之外的一种组件。<br>(2) Combiner组件的父类就是Reducer。<br>(3) Combiner和Reducer的区别在 于运行的位置<br>      Combiner是在每一个MapTask所在的节点运行;<br>      Reducer是接收全局所有Mapper的输出结果;<br>(4) Combiner的意义就是对每一个 MapTask的输出进行局部汇总，以减小网络传输量。<br>(5) Combiner能够应 用的前提是不能影响最终的业务逻辑，而且，Combiner的输 出kv应该跟Reducer的输，入Iv类型要对应起来。</p>\n<p><strong>自定义Combiner实现步骤</strong></p>\n<p>（a）自定义一个Combiner继承Reducer，重写Reduce方法</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token keyword\">public</span> <span class=\"token keyword\">class</span> <span class=\"token class-name\">WordCountCombiner</span> <span class=\"token keyword\">extends</span> <span class=\"token class-name\">Reducer</span><span class=\"token generics\"><span class=\"token punctuation\">&lt;</span><span class=\"token class-name\">Text</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">IntWritable</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">Text</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">IntWritable</span><span class=\"token punctuation\">&gt;</span></span> <span class=\"token punctuation\">{</span>\n\n    <span class=\"token keyword\">private</span> <span class=\"token class-name\">IntWritable</span> outV <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">IntWritable</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token annotation punctuation\">@Override</span>\n    <span class=\"token keyword\">protected</span> <span class=\"token keyword\">void</span> <span class=\"token function\">reduce</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">Text</span> key<span class=\"token punctuation\">,</span> <span class=\"token class-name\">Iterable</span><span class=\"token generics\"><span class=\"token punctuation\">&lt;</span><span class=\"token class-name\">IntWritable</span><span class=\"token punctuation\">&gt;</span></span> values<span class=\"token punctuation\">,</span> <span class=\"token class-name\">Context</span> context<span class=\"token punctuation\">)</span> <span class=\"token keyword\">throws</span> <span class=\"token class-name\">IOException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">InterruptedException</span> <span class=\"token punctuation\">{</span>\n\n        <span class=\"token keyword\">int</span> sum <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n        <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">IntWritable</span> value <span class=\"token operator\">:</span> values<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n            sum <span class=\"token operator\">+=</span> value<span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n        <span class=\"token punctuation\">}</span>\n     \n        outV<span class=\"token punctuation\">.</span><span class=\"token function\">set</span><span class=\"token punctuation\">(</span>sum<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n     \n        context<span class=\"token punctuation\">.</span><span class=\"token function\">write</span><span class=\"token punctuation\">(</span>key<span class=\"token punctuation\">,</span>outV<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>（b）在Job驱动类中设置： </p>\n<p>job.setCombinerClass(WordCountCombiner.class);</p>\n<h3 id=\"OutputFormat数据输出\"><a href=\"#OutputFormat数据输出\" class=\"headerlink\" title=\"OutputFormat数据输出\"></a>OutputFormat数据输出</h3><p>OupltFoma是Mpduce输出的基类,所有实现MapReduce输出都实现了OupuFormat接口。下面我们介绍几种常见的OutputFormat实现类。</p>\n<ol>\n<li>OutputFormat实现类</li>\n<li>默认输出格式TextOutputFormat</li>\n<li>自定义OutputFormat</li>\n</ol>\n<p>应用场景:输出数据到MySQL/HBase/Elasticsearch等存储框架中。<br><strong>自定义OutputFormat步骤</strong><br>➢自定义一个类继承FileOutputF ormat。<br>➢改写RecordWriter， 具体改写输出数据的方法write0。</p>\n<h3 id=\"MapReduce内核源码解析\"><a href=\"#MapReduce内核源码解析\" class=\"headerlink\" title=\"MapReduce内核源码解析\"></a>MapReduce内核源码解析</h3><p><img src=\"https://img-blog.csdnimg.cn/ff7a55e018cb4455b4c30cd05a7eb4d7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p>1）Read阶段：MapTask通过InputFormat获得的RecordReader，从输入InputSplit中解析出一个个key/value。</p>\n<p>2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p>\n<p>3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p>\n<p>4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>\n<p><strong>溢写阶段详情：</strong></p>\n<p>​    步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p>\n<p>​    步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>\n<p>​    步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p>\n<p> 5）Merge阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>\n<p>​    当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。</p>\n<p>​    在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并mapreduce.task.io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>\n<p>​    让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>\n<h3 id=\"ReduceTask工作机制\"><a href=\"#ReduceTask工作机制\" class=\"headerlink\" title=\"ReduceTask工作机制\"></a>ReduceTask工作机制</h3><p><img src=\"https://img-blog.csdnimg.cn/77ce481637b5486893fe9a8603413aaf.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p> 1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>\n<p> 2）Sort阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</p>\n<p> 3）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p>\n<h3 id=\"ReduceTask并行度决定机制\"><a href=\"#ReduceTask并行度决定机制\" class=\"headerlink\" title=\"ReduceTask并行度决定机制\"></a>ReduceTask并行度决定机制</h3><p><strong>回顾：</strong>MapTask并行度由切片个数决定，切片个数由输入文件和切片规则决定。</p>\n<p><strong>思考：</strong>ReduceTask并行度由谁决定？</p>\n<p>1）设置ReduceTask并行度（个数）</p>\n<p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置：</p>\n<p>// 默认值是1，手动设置为4</p>\n<p>job.setNumReduceTasks(4);</p>\n<p><strong>注意事项</strong></p>\n<p>(1) ReduceTask=0, 表示没有Reduce阶段，输出文件个数和Map个数-致。<br>(2) ReduceTask默认值就是1 ，所以输出文件个数为一一个。<br>(3)如果数据分布不均匀，就有可能在R educe阶段产生数据倾斜<br>(4) ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask.<br>(5)具体多少个ReduceTask,需要根据集群性能而定。<br>(6)如果分区数不是1,但是ReduceTask为1，是否执行分区过程。答案是:不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1 .肯定不执行。.</p>\n<h3 id=\"Join应用\"><a href=\"#Join应用\" class=\"headerlink\" title=\"Join应用\"></a>Join应用</h3><h4 id=\"Reduce-Join\"><a href=\"#Reduce-Join\" class=\"headerlink\" title=\"Reduce Join\"></a>Reduce Join</h4><p>​        Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。</p>\n<pre><code>     Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志）分开，最后进行合并就ok了。\n</code></pre><h4 id=\"Map-Join\"><a href=\"#Map-Join\" class=\"headerlink\" title=\"Map Join\"></a>Map Join</h4><p>1）使用场景</p>\n<p>Map Join适用于一张表十分小、一张表很大的场景。</p>\n<p>2）优点</p>\n<p>思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？</p>\n<p>在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。</p>\n<p>3）具体办法：采用DistributedCache</p>\n<p>​    （1）在Mapper的setup阶段，将文件读取到缓存集合中。</p>\n<p>​    （2）在Driver驱动类中加载缓存。</p>\n<p>//缓存普通文件到Task运行节点。</p>\n<p>job.addCacheFile(new URI(“file:///e:/cache/pd.txt”));</p>\n<p>//如果是集群运行,需要设置HDFS路径</p>\n<p>job.addCacheFile(new URI(“hdfs://hadoop102:8020/cache/pd.txt”));</p>\n<h3 id=\"数据清洗（ETL）\"><a href=\"#数据清洗（ETL）\" class=\"headerlink\" title=\"数据清洗（ETL）\"></a>数据清洗（ETL）</h3><p>“ETL，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL一词较常用在数据仓库，但其对象并不限于数据仓库</p>\n<p>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。</p>\n<h3 id=\"MapReduce开发总结\"><a href=\"#MapReduce开发总结\" class=\"headerlink\" title=\"MapReduce开发总结\"></a>MapReduce开发总结</h3><p>1）输入数据接口：InputFormat<br>（1）默认使用的实现类是：TextInputFormat<br>（2）TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。<br>（3）CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。</p>\n<p>2）逻辑处理接口：Mapper<br>用户根据业务需求实现其中三个方法：map()   setup()   cleanup () </p>\n<p>3）Partitioner分区<br>（1）有默认实现 HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces<br>（2）如果业务上有特别的需求，可以自定义分区。</p>\n<p>4）Comparable排序<br>（1）当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法。<br>（2）部分排序：对最终输出的每一个文件进行内部排序。<br>（3）全排序：对所有数据进行排序，通常只有一个Reduce。<br>（4）二次排序：排序的条件有两个。</p>\n<p>5）Combiner合并<br>Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果。</p>\n<p>6）逻辑处理接口：Reducer<br>用户根据业务需求实现其中三个方法：reduce()   setup()   cleanup () </p>\n<p>7）输出数据接口：OutputFormat<br>（1）默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对，向目标文本文件输出一行。<br>（2）用户还可以自定义OutputFormat。</p>\n<h2 id=\"Hadoop数据压缩\"><a href=\"#Hadoop数据压缩\" class=\"headerlink\" title=\"Hadoop数据压缩\"></a>Hadoop数据压缩</h2><p>1）压缩的好处和坏处<br>压缩的优点：以减少磁盘IO、减少磁盘存储空间。<br>压缩的缺点：增加CPU开销。<br>2）压缩原则<br>（1）运算密集型的Job，少用压缩<br>（2）IO密集型的Job，多用压缩</p>\n<p><strong>MR支持的压缩编码</strong></p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>压缩格式</th>\n<th>Hadoop自带？</th>\n<th>算法</th>\n<th>文件扩展名</th>\n<th>是否可切片</th>\n<th>换成压缩格式后，原来的程序是否需要修改</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>DEFLATE</td>\n<td>是，直接使用</td>\n<td>DEFLATE</td>\n<td>.deflate</td>\n<td>否</td>\n<td>和文本处理一样，不需要修改</td>\n</tr>\n<tr>\n<td>Gzip</td>\n<td>是，直接使用</td>\n<td>DEFLATE</td>\n<td>.gz</td>\n<td>否</td>\n<td>和文本处理一样，不需要修改</td>\n</tr>\n<tr>\n<td>bzip2</td>\n<td>是，直接使用</td>\n<td>bzip2</td>\n<td>.bz2</td>\n<td>是</td>\n<td>和文本处理一样，不需要修改</td>\n</tr>\n<tr>\n<td>LZO</td>\n<td>否，需要安装</td>\n<td>LZO</td>\n<td>.lzo</td>\n<td>是</td>\n<td>需要建索引，还需要指定输入格式</td>\n</tr>\n<tr>\n<td>Snappy</td>\n<td>是，直接使用</td>\n<td>Snappy</td>\n<td>.snappy</td>\n<td>否</td>\n<td>和文本处理一样，不需要修改</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>压缩性能的比较</strong></p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>压缩算法</th>\n<th>原始文件大小</th>\n<th>压缩文件大小</th>\n<th>压缩速度</th>\n<th>解压速度</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>gzip</td>\n<td>8.3GB</td>\n<td>1.8GB</td>\n<td>17.5MB/s</td>\n<td>58MB/s</td>\n</tr>\n<tr>\n<td>bzip2</td>\n<td>8.3GB</td>\n<td>1.1GB</td>\n<td>2.4MB/s</td>\n<td>9.5MB/s</td>\n</tr>\n<tr>\n<td>LZO</td>\n<td>8.3GB</td>\n<td>2.9GB</td>\n<td>49.3MB/s</td>\n<td>74.6MB/s</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"压缩方式选择\"><a href=\"#压缩方式选择\" class=\"headerlink\" title=\"压缩方式选择\"></a>压缩方式选择</h3><p>压缩方式选择时重点考虑：压缩/解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片。</p>\n<p><strong>Gzip压缩</strong></p>\n<p>优点：压缩率比较高； </p>\n<p>缺点：不支持Split；压缩/解压速度一般；</p>\n<p><strong>Bzip2压缩</strong></p>\n<p>优点：压缩率高；支持Split； </p>\n<p>缺点：压缩/解压速度慢。</p>\n<p><strong>Lzo压缩</strong></p>\n<p>优点：压缩/解压速度比较快；支持Split；</p>\n<p>缺点：压缩率一般；想支持切片需要额外创建索引。</p>\n<p><strong>Snappy压缩</strong></p>\n<p>优点：压缩和解压缩速度快； </p>\n<p>缺点：不支持Split；压缩率一般； </p>\n<p><strong>压缩位置选择</strong></p>\n<p>压缩可以在MapReduce作用的任意阶段启用。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/605d9fc06f1341d097660b71afa59984.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<h3 id=\"压缩参数配置\"><a href=\"#压缩参数配置\" class=\"headerlink\" title=\"压缩参数配置\"></a>压缩参数配置</h3><p>1）为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>压缩格式</th>\n<th>对应的编码/解码器</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>DEFLATE</td>\n<td>org.apache.hadoop.io.compress.DefaultCodec</td>\n</tr>\n<tr>\n<td>gzip</td>\n<td>org.apache.hadoop.io.compress.GzipCodec</td>\n</tr>\n<tr>\n<td>bzip2</td>\n<td>org.apache.hadoop.io.compress.BZip2Codec</td>\n</tr>\n<tr>\n<td>LZO</td>\n<td>com.hadoop.compression.lzo.LzopCodec</td>\n</tr>\n<tr>\n<td>Snappy</td>\n<td>org.apache.hadoop.io.compress.SnappyCodec</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>2）要在Hadoop中启用压缩，可以配置如下参数</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>默认值</th>\n<th>阶段</th>\n<th>建议</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>io.compression.codecs    （在core-site.xml中配置）</td>\n<td>无，这个需要在命令行输入hadoop checknative查看</td>\n<td>输入压缩</td>\n<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>\n</tr>\n<tr>\n<td>mapreduce.map.output.compress（在mapred-site.xml中配置）</td>\n<td>false</td>\n<td>mapper输出</td>\n<td>这个参数设为true启用压缩</td>\n</tr>\n<tr>\n<td>mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td>\n<td>org.apache.hadoop.io.compress.DefaultCodec</td>\n<td>mapper输出</td>\n<td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td>\n</tr>\n<tr>\n<td>mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td>\n<td>false</td>\n<td>reducer输出</td>\n<td>这个参数设为true启用压缩</td>\n</tr>\n<tr>\n<td>mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td>\n<td>org.apache.hadoop.io.compress.DefaultCodec</td>\n<td>reducer输出</td>\n<td>使用标准工具或者编解码器，如gzip和bzip2</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"常见错误及解决方案\"><a href=\"#常见错误及解决方案\" class=\"headerlink\" title=\"常见错误及解决方案\"></a>常见错误及解决方案</h2><p>1）导包容易出错。尤其Text和CombineTextInputFormat。<br>2）Mapper中第一个输入的参数必须是LongWritable或者NullWritable，不可以是IntWritable.  报的错误是类型转换异常。<br>3）java.lang.Exception: java.io.IOException: Illegal partition for 13926435656 (4)，说明Partition和ReduceTask个数没对上，调整ReduceTask个数。<br>4）如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。<br>5）在Windows环境编译的jar包导入到Linux环境中运行，</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">hadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/ /user/atguigu/output<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>报如下错误：</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">Exception in thread \"main\" java.lang.UnsupportedClassVersionError: com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version 52.0<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>原因是Windows环境用的jdk1.7，Linux环境用的jdk1.8。<br>解决方案：统一jdk版本。<br>6）缓存pd.txt小文件案例中，报找不到pd.txt文件<br>原因：大部分为路径书写错误。还有就是要检查pd.txt.txt的问题。还有个别电脑写相对路径找不到pd.txt，可以修改为绝对路径。<br>7）报类型转换异常。<br>通常都是在驱动函数中设置Map输出和最终输出时编写错误。<br>Map输出的key如果没有排序，也会报类型转换异常。<br>8）集群中运行wc.jar时出现了无法获得输入文件。<br>原因：WordCount案例的输入文件不能放用HDFS集群的根目录。<br>9）出现了如下相关异常</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:609)\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)\njava.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n\tat org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:356)\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:371)\n\tat org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:364)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>解决方案：拷贝hadoop.dll文件到Windows目录C:\\Windows\\System32。个别同学电脑还需要修改Hadoop源码。<br>方案二：创建如下包名，并将NativeIO.java拷贝到该包名下</p>\n<p>10）自定义Outputformat时，注意在RecordWirter中的close方法必须关闭流资源。否则输出的文件内容中数据为空。</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">@Override\npublic void close(TaskAttemptContext context) throws IOException, InterruptedException {\n\t\tif (atguigufos != null) {\n\t\t\tatguigufos.close();\n\t\t}\n\t\tif (otherfos != null) {\n\t\t\totherfos.close();\n\t\t}\n}<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"5-Yarn\"><a href=\"#5-Yarn\" class=\"headerlink\" title=\"5. Yarn\"></a>5. Yarn</h1><p>​        Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p>\n<h2 id=\"Yarn资源调度器\"><a href=\"#Yarn资源调度器\" class=\"headerlink\" title=\"Yarn资源调度器\"></a>Yarn资源调度器</h2><h3 id=\"Yarn基础架构\"><a href=\"#Yarn基础架构\" class=\"headerlink\" title=\"Yarn基础架构\"></a>Yarn基础架构</h3><p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/68cea7e630e74f839010c0423fd2caa3.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<h3 id=\"Yarn工作机制\"><a href=\"#Yarn工作机制\" class=\"headerlink\" title=\"Yarn工作机制\"></a>Yarn工作机制</h3><p><img src=\"https://img-blog.csdnimg.cn/861055274edd4af3a11adc6c2ab8aff2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p>1）MR程序提交到客户端所在的节点。</p>\n<p>2）YarnRunner向ResourceManager申请一个Application。</p>\n<p>3）RM将该应用程序的资源路径返回给YarnRunner。</p>\n<p>4）该程序将运行所需资源提交到HDFS上。</p>\n<p>5）程序资源提交完毕后，申请运行mrAppMaster。</p>\n<p>6）RM将用户的请求初始化成一个Task。</p>\n<p>7）其中一个NodeManager领取到Task任务。</p>\n<p>8）该NodeManager创建容器Container，并产生MRAppmaster。</p>\n<p>9）Container从HDFS上拷贝资源到本地。</p>\n<p>10）MRAppmaster向RM 申请运行MapTask资源。</p>\n<p>11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>\n<p>12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</p>\n<p>13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</p>\n<p>14）ReduceTask向MapTask获取相应分区的数据。</p>\n<p>15）程序运行完毕后，MR会向RM申请注销自己。</p>\n<h3 id=\"作业提交全过程\"><a href=\"#作业提交全过程\" class=\"headerlink\" title=\"作业提交全过程\"></a>作业提交全过程</h3><p><img src=\"https://img-blog.csdnimg.cn/c8f67701bff147a69423c8a2c87e0411.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/c91a4a9ca18b4b42b406cf255d81daa6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/85948539a3d046ae9e9796cccafa4d8c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p><strong>作业提交全过程详解</strong></p>\n<p><strong>（1）作业提交</strong></p>\n<p>第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。</p>\n<p>第2步：Client向RM申请一个作业id。</p>\n<p>第3步：RM给Client返回该job资源的提交路径和作业id。</p>\n<p>第4步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。</p>\n<p>第5步：Client提交完资源后，向RM申请运行MrAppMaster。</p>\n<p><strong>（2）作业初始化</strong></p>\n<p>第6步：当RM收到Client的请求后，将该job添加到容量调度器中。</p>\n<p>第7步：某一个空闲的NM领取到该Job。</p>\n<p>第8步：该NM创建Container，并产生MRAppmaster。</p>\n<p>第9步：下载Client提交的资源到本地。</p>\n<p><strong>（3）任务分配</strong></p>\n<p>第10步：MrAppMaster向RM申请运行多个MapTask任务资源。</p>\n<p>第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>\n<p><strong>（4）任务运行</strong></p>\n<p>第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</p>\n<p>第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</p>\n<p>第14步：ReduceTask向MapTask获取相应分区的数据。</p>\n<p>第15步：程序运行完毕后，MR会向RM申请注销自己。</p>\n<p><strong>（5）进度和状态更新</strong></p>\n<p>​        YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。</p>\n<p><strong>（6）作业完成</strong></p>\n<p>​        除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</p>\n<h3 id=\"Yarn调度器和调度算法\"><a href=\"#Yarn调度器和调度算法\" class=\"headerlink\" title=\"Yarn调度器和调度算法\"></a>Yarn调度器和调度算法</h3><p>目前，Hadoop作业调度器主要有三种：FIFO、容量（Capacity Scheduler）和公平（Fair Scheduler）。Apache Hadoop3.1.3默认的资源调度器是Capacity Scheduler。</p>\n<p>CDH框架默认调度器是Fair Scheduler。具体设置详见：yarn-default.xml文件</p>\n<pre class=\"line-numbers language-xml\" data-language=\"xml\"><code class=\"language-xml\"><span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>description</span><span class=\"token punctuation\">&gt;</span></span>The class to use as the resource scheduler.<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>description</span><span class=\"token punctuation\">&gt;</span></span>\n    <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>yarn.resourcemanager.scheduler.class<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h4 id=\"先进先出调度器（FIFO）\"><a href=\"#先进先出调度器（FIFO）\" class=\"headerlink\" title=\"先进先出调度器（FIFO）\"></a>先进先出调度器（FIFO）</h4><p>FIFO调度器（First In First Out）：单队列，根据提交作业的先后顺序，先来先服务。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/bea35917907c4da2ad3806bb183f7c0a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p>优点：简单易懂；</p>\n<p>缺点：不支持多队列，生产环境很少使用；</p>\n<h4 id=\"容量调度器（Capacity-Scheduler）\"><a href=\"#容量调度器（Capacity-Scheduler）\" class=\"headerlink\" title=\"容量调度器（Capacity Scheduler）\"></a>容量调度器（Capacity Scheduler）</h4><p>Capacity Scheduler是Yahoo开发的多用户调度器。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/b64bd3b9d16c409f88c7c34b86352224.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p>1、多队列:每个队列可配置-定的资源量, 每个队列采用FIF0调度策略。<br>2、容量保证:管理员可为每个队列设置资源最低保证和资源使用上限<br>3、灵活性:如果一个队列中的资源有剩余,可以暂时共享给那些需要资源的队列，而- -旦该队列有新的应用<br>程序提交，则其他队列借调的资源会归还给该队列。<br>4、多租户:<br>支持多用户共享集群和多应用程序同时运行。<br>为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/65f4d4aabe334ad2a6bc1668c5ab5dc6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"****\"></p>\n<h4 id=\"公平调度器（Fair-Scheduler）\"><a href=\"#公平调度器（Fair-Scheduler）\" class=\"headerlink\" title=\"公平调度器（Fair Scheduler）\"></a>公平调度器（Fair Scheduler）</h4><p>Fair Schedulere是Facebook开发的多用户调度器。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/eff34279182f49659fc6772cb42b7a8f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p>1)与容量调度器相同点<br>(1)多队列:支持多队列多作业<br>(2)容量保证:管理员可为每个队列设置资源最低保证和资源使用上线<br>(3)灵活性:如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提<br>交，则其他队列借调的资源会归还给该队列。<br>(4)多租户:支持多用户共享集群和多应用程序同时运行;为了防止同一个用户的作业独占队列中的资源，该调度器:<br>会对同一用户提交的作业所占资源量进行限定。</p>\n<p>2)与容量调度器不同点<br>(1)核心调度策略不同</p>\n<p>容量调度器:优先选择资源利用率低的队列</p>\n<p>公平调度器:优先选择对资源的缺额比例大的</p>\n<p>(2)每个队列可以单独设置资源分配方式</p>\n<p>容量调度器: FIFO、DRF<br>公平调度器: FIFO、 FAIR、k DRF</p>\n<p><img src=\"https://img-blog.csdnimg.cn/607e937525aa4d3fac4c8db264aa0e9d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p>公平调度器设计目标是: 在时我尔度占。所有作业获得公平的资源。某一时刻一个作业应获资源际获取资的差叫’缺额”<br>调度器会优先为缺额大的作业分配资源</p>\n<p><strong>公平调度器队列资源分配方式</strong></p>\n<p>1) FIFO策略<br>公平调度器每个队列资源分配策略如果选择FIFO的话，此时公平调度器相当于上面讲过的容量调度器。<br>2) Fair策略<br>    Fair策略(默认)是一-种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资<br>    源。这意味着，如果一-个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源;如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。<br>    具体资源分配流程和容量调度器一致;<br>    (l)选择队列<br>    (2)选择作业<br>    (3)选择容器<br>    以上三步，每一一步都是按照公平策略分配资源<br>    ➢实际最小资源份额: mindshare = Min (资源需求量，配置的最小资源)<br>    ➢是否饥饿: isNeedy =资源使用量&lt; mindshare (实际最小资源份额)<br>    ➢资源分配比: minShareRatio =资源使用量/ Max (mindshare, 1 )<br>    ➢资源使用权重比: use ToWeightRatio=资源使用量/权重</p>\n<p><img src=\"https://img-blog.csdnimg.cn/47d8ef9835de42268986432c4808cb1f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/bfc2a75fda5b4484a6950429c15afe7c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"\"></p>\n<p>3)DRF策略</p>\n<p>​        DRF ( Dominant Resource Faimness)，我们之前说的资源，都是单一标准，例如只考虑内存(也是Yarn默<br>认的情况)。但是很多时候我们资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用<br>应该分配的资源比例。<br>​        那么在YARN中，我们用DRF来决定如何调度:<br>​        假设集群- :共有100 CPU和10T内存，而应用A需要(2 CPU, 300GB)，应用B需要(6 CPU，100GB) 。<br>则两个应用分别需要A (2%CPU, 3%内存)和B (6%CPU, 1%内存)的资源，这就意味着A是内存主导的，B是<br>CPU主导的，针对这种情况，我们可以选择DRF策略对不同应用进行不同资源(CPU和内存)的一个不同比<br>例的限制。</p>\n<h3 id=\"Yarn常用命令\"><a href=\"#Yarn常用命令\" class=\"headerlink\" title=\"Yarn常用命令\"></a>Yarn常用命令</h3><h4 id=\"yarn-application查看任务\"><a href=\"#yarn-application查看任务\" class=\"headerlink\" title=\"yarn application查看任务\"></a>yarn application查看任务</h4><p>Yarn状态的查询，除了可以在hadoop103:8088页面查看外，还可以通过命令操作。常见的命令操作如下所示：</p>\n<p>需求：执行WordCount案例，并用Yarn命令查看任务运行情况。</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 hadoop-3.1.3]$ myhadoop.sh start\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n<p><strong>yarn application查看任务</strong></p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">yarn application -list<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p><strong>根据Application状态过滤：</strong></p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">yarn application -list -appStates （所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p><strong>Kill掉Application：</strong></p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">yarn application -kill application_1612577921195_0001<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<h4 id=\"yarn-logs查看日志\"><a href=\"#yarn-logs查看日志\" class=\"headerlink\" title=\"yarn logs查看日志\"></a>yarn logs查看日志</h4><p><strong>查询Application日志：yarn logs -applicationId <applicationid></applicationid></strong></p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">yarn logs -applicationId application_1612577921195_0001<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p><strong>查询Container日志：yarn logs -applicationId <applicationid> -containerId <containerid></containerid></applicationid></strong></p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">yarn logs -applicationId application_1612577921195_0001 -containerId container_1612577921195_0001_01_000001<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<h4 id=\"yarn-applicationattempt查看尝试运行的任务\"><a href=\"#yarn-applicationattempt查看尝试运行的任务\" class=\"headerlink\" title=\"yarn applicationattempt查看尝试运行的任务\"></a>yarn applicationattempt查看尝试运行的任务</h4><p><strong>列出所有Application尝试的列表：yarn applicationattempt -list <applicationid></applicationid></strong></p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">yarn applicationattempt -list application_1612577921195_0001<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p><strong>打印ApplicationAttemp状态：yarn applicationattempt -status <applicationattemptid></applicationattemptid></strong></p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">yarn applicationattempt -status appattempt_1612577921195_0001_000001<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<h4 id=\"yarn-container查看容器\"><a href=\"#yarn-container查看容器\" class=\"headerlink\" title=\"yarn container查看容器\"></a>yarn container查看容器</h4><p><strong>列出所有Container：yarn container -list <applicationattemptid></applicationattemptid></strong></p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">yarn container -list appattempt_1612577921195_0001_000001<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p><strong>打印Container状态： yarn container -status <containerid></containerid></strong></p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">yarn container -status container_1612577921195_0001_01_000001<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<h4 id=\"yarn-node查看节点状态\"><a href=\"#yarn-node查看节点状态\" class=\"headerlink\" title=\"yarn node查看节点状态\"></a>yarn node查看节点状态</h4><p><strong>列出所有节点：yarn node -list -all</strong></p>\n<h4 id=\"yarn-rmadmin更新配置\"><a href=\"#yarn-rmadmin更新配置\" class=\"headerlink\" title=\"yarn rmadmin更新配置\"></a>yarn rmadmin更新配置</h4><p><strong>加载队列配置：yarn rmadmin -refreshQueues</strong></p>\n<h4 id=\"yarn-queue查看队列\"><a href=\"#yarn-queue查看队列\" class=\"headerlink\" title=\"yarn queue查看队列\"></a>yarn queue查看队列</h4><p><strong>打印队列信息：yarn queue -status <queuename></queuename></strong></p>\n<h3 id=\"Yarn生产环境核心参数\"><a href=\"#Yarn生产环境核心参数\" class=\"headerlink\" title=\"Yarn生产环境核心参数\"></a>Yarn生产环境核心参数</h3><p><img src=\"https://img-blog.csdnimg.cn/e9fb2909407c41dc83456a46e24780ca.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<h3 id=\"向Hive队列提交任务\"><a href=\"#向Hive队列提交任务\" class=\"headerlink\" title=\"向Hive队列提交任务\"></a>向Hive队列提交任务</h3><p>1）hadoop jar的方式</p>\n<p>[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output</p>\n<p>注: -D表示运行时改变参数值</p>\n<p>2）打jar包的方式</p>\n<p>默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\"><span class=\"token keyword\">public</span> <span class=\"token keyword\">class</span> <span class=\"token class-name\">WcDrvier</span> <span class=\"token punctuation\">{</span>\n\n    <span class=\"token keyword\">public</span> <span class=\"token keyword\">static</span> <span class=\"token keyword\">void</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">String</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span> args<span class=\"token punctuation\">)</span> <span class=\"token keyword\">throws</span> <span class=\"token class-name\">IOException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">ClassNotFoundException</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">InterruptedException</span> <span class=\"token punctuation\">{</span>\n\n        <span class=\"token class-name\">Configuration</span> conf <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Configuration</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n        conf<span class=\"token punctuation\">.</span><span class=\"token function\">set</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"mapreduce.job.queuename\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"hive\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n        <span class=\"token comment\">//1. 获取一个Job实例</span>\n        <span class=\"token class-name\">Job</span> job <span class=\"token operator\">=</span> <span class=\"token class-name\">Job</span><span class=\"token punctuation\">.</span><span class=\"token function\">getInstance</span><span class=\"token punctuation\">(</span>conf<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n        。。。 。。。\n\n        <span class=\"token comment\">//6. 提交Job</span>\n        <span class=\"token keyword\">boolean</span> b <span class=\"token operator\">=</span> job<span class=\"token punctuation\">.</span><span class=\"token function\">waitForCompletion</span><span class=\"token punctuation\">(</span><span class=\"token boolean\">true</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n        <span class=\"token class-name\">System</span><span class=\"token punctuation\">.</span><span class=\"token function\">exit</span><span class=\"token punctuation\">(</span>b <span class=\"token operator\">?</span> <span class=\"token number\">0</span> <span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>这样，这个任务在集群提交时，就会提交到hive队列：</p>\n<h3 id=\"任务优先级\"><a href=\"#任务优先级\" class=\"headerlink\" title=\"任务优先级\"></a>任务优先级</h3><p>容量调度器，支持任务优先级的配置，在资源紧张时，优先级高的任务将优先获取资源。默认情况，Yarn将所有任务的优先级限制为0，若想使用任务的优先级功能，须开放该限制。</p>\n<p>1）修改yarn-site.xml文件，增加以下参数</p>\n<pre class=\"line-numbers language-xml\" data-language=\"xml\"><code class=\"language-xml\"><span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>property</span><span class=\"token punctuation\">&gt;</span></span>\n  <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>name</span><span class=\"token punctuation\">&gt;</span></span>yarn.cluster.max-application-priority<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>name</span><span class=\"token punctuation\">&gt;</span></span>\n  <span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>value</span><span class=\"token punctuation\">&gt;</span></span>5<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>value</span><span class=\"token punctuation\">&gt;</span></span>\n<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>property</span><span class=\"token punctuation\">&gt;</span></span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n<p>2）分发配置，并重启Yarn</p>\n<p>[atguigu@hadoop102 hadoop]$ xsync yarn-site.xml</p>\n<p>[atguigu@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh</p>\n<p>[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</p>\n<p>3）模拟资源紧张环境，可连续提交以下任务，直到新提交的任务申请不到资源为止。</p>\n<p>[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 2000000</p>\n<p>4）再次重新提交优先级高的任务</p>\n<p>[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi  -D mapreduce.job.priority=5 5 2000000</p>\n<p>5）也可以通过以下命令修改正在执行的任务的优先级。</p>\n<p>yarn application -appID <applicationid> -updatePriority 优先级</applicationid></p>\n<p>[atguigu@hadoop102 hadoop-3.1.3]$ yarn application -appID application_1611133087930_0009 -updatePriority 5</p>\n","text":"0. 大数据概念大数据（Big Data）：指无法在==一定时间范围内==用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的==海量、高增长率和多样化==的==信息资产==。大数据主要解决，海量数据的采集、存储和分析计算...","link":"","photos":[],"count_time":{"symbolsCount":"43k","symbolsTime":"39 mins."},"categories":[{"name":"大数据","slug":"大数据","count":7,"path":"api/categories/大数据.json"}],"tags":[{"name":"Hadoop","slug":"Hadoop","count":3,"path":"api/tags/Hadoop.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E5%BF%B5\"><span class=\"toc-text\">0. 大数据概念</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#1-Hadoop-%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">1. Hadoop 概述</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Hadoop-%E4%BC%98%E5%8A%BF%EF%BC%884-%E9%AB%98%EF%BC%89\"><span class=\"toc-text\">Hadoop 优势（4 高）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Hadoop-%E7%BB%84%E6%88%90%EF%BC%88%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%EF%BC%89-%E2%AD%90%E2%AD%90\"><span class=\"toc-text\">Hadoop 组成（面试重点） ⭐⭐</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#HDFS-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">HDFS 架构概述</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#YARN-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">YARN 架构概述</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#MapReduce-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">MapReduce 架构概述</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%E4%BD%93%E7%B3%BB\"><span class=\"toc-text\">大数据技术生态体系</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#2-Hadoop-%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%88%E5%BC%80%E5%8F%91%E9%87%8D%E7%82%B9%EF%BC%89\"><span class=\"toc-text\">2. Hadoop 运行环境搭建（开发重点）</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87\"><span class=\"toc-text\">基础环境准备</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Hadoop-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F\"><span class=\"toc-text\">Hadoop 运行模式</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F\"><span class=\"toc-text\">本地运行模式</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%BC%80%E5%8F%91%E9%87%8D%E7%82%B9%EF%BC%89%E2%AD%90%E2%AD%90%E2%AD%90\"><span class=\"toc-text\">完全分布式运行模式（开发重点）⭐⭐⭐</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%B8%B8%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7%E8%AF%B4%E6%98%8E%E2%AD%90%E2%AD%90\"><span class=\"toc-text\">常用端口号说明⭐⭐</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E2%AD%90%E2%AD%90\"><span class=\"toc-text\">常用配置文件⭐⭐</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#3-HDFS\"><span class=\"toc-text\">3. HDFS</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#HDFS%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">HDFS概述</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#HDFS%E4%BC%98%E7%BC%BA%E7%82%B9\"><span class=\"toc-text\">HDFS优缺点</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#HDFS%E7%BB%84%E6%88%90%E6%9E%B6%E6%9E%84\"><span class=\"toc-text\">HDFS组成架构</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#HDFS-%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F%EF%BC%88%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%EF%BC%89-%E2%AD%90%E2%AD%90\"><span class=\"toc-text\">HDFS 文件块大小（面试重点） ⭐⭐</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#HDFS-%E7%9A%84Shell-%E6%93%8D%E4%BD%9C\"><span class=\"toc-text\">HDFS 的Shell 操作</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#HDFS%E7%9A%84API%E6%93%8D%E4%BD%9C\"><span class=\"toc-text\">HDFS的API操作</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#HDFS%E7%9A%84%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%EF%BC%88%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%EF%BC%89%E2%AD%90%E2%AD%90\"><span class=\"toc-text\">HDFS的读写流程（面试重点）⭐⭐</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B\"><span class=\"toc-text\">HDFS写数据流程</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97\"><span class=\"toc-text\">网络拓扑-节点距离计算</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Hadoop3-1-3%E5%89%AF%E6%9C%AC%E8%8A%82%E7%82%B9%E9%80%89%E6%8B%A9\"><span class=\"toc-text\">Hadoop3.1.3副本节点选择</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#HDFS%E7%9A%84%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B\"><span class=\"toc-text\">HDFS的读数据流程</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#NameNode%E5%92%8CSecondaryNameNode\"><span class=\"toc-text\">NameNode和SecondaryNameNode</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#NN%E5%92%8C2NN%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">NN和2NN工作机制</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#DataNode\"><span class=\"toc-text\">DataNode</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#DataNode-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">DataNode 工作机制</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#DataNode%E8%8A%82%E7%82%B9%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%9A\"><span class=\"toc-text\">DataNode节点保证数据完整性的方法：</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#DataNode%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE\"><span class=\"toc-text\">DataNode掉线时限参数设置</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#4-MapReduce\"><span class=\"toc-text\">4. MapReduce</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#MapReduce%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">MapReduce概述</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#MapReduce%E5%AE%9A%E4%B9%89\"><span class=\"toc-text\">MapReduce定义</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BC%98%E7%BC%BA%E7%82%B9\"><span class=\"toc-text\">优缺点</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#MapReduce%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3\"><span class=\"toc-text\">MapReduce核心思想</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#MapReduce%E8%BF%9B%E7%A8%8B\"><span class=\"toc-text\">MapReduce进程</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96%E7%B1%BB%E5%9E%8B\"><span class=\"toc-text\">常用数据序列化类型</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#MapReduce%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83\"><span class=\"toc-text\">MapReduce编程规范</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Hadoop%E5%BA%8F%E5%88%97%E5%8C%96\"><span class=\"toc-text\">Hadoop序列化</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%87%AA%E5%AE%9A%E4%B9%89bean%E5%AF%B9%E8%B1%A1%E5%AE%9E%E7%8E%B0%E5%BA%8F%E5%88%97%E5%8C%96%E6%8E%A5%E5%8F%A3%EF%BC%88Writable%EF%BC%89\"><span class=\"toc-text\">自定义bean对象实现序列化接口（Writable）</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86\"><span class=\"toc-text\">MapReduce框架原理</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#InputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5\"><span class=\"toc-text\">InputFormat数据输入</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#TextInputFormat\"><span class=\"toc-text\">TextInputFormat</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#CombineTextInputFormat%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">CombineTextInputFormat切片机制</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#MapReduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B\"><span class=\"toc-text\">MapReduce工作流程</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Shuffle%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">Shuffle机制</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Partition%E5%88%86%E5%8C%BA\"><span class=\"toc-text\">Partition分区</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#WritableComparable%E6%8E%92%E5%BA%8F\"><span class=\"toc-text\">WritableComparable排序</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Combiner%E5%90%88%E5%B9%B6\"><span class=\"toc-text\">Combiner合并</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#OutputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA\"><span class=\"toc-text\">OutputFormat数据输出</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#MapReduce%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90\"><span class=\"toc-text\">MapReduce内核源码解析</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#ReduceTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">ReduceTask工作机制</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#ReduceTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">ReduceTask并行度决定机制</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Join%E5%BA%94%E7%94%A8\"><span class=\"toc-text\">Join应用</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Reduce-Join\"><span class=\"toc-text\">Reduce Join</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Map-Join\"><span class=\"toc-text\">Map Join</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%88ETL%EF%BC%89\"><span class=\"toc-text\">数据清洗（ETL）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#MapReduce%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93\"><span class=\"toc-text\">MapReduce开发总结</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9\"><span class=\"toc-text\">Hadoop数据压缩</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8E%8B%E7%BC%A9%E6%96%B9%E5%BC%8F%E9%80%89%E6%8B%A9\"><span class=\"toc-text\">压缩方式选择</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8E%8B%E7%BC%A9%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE\"><span class=\"toc-text\">压缩参数配置</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88\"><span class=\"toc-text\">常见错误及解决方案</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#5-Yarn\"><span class=\"toc-text\">5. Yarn</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8\"><span class=\"toc-text\">Yarn资源调度器</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Yarn%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84\"><span class=\"toc-text\">Yarn基础架构</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Yarn%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">Yarn工作机制</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%E5%85%A8%E8%BF%87%E7%A8%8B\"><span class=\"toc-text\">作业提交全过程</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Yarn%E8%B0%83%E5%BA%A6%E5%99%A8%E5%92%8C%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">Yarn调度器和调度算法</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%85%88%E8%BF%9B%E5%85%88%E5%87%BA%E8%B0%83%E5%BA%A6%E5%99%A8%EF%BC%88FIFO%EF%BC%89\"><span class=\"toc-text\">先进先出调度器（FIFO）</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8%EF%BC%88Capacity-Scheduler%EF%BC%89\"><span class=\"toc-text\">容量调度器（Capacity Scheduler）</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8%EF%BC%88Fair-Scheduler%EF%BC%89\"><span class=\"toc-text\">公平调度器（Fair Scheduler）</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Yarn%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4\"><span class=\"toc-text\">Yarn常用命令</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#yarn-application%E6%9F%A5%E7%9C%8B%E4%BB%BB%E5%8A%A1\"><span class=\"toc-text\">yarn application查看任务</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#yarn-logs%E6%9F%A5%E7%9C%8B%E6%97%A5%E5%BF%97\"><span class=\"toc-text\">yarn logs查看日志</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#yarn-applicationattempt%E6%9F%A5%E7%9C%8B%E5%B0%9D%E8%AF%95%E8%BF%90%E8%A1%8C%E7%9A%84%E4%BB%BB%E5%8A%A1\"><span class=\"toc-text\">yarn applicationattempt查看尝试运行的任务</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#yarn-container%E6%9F%A5%E7%9C%8B%E5%AE%B9%E5%99%A8\"><span class=\"toc-text\">yarn container查看容器</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#yarn-node%E6%9F%A5%E7%9C%8B%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81\"><span class=\"toc-text\">yarn node查看节点状态</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#yarn-rmadmin%E6%9B%B4%E6%96%B0%E9%85%8D%E7%BD%AE\"><span class=\"toc-text\">yarn rmadmin更新配置</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#yarn-queue%E6%9F%A5%E7%9C%8B%E9%98%9F%E5%88%97\"><span class=\"toc-text\">yarn queue查看队列</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Yarn%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0\"><span class=\"toc-text\">Yarn生产环境核心参数</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%90%91Hive%E9%98%9F%E5%88%97%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1\"><span class=\"toc-text\">向Hive队列提交任务</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BB%BB%E5%8A%A1%E4%BC%98%E5%85%88%E7%BA%A7\"><span class=\"toc-text\">任务优先级</span></a></li></ol></li></ol></li></ol>","author":{"name":"YFR718","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/47553658?s=400&u=5e0a85700c66d7a2661c5664ec5f24d94e5ed01a&v=4","link":"/","description":"大数据萌新","socials":{"github":"https://github.com/YFR718","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"d","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"Linux基础","uid":"144f0b4d9b9f80ea377106e9c6a54a45","slug":"B0-Linux基础","date":"2021-11-16T08:35:41.000Z","updated":"2021-11-16T09:04:08.607Z","comments":true,"path":"api/articles/B0-Linux基础.json","keywords":null,"cover":"https://img-blog.csdnimg.cn/c2607b79df8149799253f31133417976.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center","text":"Linux概述Linux的创始人:Linus Torvalds 林纳斯 Linux主要的发行版:Ubuntu(乌班图)、RedHat(红帽)、CentOS、Debain[蝶变]、Fedora、SuSE、OpenSUSE Linux由Unix发展而来。 linux与windows区...","link":"","photos":[],"count_time":{"symbolsCount":"13k","symbolsTime":"12 mins."},"categories":[{"name":"大数据","slug":"大数据","count":7,"path":"api/categories/大数据.json"}],"tags":[{"name":"Linux","slug":"Linux","count":1,"path":"api/tags/Linux.json"}],"author":{"name":"YFR718","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/47553658?s=400&u=5e0a85700c66d7a2661c5664ec5f24d94e5ed01a&v=4","link":"/","description":"大数据萌新","socials":{"github":"https://github.com/YFR718","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"d","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"meta","uid":"047a0157ce631e99397979ea9f68fcda","slug":"000meta","date":"2021-11-12T06:22:18.000Z","updated":"2021-12-10T12:08:05.744Z","comments":true,"path":"api/articles/000meta.json","keywords":null,"cover":"https://img-blog.csdnimg.cn/728718d7b95643cfafdbac6f3ebf65f8.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center","text":"","link":"","photos":[],"count_time":{"symbolsCount":0,"symbolsTime":"1 mins."},"categories":[{"name":"学习规划","slug":"学习规划","count":2,"path":"api/categories/学习规划.json"}],"tags":[],"author":{"name":"YFR718","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/47553658?s=400&u=5e0a85700c66d7a2661c5664ec5f24d94e5ed01a&v=4","link":"/","description":"大数据萌新","socials":{"github":"https://github.com/YFR718","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"d","csdn":"","juejin":"","customs":{}}},"feature":null}}