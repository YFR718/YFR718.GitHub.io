{"title":"Flume","uid":"f6c5ab269c53087419eb1c0be5e917d0","slug":"B10-Flume","date":"2021-11-26T06:25:34.000Z","updated":"2021-12-10T13:19:22.082Z","comments":true,"path":"api/articles/B10-Flume.json","keywords":null,"cover":"https://img-blog.csdnimg.cn/8eed3c777efa48089151681355ae85e6.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center","content":"<h1 id=\"1-Flume-概述\"><a href=\"#1-Flume-概述\" class=\"headerlink\" title=\"1. Flume 概述\"></a>1. Flume 概述</h1><p>​        Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量<strong>日志采集、聚合和传输</strong>的系统。Flume 基于流式架构，灵活简单。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/3d146ed838f64eb98d6c877b76176b77.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p><strong>Flume 组成架构如下图所示：</strong></p>\n<p><img src=\"https://img-blog.csdnimg.cn/2429c27d645842a6a8a31fb2ac3078a1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<ol>\n<li><strong>Agent</strong><br>Agent 是一个 JVM 进程，它以事件的形式将数据从源头送至目的。<br>Agent 主要有 3 个部分组成，Source、Channel、Sink。</li>\n<li><strong>Source</strong><br>Source 是负责接收数据到 Flume Agent 的组件。<br>Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、taildir、sequence generator、syslog、http、legacy。</li>\n<li><strong>Sink</strong><br>Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。<br>Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。</li>\n<li><strong>Channel</strong><br>Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上。Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个Sink 的读取操作。<br>Flume 自带两种 Channel：Memory Channel 和 File Channel。<ul>\n<li>Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</li>\n<li>File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</li>\n</ul>\n</li>\n<li><strong>Event</strong><br>传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。<br>Event 由 Header 和 Body 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构，Body 用来存放该条数据，形式为字节数组。</li>\n</ol>\n<h1 id=\"2-Flume-入门\"><a href=\"#2-Flume-入门\" class=\"headerlink\" title=\"2. Flume 入门\"></a>2. Flume 入门</h1><h2 id=\"2-1-Flume-安装部署\"><a href=\"#2-1-Flume-安装部署\" class=\"headerlink\" title=\"2.1 Flume 安装部署\"></a>2.1 Flume 安装部署</h2><p><strong>安装地址：</strong></p>\n<p>（1）Flume 官网地址：<a href=\"http://flume.apache.org/\">http://flume.apache.org/</a><br>（2）文档查看地址：<a href=\"http://flume.apache.org/FlumeUserGuide.html\">http://flume.apache.org/FlumeUserGuide.html</a><br>（3）下载地址：<a href=\"http://archive.apache.org/dist/flume/、\">http://archive.apache.org/dist/flume/、</a></p>\n<p><strong>安装部署：</strong></p>\n<p>（1）将 apache-flume-1.9.0-bin.tar.gz 上传到 linux 的/opt/software 目录下<br>（2）解压 apache-flume-1.9.0-bin.tar.gz 到/opt/module/目录下</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 software]$ tar -zxf /opt/software/apache flume-1.9.0-bin.tar.gz -C /opt/module/<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>（3）修改 apache-flume-1.9.0-bin 的名称为 flume</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 module]$ mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>（4）将 lib 文件夹下的 guava-11.0.2.jar 删除以兼容 Hadoop 3.1.3</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 lib]$ rm /opt/module/flume/lib/guava-11.0.2.jar<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<h2 id=\"2-2-入门案例\"><a href=\"#2-2-入门案例\" class=\"headerlink\" title=\"2.2 入门案例\"></a>2.2 入门案例</h2><h3 id=\"1）-监控端口数据官方案例\"><a href=\"#1）-监控端口数据官方案例\" class=\"headerlink\" title=\"1） 监控端口数据官方案例\"></a>1） 监控端口数据官方案例</h3><ol>\n<li><strong>案例需求：</strong><br>使用 Flume 监听一个端口，收集该端口数据，并打印到控制台。</li>\n<li><strong>需求分析：</strong></li>\n</ol>\n<p><img src=\"https://img-blog.csdnimg.cn/c6432bae5a1d4a619fd6f6078de5ca93.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<ol>\n<li><p><strong>实现步骤：</strong><br>（1）安装 netcat 工具</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 software]$ sudo yum install -y nc<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>（2）判断 44444 端口是否被占用</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 flume-telnet]$ sudo netstat -nlp | grep 44444<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>（3）创建 Flume Agent 配置文件 flume-netcat-logger.conf<br>（4）在 flume 目录下创建 job 文件夹并进入 job 文件夹。</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 flume]$ mkdir job\n[atguigu@hadoop102 flume]$ cd job/<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n<p>（5）在 job 文件夹下创建 Flume Agent 配置文件 flume-netcat-logger.conf。</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 job]$ vim flume-netcat-logger.conf<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>（6）在 flume-netcat-logger.conf 文件中添加如下内容。<br>添加内容如下：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\"># Name the components on this agent</span>\n<span class=\"token comment\"># a1:表示agent的名称</span>\n<span class=\"token comment\"># r1:表示a1的Source的名称</span>\n<span class=\"token comment\"># k1:表示a1的Sink的名称</span>\n<span class=\"token comment\"># c1:表示a1的Channel的名称</span>\na1<span class=\"token punctuation\">.</span>sources <span class=\"token operator\">=</span> r1\na1<span class=\"token punctuation\">.</span>sinks <span class=\"token operator\">=</span> k1\na1<span class=\"token punctuation\">.</span>channels <span class=\"token operator\">=</span> c1\n\n<span class=\"token comment\"># Describe/configure the source</span>\n<span class=\"token comment\"># 表示a1的输入源类型为netcat端口类型</span>\n<span class=\"token comment\"># 表示a1的监听的主机</span>\n<span class=\"token comment\"># 表示a1的监听的端口号</span>\na1<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r1<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> netcat\na1<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r1<span class=\"token punctuation\">.</span>bind <span class=\"token operator\">=</span> localhost\na1<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r1<span class=\"token punctuation\">.</span>port <span class=\"token operator\">=</span> <span class=\"token number\">44444</span>\n\n<span class=\"token comment\"># Describe the sink</span>\n<span class=\"token comment\"># 表示a1的输出目的地是控制台logger类型</span>\na1<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k1<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> logger\n\n<span class=\"token comment\"># Use a channel which buffers events in memory</span>\n<span class=\"token comment\"># 表示a1的channel类型是memory内存型</span>\n<span class=\"token comment\"># 表示a1的channel总容量1000个event</span>\n<span class=\"token comment\"># 表示a1的channel传输时收集到了100条event以后再去提交事务</span>\na1<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c1<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> memory\na1<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c1<span class=\"token punctuation\">.</span>capacity <span class=\"token operator\">=</span> <span class=\"token number\">1000</span>\na1<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c1<span class=\"token punctuation\">.</span>transactionCapacity <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\n\n<span class=\"token comment\"># Bind the source and sink to the channel</span>\n<span class=\"token comment\"># 表示将r1和c1连接起来</span>\n<span class=\"token comment\"># 表示将k1和c1连接起来</span>\na1<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r1<span class=\"token punctuation\">.</span>channels <span class=\"token operator\">=</span> c1\na1<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k1<span class=\"token punctuation\">.</span>channel <span class=\"token operator\">=</span> c1<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>（7）先开启 flume 监听端口<br>第一种写法：</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>第二种写法：</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p><strong>参数说明：</strong><br>—conf/-c：表示配置文件存储在 conf/目录<br>—name/-n：表示给 agent 起名为 a1<br>—conf-file/-f：flume 本次启动读取的配置文件是在 job 文件夹下的 flume-telnet.conf文件。<br>-Dflume.root.logger=INFO,console ：-D 表示 flume 运行时动态修改 flume.root.logger参数属性值，并将控制台日志打印级别设置为 INFO 级别。日志级别包括:log、info、warn、error。</p>\n<p>（8）使用 netcat 工具向本机的 44444 端口发送内容</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 ~]$ nc localhost 44444\nhello \natguigu<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<p>（9）在 Flume 监听页面观察接收数据情况</p>\n<p><img src=\"https://img-blog.csdnimg.cn/a51e78719b424c6cbe120701b1b37e73.png\" alt=\"\"></p>\n</li>\n</ol>\n<h3 id=\"2）-实时监控单个追加文件\"><a href=\"#2）-实时监控单个追加文件\" class=\"headerlink\" title=\"2） 实时监控单个追加文件\"></a>2） 实时监控单个追加文件</h3><ol>\n<li><p><strong>案例需求：</strong>实时监控 Hive 日志，并上传到 HDFS 中 </p>\n</li>\n<li><p><strong>需求分析：</strong></p>\n<p><img src=\"https://img-blog.csdnimg.cn/241d489006bc4bce91984ba3ccebe9dc.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n</li>\n</ol>\n<ol>\n<li><p><strong>实现步骤：</strong><br>（1)Flume 要想将数据输出到 HDFS，依赖 Hadoop 相关 jar 包<br>检查/etc/profile.d/my_env.sh 文件，确认 Hadoop 和 Java 环境变量配置正确</p>\n<pre class=\"line-numbers language-sh\" data-language=\"sh\"><code class=\"language-sh\">JAVA_HOME=/opt/module/jdk1.8.0_212\nHADOOP_HOME=/opt/module/ha/hadoop-3.1.3\nPATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\nexport PATH JAVA_HOME HADOOP_HOME<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n<p>（2)创建 flume-file-hdfs.conf 文件</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 job]$ vim flume-file-hdfs.conf<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>注：要想读取 Linux 系统中的文件，就得按照 Linux 命令的规则执行命令。由于 Hive日志在 Linux 系统中所以读取文件的类型选择：exec 即 execute 执行的意思。表示执行Linux 命令来读取文件。<br><strong>添加如下内容:</strong></p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\"># Name the components on this agent</span>\n<span class=\"token comment\"># 定义source</span>\n<span class=\"token comment\"># 定义sink</span>\n<span class=\"token comment\"># 定义channe</span>\na2<span class=\"token punctuation\">.</span>sources <span class=\"token operator\">=</span> r2\na2<span class=\"token punctuation\">.</span>sinks <span class=\"token operator\">=</span> k2\na2<span class=\"token punctuation\">.</span>channels <span class=\"token operator\">=</span> c2\n\n<span class=\"token comment\"># Describe/configure the source</span>\n<span class=\"token comment\">#定义source类型为exec可执行命令的</span>\na2<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r2<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> <span class=\"token keyword\">exec</span>\n<span class=\"token comment\">#执行shell脚本的绝对路径</span>\na2<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r2<span class=\"token punctuation\">.</span>command <span class=\"token operator\">=</span> tail <span class=\"token operator\">-</span>F <span class=\"token operator\">/</span>opt<span class=\"token operator\">/</span>module<span class=\"token operator\">/</span>hive<span class=\"token operator\">/</span>logs<span class=\"token operator\">/</span>hive<span class=\"token punctuation\">.</span>log\na2<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r2<span class=\"token punctuation\">.</span>shell <span class=\"token operator\">=</span> <span class=\"token operator\">/</span><span class=\"token builtin\">bin</span><span class=\"token operator\">/</span>bash <span class=\"token operator\">-</span>c\n\n<span class=\"token comment\"># Describe the sink</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> hdfs\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>path <span class=\"token operator\">=</span> hdfs<span class=\"token punctuation\">:</span><span class=\"token operator\">//</span>hadoop102<span class=\"token punctuation\">:</span><span class=\"token number\">9820</span><span class=\"token operator\">/</span>flume<span class=\"token operator\">/</span><span class=\"token operator\">%</span>Y<span class=\"token operator\">%</span>m<span class=\"token operator\">%</span>d<span class=\"token operator\">/</span><span class=\"token operator\">%</span>H\n<span class=\"token comment\">#上传文件的前缀</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>filePrefix <span class=\"token operator\">=</span> logs<span class=\"token operator\">-</span> \n<span class=\"token comment\">#是否按照时间滚动文件夹</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span><span class=\"token builtin\">round</span> <span class=\"token operator\">=</span> true\n<span class=\"token comment\">#多少时间单位创建一个新的文件夹</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>roundValue <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n<span class=\"token comment\">#重新定义时间单位</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>roundUnit <span class=\"token operator\">=</span> hour\n<span class=\"token comment\">#是否使用本地时间戳</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>useLocalTimeStamp <span class=\"token operator\">=</span> true\n<span class=\"token comment\">#积攒多少个 Event 才 flush 到 HDFS 一次</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>batchSize <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\n<span class=\"token comment\">#设置文件类型，可支持压缩</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>fileType <span class=\"token operator\">=</span> DataStream\n<span class=\"token comment\">#多久生成一个新的文件</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>rollInterval <span class=\"token operator\">=</span> <span class=\"token number\">60</span>\n<span class=\"token comment\">#设置每个文件的滚动大小</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>rollSize <span class=\"token operator\">=</span> <span class=\"token number\">134217700</span>\n<span class=\"token comment\">#文件的滚动与 Event 数量无关</span>\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>rollCount <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n<span class=\"token comment\"># Use a channel which buffers events in memory</span>\na2<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c2<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> memory\na2<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c2<span class=\"token punctuation\">.</span>capacity <span class=\"token operator\">=</span> <span class=\"token number\">1000</span>\na2<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c2<span class=\"token punctuation\">.</span>transactionCapacity <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\n<span class=\"token comment\"># Bind the source and sink to the channel</span>\na2<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r2<span class=\"token punctuation\">.</span>channels <span class=\"token operator\">=</span> c2\na2<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k2<span class=\"token punctuation\">.</span>channel <span class=\"token operator\">=</span> c2<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>注意：对于所有与时间相关的转义序列，Event Header 中必须存在以 “timestamp”的key（除非 hdfs.useLocalTimeStamp 设置为 true，此方法会使用 TimestampInterceptor 自动添加 timestamp）。<br><strong>a3.sinks.k3.hdfs.useLocalTimeStamp = true</strong></p>\n<p>（3）运行 Flume</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p>（4）开启 Hadoop 和 Hive 并操作 Hive 产生日志</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh\n[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh\n[atguigu@hadoop102 hive]$ bin/hive\nhive (default)&gt;<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n<p>（5）在 HDFS 上查看文件。</p>\n</li>\n</ol>\n<h3 id=\"3）实时监控目录下多个新文件\"><a href=\"#3）实时监控目录下多个新文件\" class=\"headerlink\" title=\"3）实时监控目录下多个新文件\"></a>3）实时监控目录下多个新文件</h3><ol>\n<li><p>案例需求：使用 Flume 监听整个目录的文件，并上传至 HDFS </p>\n</li>\n<li><p>需求分析：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/418a177433b145ba8e8bcfdc474f6696.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n</li>\n<li><p>实现步骤：<br>（1）创建配置文件 flume-dir-hdfs.conf</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 job]$ vim flume-dir-hdfs.conf<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n <pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\">#定义source</span>\n<span class=\"token comment\">#定义sink</span>\n<span class=\"token comment\">#定义channel</span>\na3<span class=\"token punctuation\">.</span>sources <span class=\"token operator\">=</span> r3\na3<span class=\"token punctuation\">.</span>sinks <span class=\"token operator\">=</span> k3\na3<span class=\"token punctuation\">.</span>channels <span class=\"token operator\">=</span> c3\n\n<span class=\"token comment\"># Describe/configure the source</span>\n<span class=\"token comment\">#定义source类型为目录</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> spooldir\n<span class=\"token comment\">#定义监控目录</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span>spoolDir <span class=\"token operator\">=</span> <span class=\"token operator\">/</span>opt<span class=\"token operator\">/</span>module<span class=\"token operator\">/</span>flume<span class=\"token operator\">/</span>upload\n<span class=\"token comment\">#定义文件上传完，后缀</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span>fileSuffix <span class=\"token operator\">=</span> <span class=\"token punctuation\">.</span>COMPLETED\n<span class=\"token comment\">#是否有文件头</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span>fileHeader <span class=\"token operator\">=</span> true\n<span class=\"token comment\">#忽略所有以.tmp 结尾的文件，不上传</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span>ignorePattern <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token operator\">^</span> <span class=\"token punctuation\">]</span><span class=\"token operator\">*</span>\\<span class=\"token punctuation\">.</span>tmp<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Describe the sink</span>\n<span class=\"token comment\">#sink类型为hdfs</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> hdfs\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>path <span class=\"token operator\">=</span> hdfs<span class=\"token punctuation\">:</span><span class=\"token operator\">//</span>hadoop102<span class=\"token punctuation\">:</span><span class=\"token number\">9820</span><span class=\"token operator\">/</span>flume<span class=\"token operator\">/</span>upload<span class=\"token operator\">/</span><span class=\"token operator\">%</span>Y<span class=\"token operator\">%</span>m<span class=\"token operator\">%</span>d<span class=\"token operator\">/</span><span class=\"token operator\">%</span>H\n<span class=\"token comment\">#上传文件的前缀</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>filePrefix <span class=\"token operator\">=</span> upload<span class=\"token operator\">-</span> \n<span class=\"token comment\">#是否按照时间滚动文件夹</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span><span class=\"token builtin\">round</span> <span class=\"token operator\">=</span> true\n<span class=\"token comment\">#多少时间单位创建一个新的文件夹</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>roundValue <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n<span class=\"token comment\">#重新定义时间单位</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>roundUnit <span class=\"token operator\">=</span> hour\n<span class=\"token comment\">#是否使用本地时间戳</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>useLocalTimeStamp <span class=\"token operator\">=</span> true\n<span class=\"token comment\">#积攒多少个 Event 才 flush 到 HDFS 一次</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>batchSize <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\n<span class=\"token comment\">#设置文件类型，可支持压缩</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>fileType <span class=\"token operator\">=</span> DataStream\n<span class=\"token comment\">#多久生成一个新的文件</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>rollInterval <span class=\"token operator\">=</span> <span class=\"token number\">60</span>\n<span class=\"token comment\">#设置每个文件的滚动大小大概是 128M</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>rollSize <span class=\"token operator\">=</span> <span class=\"token number\">134217700</span>\n<span class=\"token comment\">#文件的滚动与 Event 数量无关</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>rollCount <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n<span class=\"token comment\"># Use a channel which buffers events in memory</span>\na3<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c3<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> memory\na3<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c3<span class=\"token punctuation\">.</span>capacity <span class=\"token operator\">=</span> <span class=\"token number\">1000</span>\na3<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c3<span class=\"token punctuation\">.</span>transactionCapacity <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\n\n<span class=\"token comment\"># Bind the source and sink to the channel</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span>channels <span class=\"token operator\">=</span> c3\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>channel <span class=\"token operator\">=</span> c3<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p> （2）启动监控文件夹命令</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p> 说明：在使用 Spooling Directory Source 时，不要在监控目录中创建并持续修改文 件；上传完成的文件会以.COMPLETED 结尾；被监控文件夹每 500 毫秒扫描一次文件变动。 </p>\n<p>（3）向 upload 文件夹中添加文件<br> 在/opt/module/flume 目录下创建 upload 文件夹</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 flume]$ mkdir upload<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p> 向 upload 文件夹中添加文件</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 upload]$ touch atguigu.txt\n[atguigu@hadoop102 upload]$ touch atguigu.tmp\n[atguigu@hadoop102 upload]$ touch atguigu.log<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<p> （4）查看 HDFS 上的数据</p>\n</li>\n</ol>\n<h3 id=\"4）实时监控目录下的多个追加文件\"><a href=\"#4）实时监控目录下的多个追加文件\" class=\"headerlink\" title=\"4）实时监控目录下的多个追加文件\"></a>4）实时监控目录下的多个追加文件</h3><ul>\n<li>Exec source 适用于监控一个实时追加的文件，不能实现断点续传；</li>\n<li>Spooldir Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；</li>\n<li>Taildir Source适合用于监听多个实时追加的文件，并且能够实现断点续传。</li>\n</ul>\n<ol>\n<li><p><strong>案例需求:</strong>使用 Flume 监听整个目录的实时追加文件，并上传至 HDFS </p>\n</li>\n<li><p><strong>需求分析:</strong></p>\n<p><img src=\"https://img-blog.csdnimg.cn/bdc13535e3aa4e4aadfc56f69230bd12.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n</li>\n<li><p><strong>实现步骤：</strong><br>（1）创建配置文件 flume-taildir-hdfs.conf</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 job]$ vim flume-taildir-hdfs.conf<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\">#定义source</span>\n<span class=\"token comment\">#定义sink</span>\n<span class=\"token comment\">#定义channel</span>\na3<span class=\"token punctuation\">.</span>sources <span class=\"token operator\">=</span> r3\na3<span class=\"token punctuation\">.</span>sinks <span class=\"token operator\">=</span> k3\na3<span class=\"token punctuation\">.</span>channels <span class=\"token operator\">=</span> c3\n\n<span class=\"token comment\"># Describe/configure the source</span>\n<span class=\"token comment\">#定义source类型</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> TAILDIR\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span>positionFile <span class=\"token operator\">=</span> <span class=\"token operator\">/</span>opt<span class=\"token operator\">/</span>module<span class=\"token operator\">/</span>flume<span class=\"token operator\">/</span>tail_dir<span class=\"token punctuation\">.</span>json\n<span class=\"token comment\">#指定position_file位置</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span>filegroups <span class=\"token operator\">=</span> f1 f2\n<span class=\"token comment\">#定义监控目录文件</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span>filegroups<span class=\"token punctuation\">.</span>f1 <span class=\"token operator\">=</span> <span class=\"token operator\">/</span>opt<span class=\"token operator\">/</span>module<span class=\"token operator\">/</span>flume<span class=\"token operator\">/</span>files<span class=\"token operator\">/</span><span class=\"token punctuation\">.</span><span class=\"token operator\">*</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">.</span><span class=\"token operator\">*</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span>filegroups<span class=\"token punctuation\">.</span>f2 <span class=\"token operator\">=</span> <span class=\"token operator\">/</span>opt<span class=\"token operator\">/</span>module<span class=\"token operator\">/</span>flume<span class=\"token operator\">/</span>files2<span class=\"token operator\">/</span><span class=\"token punctuation\">.</span><span class=\"token operator\">*</span>log<span class=\"token punctuation\">.</span><span class=\"token operator\">*</span>\n\n<span class=\"token comment\"># Describe the sink</span>\n<span class=\"token comment\">#sink类型为hdfs</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> hdfs\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>path <span class=\"token operator\">=</span> hdfs<span class=\"token punctuation\">:</span><span class=\"token operator\">//</span>hadoop102<span class=\"token punctuation\">:</span><span class=\"token number\">9820</span><span class=\"token operator\">/</span>flume<span class=\"token operator\">/</span>upload2<span class=\"token operator\">/</span><span class=\"token operator\">%</span>Y<span class=\"token operator\">%</span>m<span class=\"token operator\">%</span>d<span class=\"token operator\">/</span><span class=\"token operator\">%</span>H\n<span class=\"token comment\">#上传文件的前缀</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>filePrefix <span class=\"token operator\">=</span> upload<span class=\"token operator\">-</span>\n<span class=\"token comment\">#是否按照时间滚动文件夹</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span><span class=\"token builtin\">round</span> <span class=\"token operator\">=</span> true\n<span class=\"token comment\">#多少时间单位创建一个新的文件夹</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>roundValue <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n<span class=\"token comment\">#重新定义时间单位</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>roundUnit <span class=\"token operator\">=</span> hour\n<span class=\"token comment\">#是否使用本地时间戳</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>useLocalTimeStamp <span class=\"token operator\">=</span> true\n<span class=\"token comment\">#积攒多少个 Event 才 flush 到 HDFS 一次</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>batchSize <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\n<span class=\"token comment\">#设置文件类型，可支持压缩</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>fileType <span class=\"token operator\">=</span> DataStream\n<span class=\"token comment\">#多久生成一个新的文件</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>rollInterval <span class=\"token operator\">=</span> <span class=\"token number\">60</span>\n<span class=\"token comment\">#设置每个文件的滚动大小大概是 128M</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>rollSize <span class=\"token operator\">=</span> <span class=\"token number\">134217700</span>\n<span class=\"token comment\">#文件的滚动与 Event 数量无关</span>\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>hdfs<span class=\"token punctuation\">.</span>rollCount <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n<span class=\"token comment\"># Use a channel which buffers events in memory</span>\na3<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c3<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span> <span class=\"token operator\">=</span> memory\na3<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c3<span class=\"token punctuation\">.</span>capacity <span class=\"token operator\">=</span> <span class=\"token number\">1000</span>\na3<span class=\"token punctuation\">.</span>channels<span class=\"token punctuation\">.</span>c3<span class=\"token punctuation\">.</span>transactionCapacity <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\n\n<span class=\"token comment\"># Bind the source and sink to the channel</span>\na3<span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>r3<span class=\"token punctuation\">.</span>channels <span class=\"token operator\">=</span> c3\na3<span class=\"token punctuation\">.</span>sinks<span class=\"token punctuation\">.</span>k3<span class=\"token punctuation\">.</span>channel <span class=\"token operator\">=</span> c3<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p> （2）启动监控文件夹命令</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-taildir-hdfs.conf<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p> （3）向 files 文件夹中追加内容<br> 在/opt/module/flume 目录下创建 files 文件夹</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">[atguigu@hadoop102 flume]$ mkdir files向 upload 文件夹中添加文件\n[atguigu@hadoop102 files]$ echo hello &gt;&gt; file1.txt\n[atguigu@hadoop102 files]$ echo atguigu &gt;&gt; file2.txt<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<p> （4）查看 HDFS 上的数据<br><strong>Taildir 说明：</strong> </p>\n<pre><code>    Taildir Source 维护了一个 json 格式的 position File，其会定期的往 position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。Position File 的格式如下：\n</code></pre><pre class=\"line-numbers language-json\" data-language=\"json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span><span class=\"token property\">\"inode\"</span><span class=\"token operator\">:</span><span class=\"token number\">2496272</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"pos\"</span><span class=\"token operator\">:</span><span class=\"token number\">12</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"file\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"/opt/module/flume/files/file1.txt\"</span><span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">{</span><span class=\"token property\">\"inode\"</span><span class=\"token operator\">:</span><span class=\"token number\">2496275</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"pos\"</span><span class=\"token operator\">:</span><span class=\"token number\">12</span><span class=\"token punctuation\">,</span><span class=\"token property\">\"file\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"/opt/module/flume/files/file2.txt\"</span><span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n<p> 注：Linux 中储存文件元数据的区域就叫做 inode，每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件，Unix/Linux 系统内部不使用文件名，而使用 inode 号码来识别文件。</p>\n</li>\n</ol>\n<h1 id=\"3-Flume-进阶\"><a href=\"#3-Flume-进阶\" class=\"headerlink\" title=\"3. Flume 进阶\"></a>3. Flume 进阶</h1><h2 id=\"3-1-Flume-事务\"><a href=\"#3-1-Flume-事务\" class=\"headerlink\" title=\"3.1 Flume 事务\"></a>3.1 Flume 事务</h2><p><img src=\"https://img-blog.csdnimg.cn/b0b491eb3c4849268adb3d50db39eb09.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"><strong>Put事务流程</strong></p>\n<ol>\n<li>doPut:将批数据先写入临时缓冲区putList</li>\n<li>doCommit:检查channel内存队列是否足够合并。</li>\n<li>doRolback:channel内存队列空间不足，回滚数据</li>\n</ol>\n<p><strong>Take事务</strong></p>\n<ol>\n<li>doTake:将数据取到临时缓冲区takeList,并将数据发送到HDFS</li>\n<li>doCommit:如果数据全部发送成功,则清除临时缓冲区takeList</li>\n<li>doRollback:数据发送过程中如果出现异常, rollback将临时缓冲区takeList中的数据归还给channel内存队列。</li>\n</ol>\n<h2 id=\"3-2-Flume-Agent-内部原理\"><a href=\"#3-2-Flume-Agent-内部原理\" class=\"headerlink\" title=\"3.2 Flume Agent 内部原理\"></a>3.2 Flume Agent 内部原理</h2><p><img src=\"https://img-blog.csdnimg.cn/7d1f37afeb3d48f9a10ba4376d3d09c0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p><strong>重要组件：</strong> </p>\n<ol>\n<li><p>ChannelSelector </p>\n<ul>\n<li>ChannelSelector 的作用就是选出 Event 将要被发往哪个 Channel。其共有两种类型，分别是 Replicating（复制）和 Multiplexing（多路复用）。</li>\n<li>ReplicatingSelector 会将同一个 Event 发往所有的 Channel，Multiplexing 会根据相应的原则，将不同的 Event 发往不同的 Channel。 </li>\n</ul>\n</li>\n<li><p>SinkProcessor<br>SinkProcessor 共 有 三 种 类 型 </p>\n<ul>\n<li>DefaultSinkProcessor 对 应 的 是 单 个 的 Sink </li>\n<li>LoadBalancingSinkProcessor 和FailoverSinkProcessor 对应的是 Sink Group，LoadBalancingSinkProcessor 可以实现负载均衡的功能，FailoverSinkProcessor 可以错误恢复的功能。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"3-3-Flume-拓扑结构\"><a href=\"#3-3-Flume-拓扑结构\" class=\"headerlink\" title=\"3.3 Flume 拓扑结构\"></a>3.3 Flume 拓扑结构</h2><h3 id=\"1）简单串联\"><a href=\"#1）简单串联\" class=\"headerlink\" title=\"1）简单串联\"></a>1）简单串联</h3><p><img src=\"https://img-blog.csdnimg.cn/16af6190f86e42bdbbb1ae810c428e93.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>​        这种模式是将多个 flume 顺序连接起来了，从最初的 source 开始到最终 sink 传送的目的存储系统。此模式不建议桥接过多的 flume 数量， flume 数量过多不仅会影响传输速率，而且一旦传输过程中某个节点 flume 宕机，会影响整个传输系统。</p>\n<h3 id=\"2）复制和多路复用\"><a href=\"#2）复制和多路复用\" class=\"headerlink\" title=\"2）复制和多路复用\"></a>2）复制和多路复用</h3><p><img src=\"https://img-blog.csdnimg.cn/05cac1a28a394ef583e61af0127ebe6e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>​        Flume 支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel 中，或者将不同数据分发到不同的 channel 中，sink 可以选择传送到不同的目的地。</p>\n<h3 id=\"3）负载均衡和故障转移\"><a href=\"#3）负载均衡和故障转移\" class=\"headerlink\" title=\"3）负载均衡和故障转移\"></a>3）负载均衡和故障转移</h3><p><img src=\"https://img-blog.csdnimg.cn/32693033a7b54558b4c9c93c2369e87a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>​        Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能。</p>\n<h3 id=\"4）聚合\"><a href=\"#4）聚合\" class=\"headerlink\" title=\"4）聚合\"></a>4）聚合</h3><p><img src=\"https://img-blog.csdnimg.cn/01d97924805f44c98a0d6123a51caa73.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>​        这种模式是我们最常见的，也非常实用，日常 web 应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用 flume 的这种组合方式能很好的解决这一问题，每台服务器部署一个 flume 采集日志，传送到一个集中收集日志的flume，再由此 flume 上传到 hdfs、hive、hbase 等，进行日志分析。</p>\n<h2 id=\"3-4-Flume-企业开发案例\"><a href=\"#3-4-Flume-企业开发案例\" class=\"headerlink\" title=\"3.4 Flume 企业开发案例\"></a>3.4 Flume 企业开发案例</h2><p>learning。。。。。。。。</p>\n<h2 id=\"3-5-自定义-Interceptor\"><a href=\"#3-5-自定义-Interceptor\" class=\"headerlink\" title=\"3.5 自定义 Interceptor\"></a>3.5 自定义 Interceptor</h2><h2 id=\"3-6-自定义-Source\"><a href=\"#3-6-自定义-Source\" class=\"headerlink\" title=\"3.6 自定义 Source\"></a>3.6 自定义 Source</h2><h2 id=\"3-7-自定义-Sink\"><a href=\"#3-7-自定义-Sink\" class=\"headerlink\" title=\"3.7 自定义 Sink\"></a>3.7 自定义 Sink</h2><h2 id=\"3-8-Flume-数据流监控\"><a href=\"#3-8-Flume-数据流监控\" class=\"headerlink\" title=\"3.8 Flume 数据流监控\"></a>3.8 Flume 数据流监控</h2><h1 id=\"4-企业真实面试题（重点）\"><a href=\"#4-企业真实面试题（重点）\" class=\"headerlink\" title=\"4. 企业真实面试题（重点）\"></a>4. 企业真实面试题（重点）</h1><p><strong>1）你是如何实现 Flume 数据传输的监控的</strong></p>\n<p>使用第三方框架 Ganglia 实时监控 Flume。 </p>\n<p><strong>2）Flume 的 Source，Sink，Channel 的作用？你们 Source 是什么类型？</strong></p>\n<ol>\n<li>作用 <ul>\n<li>Source 组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy</li>\n<li>Channel 组件对采集到的数据进行缓存，可以存放在 Memory 或 File 中。</li>\n<li>Sink 组件是用于把数据发送到目的地的组件，目的地包括 Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。</li>\n</ul>\n</li>\n<li>我公司采用的 Source 类型为：<ul>\n<li>监控后台日志：exec</li>\n<li>监控后台产生日志的端口：netcat</li>\n</ul>\n</li>\n</ol>\n<p><strong>3）Flume 的 Channel Selectors</strong></p>\n<p><img src=\"https://img-blog.csdnimg.cn/8ddc6e0923004edb9afed5cb1aeedc69.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16\" alt=\"\"></p>\n<p>​        Channel Selectors，可以让不同的项目日志通过不同的Channel到不同的Sink中去。官方文档上Channel Selectors 有两种类型:Replicating Channel Selector (default)和Multiplexing Channel Selector<br>​        这两种Selector的区别是:Replicating 会 将source过来的events发往所有channel,而Multiplexing可以选择该发往哪些Channel。 </p>\n<p><strong>4） Flume 参数调优</strong></p>\n<ol>\n<li><p>Source </p>\n<pre><code>    增加 Source 个数（使用 Tair Dir Source 时可增加 FileGroups 个数）可以增大 Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个 Source 以保证 Source 有足够的能力获取到新产生的数据。\n    batchSize 参数决定 Source 一次批量运输到 Channel 的 event 条数，适当调大这个参数可以提高 Source 搬运 Event 到 Channel 时的性能。\n</code></pre></li>\n<li><p>Channel </p>\n<pre><code>    type 选择 memory 时 Channel 的性能最好，但是如果 Flume 进程意外挂掉可能会丢失数据。type 选择 file 时 Channel 的容错性更好，但是性能上会比 memory channel 差。\n    使用 file Channel 时 dataDirs 配置多个不同盘下的目录可以提高性能。\n    Capacity 参数决定 Channel 可容纳最大的 event 条数。transactionCapacity 参数决定每次 Source 往 channel 里面写的最大 event 条数和每次 Sink 从 channel 里面读的最大event 条数。transactionCapacity 需要大于 Source 和 Sink 的 batchSize 参数。\n</code></pre><ol>\n<li>Sink <pre><code>增加 Sink 的个数可以增加 Sink 消费 event 的能力。Sink 也不是越多越好够用就行过多的 Sink 会占用系统资源，造成系统资源不必要的浪费。\n    batchSize 参数决定 Sink 一次批量从 Channel 读取的 event 条数，适当调大这个参数可以提高 Sink 从 Channel 搬出 event 的性能。\n</code></pre></li>\n</ol>\n</li>\n</ol>\n<p><strong>5）Flume 的事务机制</strong><br>        Flume 的事务机制（类似数据库的事务机制）：Flume 使用两个独立的事务分别负责从Soucrce 到 Channel，以及从 Channel 到 Sink 的事件传递。<br>        比如 spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到 Channel 且提交成功，那么 Soucrce 就将该文件标记为完成。<br>        同理，事务以类似的方式处理从 Channel 到 Sink 的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到 Channel 中，等待重新传递。<br><strong>6） Flume 采集数据会丢失吗?</strong><br>        根据 Flume 的架构原理，Flume 是不可能丢失数据的，其内部有完善的事务机制，Source 到 Channel 是事务性的，Channel 到 Sink 是事务性的，因此这两个环节不会出现数据的丢失，唯一可能丢失数据的情况是 Channel 采用 memoryChannel，agent 宕机导致数据丢失，或者 Channel 存储数据已满，导致 Source 不再写入，未写入的数据丢失。</p>\n<p>​        <strong>Flume 不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由 Sink 发出，但是没有接收到响应，Sink 会再次发送数据，此时可能会导致数据的重复。</strong></p>\n","text":"1. Flume 概述​ Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单。 Flume 组成架构如下图所示： AgentAgent 是一个 JVM 进程，它以事件的形式将数据从源头送至目的...","link":"","photos":[],"count_time":{"symbolsCount":"14k","symbolsTime":"12 mins."},"categories":[{"name":"大数据","slug":"大数据","count":7,"path":"api/categories/大数据.json"}],"tags":[{"name":"Flume","slug":"Flume","count":1,"path":"api/tags/Flume.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#1-Flume-%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">1. Flume 概述</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#2-Flume-%E5%85%A5%E9%97%A8\"><span class=\"toc-text\">2. Flume 入门</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-1-Flume-%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2\"><span class=\"toc-text\">2.1 Flume 安装部署</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-2-%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B\"><span class=\"toc-text\">2.2 入门案例</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1%EF%BC%89-%E7%9B%91%E6%8E%A7%E7%AB%AF%E5%8F%A3%E6%95%B0%E6%8D%AE%E5%AE%98%E6%96%B9%E6%A1%88%E4%BE%8B\"><span class=\"toc-text\">1） 监控端口数据官方案例</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2%EF%BC%89-%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E5%8D%95%E4%B8%AA%E8%BF%BD%E5%8A%A0%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">2） 实时监控单个追加文件</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3%EF%BC%89%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E7%9B%AE%E5%BD%95%E4%B8%8B%E5%A4%9A%E4%B8%AA%E6%96%B0%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">3）实时监控目录下多个新文件</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#4%EF%BC%89%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84%E5%A4%9A%E4%B8%AA%E8%BF%BD%E5%8A%A0%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">4）实时监控目录下的多个追加文件</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#3-Flume-%E8%BF%9B%E9%98%B6\"><span class=\"toc-text\">3. Flume 进阶</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-1-Flume-%E4%BA%8B%E5%8A%A1\"><span class=\"toc-text\">3.1 Flume 事务</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-2-Flume-Agent-%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86\"><span class=\"toc-text\">3.2 Flume Agent 内部原理</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-3-Flume-%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84\"><span class=\"toc-text\">3.3 Flume 拓扑结构</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1%EF%BC%89%E7%AE%80%E5%8D%95%E4%B8%B2%E8%81%94\"><span class=\"toc-text\">1）简单串联</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2%EF%BC%89%E5%A4%8D%E5%88%B6%E5%92%8C%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8\"><span class=\"toc-text\">2）复制和多路复用</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3%EF%BC%89%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%92%8C%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB\"><span class=\"toc-text\">3）负载均衡和故障转移</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#4%EF%BC%89%E8%81%9A%E5%90%88\"><span class=\"toc-text\">4）聚合</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-4-Flume-%E4%BC%81%E4%B8%9A%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B\"><span class=\"toc-text\">3.4 Flume 企业开发案例</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-5-%E8%87%AA%E5%AE%9A%E4%B9%89-Interceptor\"><span class=\"toc-text\">3.5 自定义 Interceptor</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-6-%E8%87%AA%E5%AE%9A%E4%B9%89-Source\"><span class=\"toc-text\">3.6 自定义 Source</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-7-%E8%87%AA%E5%AE%9A%E4%B9%89-Sink\"><span class=\"toc-text\">3.7 自定义 Sink</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-8-Flume-%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9B%91%E6%8E%A7\"><span class=\"toc-text\">3.8 Flume 数据流监控</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#4-%E4%BC%81%E4%B8%9A%E7%9C%9F%E5%AE%9E%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E9%87%8D%E7%82%B9%EF%BC%89\"><span class=\"toc-text\">4. 企业真实面试题（重点）</span></a></li></ol>","author":{"name":"YFR718","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/47553658?s=400&u=5e0a85700c66d7a2661c5664ec5f24d94e5ed01a&v=4","link":"/","description":"大数据萌新","socials":{"github":"https://github.com/YFR718","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"d","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"B11.kafka","uid":"b244e3b85104863ef08f66c0cb6a98b0","slug":"B11-kafka","date":"2021-11-26T06:25:49.000Z","updated":"2021-11-26T06:25:49.809Z","comments":true,"path":"api/articles/B11-kafka.json","keywords":null,"cover":null,"text":"","link":"","photos":[],"count_time":{"symbolsCount":0,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"YFR718","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/47553658?s=400&u=5e0a85700c66d7a2661c5664ec5f24d94e5ed01a&v=4","link":"/","description":"大数据萌新","socials":{"github":"https://github.com/YFR718","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"d","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"Zookeeper","uid":"a21b04ca115a88f58bf8270c3847c249","slug":"B9-zookeeper","date":"2021-11-26T06:25:15.000Z","updated":"2021-12-06T07:36:49.255Z","comments":true,"path":"api/articles/B9-zookeeper.json","keywords":null,"cover":"https://img-blog.csdnimg.cn/799dba0361cb4697a0776df265d40e21.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAWUZSNzE4,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center","text":"Zookeeper简介​ Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目。 ​ Zookeeper从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然 后接受观察者的注 册，一旦这些数据...","link":"","photos":[],"count_time":{"symbolsCount":"3.4k","symbolsTime":"3 mins."},"categories":[{"name":"大数据","slug":"大数据","count":7,"path":"api/categories/大数据.json"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","count":1,"path":"api/tags/Zookeeper.json"}],"author":{"name":"YFR718","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/47553658?s=400&u=5e0a85700c66d7a2661c5664ec5f24d94e5ed01a&v=4","link":"/","description":"大数据萌新","socials":{"github":"https://github.com/YFR718","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"d","csdn":"","juejin":"","customs":{}}}}}